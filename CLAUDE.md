<!--
🤖 GENERATED FILE - DO NOT EDIT DIRECTLY
===========================================

This file was automatically generated by ai-rulez from .ai-rulez.yaml.

⚠️  IMPORTANT FOR AI ASSISTANTS AND DEVELOPERS:
- DO NOT modify this file directly
- DO NOT add, remove, or change rules in this file
- Changes made here will be OVERWRITTEN on next generation

✅ TO UPDATE RULES:
1. Edit the source configuration: .ai-rulez.yaml
2. Regenerate this file: ai-rulez generate
3. The updated CLAUDE.md will be created automatically

📝 Generated: 2025-07-13 07:59:15
📁 Source: .ai-rulez.yaml
🎯 Target: CLAUDE.md
📊 Content: 7 rules, 4 sections

Learn more: https://github.com/Goldziher/ai-rulez
===========================================
-->

# CLAUDE.md - Repository Knowledge Base

## Project Overview

Comprehensive benchmarking suite for Python text extraction libraries

## Development Commands

### Dependencies

- Install dependencies: `uv sync`
- Install with all extras: `uv sync --all-extras`
- Install OCR backends: `uv sync --extra ocr`

### Running Benchmarks

- Run all frameworks: `uv run python -m src.cli benchmark`
- Test specific frameworks: `uv run python -m src.cli benchmark --framework kreuzberg_sync,extractous`
- Test by category: `uv run python -m src.cli benchmark --category small,medium`
- List available options: `uv run python -m src.cli list-frameworks`

### Analysis and Reporting

- Generate reports: `uv run python -m src.cli report --output-format html`
- Create visualizations: `uv run python -m src.cli visualize`
- Aggregate results: `uv run python -m src.cli aggregate`
- Quality assessment: `uv run python -m src.cli quality-assess`

### Code Quality

- Format code: `ruff format`
- Lint code: `ruff check`
- Fix linting issues: `ruff check --fix`
- Type check: `mypy src/`

### Pre-commit

- Install hooks: `uv run pre-commit install --hook-type commit-msg && uv run pre-commit install`
- Run manually: `uv run pre-commit run --all-files`

## Important Instructions

- Do what has been asked; nothing more, nothing less
- NEVER create files unless they're absolutely necessary for achieving your goal
- ALWAYS prefer editing an existing file to creating a new one
- NEVER proactively create documentation files (\*.md) or README files
- Only create documentation files if explicitly requested by the User
- NO framework recommendations - present data objectively
- Focus on benchmarking accuracy and fairness
- Filter out PDF specialist frameworks (PyMuPDF, PDFPlumber, Playa) - focus on multi-format frameworks only

## Quick Reference

- **Main CLI**: `uv run python -m src.cli <command>`
- **Test Documents**: `test_documents/` (94 files, ~210MB)
- **CI Workflows**: `.github/workflows/benchmark-by-framework.yml` (recommended)
- **Key Timeouts**: 300s per extraction, 150min per CI job
- **Cache Clearing**: Kreuzberg cache cleared before each benchmark

## Architecture

### Core Components

1. **Benchmark Engine** (`benchmark.py`): Orchestrates benchmark execution

    - Supports both sync and async extractors
    - Thread pool execution for sync extractors
    - Comprehensive timeout and error handling

1. **Extractors** (`extractors.py`): Framework wrappers

    - Protocol-based interface for consistency
    - Factory pattern for instantiation
    - Language configuration support

1. **Profiler** (`profiler.py`): Performance monitoring

    - CPU usage sampling at 100ms intervals
    - Memory (RSS) tracking with peak detection
    - Context managers for clean resource management

1. **Reporting** (`reporting.py`): Results analysis

    - Console tables with Rich
    - CSV exports for external analysis
    - Visualization charts (time, memory, success rate)
    - Colorblind-accessible visualizations

1. **CLI** (`cli.py`): Command interface

    - Commands: benchmark, report, visualize, aggregate, quality-assess
    - Framework/category/file-type filtering
    - Configurable timeouts and iterations

### Adding New Frameworks

1. Create extractor class in `extractors.py`
1. Add to Framework enum in `types.py`
1. Register in `get_extractor()` factory
1. Add dependencies to `pyproject.toml`
1. Configure language support in `get_language_config()`
1. Add to FRAMEWORK_EXCLUSIONS in `config.py` if needed

## Testing Guidelines

### Test Document Collection

- **Total**: 94 documents (~210MB)
- **Categories**: tiny (\<100KB), small (100KB-1MB), medium (1-10MB), large (10-50MB), huge (>50MB)
- **Formats**: PDF, DOCX, PPTX, XLSX, HTML, images, emails, text, data formats
- **Languages**: English, Hebrew, German, Chinese, Japanese, Korean

### Performance Metrics

- **Extraction Time**: Wall-clock time from start to completion
- **Memory Usage**: Peak RSS (Resident Set Size)
- **CPU Utilization**: Average percentage during processing
- **Success Rate**: Percentage of successful extractions
- **Throughput**: Files/second and MB/second

### Benchmark Methodology

- Multiple iterations (default 3) per document
- Cold-start performance (no warmup)
- Resource monitoring at 50ms intervals
- Timeout protection (300s per file)
- Framework isolation in CI jobs

## CI/CD and Deployment

### GitHub Actions Workflow

- **Schedule**: Every Monday at 6 AM UTC
- **Manual Trigger**: Via GitHub Actions UI
- **Framework Isolation**: Each framework runs in separate job
- **Timeouts**: 2 hours per framework, 300s per file
- **Artifacts**: Results stored for 90 days

### Deployment Pipeline

1. Run benchmarks for each framework
1. Aggregate results from successful jobs
1. Generate charts and reports
1. Deploy to GitHub Pages
1. Store raw data in repository

### Key Optimizations

- Clear Kreuzberg cache before each run
- Continue on error for robustness
- Parallel framework execution
- Automatic result aggregation

## Data Management

### Output Files

- **Raw Results**: `results/results.json` (msgspec format)
- **Summaries**: `results/summaries.json`
- **CSV Exports**: `results/detailed_results.csv`, `results/summary_results.csv`
- **Visualizations**: `results/charts/*.png`
- **Interactive**: `results/charts/interactive_dashboard.html`

### Data Structure

- BenchmarkResult: Per-file extraction details
- BenchmarkSummary: Aggregated statistics by framework/category
- AggregatedResults: Combined results from all frameworks
- Quality metrics when enabled with --enable-quality-assessment

## Future Extensibility

### Adding New Frameworks

1. Create extractor class inheriting from Protocol
1. Add to Framework enum
1. Register in factory function
1. Add dependencies
1. Configure language support
1. Add format exclusions if needed

### Potential Enhancements

- GPU acceleration support
- Multi-threaded extraction
- Streaming extraction for large files
- Real-time monitoring dashboard
- Cloud deployment options
- API endpoint benchmarking

## Language Configuration

### Auto-Detection Based on Filenames

- **Hebrew**: Files containing "hebrew", "israel", "tel_aviv"
- **German**: Files containing "german", "germany", "berlin"
- **Chinese**: Files containing "chinese", "china", "beijing", "chi_sim"
- **Japanese**: Files containing "japanese", "japan", "jpn"
- **Korean**: Files containing "korean", "korea", "kor"
- **Default**: English with German and French as fallbacks

### Framework-Specific Handling

- **Kreuzberg**: Configures Tesseract/EasyOCR/PaddleOCR with appropriate models
- **Unstructured**: Passes language hints for OCR
- **Docling**: Auto-detects using ML models
- **MarkItDown**: Uses ONNX models with multilingual support

## Performance Observations

### Framework Speed Rankings (medium PDFs)

- Kreuzberg: ~2 minutes for 24 extractions
- Markitdown: Moderate speed
- Unstructured: Slower but reliable
- Docling: Very slow on complex PDFs (>60 min timeout)

### Common Issues

- **Docling Timeouts**: Switch to text export for better performance
- **Memory Spikes**: Some frameworks use 1GB+ on large documents
- **Cache Impact**: Kreuzberg cache must be cleared for fair comparison
- **Language Config**: Affects OCR accuracy significantly

## Tested Frameworks

### Multi-Format Frameworks (6 total)

1. **Kreuzberg** (v3.8.0+)

    - Sync/async APIs, multiple OCR backends
    - 71MB installation

1. **Extractous** (v0.1.0+)

    - Rust-based with Python bindings
    - Native performance characteristics

1. **Unstructured** (v0.18.5+)

    - 64+ file types support
    - 146MB installation

1. **MarkItDown** (v0.0.1a2+)

    - Microsoft's Markdown converter
    - 251MB with ONNX Runtime

1. **Docling** (v2.41.0+)

    - IBM Research's ML-based extraction
    - 1GB+ installation with PyTorch

1. **Various Kreuzberg OCR backends**

    - kreuzberg_tesseract, kreuzberg_easyocr, kreuzberg_paddleocr
    - Both sync and async variants
