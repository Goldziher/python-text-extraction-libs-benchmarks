[{"file_path":"test_documents/pdfs/Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf","file_size":35493143,"file_type":"pdf","category":"large","framework":"extractous","iteration":0,"extraction_time":29.399924755096436,"startup_time":null,"peak_memory_mb":457.7109375,"avg_memory_mb":418.56484375,"peak_cpu_percent":31.8,"avg_cpu_percent":6.36,"total_io_mb":null,"status":"success","character_count":500000,"word_count":81636,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":81636,"sentence_count":5185,"paragraph_count":3135,"avg_word_length":5.082439119996081,"avg_sentence_length":15.972999035679846,"extraction_completeness":1.0,"text_coherence":0.751147842056933,"noise_ratio":0.31442000000000003,"gibberish_ratio":0.021897810218978103,"flesch_reading_ease":40.31429394808873,"gunning_fog_index":15.859068527521277,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":false,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5587323279493441,"extracted_text":"\nThis electronic edition is for non-commercial purposes only.\n\nBayesian Data Analysis\n\nThird edition\n\n(with errors fixed as of 13 February 2020)\n\nAndrew Gelman\n\nColumbia University\n\nJohn B. Carlin\n\nUniversity of Melbourne\n\nHal S. Stern\n\nUniversity of California, Irvine\n\nDavid B. Dunson\n\nDuke University\n\nAki Vehtari\n\nAalto University\n\nDonald B. Rubin\n\nHarvard University\n\nCopyright c©1995–2020, by Andrew Gelman, John Carlin, Hal Stern,\n\nDonald Rubin, David Dunson, and Aki Vehtari\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nContents\n\nPreface xiii\n\nPart I: Fundamentals of Bayesian Inference 1\n\n1 Probability and inference 3\n1.1 The three steps of Bayesian data analysis 3\n1.2 General notation for statistical inference 4\n1.3 Bayesian inference 6\n1.4 Discrete examples: genetics and spell checking 8\n1.5 Probability as a measure of uncertainty 11\n1.6 Example: probabilities from football point spreads 13\n1.7 Example: calibration for record linkage 16\n1.8 Some useful results from probability theory 19\n1.9 Computation and software 22\n1.10 Bayesian inference in applied statistics 24\n1.11 Bibliographic note 25\n1.12 Exercises 27\n\n2 Single-parameter models 29\n2.1 Estimating a probability from binomial data 29\n2.2 Posterior as compromise between data and prior information 32\n2.3 Summarizing posterior inference 32\n2.4 Informative prior distributions 34\n2.5 Normal distribution with known variance 39\n2.6 Other standard single-parameter models 42\n2.7 Example: informative prior distribution for cancer rates 46\n2.8 Noninformative prior distributions 51\n2.9 Weakly informative prior distributions 55\n2.10 Bibliographic note 56\n2.11 Exercises 57\n\n3 Introduction to multiparameter models 63\n3.1 Averaging over ‘nuisance parameters’ 63\n3.2 Normal data with a noninformative prior distribution 64\n3.3 Normal data with a conjugate prior distribution 67\n3.4 Multinomial model for categorical data 69\n3.5 Multivariate normal model with known variance 70\n3.6 Multivariate normal with unknown mean and variance 72\n3.7 Example: analysis of a bioassay experiment 74\n3.8 Summary of elementary modeling and computation 78\n3.9 Bibliographic note 78\n3.10 Exercises 79\n\nvii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nviii CONTENTS\n\n4 Asymptotics and connections to non-Bayesian approaches 83\n4.1 Normal approximations to the posterior distribution 83\n4.2 Large-sample theory 87\n4.3 Counterexamples to the theorems 89\n4.4 Frequency evaluations of Bayesian inferences 91\n4.5 Bayesian interpretations of other statistical methods 92\n4.6 Bibliographic note 97\n4.7 Exercises 98\n\n5 Hierarchical models 101\n5.1 Constructing a parameterized prior distribution 102\n5.2 Exchangeability and hierarchical models 104\n5.3 Bayesian analysis of conjugate hierarchical models 108\n5.4 Normal model with exchangeable parameters 113\n5.5 Example: parallel experiments in eight schools 119\n5.6 Hierarchical modeling applied to a meta-analysis 124\n5.7 Weakly informative priors for variance parameters 128\n5.8 Bibliographic note 132\n5.9 Exercises 134\n\nPart II: Fundamentals of Bayesian Data Analysis 139\n\n6 Model checking 141\n6.1 The place of model checking in applied Bayesian statistics 141\n6.2 Do the inferences from the model make sense? 142\n6.3 Posterior predictive checking 143\n6.4 Graphical posterior predictive checks 153\n6.5 Model checking for the educational testing example 159\n6.6 Bibliographic note 161\n6.7 Exercises 163\n\n7 Evaluating, comparing, and expanding models 165\n7.1 Measures of predictive accuracy 166\n7.2 Information criteria and cross-validation 169\n7.3 Model comparison based on predictive performance 178\n7.4 Model comparison using Bayes factors 182\n7.5 Continuous model expansion 184\n7.6 Implicit assumptions and model expansion: an example 187\n7.7 Bibliographic note 192\n7.8 Exercises 194\n\n8 Modeling accounting for data collection 197\n8.1 Bayesian inference requires a model for data collection 197\n8.2 Data-collection models and ignorability 199\n8.3 Sample surveys 205\n8.4 Designed experiments 214\n8.5 Sensitivity and the role of randomization 218\n8.6 Observational studies 220\n8.7 Censoring and truncation 224\n8.8 Discussion 229\n8.9 Bibliographic note 229\n8.10 Exercises 230\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS ix\n\n9 Decision analysis 237\n9.1 Bayesian decision theory in different contexts 237\n9.2 Using regression predictions: survey incentives 239\n9.3 Multistage decision making: medical screening 245\n9.4 Hierarchical decision analysis for home radon 246\n9.5 Personal vs. institutional decision analysis 256\n9.6 Bibliographic note 257\n9.7 Exercises 257\n\nPart III: Advanced Computation 259\n\n10 Introduction to Bayesian computation 261\n10.1 Numerical integration 261\n10.2 Distributional approximations 262\n10.3 Direct simulation and rejection sampling 263\n10.4 Importance sampling 265\n10.5 How many simulation draws are needed? 267\n10.6 Computing environments 268\n10.7 Debugging Bayesian computing 270\n10.8 Bibliographic note 271\n10.9 Exercises 272\n\n11 Basics of Markov chain simulation 275\n11.1 Gibbs sampler 276\n11.2 Metropolis and Metropolis-Hastings algorithms 278\n11.3 Using Gibbs and Metropolis as building blocks 280\n11.4 Inference and assessing convergence 281\n11.5 Effective number of simulation draws 286\n11.6 Example: hierarchical normal model 288\n11.7 Bibliographic note 291\n11.8 Exercises 291\n\n12 Computationally efficient Markov chain simulation 293\n12.1 Efficient Gibbs samplers 293\n12.2 Efficient Metropolis jumping rules 295\n12.3 Further extensions to Gibbs and Metropolis 297\n12.4 Hamiltonian Monte Carlo 300\n12.5 Hamiltonian Monte Carlo for a hierarchical model 305\n12.6 Stan: developing a computing environment 307\n12.7 Bibliographic note 308\n12.8 Exercises 309\n\n13 Modal and distributional approximations 311\n13.1 Finding posterior modes 311\n13.2 Boundary-avoiding priors for modal summaries 313\n13.3 Normal and related mixture approximations 318\n13.4 Finding marginal posterior modes using EM 320\n13.5 Conditional and marginal posterior approximations 325\n13.6 Example: hierarchical normal model (continued) 326\n13.7 Variational inference 331\n13.8 Expectation propagation 338\n13.9 Other approximations 343\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nx CONTENTS\n\n13.10 Unknown normalizing factors 345\n\n13.11 Bibliographic note 348\n\n13.12 Exercises 349\n\nPart IV: Regression Models 351\n\n14 Introduction to regression models 353\n\n14.1 Conditional modeling 353\n\n14.2 Bayesian analysis of classical regression 354\n\n14.3 Regression for causal inference: incumbency and voting 358\n\n14.4 Goals of regression analysis 364\n\n14.5 Assembling the matrix of explanatory variables 365\n\n14.6 Regularization and dimension reduction 367\n\n14.7 Unequal variances and correlations 369\n\n14.8 Including numerical prior information 376\n\n14.9 Bibliographic note 378\n\n14.10 Exercises 378\n\n15 Hierarchical linear models 381\n\n15.1 Regression coefficients exchangeable in batches 382\n\n15.2 Example: forecasting U.S. presidential elections 383\n\n15.3 Interpreting a normal prior distribution as extra data 388\n\n15.4 Varying intercepts and slopes 390\n\n15.5 Computation: batching and transformation 392\n\n15.6 Analysis of variance and the batching of coefficients 395\n\n15.7 Hierarchical models for batches of variance components 398\n\n15.8 Bibliographic note 400\n\n15.9 Exercises 402\n\n16 Generalized linear models 405\n\n16.1 Standard generalized linear model likelihoods 406\n\n16.2 Working with generalized linear models 407\n\n16.3 Weakly informative priors for logistic regression 412\n\n16.4 Overdispersed Poisson regression for police stops 420\n\n16.5 State-level opinons from national polls 422\n\n16.6 Models for multivariate and multinomial responses 423\n\n16.7 Loglinear models for multivariate discrete data 428\n\n16.8 Bibliographic note 431\n\n16.9 Exercises 432\n\n17 Models for robust inference 435\n\n17.1 Aspects of robustness 435\n\n17.2 Overdispersed versions of standard models 437\n\n17.3 Posterior inference and computation 439\n\n17.4 Robust inference for the eight schools 441\n\n17.5 Robust regression using t-distributed errors 444\n\n17.6 Bibliographic note 445\n\n17.7 Exercises 446\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS xi\n\n18 Models for missing data 449\n18.1 Notation 449\n18.2 Multiple imputation 451\n18.3 Missing data in the multivariate normal and t models 454\n18.4 Example: multiple imputation for a series of polls 456\n18.5 Missing values with counted data 462\n18.6 Example: an opinion poll in Slovenia 463\n18.7 Bibliographic note 466\n18.8 Exercises 467\n\nPart V: Nonlinear and Nonparametric Models 469\n\n19 Parametric nonlinear models 471\n19.1 Example: serial dilution assay 471\n19.2 Example: population toxicokinetics 477\n19.3 Bibliographic note 485\n19.4 Exercises 486\n\n20 Basis function models 487\n20.1 Splines and weighted sums of basis functions 487\n20.2 Basis selection and shrinkage of coefficients 490\n20.3 Non-normal models and regression surfaces 494\n20.4 Bibliographic note 498\n20.5 Exercises 498\n\n21 Gaussian process models 501\n21.1 Gaussian process regression 501\n21.2 Example: birthdays and birthdates 505\n21.3 Latent Gaussian process models 510\n21.4 Functional data analysis 512\n21.5 Density estimation and regression 513\n21.6 Bibliographic note 516\n21.7 Exercises 516\n\n22 Finite mixture models 519\n22.1 Setting up and interpreting mixture models 519\n22.2 Example: reaction times and schizophrenia 524\n22.3 Label switching and posterior computation 533\n22.4 Unspecified number of mixture components 536\n22.5 Mixture models for classification and regression 539\n22.6 Bibliographic note 542\n22.7 Exercises 543\n\n23 Dirichlet process models 545\n23.1 Bayesian histograms 545\n23.2 Dirichlet process prior distributions 546\n23.3 Dirichlet process mixtures 549\n23.4 Beyond density estimation 557\n23.5 Hierarchical dependence 560\n23.6 Density regression 568\n23.7 Bibliographic note 571\n23.8 Exercises 573\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxii CONTENTS\n\nAppendixes 575\n\nA Standard probability distributions 577\nA.1 Continuous distributions 577\nA.2 Discrete distributions 585\nA.3 Bibliographic note 586\n\nB Outline of proofs of limit theorems 587\nB.1 Bibliographic note 590\n\nC Computation in R and Stan 591\nC.1 Getting started with R and Stan 591\nC.2 Fitting a hierarchical model in Stan 592\nC.3 Direct simulation, Gibbs, and Metropolis in R 596\nC.4 Programming Hamiltonian Monte Carlo in R 603\nC.5 Further comments on computation 607\nC.6 Bibliographic note 608\n\nReferences 609\n\nAuthor Index 643\n\nSubject Index 654\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPreface\n\nThis book is intended to have three roles and to serve three associated audiences: an\nintroductory text on Bayesian inference starting from first principles, a graduate text on\neffective current approaches to Bayesian modeling and computation in statistics and related\nfields, and a handbook of Bayesian methods in applied statistics for general users of and\nresearchers in applied statistics. Although introductory in its early sections, the book is\ndefinitely not elementary in the sense of a first text in statistics. The mathematics used\nin our book is basic probability and statistics, elementary calculus, and linear algebra. A\nreview of probability notation is given in Chapter 1 along with a more detailed list of topics\nassumed to have been studied. The practical orientation of the book means that the reader’s\nprevious experience in probability, statistics, and linear algebra should ideally have included\nstrong computational components.\n\nTo write an introductory text alone would leave many readers with only a taste of the\nconceptual elements but no guidance for venturing into genuine practical applications, be-\nyond those where Bayesian methods agree essentially with standard non-Bayesian analyses.\nOn the other hand, we feel it would be a mistake to present the advanced methods with-\nout first introducing the basic concepts from our data-analytic perspective. Furthermore,\ndue to the nature of applied statistics, a text on current Bayesian methodology would be\nincomplete without a variety of worked examples drawn from real applications. To avoid\ncluttering the main narrative, there are bibliographic notes at the end of each chapter and\nreferences at the end of the book.\n\nExamples of real statistical analyses appear throughout the book, and we hope thereby\nto give an applied flavor to the entire development. Indeed, given the conceptual simplicity\nof the Bayesian approach, it is only in the intricacy of specific applications that novelty\narises. Non-Bayesian approaches dominated statistical theory and practice for most of the\nlast century, but the last few decades have seen a re-emergence of Bayesian methods. This\nhas been driven more by the availability of new computational techniques than by what\nmany would see as the theoretical and logical advantages of Bayesian thinking.\n\nIn our treatment of Bayesian inference, we focus on practice rather than philosophy. We\ndemonstrate our attitudes via examples that have arisen in the applied research of ourselves\nand others. Chapter 1 presents our views on the foundations of probability as empirical\nand measurable; see in particular Sections 1.4–1.7.\n\nChanges for the third edition\n\nThe biggest change for this new edition is the addition of Chapters 20–23 on nonparametric\nmodeling. Other major changes include weakly informative priors in Chapters 2, 5, and\nelsewhere; boundary-avoiding priors in Chapter 13; an updated discussion of cross-validation\nand predictive information criteria in the new Chapter 7; improved convergence monitoring\nand effective sample size calculations for iterative simulation in Chapter 11; presentations of\nHamiltonian Monte Carlo, variational Bayes, and expectation propagation in Chapters 12\nand 13; and new and revised code in Appendix C. We have made other changes throughout.\n\nDuring the eighteen years since completing the first edition of Bayesian Data Analysis,\nwe have worked on dozens of interesting applications which, for reasons of space, we are not\nable to add to this new edition. Many of these examples appear in our book, Data Analysis\n\nxiii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxiv PREFACE\n\nUsing Regression and Hierarchical/Multilevel Models, as well as in our published research\narticles.\n\nWe have made some small corrections and updates for the second printing of the third\nedition.\n\nOnline information\n\nAdditional materials, including the data used in the examples, solutions to many of the\nend-of-chapter exercises, and any errors found after the book goes to press, are posted at\nhttp://www.stat.columbia.edu/∼gelman/book/. Feel free to send any comments to us\ndirectly.\n\nAcknowledgments\n\nWe thank many students, colleagues, and friends for comments and advice and also ac-\nknowledge the public funding that made much of this work possible.\n\nIn particular, we thank Stephen Ansolabehere, Adriano Azevedo, Jarrett Barber, Richard\nBarker, Tom Belin, Michael Betancourt, Suzette Blanchard, Rob Calver, Brad Carlin, Bob\nCarpenter, Alicia Carriquiry, Samantha Cook, Alex Damour, Victor De Oliveira, Vince\nDorie, David Draper, Greg Dropkin, John Emerson, Steve Fienberg, Alex Franks, Byron\nGajewski, Yuanjun Gao, Daniel Gianola, Yuri Goegebeur, David Hammill, Chad Heilig,\nMatt Hoffman, Chuanpu Hu, Zaiying Huang, Shane Jensen, Yoon-Sook Jeon, Pasi Jy-\nlanki, Jay Kadane, Jouni Kerman, Gary King, Lucien Le Cam, Yew Jin Lim, Rod Little,\nTom Little, Chuanhai Liu, Xuecheng Liu, Tomoki Matsumoto, Peter McCullagh, Mary\nSara McPeek, Xiao-Li Meng, Baback Moghaddam, Sergei Morozov, Jarad Niemi, Olivier\nNimeskern, Peter Norvig, Ali Rahimi, Thomas Richardson, Christian Robert, Scott Schmi-\ndler, Matt Schofield, Andrea Siegel, Raghav Singal, Sandip Sinharay, Elizabeth Stuart,\nDwight Sunada, Andrew Swift, Eric Tassone, Francis Tuerlinckx, Iven Van Mechelen,\nAmos Waterland, Rob Weiss, Lo-Hua Yuan, and Alan Zaslavsky. We especially thank\nJohn Boscardin, Jessica Hwang, Daniel Lee, Phillip Price, and Radford Neal.\n\nThis work was partially supported by research grants from the National Science Foun-\ndation, National Institutes of Health, Institute of Education Sciences, National Security\nAgency, Department of Energy, and Academy of Finland.\n\nMany of our examples have appeared in books and articles written by ourselves and\nothers, as we indicate in the bibliographic notes and exercises in the chapters where they\nappear.1\n\nFinally, we thank Caroline, Nancy, Hara, Amy, Ilona, and other family and friends for\ntheir love and support during the writing and revision of this book.\n\n1In particular: Figures 1.3–1.5 are adapted from the Journal of the American Statistical Association 90\n(1995), pp. 696, 702, and 703, and are reprinted with permission of the American Statistical Association.\nFigures 2.6 and 2.7 come from Gelman, A., and Nolan, D., Teaching Statistics: A Bag of Tricks, Oxford\nUniversity Press (1992), pp. 14 and 15, and are reprinted with permission of Oxford University Press.\nFigures 19.8–19.10 come from the Journal of the American Statistical Association 91 (1996), pp. 1407 and\n1409, and are reprinted with permission of the American Statistical Association. Table 19.1 comes from\nBerry, D., Statistics: A Bayesian Perspective, first edition, copyright 1996 Wadsworth, a part of Cengage\nLearning, Inc. Reproduced by permission. www.cengage.com/permissions. Figures 18.1 and 18.2 come\nfrom the Journal of the American Statistical Association 93 (1998), pp. 851 and 853, and are reprinted\nwith permission of the American Statistical Association. Figures 9.1–9.3 are adapted from the Journal of\n\nBusiness and Economic Statistics 21 (2003), pp. 219 and 223, and are reprinted with permission of the\nAmerican Statistical Association. We thank Jack Taylor for the data used to produce Figure 23.4.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart I: Fundamentals of Bayesian Inference\n\nBayesian inference is the process of fitting a probability model to a set of data and sum-\nmarizing the result by a probability distribution on the parameters of the model and on\nunobserved quantities such as predictions for new observations. In Chapters 1–3, we in-\ntroduce several useful families of models and illustrate their application in the analysis of\nrelatively simple data structures. Some mathematics arises in the analytical manipulation of\nthe probability distributions, notably in transformation and integration in multiparameter\nproblems. We differ somewhat from other introductions to Bayesian inference by emphasiz-\ning stochastic simulation, and the combination of mathematical analysis and simulation, as\ngeneral methods for summarizing distributions. Chapter 4 outlines the fundamental con-\nnections between Bayesian and other approaches to statistical inference. The early chapters\nfocus on simple examples to develop the basic ideas of Bayesian inference; examples in which\nthe Bayesian approach makes a practical difference relative to more traditional approaches\nbegin to appear in Chapter 3. The major practical advantages of the Bayesian approach\nappear in Chapter 5, where we introduce hierarchical models, which allow the parameters\nof a prior, or population, distribution themselves to be estimated from data.\n\n1\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 1\n\nProbability and inference\n\n1.1 The three steps of Bayesian data analysis\n\nThis book is concerned with practical methods for making inferences from data using prob-\nability models for quantities we observe and for quantities about which we wish to learn.\nThe essential characteristic of Bayesian methods is their explicit use of probability for quan-\ntifying uncertainty in inferences based on statistical data analysis.\n\nThe process of Bayesian data analysis can be idealized by dividing it into the following\nthree steps:\n\n1. Setting up a full probability model—a joint probability distribution for all observable and\nunobservable quantities in a problem. The model should be consistent with knowledge\nabout the underlying scientific problem and the data collection process.\n\n2. Conditioning on observed data: calculating and interpreting the appropriate posterior\ndistribution—the conditional probability distribution of the unobserved quantities of ul-\ntimate interest, given the observed data.\n\n3. Evaluating the fit of the model and the implications of the resulting posterior distribution:\nhow well does the model fit the data, are the substantive conclusions reasonable, and\nhow sensitive are the results to the modeling assumptions in step 1? In response, one\ncan alter or expand the model and repeat the three steps.\n\nGreat advances in all these areas have been made in the last forty years, and many\nof these are reviewed and used in examples throughout the book. Our treatment covers\nall three steps, the second involving computational methodology and the third a delicate\nbalance of technique and judgment, guided by the applied context of the problem. The first\nstep remains a major stumbling block for much Bayesian analysis: just where do our models\ncome from? How do we go about constructing appropriate probability specifications? We\nprovide some guidance on these issues and illustrate the importance of the third step in\nretrospectively evaluating the fit of models. Along with the improved techniques available\nfor computing conditional probability distributions in the second step, advances in carrying\nout the third step alleviate to some degree the need to assume correct model specification at\nthe first attempt. In particular, the much-feared dependence of conclusions on ‘subjective’\nprior distributions can be examined and explored.\n\nA primary motivation for Bayesian thinking is that it facilitates a common-sense in-\nterpretation of statistical conclusions. For instance, a Bayesian (probability) interval for\nan unknown quantity of interest can be directly regarded as having a high probability of\ncontaining the unknown quantity, in contrast to a frequentist (confidence) interval, which\nmay strictly be interpreted only in relation to a sequence of similar inferences that might\nbe made in repeated practice. Recently in applied statistics, increased emphasis has been\nplaced on interval estimation rather than hypothesis testing, and this provides a strong im-\npetus to the Bayesian viewpoint, since it seems likely that most users of standard confidence\nintervals give them a common-sense Bayesian interpretation. One of our aims in this book\n\n3\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4 1. PROBABILITY AND INFERENCE\n\nis to indicate the extent to which Bayesian interpretations of common simple statistical\nprocedures are justified.\n\nRather than argue the foundations of statistics—see the bibliographic note at the end\nof this chapter for references to foundational debates—we prefer to concentrate on the\npragmatic advantages of the Bayesian framework, whose flexibility and generality allow\nit to cope with complex problems. The central feature of Bayesian inference, the direct\nquantification of uncertainty, means that there is no impediment in principle to fitting\nmodels with many parameters and complicated multilayered probability specifications. In\npractice, the problems are ones of setting up and computing with such large models, and\na large part of this book focuses on recently developed and still developing techniques\nfor handling these modeling and computational challenges. The freedom to set up complex\nmodels arises in large part from the fact that the Bayesian paradigm provides a conceptually\nsimple method for coping with multiple parameters, as we discuss in detail from Chapter 3\non.\n\n1.2 General notation for statistical inference\n\nStatistical inference is concerned with drawing conclusions, from numerical data, about\nquantities that are not observed. For example, a clinical trial of a new cancer drug might\nbe designed to compare the five-year survival probability in a population given the new drug\nto that in a population under standard treatment. These survival probabilities refer to a\nlarge population of patients, and it is neither feasible nor ethically acceptable to experiment\non an entire population. Therefore inferences about the true probabilities and, in particular,\ntheir differences must be based on a sample of patients. In this example, even if it were\npossible to expose the entire population to one or the other treatment, it is never possible to\nexpose anyone to both treatments, and therefore statistical inference would still be needed to\nassess the causal inference—the comparison between the observed outcome in each patient\nand that patient’s unobserved outcome if exposed to the other treatment.\n\nWe distinguish between two kinds of estimands—unobserved quantities for which sta-\ntistical inferences are made—first, potentially observable quantities, such as future obser-\nvations of a process, or the outcome under the treatment not received in the clinical trial\nexample; and second, quantities that are not directly observable, that is, parameters that\ngovern the hypothetical process leading to the observed data (for example, regression coef-\nficients). The distinction between these two kinds of estimands is not always precise, but is\ngenerally useful as a way of understanding how a statistical model for a particular problem\nfits into the real world.\n\nParameters, data, and predictions\n\nAs general notation, we let θ denote unobservable vector quantities or population parameters\nof interest (such as the probabilities of survival under each treatment for randomly chosen\nmembers of the population in the example of the clinical trial), y denote the observed\ndata (such as the numbers of survivors and deaths in each treatment group), and ỹ denote\nunknown, but potentially observable, quantities (such as the outcomes of the patients under\nthe other treatment, or the outcome under each of the treatments for a new patient similar\nto those already in the trial). In general these symbols represent multivariate quantities.\nWe generally use Greek letters for parameters, lower case Roman letters for observed or\nobservable scalars and vectors (and sometimes matrices), and upper case Roman letters\nfor observed or observable matrices. When using matrix notation, we consider vectors as\ncolumn vectors throughout; for example, if u is a vector with n components, then uTu is a\nscalar and uuT an n× n matrix.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.2. GENERAL NOTATION FOR STATISTICAL INFERENCE 5\n\nObservational units and variables\n\nIn many statistical studies, data are gathered on each of a set of n objects or units, and\nwe can write the data as a vector, y = (y1, . . . , yn). In the clinical trial example, we might\nlabel yi as 1 if patient i is alive after five years or 0 if the patient dies. If several variables\nare measured on each unit, then each yi is actually a vector, and the entire dataset y is a\nmatrix (usually taken to have n rows). The y variables are called the ‘outcomes’ and are\nconsidered ‘random’ in the sense that, when making inferences, we wish to allow for the\npossibility that the observed values of the variables could have turned out otherwise, due\nto the sampling process and the natural variation of the population.\n\nExchangeability\n\nThe usual starting point of a statistical analysis is the (often tacit) assumption that the\nn values yi may be regarded as exchangeable, meaning that we express uncertainty as a\njoint probability density p(y1, . . . , yn) that is invariant to permutations of the indexes. A\nnonexchangeable model would be appropriate if information relevant to the outcome were\nconveyed in the unit indexes rather than by explanatory variables (see below). The idea of\nexchangeability is fundamental to statistics, and we return to it repeatedly throughout the\nbook.\n\nWe commonly model data from an exchangeable distribution as independently and iden-\ntically distributed (iid) given some unknown parameter vector θ with distribution p(θ). In\nthe clinical trial example, we might model the outcomes yi as iid, given θ, the unknown\nprobability of survival.\n\nExplanatory variables\n\nIt is common to have observations on each unit that we do not bother to model as random.\nIn the clinical trial example, such variables might include the age and previous health status\nof each patient in the study. We call this second class of variables explanatory variables, or\ncovariates, and label them x. We use X to denote the entire set of explanatory variables\nfor all n units; if there are k explanatory variables, then X is a matrix with n rows and k\ncolumns. Treating X as random, the notion of exchangeability can be extended to require\nthe distribution of the n values of (x, y)i to be unchanged by arbitrary permutations of\nthe indexes. It is always appropriate to assume an exchangeable model after incorporating\nsufficient relevant information inX that the indexes can be thought of as randomly assigned.\nIt follows from the assumption of exchangeability that the distribution of y, given x, is the\nsame for all units in the study in the sense that if two units have the same value of x, then\ntheir distributions of y are the same. Any of the explanatory variables x can be moved into\nthe y category if we wish to model them. We discuss the role of explanatory variables (also\ncalled predictors) in detail in Chapter 8 in the context of analyzing surveys, experiments,\nand observational studies, and in the later parts of this book in the context of regression\nmodels.\n\nHierarchical modeling\n\nIn Chapter 5 and subsequent chapters, we focus on hierarchical models (also called mul-\ntilevel models), which are used when information is available on several different levels of\nobservational units. In a hierarchical model, it is possible to speak of exchangeability at\neach level of units. For example, suppose two medical treatments are applied, in separate\nrandomized experiments, to patients in several different cities. Then, if no other information\nwere available, it would be reasonable to treat the patients within each city as exchangeable\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6 1. PROBABILITY AND INFERENCE\n\nand also treat the results from different cities as themselves exchangeable. In practice it\nwould make sense to include, as explanatory variables at the city level, whatever relevant\ninformation we have on each city, as well as the explanatory variables mentioned before at\nthe individual level, and then the conditional distributions given these explanatory variables\nwould be exchangeable.\n\n1.3 Bayesian inference\n\nBayesian statistical conclusions about a parameter θ, or unobserved data ỹ, are made in\nterms of probability statements. These probability statements are conditional on the ob-\nserved value of y, and in our notation are written simply as p(θ|y) or p(ỹ|y). We also\nimplicitly condition on the known values of any covariates, x. It is at the fundamental\nlevel of conditioning on observed data that Bayesian inference departs from the approach\nto statistical inference described in many textbooks, which is based on a retrospective eval-\nuation of the procedure used to estimate θ (or ỹ) over the distribution of possible y values\nconditional on the true unknown value of θ. Despite this difference, it will be seen that\nin many simple analyses, superficially similar conclusions result from the two approaches\nto statistical inference. However, analyses obtained using Bayesian methods can be easily\nextended to more complex problems. In this section, we present the basic mathematics and\nnotation of Bayesian inference, followed in the next section by an example from genetics.\n\nProbability notation\nSome comments on notation are needed at this point. First, p(·|·) denotes a conditional\nprobability density with the arguments determined by the context, and similarly for\np(·), which denotes a marginal distribution. We use the terms ‘distribution’ and\n‘density’ interchangeably. The same notation is used for continuous density functions\nand discrete probability mass functions. Different distributions in the same equation\n(or expression) will each be denoted by p(·), as in (1.1) below, for example. Although\nan abuse of standard mathematical notation, this method is compact and similar to\nthe standard practice of using p(·) for the probability of any discrete event, where\nthe sample space is also suppressed in the notation. Depending on context, to avoid\nconfusion, we may use the notation Pr(·) for the probability of an event; for example,\nPr(θ > 2) =\n\n∫\nθ>2\n\np(θ)dθ. When using a standard distribution, we use a notation based\non the name of the distribution; for example, if θ has a normal distribution with mean\nµ and variance σ2, we write θ ∼ N(µ, σ2) or p(θ) = N(θ|µ, σ2) or, to be even more\nexplicit, p(θ|µ, σ2) = N(θ|µ, σ2). Throughout, we use notation such as N(µ, σ2) for\nrandom variables and N(θ|µ, σ2) for density functions. Notation and formulas for\nseveral standard distributions appear in Appendix A.\nWe also occasionally use the following expressions for random variables θ: the coeffi-\ncient of variation is defined as sd(θ)/E(θ), the geometric mean is exp(E[log(θ)]), and\nthe geometric standard deviation is exp(sd[log(θ)]).\n\nBayes’ rule\n\nIn order to make probability statements about θ given y, we must begin with a model\nproviding a joint probability distribution for θ and y. The joint probability mass or density\nfunction can be written as a product of two densities that are often referred to as the prior\ndistribution p(θ) and the sampling distribution (or data distribution) p(y|θ), respectively:\n\np(θ, y) = p(θ)p(y|θ).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.3. BAYESIAN INFERENCE 7\n\nSimply conditioning on the known value of the data y, using the basic property of conditional\nprobability known as Bayes’ rule, yields the posterior density:\n\np(θ|y) = p(θ, y)\n\np(y)\n=\np(θ)p(y|θ)\np(y)\n\n, (1.1)\n\nwhere p(y) =\n∑\n\nθp(θ)p(y|θ), and the sum is over all possible values of θ (or p(y) =∫\np(θ)p(y|θ)dθ in the case of continuous θ). An equivalent form of (1.1) omits the fac-\n\ntor p(y), which does not depend on θ and, with fixed y, can thus be considered a constant,\nyielding the unnormalized posterior density, which is the right side of (1.2):\n\np(θ|y) ∝ p(θ)p(y|θ). (1.2)\n\nThe second term in this expression, p(y|θ), is taken here as a function of θ, not of y. These\nsimple formulas encapsulate the technical core of Bayesian inference: the primary task of\nany specific application is to develop the model p(θ, y) and perform the computations to\nsummarize p(θ|y) in appropriate ways.\n\nPrediction\n\nTo make inferences about an unknown observable, often called predictive inferences, we\nfollow a similar logic. Before the data y are considered, the distribution of the unknown\nbut observable y is\n\np(y) =\n\n∫\np(y, θ)dθ =\n\n∫\np(θ)p(y|θ)dθ. (1.3)\n\nThis is often called the marginal distribution of y, but a more informative name is the prior\npredictive distribution: prior because it is not conditional on a previous observation of the\nprocess, and predictive because it is the distribution for a quantity that is observable.\n\nAfter the data y have been observed, we can predict an unknown observable, ỹ, from\nthe same process. For example, y = (y1, . . . , yn) may be the vector of recorded weights of\nan object weighed n times on a scale, θ = (µ, σ2) may be the unknown true weight of the\nobject and the measurement variance of the scale, and ỹ may be the yet to be recorded\nweight of the object in a planned new weighing. The distribution of ỹ is called the posterior\npredictive distribution, posterior because it is conditional on the observed y and predictive\nbecause it is a prediction for an observable ỹ:\n\np(ỹ|y) =\n\n∫\np(ỹ, θ|y)dθ\n\n=\n\n∫\np(ỹ|θ, y)p(θ|y)dθ\n\n=\n\n∫\np(ỹ|θ)p(θ|y)dθ. (1.4)\n\nThe second and third lines display the posterior predictive distribution as an average of\nconditional predictions over the posterior distribution of θ. The last step follows from the\nassumed conditional independence of y and ỹ given θ.\n\nLikelihood\n\nUsing Bayes’ rule with a chosen probability model means that the data y affect the posterior\ninference (1.2) only through p(y|θ), which, when regarded as a function of θ, for fixed y, is\ncalled the likelihood function. In this way Bayesian inference is obeying what is sometimes\ncalled the likelihood principle.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n8 1. PROBABILITY AND INFERENCE\n\nThe likelihood principle is reasonable, but only within the framework of the model or\nfamily of models adopted for a particular analysis. In practice, one can rarely be confident\nthat the chosen model is correct. We shall see in Chapter 6 that sampling distributions\n(imagining repeated realizations of our data) can play an important role in checking model\nassumptions. In fact, our view of an applied Bayesian statistician is one who is willing to\napply Bayes’ rule under a variety of possible models.\n\nLikelihood and odds ratios\n\nThe ratio of the posterior density p(θ|y) evaluated at the points θ1 and θ2 under a given\nmodel is called the posterior odds for θ1 compared to θ2. The most familiar application of\nthis concept is with discrete parameters, with θ2 taken to be the complement of θ1. Odds\nprovide an alternative representation of probabilities and have the attractive property that\nBayes’ rule takes a particularly simple form when expressed in terms of them:\n\np(θ1|y)\np(θ2|y)\n\n=\np(θ1)p(y|θ1)/p(y)\np(θ2)p(y|θ2)/p(y)\n\n=\np(θ1)\n\np(θ2)\n\np(y|θ1)\np(y|θ2)\n\n. (1.5)\n\nIn words, the posterior odds are equal to the prior odds multiplied by the likelihood ratio,\np(y|θ1)/p(y|θ2).\n\n1.4 Discrete examples: genetics and spell checking\n\nWe next demonstrate Bayes’ theorem with two examples in which the immediate goal is\ninference about a particular discrete quantity rather than with the estimation of a parameter\nthat describes an entire population. These discrete examples allow us to see the prior,\nlikelihood, and posterior probabilities directly.\n\nInference about a genetic status\n\nHuman males have one X-chromosome and one Y-chromosome, whereas females have two\nX-chromosomes, each chromosome being inherited from one parent. Hemophilia is a disease\nthat exhibits X-chromosome-linked recessive inheritance, meaning that a male who inherits\nthe gene that causes the disease on the X-chromosome is affected, whereas a female carrying\nthe gene on only one of her two X-chromosomes is not affected. The disease is generally fatal\nfor women who inherit two such genes, and this is rare, since the frequency of occurrence\nof the gene is low in human populations.\n\nPrior distribution. Consider a woman who has an affected brother, which implies that her\nmother must be a carrier of the hemophilia gene with one ‘good’ and one ‘bad’ hemophilia\ngene. We are also told that her father is not affected; thus the woman herself has a fifty-fifty\nchance of having the gene. The unknown quantity of interest, the state of the woman, has\njust two values: the woman is either a carrier of the gene (θ = 1) or not (θ = 0). Based on\nthe information provided thus far, the prior distribution for the unknown θ can be expressed\nsimply as Pr(θ = 1) = Pr(θ = 0) = 1\n\n2 .\n\nData model and likelihood. The data used to update the prior information consist of the\naffection status of the woman’s sons. Suppose she has two sons, neither of whom is affected.\nLet yi=1 or 0 denote an affected or unaffected son, respectively. The outcomes of the two\nsons are exchangeable and, conditional on the unknown θ, are independent; we assume the\nsons are not identical twins. The two items of independent data generate the following\nlikelihood function:\n\nPr(y1=0, y2 = 0 | θ=1) = (0.5)(0.5) = 0.25\n\nPr(y1=0, y2 = 0 | θ=0) = (1)(1) = 1.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.4. DISCRETE EXAMPLES: GENETICS AND SPELL CHECKING 9\n\nThese expressions follow from the fact that if the woman is a carrier, then each of her sons\nwill have a 50% chance of inheriting the gene and so being affected, whereas if she is not a\ncarrier then there is a probability close to 1 that a son of hers will be unaffected. (In fact,\nthere is a nonzero probability of being affected even if the mother is not a carrier, but this\nrisk—the mutation rate—is small and can be ignored for this example.)\n\nPosterior distribution. Bayes’ rule can now be used to combine the information in the\ndata with the prior probability; in particular, interest is likely to focus on the posterior\nprobability that the woman is a carrier. Using y to denote the joint data (y1, y2), this is\nsimply\n\nPr(θ = 1|y) =\np(y|θ = 1)Pr(θ = 1)\n\np(y|θ = 1)Pr(θ = 1) + p(y|θ = 0)Pr(θ = 0)\n\n=\n(0.25)(0.5)\n\n(0.25)(0.5) + (1.0)(0.5)\n=\n\n0.125\n\n0.625\n= 0.20.\n\nIntuitively it is clear that if a woman has unaffected children, it is less probable that she is\na carrier, and Bayes’ rule provides a formal mechanism for determining the extent of the\ncorrection. The results can also be described in terms of prior and posterior odds. The\nprior odds of the woman being a carrier are 0.5/0.5 = 1. The likelihood ratio based on\nthe information about her two unaffected sons is 0.25/1 = 0.25, so the posterior odds are\n1 ·0.25 = 0.25. Converting back to a probability, we obtain 0.25/(1+0.25) = 0.2, as before.\n\nAdding more data. A key aspect of Bayesian analysis is the ease with which sequential\nanalyses can be performed. For example, suppose that the woman has a third son, who\nis also unaffected. The entire calculation does not need to be redone; rather we use the\nprevious posterior distribution as the new prior distribution, to obtain:\n\nPr(θ = 1|y1, y2, y3) =\n(0.5)(0.20)\n\n(0.5)(0.20) + (1)(0.8)\n= 0.111.\n\nAlternatively, if we suppose that the third son is affected, it is easy to check that the\nposterior probability of the woman being a carrier becomes 1 (again ignoring the possibility\nof a mutation).\n\nSpelling correction\n\nClassification of words is a problem of managing uncertainty. For example, suppose someone\ntypes ‘radom.’ How should that be read? It could be a misspelling or mistyping of ‘random’\nor ‘radon’ or some other alternative, or it could be the intentional typing of ‘radom’ (as\nin its first use in this paragraph). What is the probability that ‘radom’ actually means\nrandom? If we label y as the data and θ as the word that the person was intending to type,\nthen\n\nPr(θ | y=‘radom’) ∝ p(θ) Pr(y=‘radom’ | θ). (1.6)\n\nThis product is the unnormalized posterior density. In this case, if for simplicity we consider\nonly three possibilities for the intended word, θ (random, radon, or radom), we can compute\nthe posterior probability of interest by first computing the unnormalized density for all three\nvalues of theta and then normalizing:\n\np(random|‘radom’) =\np(θ1)p(‘radom’|θ1)∑3\nj=1 p(θj)p(‘radom’|θj)\n\n,\n\nwhere θ1=random, θ2=radon, and θ3=radom. The prior probabilities p(θj) can most simply\ncome from frequencies of these words in some large database, ideally one that is adapted\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n10 1. PROBABILITY AND INFERENCE\n\nto the problem at hand (for example, a database of recent student emails if the word in\nquestion is appearing in such a document). The likelihoods p(y|θj) can come from some\nmodeling of spelling and typing errors, perhaps fit using some study in which people were\nfollowed up after writing emails to identify any questionable words.\n\nPrior distribution. Without any other context, it makes sense to assign the prior proba-\nbilities p(θj) based on the relative frequencies of these three words in some databases. Here\nare probabilities supplied by researchers at Google:\n\nθ p(θ)\nrandom 7.60× 10−5\n\nradon 6.05× 10−6\n\nradom 3.12× 10−7\n\nSince we are considering only these possibilities, we could renormalize the three numbers to\nsum to 1 (p(random) = 760\n\n760+60.5+3.12 , etc.) but there is no need, as the adjustment would\nmerely be absorbed into the proportionality constant in (1.6).\n\nReturning to the table above, we were surprised to see the probability of ‘radom’ in the\ncorpus being as high as it was. We looked up the word in Wikipedia and found that it is a\nmedium-sized city: home to ‘the largest and best-attended air show in Poland . . . also the\npopular unofficial name for a semiautomatic 9 mm Para pistol of Polish design . . . ’ For\nthe documents that we encounter, the relative probability of ‘radom’ seems much too high.\nIf the probabilities above do not seem appropriate for our application, this implies that we\nhave prior information or beliefs that have not yet been included in the model. We shall\nreturn to this point after first working out the model’s implications for this example.\n\nLikelihood. Here are some conditional probabilities from Google’s model of spelling and\ntyping errors:\n\nθ p(‘radom’|θ)\nrandom 0.00193\nradon 0.000143\nradom 0.975\n\nWe emphasize that this likelihood function is not a probability distribution. Rather, it is a\nset of conditional probabilities of a particular outcome (‘radom’) from three different proba-\nbility distributions, corresponding to three different possibilities for the unknown parameter\nθ.\n\nThese particular values look reasonable enough—a 97% chance that this particular five-\nletter word will be typed correctly, a 0.2% chance of obtaining this character string by\nmistakenly dropping a letter from ‘random,’ and a much lower chance of obtaining it by\nmistyping the final letter of ‘radon.’ We have no strong intuition about these probabilities\nand will trust the Google engineers here.\n\nPosterior distribution. We multiply the prior probability and the likelihood to get joint\nprobabilities and then renormalize to get posterior probabilities:\n\nθ p(θ)p(‘radom’|θ) p(θ|‘radom’)\nrandom 1.47× 10−7 0.325\nradon 8.65× 10−10 0.002\nradom 3.04× 10−7 0.673\n\nThus, conditional on the model, the typed word ‘radom’ is about twice as likely to be correct\nas to be a typographical error for ‘random,’ and it is very unlikely to be a mistaken instance\nof ‘radon.’ A fuller analysis would include possibilities beyond these three words, but the\nbasic idea is the same.\n\nDecision making, model checking, and model improvement. We can envision two directions\nto go from here. The first approach is to accept the two-thirds probability that the word\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.5. PROBABILITY AS A MEASURE OF UNCERTAINTY 11\n\nwas typed correctly or even to simply declare ‘radom’ as correct on first pass. The second\noption would be to question this probability by saying, for example, that ‘radom’ looks like\na typo and that the estimated probability of it being correct seems much too high.\n\nWhen we dispute the claims of a posterior distribution, we are saying that the model\ndoes not fit the data or that we have additional prior information not included in the model\nso far. In this case, we are only examining one word so lack of fit is not the issue; thus a\ndispute over the posterior must correspond to a claim of additional information, either in\nthe prior or the likelihood.\n\nFor this problem we have no particular grounds on which to criticize the likelihood. The\nprior probabilities, on the other hand, are highly context dependent. The word ‘random’ is\nof course highly frequent in our own writing on statistics, ‘radon’ occurs occasionally (see\nSection 9.4), while ‘radom’ was entirely new to us. Our surprise at the high probability of\n‘radom’ represents additional knowledge relevant to our particular problem.\n\nThe model can be elaborated most immediately by including contextual information in\nthe prior probabilities. For example, if the document under study is a statistics book, then\nit becomes more likely that the person intended to type ‘random.’ If we label x as the\ncontextual information used by the model, the Bayesian calculation then becomes,\n\np(θ|x, y) ∝ p(θ|x)p(y|θ, x).\n\nTo first approximation, we can simplify that last term to p(y|θ), so that the probability\nof any particular error (that is, the probability of typing a particular string y given the\nintended word θ) does not depend on context. This is not a perfect assumption but could\nreduce the burden of modeling and computation.\n\nThe practical challenges in Bayesian inference involve setting up models to estimate all\nthese probabilities from data. At that point, as shown above, Bayes’ rule can be easily\napplied to determine the implications of the model for the problem at hand.\n\n1.5 Probability as a measure of uncertainty\n\nWe have already used concepts such as probability density, and indeed we assume that the\nreader has a fair degree of familiarity with basic probability theory (although in Section\n1.8 we provide a brief technical review of some probability calculations that often arise\nin Bayesian analysis). But since the uses of probability within a Bayesian framework are\nmuch broader than within non-Bayesian statistics, it is important to consider at least briefly\nthe foundations of the concept of probability before considering more detailed statistical\nexamples. We take for granted a common understanding on the part of the reader of the\nmathematical definition of probability: that probabilities are numerical quantities, defined\non a set of ‘outcomes,’ that are nonnegative, additive over mutually exclusive outcomes,\nand sum to 1 over all possible mutually exclusive outcomes.\n\nIn Bayesian statistics, probability is used as the fundamental measure or yardstick of\nuncertainty. Within this paradigm, it is equally legitimate to discuss the probability of\n‘rain tomorrow’ or of a Brazilian victory in the soccer World Cup as it is to discuss the\nprobability that a coin toss will land heads. Hence, it becomes as natural to consider the\nprobability that an unknown estimand lies in a particular range of values as it is to consider\nthe probability that the mean of a random sample of 10 items from a known fixed population\nof size 100 will lie in a certain range. The first of these two probabilities is of more interest\nafter data have been acquired whereas the second is more relevant beforehand. Bayesian\nmethods enable statements to be made about the partial knowledge available (based on\ndata) concerning some situation or ‘state of nature’ (unobservable or as yet unobserved) in\na systematic way, using probability as the yardstick. The guiding principle is that the state\nof knowledge about anything unknown is described by a probability distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n12 1. PROBABILITY AND INFERENCE\n\nWhat is meant by a numerical measure of uncertainty? For example, the probability of\n‘heads’ in a coin toss is widely agreed to be 1\n\n2 . Why is this so? Two justifications seem to\nbe commonly given:\n\n1. Symmetry or exchangeability argument:\n\nprobability =\nnumber of favorable cases\n\nnumber of possibilities\n,\n\nassuming equally likely possibilities. For a coin toss this is really a physical argument,\nbased on assumptions about the forces at work in determining the manner in which the\ncoin will fall, as well as the initial physical conditions of the toss.\n\n2. Frequency argument: probability = relative frequency obtained in a long sequence of\ntosses, assumed to be performed in an identical manner, physically independently of\neach other.\n\nBoth the above arguments are in a sense subjective, in that they require judgments about\nthe nature of the coin and the tossing procedure, and both involve semantic arguments\nabout the meaning of equally likely events, identical measurements, and independence.\nThe frequency argument may be perceived to have certain special difficulties, in that it\ninvolves the hypothetical notion of a long sequence of identical tosses. If taken strictly, this\npoint of view does not allow a statement of probability for a single coin toss that does not\nhappen to be embedded, at least conceptually, in a long sequence of identical events.\n\nThe following examples illustrate how probability judgments can be increasingly subjec-\ntive. First, consider the following modified coin experiment. Suppose that a particular coin\nis stated to be either double-headed or double-tailed, with no further information provided.\nCan one still talk of the probability of heads? It seems clear that in common parlance one\ncertainly can. It is less clear, perhaps, how to assess this new probability, but many would\nagree on the same value of 1\n\n2 , perhaps based on the exchangeability of the labels ‘heads’\nand ‘tails.’\n\nNow consider some further examples. Suppose Colombia plays Brazil in soccer to-\nmorrow: what is the probability of Colombia winning? What is the probability of rain\ntomorrow? What is the probability that Colombia wins, if it rains tomorrow? What is\nthe probability that a specified rocket launch will fail? Although each of these questions\nseems reasonable in a common-sense way, it is difficult to contemplate strong frequency\ninterpretations for the probabilities being referenced. Frequency interpretations can usually\nbe constructed, however, and this is an extremely useful tool in statistics. For example, one\ncan consider the future rocket launch as a sample from the population of potential launches\nof the same type, and look at the frequency of past launches that have failed (see the bib-\nliographic note at the end of this chapter for more details on this example). Doing this\nsort of thing scientifically means creating a probability model (or, at least, a ‘reference set’\nof comparable events), and this brings us back to a situation analogous to the simple coin\ntoss, where we must consider the outcomes in question as exchangeable and thus equally\nlikely.\n\nWhy is probability a reasonable way of quantifying uncertainty? The following reasons\nare often advanced.\n\n1. By analogy: physical randomness induces uncertainty, so it seems reasonable to describe\nuncertainty in the language of random events. Common speech uses many terms such\nas ‘probably’ and ‘unlikely,’ and it appears consistent with such usage to extend a more\nformal probability calculus to problems of scientific inference.\n\n2. Axiomatic or normative approach: related to decision theory, this approach places all sta-\ntistical inference in the context of decision-making with gains and losses. Then reasonable\naxioms (ordering, transitivity, and so on) imply that uncertainty must be represented in\nterms of probability. We view this normative rationale as suggestive but not compelling.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 13\n\n3. Coherence of bets. Define the probability p attached (by you) to an event E as the\nfraction (p ∈ [0, 1]) at which you would exchange (that is, bet) $p for a return of $1 if E\noccurs. That is, if E occurs, you gain $(1− p); if the complement of E occurs, you lose\n$p. For example:\n\n• Coin toss: thinking of the coin toss as a fair bet suggests even odds corresponding to\np = 1\n\n2 .\n\n• Odds for a game: if you are willing to bet on team A to win a game at 10 to 1 odds\nagainst team B (that is, you bet 1 to win 10), your ‘probability’ for team A winning\nis at least 1\n\n11 .\n\nThe principle of coherence states that your assignment of probabilities to all possible\nevents should be such that it is not possible to make a definite gain by betting with you.\nIt can be proved that probabilities constructed under this principle must satisfy the basic\naxioms of probability theory.\nThe betting rationale has some fundamental difficulties:\n\n• Exact odds are required, on which you would be willing to bet in either direction, for\nall events. How can you assign exact odds if you are not sure?\n\n• If a person is willing to bet with you, and has information you do not, it might not\nbe wise for you to take the bet. In practice, probability is an incomplete (necessary\nbut not sufficient) guide to betting.\n\nAll of these considerations suggest that probabilities may be a reasonable approach to\nsummarizing uncertainty in applied statistics, but the ultimate proof is in the success of the\napplications. The remaining chapters of this book demonstrate that probability provides a\nrich and flexible framework for handling uncertainty in statistical applications.\n\nSubjectivity and objectivity\n\nAll statistical methods that use probability are subjective in the sense of relying on math-\nematical idealizations of the world. Bayesian methods are sometimes said to be especially\nsubjective because of their reliance on a prior distribution, but in most problems, scientific\njudgment is necessary to specify both the ‘likelihood’ and the ‘prior’ parts of the model. For\nexample, linear regression models are generally at least as suspect as any prior distribution\nthat might be assumed about the regression parameters. A general principle is at work\nhere: whenever there is replication, in the sense of many exchangeable units observed, there\nis scope for estimating features of a probability distribution from data and thus making the\nanalysis more ‘objective.’ If an experiment as a whole is replicated several times, then the\nparameters of the prior distribution can themselves be estimated from data, as discussed in\nChapter 5. In any case, however, certain elements requiring scientific judgment will remain,\nnotably the choice of data included in the analysis, the parametric forms assumed for the\ndistributions, and the ways in which the model is checked.\n\n1.6 Example: probabilities from football point spreads\n\nAs an example of how probabilities might be assigned using empirical data and plausible\nsubstantive assumptions, we consider methods of estimating the probabilities of certain\noutcomes in professional (American) football games. This is an example only of probability\nassignment, not of Bayesian inference. A number of approaches to assigning probabilities\nfor football game outcomes are illustrated: making subjective assessments, using empirical\nprobabilities based on observed data, and constructing a parametric probability model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n14 1. PROBABILITY AND INFERENCE\n\nFigure 1.1 Scatterplot of actual outcome vs. point spread for each of 672 professional football games.\nThe x and y coordinates are jittered by adding uniform random numbers to each point’s coordinates\n(between −0.1 and 0.1 for the x coordinate; between −0.2 and 0.2 for the y coordinate) in order to\ndisplay multiple values but preserve the discrete-valued nature of each.\n\nFootball point spreads and game outcomes\n\nFootball experts provide a point spread for every football game as a measure of the difference\nin ability between the two teams. For example, team A might be a 3.5-point favorite to\ndefeat team B. The implication of this point spread is that the proposition that team A,\nthe favorite, defeats team B, the underdog, by 4 or more points is considered a fair bet; in\nother words, the probability that A wins by more than 3.5 points is 1\n\n2 . If the point spread\nis an integer, then the implication is that team A is as likely to win by more points than\nthe point spread as it is to win by fewer points than the point spread (or to lose); there is\npositive probability that A will win by exactly the point spread, in which case neither side\nis paid off. The assignment of point spreads is itself an interesting exercise in probabilistic\nreasoning; one interpretation is that the point spread is the median of the distribution of\nthe gambling population’s beliefs about the possible outcomes of the game. For the rest\nof this example, we treat point spreads as given and do not worry about how they were\nderived.\n\nThe point spread and actual game outcome for 672 professional football games played\nduring the 1981, 1983, and 1984 seasons are graphed in Figure 1.1. (Much of the 1982\nseason was canceled due to a labor dispute.) Each point in the scatterplot displays the\npoint spread, x, and the actual outcome (favorite’s score minus underdog’s score), y. (In\ngames with a point spread of zero, the labels ‘favorite’ and ‘underdog’ were assigned at\nrandom.) A small random jitter is added to the x and y coordinate of each point on the\ngraph so that multiple points do not fall exactly on top of each other.\n\nAssigning probabilities based on observed frequencies\n\nIt is of interest to assign probabilities to particular events: Pr(favorite wins), Pr(favorite\nwins | point spread is 3.5 points), Pr(favorite wins by more than the point spread), Pr(favorite\nwins by more than the point spread | point spread is 3.5 points), and so forth. We might\nreport a subjective probability based on informal experience gathered by reading the news-\npaper and watching football games. The probability that the favored team wins a game\nshould certainly be greater than 0.5, perhaps between 0.6 and 0.75? More complex events\nrequire more intuition or knowledge on our part. A more systematic approach is to assign\nprobabilities based on the data in Figure 1.1. Counting a tied game as one-half win and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 15\n\nFigure 1.2 (a) Scatterplot of (actual outcome − point spread) vs. point spread for each of 672\nprofessional football games (with uniform random jitter added to x and y coordinates). (b) His-\ntogram of the differences between the game outcome and the point spread, with the N(0, 142) density\nsuperimposed.\n\none-half loss, and ignoring games for which the point spread is zero (and thus there is no\nfavorite), we obtain empirical estimates such as:\n\n• Pr(favorite wins) = 410.5\n655 = 0.63\n\n• Pr(favorite wins |x = 3.5) = 36\n59 = 0.61\n\n• Pr(favorite wins by more than the point spread) = 308\n655 = 0.47\n\n• Pr(favorite wins by more than the point spread |x = 3.5) = 32\n59 = 0.54.\n\nThese empirical probability assignments all seem sensible in that they match the intu-\nition of knowledgeable football fans. However, such probability assignments are problematic\nfor events with few directly relevant data points. For example, 8.5-point favorites won five\nout of five times during this three-year period, whereas 9-point favorites won thirteen out of\ntwenty times. However, we realistically expect the probability of winning to be greater for\na 9-point favorite than for an 8.5-point favorite. The small sample size with point spread\n8.5 leads to imprecise probability assignments. We consider an alternative method using a\nparametric model.\n\nA parametric model for the difference between outcome and point spread\n\nFigure 1.2a displays the differences y−x between the observed game outcome and the point\nspread, plotted versus the point spread, for the games in the football dataset. (Once again,\nrandom jitter was added to both coordinates.) This plot suggests that it may be roughly\nreasonable to model the distribution of y − x as independent of x. (See Exercise 6.10.)\nFigure 1.2b is a histogram of the differences y − x for all the football games, with a fitted\nnormal density superimposed. This plot suggests that it may be reasonable to approximate\nthe marginal distribution of the random variable d = y − x by a normal distribution. The\nsample mean of the 672 values of d is 0.07, and the sample standard deviation is 13.86,\nsuggesting that the results of football games are approximately normal with mean equal to\nthe point spread and standard deviation nearly 14 points (two converted touchdowns). For\nthe remainder of the discussion we take the distribution of d to be independent of x and\nnormal with mean zero and standard deviation 14 for each x; that is,\n\nd|x ∼ N(0, 142),\n\nas displayed in Figure 1.2b. The assigned probability model is not perfect: it does not fit\nthe data exactly, and, as is often the case with real data, neither football scores nor point\nspreads are continuous-valued quantities.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n16 1. PROBABILITY AND INFERENCE\n\nAssigning probabilities using the parametric model\n\nNevertheless, the model provides a convenient approximation that can be used to assign\nprobabilities to events. If d has a normal distribution with mean zero and is independent of\nthe point spread, then the probability that the favorite wins by more than the point spread\nis 1\n\n2 , conditional on any value of the point spread, and therefore unconditionally as well.\nDenoting probabilities obtained by the normal model as Prnorm, the probability that an\nx-point favorite wins the game can be computed, assuming the normal model, as follows:\n\nPrnorm(y>0 |x) = Prnorm(d>−x |x) = 1− Φ\n(\n− x\n\n14\n\n)\n,\n\nwhere Φ is the standard normal cumulative distribution function. For example,\n\n• Prnorm(favorite wins |x = 3.5) = 0.60\n\n• Prnorm(favorite wins |x = 8.5) = 0.73\n\n• Prnorm(favorite wins |x = 9.0) = 0.74.\n\nThe probability for a 3.5-point favorite agrees with the empirical value given earlier, whereas\nthe probabilities for 8.5- and 9-point favorites make more intuitive sense than the empirical\nvalues based on small samples.\n\n1.7 Example: calibration for record linkage\n\nWe emphasize the essentially empirical (not ‘subjective’ or ‘personal’) nature of probabilities\nwith another example in which they are estimated from data.\n\nRecord linkage refers to the use of an algorithmic technique to identify records from\ndifferent databases that correspond to the same individual. Record-linkage techniques are\nused in a variety of settings. The work described here was formulated and first applied in\nthe context of record linkage between the U.S. Census and a large-scale post-enumeration\nsurvey, which is the first step of an extensive matching operation conducted to evaluate\ncensus coverage for subgroups of the population. The goal of this first step is to declare as\nmany records as possible ‘matched’ by computer without an excessive rate of error, thereby\navoiding the cost of the resulting manual processing for all records not declared ‘matched.’\n\nExisting methods for assigning scores to potential matches\n\nMuch attention has been paid in the record-linkage literature to the problem of assigning\n‘weights’ to individual fields of information in a multivariate record and obtaining a com-\nposite ‘score,’ which we call y, that summarizes the closeness of agreement between two\nrecords. Here, we assume that this step is complete in the sense that these rules have been\nchosen. The next step is the assignment of candidate matched pairs, where each pair of\nrecords consists of the best potential match for each other from the respective databases.\nThe specified weighting rules then order the candidate matched pairs. In the motivating\nproblem at the Census Bureau, a binary choice is made between the alternatives ‘declare\nmatched’ vs. ‘send to followup,’ where a cutoff score is needed above which records are\ndeclared matched. The false-match rate is then defined as the number of falsely matched\npairs divided by the number of declared matched pairs.\n\nParticularly relevant for any such decision problem is an accurate method for assessing\nthe probability that a candidate matched pair is a correct match as a function of its score.\nSimple methods exist for converting the scores into probabilities, but these lead to extremely\ninaccurate, typically grossly optimistic, estimates of false-match rates. For example, a\nmanual check of a set of records with nominal false-match probabilities ranging from 10−3\n\nto 10−7 (that is, pairs deemed almost certain to be matches) found actual false-match rates\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.7. EXAMPLE: CALIBRATION FOR RECORD LINKAGE 17\n\nFigure 1.3 Histograms of weight scores y for true and false matches in a sample of records from\nthe 1988 test Census. Most of the matches in the sample are true (because a pre-screening process\nhas already picked these as the best potential match for each case), and the two distributions are\nmostly, but not completely, separated.\n\ncloser to the 1% range. Records with nominal false-match probabilities of 1% had an actual\nfalse-match rate of 5%.\n\nWe would like to use Bayesian methods to recalibrate these to obtain objective proba-\nbilities of matching for a given decision rule—in the same way that in the football example,\nwe used past data to estimate the probabilities of different game outcomes conditional on\nthe point spread. Our approach is to work with the scores y and empirically estimate the\nprobability of a match as a function of y.\n\nEstimating match probabilities empirically\n\nWe obtain accurate match probabilities using mixture modeling, a topic we discuss in detail\nin Chapter 22. The distribution of previously obtained scores for the candidate matches\nis considered a ‘mixture’ of a distribution of scores for true matches and a distribution for\nnon-matches. The parameters of the mixture model are estimated from the data. The\nestimated parameters allow us to calculate an estimate of the probability of a false match\n(a pair declared matched that is not a true match) for any given decision threshold on the\nscores. In the procedure that was actually used, some elements of the mixture model (for\nexample, the optimal transformation required to allow a mixture of normal distributions\nto apply) were fit using ‘training’ data with known match status (separate from the data\nto which we apply our calibration procedure), but we do not describe those details here.\nInstead we focus on how the method would be used with a set of data with unknown match\nstatus.\n\nSupport for this approach is provided in Figure 1.3, which displays the distribution of\nscores for the matches and non-matches in a particular dataset obtained from 2300 records\nfrom a ‘test Census’ survey conducted in a single local area two years before the 1990 Census.\nThe two distributions, p(y|match) and p(y|non-match), are mostly distinct—meaning that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n18 1. PROBABILITY AND INFERENCE\n\nFigure 1.4 Lines show expected false-match rate (and 95% bounds) as a function of the proportion\nof cases declared matches, based on the mixture model for record linkage. Dots show the actual\nfalse-match rate for the data.\n\nin most cases it is possible to identify a candidate as a match or not given the score alone—\nbut with some overlap.\n\nIn our application dataset, we do not know the match status. Thus we are faced with a\nsingle combined histogram from which we estimate the two component distributions and the\nproportion of the population of scores that belong to each component. Under the mixture\nmodel, the distribution of scores can be written as,\n\np(y) = Pr(match) p(y|match) + Pr(non-match) p(y|non-match). (1.7)\n\nThe mixture probability (Pr(match)) and the parameters of the distributions of matches\n(p(y|match)) and non-matches (p(y|non-match)) are estimated using the mixture model\napproach (as described in Chapter 22) applied to the combined histogram from the data\nwith unknown match status.\n\nTo use the method to make record-linkage decisions, we construct a curve giving the\nfalse-match rate as a function of the decision threshold, the score above which pairs will\nbe ‘declared’ a match. For a given decision threshold, the probability distributions in (1.7)\ncan be used to estimate the probability of a false match, a score y above the threshold\noriginating from the distribution p(y|non-match). The lower the threshold, the more pairs\nwe will declare as matches. As we declare more matches, the proportion of errors increases.\nThe approach described here should provide an objective error estimate for each threshold.\n(See the validation in the next paragraph.) Then a decision maker can determine the\nthreshold that provides an acceptable balance between the goals of declaring more matches\nautomatically (thus reducing the clerical labor) and making fewer mistakes.\n\nExternal validation of the probabilities using test data\n\nThe approach described above was externally validated using data for which the match\nstatus is known. The method was applied to data from three different locations of the 1988\ntest Census, and so three tests of the methods were possible. We provide detailed results\nfor one; results for the other two were similar. The mixture model was fitted to the scores\nof all the candidate pairs at a test site. Then the estimated model was used to create the\nlines in Figure 1.4, which show the expected false-match rate (and uncertainty bounds) in\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 19\n\nFigure 1.5 Expansion of Figure 1.4 in the region where the estimated and actual match rates change\nrapidly. In this case, it would seem a good idea to match about 88% of the cases and send the rest\nto followup.\n\nterms of the proportion of cases declared matched, as the threshold varies from high (thus\nallowing no matches) to low (thus declaring almost all the candidate pairs to be matches).\nThe false-match proportion is an increasing function of the number of declared matches,\nwhich makes sense: as we move rightward on the graph, we are declaring weaker and weaker\ncases to be matches.\n\nThe lines on Figure 1.4 display the expected proportion of false matches and 95% pos-\nterior bounds for the false-match rate as estimated from the model. (These bounds give\nthe estimated range within which there is 95% posterior probability that the false-match\nrate lies. The concept of posterior intervals is discussed in more detail in the next chapter.)\nThe dots in the graph display the actual false-match proportions, which track well with the\nmodel. In particular, the model would suggest a recommendation of declaring something\nless than 90% of cases as matched and giving up on the other 10% or so, so as to avoid\nmost of the false matches, and the dots show a similar pattern.\n\nIt is clearly possible to match large proportions of the files with little or no error. Also,\nthe quality of candidate matches becomes dramatically worse at some point where the\nfalse-match rate accelerates. Figure 1.5 takes a magnifying glass to the previous display\nto highlight the behavior of the calibration procedure in the region of interest where the\nfalse-match rate accelerates. The predicted false-match rate curves bend upward, close to\nthe points where the observed false-match rate curves rise steeply, which is a particularly\nencouraging feature of the calibration method. The calibration procedure performs well\nfrom the standpoint of providing predicted probabilities that are close to the true probabili-\nties and interval estimates that are informative and include the true values. By comparison,\nthe original estimates of match probabilities, constructed by multiplying weights without\nempirical calibration, were highly inaccurate.\n\n1.8 Some useful results from probability theory\n\nWe assume the reader is familiar with elementary manipulations involving probabilities\nand probability distributions. In particular, basic probability background that must be\nwell understood for key parts of the book includes the manipulation of joint densities, the\ndefinition of simple moments, the transformation of variables, and methods of simulation. In\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n20 1. PROBABILITY AND INFERENCE\n\nthis section we briefly review these assumed prerequisites and clarify some further notational\nconventions used in the remainder of the book. Appendix A provides information on some\ncommonly used probability distributions.\n\nAs introduced in Section 1.3, we generally represent joint distributions by their joint\nprobability mass or density function, with dummy arguments reflecting the name given to\neach variable being considered. Thus for two quantities u and v, we write the joint density\nas p(u, v); if specific values need to be referenced, this notation will be further abused as\nwith, for example, p(u, v=1).\n\nIn Bayesian calculations relating to a joint density p(u, v), we will often refer to a\nconditional distribution or density function such as p(u|v) and a marginal density such as\np(u) =\n\n∫\np(u, v)dv. In this notation, either or both u and v can be vectors. Typically\n\nit will be clear from the context that the range of integration in the latter expression\nrefers to the entire range of the variable being integrated out. It is also often useful to\nfactor a joint density as a product of marginal and conditional densities; for example,\np(u, v, w) = p(u|v, w)p(v|w)p(w).\n\nSome authors use different notations for distributions on parameters and observables—\nfor example, π(θ), f(y|θ)—but this obscures the fact that all probability distributions have\nthe same logical status in Bayesian inference. We must always be careful, though, to in-\ndicate appropriate conditioning; for example, p(y|θ) is different from p(y). In the inter-\nests of conciseness, however, our notation hides the conditioning on hypotheses that hold\nthroughout—no probability judgments can be made in a vacuum—and to be more explicit\none might use a notation such as the following:\n\np(θ, y|H) = p(θ|H)p(y|θ,H),\n\nwhere H refers to the set of hypotheses or assumptions used to define the model. Also, we\nsometimes suppress explicit conditioning on known explanatory variables, x.\n\nWe use the standard notations, E(·) and var(·), for mean and variance, respectively:\n\nE(u) =\n\n∫\nup(u)du, var(u) =\n\n∫\n(u− E(u))2p(u)du.\n\nFor a vector parameter u, the expression for the mean is the same, and the covariance\nmatrix is defined as\n\nvar(u) =\n\n∫\n(u− E(u))(u− E(u))T p(u)du,\n\nwhere u is considered a column vector. (We use the terms ‘variance matrix’ and ‘covariance\nmatrix’ interchangeably.) This notation is slightly imprecise, because E(u) and var(u) are\nreally functions of the distribution function, p(u), not of the variable u. In an expression\ninvolving an expectation, any variable that does not appear explicitly as a conditioning\nvariable is assumed to be integrated out in the expectation; for example, E(u|v) refers to\nthe conditional expectation of u with v held fixed—that is, the conditional expectation as\na function of v—whereas E(u) is the expectation of u, averaging over v (as well as u).\n\nModeling using conditional probability\n\nUseful probability models often express the distribution of observables conditionally or hier-\narchically rather than through more complicated unconditional distributions. For example,\nsuppose y is the height of a university student selected at random. The marginal distri-\nbution p(y) is (essentially) a mixture of two approximately normal distributions centered\naround 160 and 175 centimeters. A more useful description of the distribution of y would\nbe based on the joint distribution of height and sex: p(male) ≈ p(female) ≈ 1\n\n2 , along with\nthe conditional specifications that p(y|female) and p(y|male) are each approximately normal\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(u|v)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over v. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nE(u) =\n\n∫ ∫\nup(u, v)dudv =\n\n∫ ∫\nu p(u|v)du p(v)dv =\n\n∫\nE(u|v)p(v)dv.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(u|v)) + var(E(u|v)). (1.9)\n\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var (E(u|v)) = E\n(\nE(u2|v)− (E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E (E(u|v)))2\n\n= E(u2)− E\n(\n(E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E(u))2\n\n= E(u2)− (E(u))2\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if u is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(·). Suppose pu(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf pu is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\npv(v) = pu(f\n−1(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for pv(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf pu is a continuous distribution, and v = f(u) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\npv(v) = |J | pu(f−1(v))\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(uly)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over vu. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nu)= / / up(u, v)dudv = / / up(ulv)dup(v)dv = / E(ulv)p(w)do.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(ulv)) + var(E(ulv)). (1.9)\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var(E(ulv)) = E(E(u*|v) — (E(ulv))*) + E ((E(ulv))*) — (E (E(u|v)))°\nE(u’) — E ((E(u|v))*) + E ((E(ulv))*) = (E(u)?\n= E(u*) — (E(u)?\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if wu is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(-). Suppose p,(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf p, is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\nPo(v) = Pu(f(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for p,(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf p,, is a continuous distribution, and v = f(wu) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\nPo(v) = |J| pul(f~*(v))\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n22 1. PROBABILITY AND INFERENCE\n\nwhere |J | is the absolute value of the determinant of the Jacobian of the transformation\nu = f−1(v) as a function of v; the Jacobian J is the square matrix of partial derivatives\n(with dimension given by the number of components of u), with the (i, j)th entry equal to\n∂ui/∂vj. Once again, if f is many-to-one, then pv(v) is a sum or integral of terms.\n\nIn one dimension, we commonly use the logarithm to transform the parameter space\nfrom (0,∞) to (−∞,∞). When working with parameters defined on the open unit interval,\n(0, 1), we often use the logistic transformation:\n\nlogit(u) = log\n\n(\nu\n\n1− u\n\n)\n, (1.10)\n\nwhose inverse transformation is\n\nlogit−1(v) =\nev\n\n1 + ev\n.\n\nAnother common choice is the probit transformation, Φ−1(u), where Φ is the standard\nnormal cumulative distribution function, to transform from (0, 1) to (−∞,∞).\n\n1.9 Computation and software\n\nAt the time of writing, the authors rely primarily on the software package R for graphs and\nbasic simulations, fitting of classical simple models (including regression, generalized linear\nmodels, and nonparametric methods such as locally weighted regression), optimization, and\nsome simple programming. We use the Bayesian inference package Stan (see Appendix C)\nfor fitting most models, but for teaching purposes in this book we describe how to perform\nmost of the computations from first principles. Even when using Stan, we typically work\nwithin R to plot and transform the data before model fitting, and to display inferences and\nmodel checks afterwards.\n\nSpecific computational tasks that arise in Bayesian data analysis include:\n\n• Vector and matrix manipulations (see Table 1.1)\n\n• Computing probability density functions (see Appendix A)\n\n• Drawing simulations from probability distributions (see Appendix A for standard distri-\nbutions and Exercise 1.9 for an example of a simple stochastic process)\n\n• Structured programming (including looping and customized functions)\n\n• Calculating the linear regression estimate and variance matrix (see Chapter 14)\n\n• Graphics, including scatterplots with overlain lines and multiple graphs per page (see\nChapter 6 for examples).\n\nOur general approach to computation is to fit many models, gradually increasing the\ncomplexity. We do not recommend the strategy of writing a model and then letting the\ncomputer run overnight to estimate it perfectly. Rather, we prefer to fit each model rela-\ntively quickly, using inferences from the previously fitted simpler models as starting values,\nand displaying inferences and comparing to data before continuing.\n\nWe discuss computation in detail in Part III of this book after first introducing the\nfundamental concepts of Bayesian modeling, inference, and model checking. Appendix C\nillustrates how to perform computations in R and Stan in several different ways for a single\nexample.\n\nSummarizing inferences by simulation\n\nSimulation forms a central part of much applied Bayesian analysis, because of the relative\nease with which samples can often be generated from a probability distribution, even when\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.9. COMPUTATION AND SOFTWARE 23\n\nthe density function cannot be explicitly integrated. In performing simulations, it is helpful\nto consider the duality between a probability density function and a histogram of a set of\nrandom draws from the distribution: given a large enough sample, the histogram can pro-\nvide practically complete information about the density, and in particular, various sample\nmoments, percentiles, and other summary statistics provide estimates of any aspect of the\ndistribution, to a level of precision that can be estimated. For example, to estimate the\n95th percentile of the distribution of θ, draw a random sample of size S from p(θ) and use\nthe 0.95Sth order statistic. For most purposes, S = 1000 is adequate for estimating the\n95th percentile in this way.\n\nAnother advantage of simulation is that extremely large or small simulated values often\nflag a problem with model specification or parameterization (for example, see Figure 4.2)\nthat might not be noticed if estimates and probability statements were obtained in analytic\nform.\n\nGenerating values from a probability distribution is often straightforward with modern\ncomputing techniques based on (pseudo)random number sequences. A well-designed pseu-\ndorandom number generator yields a deterministic sequence that appears to have the same\nproperties as a sequence of independent random draws from the uniform distribution on\n[0, 1]. Appendix A describes methods for drawing random samples from some commonly\nused distributions.\n\nSampling using the inverse cumulative distribution function\n\nAs an introduction to the ideas of simulation, we describe a method for sampling from\ndiscrete and continuous distributions using the inverse cumulative distribution function.\nThe cumulative distribution function, or cdf, F , of a one-dimensional distribution, p(v), is\ndefined by\n\nF (v∗) = Pr(v ≤ v∗)\n\n=\n\n{ ∑\nv≤v∗ p(v) if p is discrete∫ v∗\n\n−∞ p(v)dv if p is continuous.\n\nThe inverse cdf can be used to obtain random samples from the distribution p, as\nfollows. First draw a random value, U , from the uniform distribution on [0, 1], using a table\nof random numbers or, more likely, a random number function on the computer. Now let\nv = F−1(U). The function F is not necessarily one-to-one—certainly not if the distribution\nis discrete—but F−1(U) is unique with probability 1. The value v will be a random draw\nfrom p, and is easy to compute as long as F−1(U) is simple. For a discrete distribution,\nF−1 can simply be tabulated.\n\nFor a continuous example, suppose v has an exponential distribution with parameter λ\n(see Appendix A); then its cdf is F (v) = 1− e−λv, and the value of v for which U=F (v) is\n\nv = − log(1−U)\nλ . Then, recognizing that 1−U also has the uniform distribution on [0, 1], we\n\nsee we can obtain random draws from the exponential distribution as − logU\nλ . We discuss\n\nother methods of simulation in Part III of the book and Appendix A.\n\nSimulation of posterior and posterior predictive quantities\n\nIn practice, we are most often interested in simulating draws from the posterior distribu-\ntion of the model parameters θ, and perhaps from the posterior predictive distribution of\nunknown observables ỹ. Results from a set of S simulation draws can be stored in the\ncomputer in an array, as illustrated in Table 1.1. We use the notation s = 1, . . . , S to in-\ndex simulation draws; (θs, ỹs) is the corresponding joint draw of parameters and predicted\nquantities from their joint posterior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n24 1. PROBABILITY AND INFERENCE\n\nSimulation Parameters Predictive\ndraw quantities\n\nθ1 . . . θk ỹ1 . . . ỹn\n1 θ11 . . . θ1k ỹ11 . . . ỹ1n\n...\n\n...\n. . .\n\n...\n...\n\n. . .\n...\n\nS θS1 . . . θSk ỹS1 . . . ỹSn\n\nTable 1.1 Structure of posterior and posterior predictive simulations. The superscripts are indexes,\nnot powers.\n\nFrom these simulated values, we can estimate the posterior distribution of any quantity\nof interest, such as θ1/θ3, by just computing a new column in Table 1.1 using the existing S\ndraws of (θ, ỹ). We can estimate the posterior probability of any event, such as Pr(ỹ1+ ỹ2 >\neθ1), by the proportion of the S simulations for which it is true. We are often interested in\nposterior intervals; for example, the central 95% posterior interval [a, b] for the parameter\nθj , for which Pr(θj < a) = 0.025 and Pr(θj > b) = 0.025. These values can be directly\nestimated by the appropriate simulated values of θj , for example, the 25th and 976th order\nstatistics if S=1000. We commonly summarize inferences by 50% and 95% intervals.\n\nWe return to the accuracy of simulation inferences in Section 10.5 after we have gained\nsome experience using simulations of posterior distributions in some simple examples.\n\n1.10 Bayesian inference in applied statistics\n\nA pragmatic rationale for the use of Bayesian methods is the inherent flexibility introduced\nby their incorporation of multiple levels of randomness and the resultant ability to combine\ninformation from different sources, while incorporating all reasonable sources of uncertainty\nin inferential summaries. Such methods naturally lead to smoothed estimates in complicated\ndata structures and consequently have the ability to obtain better real-world answers.\n\nAnother reason for focusing on Bayesian methods is more psychological, and involves the\nrelationship between the statistician and the client or specialist in the subject matter area\nwho is the consumer of the statistician’s work. In many practical cases, clients will interpret\ninterval estimates provided by statisticians as Bayesian intervals, that is, as probability\nstatements about the likely values of unknown quantities conditional on the evidence in\nthe data. Such direct probability statements require prior probability specifications for\nunknown quantities (or more generally, probability models for vectors of unknowns), and\nthus the kinds of answers clients will assume are being provided by statisticians, Bayesian\nanswers, require full probability models—explicit or implicit.\n\nFinally, Bayesian inferences are conditional on probability models that invariably contain\napproximations in their attempt to represent complicated real-world relationships. If the\nBayesian answers vary dramatically over a range of scientifically reasonable assumptions\nthat are unassailable by the data, then the resultant range of possible conclusions must be\nentertained as legitimate, and we believe that the statistician has the responsibility to make\nthe client aware of this fact.\n\nIn this book, we focus on the construction of models (especially hierarchical ones, as\ndiscussed in Chapter 5 onward) to relate complicated data structures to scientific questions,\nchecking the fit of such models, and investigating the sensitivity of conclusions to reasonable\nmodeling assumptions. From this point of view, the strength of the Bayesian approach lies in\n(1) its ability to combine information from multiple sources (thereby in fact allowing greater\n‘objectivity’ in final conclusions), and (2) its more encompassing accounting of uncertainty\nabout the unknowns in a statistical problem.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.11. BIBLIOGRAPHIC NOTE 25\n\nOther important themes, many of which are common to much modern applied statistical\npractice, whether formally Bayesian or not, are the following:\n\n• a willingness to use many parameters\n\n• hierarchical structuring of models, which is the essential tool for achieving partial pool-\ning of estimates and compromising in a scientific way between alternative sources of\ninformation\n\n• model checking—not only by examining the internal goodness of fit of models to ob-\nserved and possible future data, but also by comparing inferences about estimands and\npredictions of interest to substantive knowledge\n\n• an emphasis on inference in the form of distributions or at least interval estimates rather\nthan simple point estimates\n\n• the use of simulation as the primary method of computation; the modern computational\ncounterpart to a ‘joint probability distribution’ is a set of randomly drawn values, and a\nkey tool for dealing with missing data is the method of multiple imputation (computation\nand multiple imputation are discussed in more detail in later chapters)\n\n• the use of probability models as tools for understanding and possibly improving data-\nanalytic techniques that may not explicitly invoke a Bayesian model\n\n• the importance of including in the analysis as much background information as possible,\nso as to approximate the goal that data can be viewed as a random sample, conditional\non all the variables in the model\n\n• the importance of designing studies to have the property that inferences for estimands\nof interest will be robust to model assumptions.\n\n1.11 Bibliographic note\n\nSeveral good introductory books have been written on Bayesian statistics, beginning with\nLindley (1965), and continuing through Hoff (2009). Berry (1996) presents, from a Bayesian\nperspective, many of the standard topics for an introductory statistics textbook. Gill\n(2002) and Jackman (2009) introduce applied Bayesian statistics for social scientists, Kr-\nuschke (2011) introduces Bayesian methods for psychology researchers, and Christensen et\nal. (2010) supply a general introduction. Carlin and Louis (2008) cover the theory and\napplications of Bayesian inference, focusing on biological applications and connections to\nclassical methods. Some resources for teaching Bayesian statistics include Sedlmeier and\nGigerenzer (2001) and Gelman (1998, 2008b).\n\nThe bibliographic notes at the ends of the chapters in this book refer to a variety of\nspecific applications of Bayesian data analysis. Several review articles in the statistical\nliterature, such as Breslow (1990) and Racine et al. (1986), have appeared that discuss,\nin general terms, areas of application in which Bayesian methods have been useful. The\nvolumes edited by Gatsonis et al. (1993–2002) are collections of Bayesian analyses, including\nextensive discussions about choices in the modeling process and the relations between the\nstatistical methods and the applications.\n\nThe foundations of probability and Bayesian statistics are an important topic that we\ntreat only briefly. Bernardo and Smith (1994) give a thorough review of the foundations\nof Bayesian models and inference with a comprehensive list of references. Jeffreys (1961) is\na self-contained book about Bayesian statistics that comprehensively presents an inductive\nview of inference; Good (1950) is another important early work. Jaynes (1983) is a collection\nof reprinted articles that present a deductive view of Bayesian inference that we believe is\nsimilar to ours. Both Jeffreys and Jaynes focus on applications in the physical sciences.\nJaynes (2003) focuses on connections between statistical inference and the philosophy of\nscience and includes several examples of physical probability.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n26 1. PROBABILITY AND INFERENCE\n\nGigerenzer and Hoffrage (1995) discuss the connections between Bayesian probability\nand frequency probabilities from a perspective similar to ours, and provide evidence that\npeople can typically understand and compute best with probabilities that are expressed\nin the form of relative frequency. Gelman (1998) presents some classroom activities for\nteaching Bayesian ideas.\n\nDe Finetti (1974) is an influential work that focuses on the crucial role of exchange-\nability. More approachable discussions of the role of exchangeability in Bayesian inference\nare provided by Lindley and Novick (1981) and Rubin (1978a, 1987a). The non-Bayesian\narticle by Draper et al. (1993) makes an interesting attempt to explain how exchangeable\nprobability models can be justified in data analysis. Berger and Wolpert (1984) give a\ncomprehensive discussion and review of the likelihood principle, and Berger (1985, Sections\n1.6, 4.1, and 4.12) reviews a range of philosophical issues from the perspective of Bayesian\ndecision theory.\n\nOur own philosophy of Bayesian statistics appears in Gelman (2011) and Gelman and\nShalizi (2013); for some contrasting views, see the discussion of that article, along with\nEfron (1986) and the discussions following Gelman (2008a).\n\nPratt (1965) and Rubin (1984) discuss the relevance of Bayesian methods for applied\nstatistics and make many connections between Bayesian and non-Bayesian approaches to\ninference. Further references on the foundations of statistical inference appear in Shafer\n(1982) and the accompanying discussion. Kahneman and Tversky (1972) and Alpert and\nRaiffa (1982) present the results of psychological experiments that assess the meaning of\n‘subjective probability’ as measured by people’s stated beliefs and observed actions. Lindley\n(1971a) surveys many different statistical ideas, all from the Bayesian perspective. Box and\nTiao (1973) is an early book on applied Bayesian methods. They give an extensive treatment\nof inference based on normal distributions, and their first chapter, a broad introduction to\nBayesian inference, provides a good counterpart to Chapters 1 and 2 of this book.\n\nThe iterative process involving modeling, inference, and model checking that we present\nin Section 1.1 is discussed at length in the first chapter of Box and Tiao (1973) and also\nin Box (1980). Cox and Snell (1981) provide a more introductory treatment of these ideas\nfrom a less model-based perspective.\n\nMany good books on the mathematical aspects of probability theory are available, such\nas Feller (1968) and Ross (1983); these are useful when constructing probability models\nand working with them. O’Hagan (1988) has written an interesting introductory text on\nprobability from an explicitly Bayesian point of view.\n\nPhysical probability models for coin tossing are discussed by Keller (1986), Jaynes\n(2003), and Gelman and Nolan (2002b). The football example of Section 1.6 is discussed\nin more detail in Stern (1991); see also Harville (1980) and Glickman (1993) and Glickman\nand Stern (1998) for analyses of football scores not using the point spread. Related analyses\nof sports scores and betting odds appear in Stern (1997, 1998). For more background on\nsports betting, see Snyder (1975) and Rombola (1984).\n\nAn interesting real-world example of probability assignment arose with the explosion\nof the Challenger space shuttle in 1986; Martz and Zimmer (1992), Dalal, Fowlkes, and\nHoadley (1989), and Lavine (1991) present and compare various methods for assigning\nprobabilities for space shuttle failures. (At the time of writing we are not aware of similar\ncontributions relating to the more recent space accident in 2003.) The record-linkage ex-\nample in Section 1.7 appears in Belin and Rubin (1995b), who discuss the mixture models\nand calibration techniques in more detail. The Census problem that motivated the record\nlinkage is described by Hogan (1992).\n\nIn all our examples, probabilities are assigned using statistical modeling and estimation,\nnot by ‘subjective’ assessment. Dawid (1986) provides a general discussion of probability\nassignment, and Dawid (1982) discusses the connections between calibration and Bayesian\nprobability assignment.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.12. EXERCISES 27\n\nThe graphical method of jittering, used in Figures 1.1 and 1.2 and elsewhere in this\nbook, is discussed in Chambers et al. (1983). For information on the statistical packages R\nand Bugs, see Becker, Chambers, and Wilks (1988), R Project (2002), Fox (2002), Venables\nand Ripley (2002), and Spiegelhalter et al. (1994, 2003).\n\nNorvig (2007) describes the principles and details of the Bayesian spelling corrector.\n\n1.12 Exercises\n\n1. Conditional probability: suppose that if θ = 1, then y has a normal distribution with\nmean 1 and standard deviation σ, and if θ = 2, then y has a normal distribution with\nmean 2 and standard deviation σ. Also, suppose Pr(θ = 1) = 0.5 and Pr(θ = 2) = 0.5.\n\n(a) For σ = 2, write the formula for the marginal probability density for y and sketch it.\n\n(b) What is Pr(θ = 1|y = 1), again supposing σ = 2?\n\n(c) Describe how the posterior density of θ changes in shape as σ is increased and as it is\ndecreased.\n\n2. Conditional means and variances: show that (1.8) and (1.9) hold if u is a vector.\n\n3. Probability calculation for genetics (from Lindley, 1965): suppose that in each individual\nof a large population there is a pair of genes, each of which can be either x or X, that\ncontrols eye color: those with xx have blue eyes, while heterozygotes (those with Xx or\nxX) and those with XX have brown eyes. The proportion of blue-eyed individuals is p2\n\nand of heterozygotes is 2p(1 − p), where 0 < p < 1. Each parent transmits one of its\nown genes to the child; if a parent is a heterozygote, the probability that it transmits the\ngene of type X is 1\n\n2 . Assuming random mating, show that among brown-eyed children\nof brown-eyed parents, the expected proportion of heterozygotes is 2p/(1+2p). Suppose\nJudy, a brown-eyed child of brown-eyed parents, marries a heterozygote, and they have\nn children, all brown-eyed. Find the posterior probability that Judy is a heterozygote\nand the probability that her first grandchild has blue eyes.\n\n4. Probability assignment: we will use the football dataset to estimate some conditional\nprobabilities about professional football games. There were twelve games with point\nspreads of 8 points; the outcomes in those games were: −7, −5, −3, −3, 1, 6, 7, 13, 15,\n16, 20, and 21, with positive values indicating wins by the favorite and negative values\nindicating wins by the underdog. Consider the following conditional probabilities:\n\nPr(favorite wins | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8 and favorite wins).\n\n(a) Estimate each of these using the relative frequencies of games with a point spread of\n8.\n\n(b) Estimate each using the normal approximation for the distribution of (outcome −\npoint spread).\n\n5. Probability assignment: the 435 U.S. Congressmembers are elected to two-year terms;\nthe number of voters in an individual congressional election varies from about 50,000 to\n350,000. We will use various sources of information to estimate roughly the probability\nthat at least one congressional election is tied in the next national election.\n\n(a) Use any knowledge you have about U.S. politics. Specify clearly what information you\nare using to construct this conditional probability, even if your answer is just a guess.\n\n(b) Use the following information: in the period 1900–1992, there were 20,597 congres-\nsional elections, out of which 6 were decided by fewer than 10 votes and 49 decided\nby fewer than 100 votes.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n28 1. PROBABILITY AND INFERENCE\n\nSee Gelman, King, and Boscardin (1998), Mulligan and Hunter (2001), and Gelman,\nKatz, and Tuerlinckx (2002) for more on this topic.\n\n6. Conditional probability: approximately 1/125 of all births are fraternal twins and 1/300\nof births are identical twins. Elvis Presley had a twin brother (who died at birth). What\nis the probability that Elvis was an identical twin? (You may approximate the probability\nof a boy or girl birth as 1\n\n2 .)\n\n7. Conditional probability: the following problem is loosely based on the television game\nshow Let’s Make a Deal. At the end of the show, a contestant is asked to choose one of\nthree large boxes, where one box contains a fabulous prize and the other two boxes contain\nlesser prizes. After the contestant chooses a box, Monty Hall, the host of the show,\nopens one of the two boxes containing smaller prizes. (In order to keep the conclusion\nsuspenseful, Monty does not open the box selected by the contestant.) Monty offers the\ncontestant the opportunity to switch from the chosen box to the remaining unopened box.\nShould the contestant switch or stay with the original choice? Calculate the probability\nthat the contestant wins under each strategy. This is an exercise in being clear about the\ninformation that should be conditioned on when constructing a probability judgment.\nSee Selvin (1975) and Morgan et al. (1991) for further discussion of this problem.\n\n8. Subjective probability: discuss the following statement. ‘The probability of event E is\nconsidered “subjective” if two rational persons A and B can assign unequal probabilities\nto E, PA(E) and PB(E). These probabilities can also be interpreted as “conditional”:\nPA(E) = P (E|IA) and PB(E) = P (E|IB), where IA and IB represent the knowledge\navailable to persons A and B, respectively.’ Apply this idea to the following examples.\n\n(a) The probability that a ‘6’ appears when a fair die is rolled, where A observes the\noutcome of the die roll and B does not.\n\n(b) The probability that Brazil wins the next World Cup, where A is ignorant of soccer\nand B is a knowledgeable sports fan.\n\n9. Simulation of a queuing problem: a clinic has three doctors. Patients come into the\nclinic at random, starting at 9 a.m., according to a Poisson process with time parameter\n10 minutes: that is, the time after opening at which the first patient appears follows an\nexponential distribution with expectation 10 minutes and then, after each patient arrives,\nthe waiting time until the next patient is independently exponentially distributed, also\nwith expectation 10 minutes. When a patient arrives, he or she waits until a doctor\nis available. The amount of time spent by each doctor with each patient is a random\nvariable, uniformly distributed between 5 and 20 minutes. The office stops admitting\nnew patients at 4 p.m. and closes when the last patient is through with the doctor.\n\n(a) Simulate this process once. How many patients came to the office? How many had to\nwait for a doctor? What was their average wait? When did the office close?\n\n(b) Simulate the process 100 times and estimate the median and 50% interval for each of\nthe summaries in (a).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 2\n\nSingle-parameter models\n\nOur first detailed discussion of Bayesian inference is in the context of statistical models\nwhere only a single scalar parameter is to be estimated; that is, the estimand θ is one-\ndimensional. In this chapter, we consider four fundamental and widely used one-dimensional\nmodels—the binomial, normal, Poisson, and exponential—and at the same time introduce\nimportant concepts and computational methods for Bayesian data analysis.\n\n2.1 Estimating a probability from binomial data\n\nIn the simple binomial model, the aim is to estimate an unknown population proportion\nfrom the results of a sequence of ‘Bernoulli trials’; that is, data y1, . . . , yn, each of which is\neither 0 or 1. This problem provides a relatively simple but important starting point for\nthe discussion of Bayesian inference. By starting with the binomial model, our discussion\nalso parallels the very first published Bayesian analysis by Thomas Bayes in 1763, and his\nseminal contribution is still of interest.\n\nThe binomial distribution provides a natural model for data that arise from a sequence\nof n exchangeable trials or draws from a large population where each trial gives rise to\none of two possible outcomes, conventionally labeled ‘success’ and ‘failure.’ Because of the\nexchangeability, the data can be summarized by the total number of successes in the n\ntrials, which we denote here by y. Converting from a formulation in terms of exchangeable\ntrials to one using independent and identically distributed random variables is achieved\nnaturally by letting the parameter θ represent the proportion of successes in the population\nor, equivalently, the probability of success in each trial. The binomial sampling model is,\n\np(y|θ) = Bin(y|n, θ) =\n(\nn\n\ny\n\n)\nθy(1 − θ)n−y, (2.1)\n\nwhere on the left side we suppress the dependence on n because it is regarded as part of the\nexperimental design that is considered fixed; all the probabilities discussed for this problem\nare assumed to be conditional on n.\n\nExample. Estimating the probability of a female birth\nAs a specific application of the binomial model, we consider the estimation of the\nsex ratio within a population of human births. The proportion of births that are\nfemale has long been a topic of interest both scientifically and to the lay public. Two\nhundred years ago it was established that the proportion of female births in European\npopulations was less than 0.5 (see Historical Note below), while in this century interest\nhas focused on factors that may influence the sex ratio. The currently accepted value\nof the proportion of female births in large European-race populations is 0.485.\nFor this example we define the parameter θ to be the proportion of female births, but\nan alternative way of reporting this parameter is as a ratio of male to female birth\nrates, φ = (1− θ)/θ.\nLet y be the number of girls in n recorded births. By applying the binomial model\n\n29\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n30 2. SINGLE-PARAMETER MODELS\n\nFigure 2.1 Unnormalized posterior density for binomial parameter θ, based on uniform prior dis-\ntribution and y successes out of n trials. Curves displayed for several values of n and y.\n\n(2.1), we are assuming that the n births are conditionally independent given θ, with\nthe probability of a female birth equal to θ for all cases. This modeling assumption\nis motivated by the exchangeability that may be judged to arise when we have no\nexplanatory information (for example, distinguishing multiple births or births within\nthe same family) that might affect the sex of the baby.\n\nTo perform Bayesian inference in the binomial model, we must specify a prior distribu-\ntion for θ. We will discuss issues associated with specifying prior distributions many times\nthroughout this book, but for simplicity at this point, we assume that the prior distribution\nfor θ is uniform on the interval [0, 1].\n\nElementary application of Bayes’ rule as displayed in (1.2), applied to (2.1), then gives\nthe posterior density for θ as\n\np(θ|y) ∝ θy(1 − θ)n−y. (2.2)\n\nWith fixed n and y, the factor\n(\nn\ny\n\n)\ndoes not depend on the unknown parameter θ, and so it\n\ncan be treated as a constant when calculating the posterior distribution of θ. As is typical\nof many examples, the posterior density can be written immediately in closed form, up to a\nconstant of proportionality. In single-parameter problems, this allows immediate graphical\npresentation of the posterior distribution. For example, in Figure 2.1, the unnormalized\ndensity (2.2) is displayed for several different experiments, that is, different values of n and\ny. Each of the four experiments has the same proportion of successes, but the sample sizes\nvary. In the present case, we can recognize (2.2) as the unnormalized form of the beta\ndistribution (see Appendix A),\n\nθ|y ∼ Beta(y + 1, n− y + 1). (2.3)\n\nHistorical note: Bayes and Laplace\nMany early writers on probability dealt with the elementary binomial model. The first\ncontributions of lasting significance, in the 17th and early 18th centuries, concentrated\non the ‘pre-data’ question: given θ, what are the probabilities of the various possible\noutcomes of the random variable y? For example, the ‘weak law of large numbers’ of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.1. ESTIMATING A PROBABILITY FROM BINOMIAL DATA 31\n\nJacob Bernoulli states that if y ∼ Bin(n, θ), then Pr( | yn − θ|>ǫ\n∣∣ θ) → 0 as n → ∞,\n\nfor any θ and any fixed value of ǫ > 0. The Reverend Thomas Bayes, an English\npart-time mathematician whose work was unpublished during his lifetime, and Pierre\nSimon Laplace, an inventive and productive mathematical scientist whose massive\noutput spanned the Napoleonic era in France, receive independent credit as the first\nto invert the probability statement and obtain probability statements about θ, given\nobserved y.\nIn his famous paper, published in 1763, Bayes sought, in our notation, the probability\nPr(θ∈ (θ1, θ2)|y); his solution was based on a physical analogy of a probability space\nto a rectangular table (such as a billiard table):\n\n1. (Prior distribution) A ballW is randomly thrown (according to a uniform distribu-\ntion on the table). The horizontal position of the ball on the table is θ, expressed\nas a fraction of the table width.\n\n2. (Likelihood) A ball O is randomly thrown n times. The value of y is the number\nof times O lands to the right of W .\n\nThus, θ is assumed to have a (prior) uniform distribution on [0, 1]. Using direct\nprobability calculations which he derived in the paper, Bayes then obtained\n\nPr(θ∈(θ1, θ2)|y) =\nPr(θ∈(θ1, θ2), y)\n\np(y)\n\n=\n\n∫ θ2\nθ1\np(y|θ)p(θ)dθ\np(y)\n\n=\n\n∫ θ2\nθ1\n\n(\nn\ny\n\n)\nθy(1− θ)n−ydθ\n\np(y)\n. (2.4)\n\nBayes succeeded in evaluating the denominator, showing that\n\np(y) =\n\n∫ 1\n\n0\n\n(\nn\n\ny\n\n)\nθy(1− θ)n−ydθ (2.5)\n\n=\n1\n\nn+ 1\nfor y = 0, . . . , n.\n\nThis calculation shows that all possible values of y are equally likely a priori.\nThe numerator of (2.4) is an incomplete beta integral with no closed-form expression\nfor large values of y and (n− y), a fact that apparently presented some difficulties for\nBayes.\nLaplace, however, independently ‘discovered’ Bayes’ theorem, and developed new ana-\nlytic tools for computing integrals. For example, he expanded the function θy(1− θ)n−y\naround its maximum at θ = y/n and evaluated the incomplete beta integral using what\nwe now know as the normal approximation.\nIn analyzing the binomial model, Laplace also used the uniform prior distribution. His\nfirst serious application was to estimate the proportion of girl births in a population.\nA total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770. Letting\nθ be the probability that any birth is female, Laplace showed that\n\nPr(θ ≥ 0.5|y = 241,945, n = 251,527+ 241,945) ≈ 1.15× 10−42,\n\nand so he was ‘morally certain’ that θ < 0.5.\n\nPrediction\n\nIn the binomial example with the uniform prior distribution, the prior predictive distribution\ncan be evaluated explicitly, as we have already noted in (2.5). Under the model, all possible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n32 2. SINGLE-PARAMETER MODELS\n\nvalues of y are equally likely, a priori. For posterior prediction from this model, we might\nbe more interested in the outcome of one new trial, rather than another set of n new trials.\nLetting ỹ denote the result of a new trial, exchangeable with the first n,\n\nPr(ỹ = 1|y) =\n\n∫ 1\n\n0\n\nPr(ỹ = 1|θ, y)p(θ|y)dθ\n\n=\n\n∫ 1\n\n0\n\nθp(θ|y)dθ = E(θ|y) = y + 1\n\nn+ 2\n, (2.6)\n\nfrom the properties of the beta distribution (see Appendix A). It is left as an exercise to\nreproduce this result using direct integration of (2.6). This result, based on the uniform\nprior distribution, is known as ‘Laplace’s law of succession.’ At the extreme observations\ny = 0 and y = n, Laplace’s law predicts probabilities of 1\n\nn+2 and n+1\nn+2 , respectively.\n\n2.2 Posterior as compromise between data and prior information\n\nThe process of Bayesian inference involves passing from a prior distribution, p(θ), to a\nposterior distribution, p(θ|y), and it is natural to expect that some general relations might\nhold between these two distributions. For example, we might expect that, because the\nposterior distribution incorporates the information from the data, it will be less variable than\nthe prior distribution. This notion is formalized in the second of the following expressions:\n\nE(θ) = E(E(θ|y)) (2.7)\n\nand\nvar(θ) = E(var(θ|y)) + var(E(θ|y)), (2.8)\n\nwhich are obtained by substituting (θ, y) for the generic (u, v) in (1.8) and (1.9). The result\nexpressed by Equation (2.7) is scarcely surprising: the prior mean of θ is the average of all\npossible posterior means over the distribution of possible data. The variance formula (2.8)\nis more interesting because it says that the posterior variance is on average smaller than\nthe prior variance, by an amount that depends on the variation in posterior means over\nthe distribution of possible data. The greater the latter variation, the more the potential\nfor reducing our uncertainty with regard to θ, as we shall see in detail for the binomial\nand normal models in the next chapter. The mean and variance relations only describe\nexpectations, and in particular situations the posterior variance can be similar to or even\nlarger than the prior variance (although this can be an indication of conflict or inconsistency\nbetween the sampling model and prior distribution).\n\nIn the binomial example with the uniform prior distribution, the prior mean is 1\n2 , and\n\nthe prior variance is 1\n12 . The posterior mean, y+1\n\nn+2 , is a compromise between the prior mean\nand the sample proportion, yn , where clearly the prior mean has a smaller and smaller role\nas the size of the data sample increases. This is a general feature of Bayesian inference: the\nposterior distribution is centered at a point that represents a compromise between the prior\ninformation and the data, and the compromise is controlled to a greater extent by the data\nas the sample size increases.\n\n2.3 Summarizing posterior inference\n\nThe posterior probability distribution contains all the current information about the pa-\nrameter θ. Ideally one might report the entire posterior distribution p(θ|y); as we have seen\nin Figure 2.1, a graphical display is useful. In Chapter 3, we use contour plots and scat-\nterplots to display posterior distributions in multiparameter problems. A key advantage of\nthe Bayesian approach, as implemented by simulation, is the flexibility with which posterior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.3. SUMMARIZING POSTERIOR INFERENCE 33\n\nFigure 2.2 Hypothetical density for which the 95% central interval and 95% highest posterior density\nregion dramatically differ: (a) central posterior interval, (b) highest posterior density region.\n\ninferences can be summarized, even after complicated transformations. This advantage is\nmost directly seen through examples, some of which will be presented shortly.\n\nFor many practical purposes, however, various numerical summaries of the distribu-\ntion are desirable. Commonly used summaries of location are the mean, median, and\nmode(s) of the distribution; variation is commonly summarized by the standard deviation,\nthe interquartile range, and other quantiles. Each summary has its own interpretation: for\nexample, the mean is the posterior expectation of the parameter, and the mode may be\ninterpreted as the single ‘most likely’ value, given the data (and the model). Furthermore,\nas we shall see, much practical inference relies on the use of normal approximations, often\nimproved by applying a symmetrizing transformation to θ, and here the mean and the stan-\ndard deviation play key roles. The mode is important in computational strategies for more\ncomplex problems because it is often easier to compute than the mean or median.\n\nWhen the posterior distribution has a closed form, such as the beta distribution in\nthe current example, summaries such as the mean, median, and standard deviation of\nthe posterior distribution are often available in closed form. For example, applying the\ndistributional results in Appendix A, the mean of the beta distribution in (2.3) is y+1\n\nn+2 , and\nthe mode is y\n\nn , which is well known from different points of view as the maximum likelihood\nand (minimum variance) unbiased estimate of θ.\n\nPosterior quantiles and intervals\n\nIn addition to point summaries, it is nearly always important to report posterior uncertainty.\nOur usual approach is to present quantiles of the posterior distribution of estimands of\ninterest or, if an interval summary is desired, a central interval of posterior probability,\nwhich corresponds, in the case of a 100(1− α)% interval, to the range of values above and\nbelow which lies exactly 100(α/2)% of the posterior probability. Such interval estimates\nare referred to as posterior intervals. For simple models, such as the binomial and normal,\nposterior intervals can be computed directly from cumulative distribution functions, often\nusing calls to standard computer functions, as we illustrate in Section 2.4 with the example\nof the human sex ratio. In general, intervals can be computed using computer simulations\nfrom the posterior distribution, as described at the end of Section 1.9.\n\nA slightly different summary of posterior uncertainty is the highest posterior density\nregion: the set of values that contains 100(1 − α)% of the posterior probability and also\nhas the characteristic that the density within the region is never lower than that outside.\nSuch a region is identical to a central posterior interval if the posterior distribution is\nunimodal and symmetric. In current practice, the central posterior interval is in common\nuse, partly because it has a direct interpretation as the posterior α/2 and 1−α/2 quantiles,\nand partly because it is directly computed using posterior simulations. Figure 2.2 shows\na case where different posterior summaries look much different: the 95% central interval\nincludes the area of zero probability in the center of the distribution, whereas the 95%\nhighest posterior density region comprises two disjoint intervals. In this situation, the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n34 2. SINGLE-PARAMETER MODELS\n\nhighest posterior density region is more cumbersome but conveys more information than\nthe central interval; however, it is probably better not to try to summarize this bimodal\ndensity by any single interval. The central interval and the highest posterior density region\ncan also differ substantially when the posterior density is highly skewed.\n\n2.4 Informative prior distributions\n\nIn the binomial example, we have so far considered only the uniform prior distribution for\nθ. How can this specification be justified, and how in general do we approach the problem\nof constructing prior distributions?\n\nWe consider two basic interpretations that can be given to prior distributions. In the\npopulation interpretation, the prior distribution represents a population of possible parame-\nter values, from which the θ of current interest has been drawn. In the more subjective state\nof knowledge interpretation, the guiding principle is that we must express our knowledge\n(and uncertainty) about θ as if its value could be thought of as a random realization from\nthe prior distribution. For many problems, such as estimating the probability of failure in\na new industrial process, there is no perfectly relevant population of θ’s from which the\ncurrent θ has been drawn, except in hypothetical contemplation. Typically, the prior distri-\nbution should include all plausible values of θ, but the distribution need not be realistically\nconcentrated around the true value, because often the information about θ contained in the\ndata will far outweigh any reasonable prior probability specification.\n\nIn the binomial example, we have seen that the uniform prior distribution for θ im-\nplies that the prior predictive distribution for y (given n) is uniform on the discrete set\n{0, 1, . . . , n}, giving equal probability to the n+1 possible values. In his original treatment\nof this problem (described in the Historical Note in Section 2.1), Bayes’ justification for the\nuniform prior distribution appears to have been based on this observation; the argument\nis appealing because it is expressed entirely in terms of the observable quantities y and n.\nLaplace’s rationale for the uniform prior density was less clear, but subsequent interpre-\ntations ascribe to him the so-called ‘principle of insufficient reason,’ which claims that a\nuniform specification is appropriate if nothing is known about θ. We shall discuss in Section\n2.8 the weaknesses of the principle of insufficient reason as a general approach for assigning\nprobability distributions.\n\nAt this point, we discuss some of the issues that arise in assigning a prior distribution\nthat reflects substantive information.\n\nBinomial example with different prior distributions\n\nWe first pursue the binomial model in further detail using a parametric family of prior\ndistributions that includes the uniform as a special case. For mathematical convenience, we\nconstruct a family of prior densities that lead to simple posterior densities.\n\nConsidered as a function of θ, the likelihood (2.1) is of the form,\n\np(y|θ) ∝ θa(1 − θ)b.\n\nThus, if the prior density is of the same form, with its own values a and b, then the posterior\ndensity will also be of this form. We will parameterize such a prior density as\n\np(θ) ∝ θα−1(1− θ)β−1,\n\nwhich is a beta distribution with parameters α and β: θ ∼ Beta(α, β). Comparing p(θ) and\np(y|θ) suggests that this prior density is equivalent to α− 1 prior successes and β − 1 prior\nfailures. The parameters of the prior distribution are often referred to as hyperparameters.\nThe beta prior distribution is indexed by two hyperparameters, which means we can specify\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 35\n\na particular prior distribution by fixing two features of the distribution, for example its mean\nand variance; see (A.3) on page 585.\n\nFor now, assume that we can select reasonable values α and β. Appropriate methods\nfor working with unknown hyperparameters in certain problems are described in Chapter\n5. The posterior density for θ is\n\np(θ|y) ∝ θy(1− θ)n−yθα−1(1− θ)β−1\n\n= θy+α−1(1− θ)n−y+β−1\n\n= Beta(θ|α + y, β + n− y).\n\nThe property that the posterior distribution follows the same parametric form as the\nprior distribution is called conjugacy; the beta prior distribution is a conjugate family for\nthe binomial likelihood. The conjugate family is mathematically convenient in that the\nposterior distribution follows a known parametric form. If information is available that\ncontradicts the conjugate parametric family, it may be necessary to use a more realistic, if\ninconvenient, prior distribution (just as the binomial likelihood may need to be replaced by\na more realistic likelihood in some cases).\n\nTo continue with the binomial model with beta prior distribution, the posterior mean of\nθ, which may be interpreted as the posterior probability of success for a future draw from\nthe population, is now\n\nE(θ|y) = α+ y\n\nα+ β + n\n,\n\nwhich always lies between the sample proportion, y/n, and the prior mean, α/(α+ β); see\nExercise 2.5b. The posterior variance is\n\nvar(θ|y) = (α+ y)(β + n− y)\n(α+ β + n)2(α+ β + n+ 1)\n\n=\nE(θ|y)[1− E(θ|y)]\nα+ β + n+ 1\n\n.\n\nAs y and n− y become large with fixed α and β, E(θ|y) ≈ y/n and var(θ|y) ≈ 1\nn\ny\nn (1−\n\ny\nn ),\n\nwhich approaches zero at the rate 1/n. In the limit, the parameters of the prior distribution\nhave no influence on the posterior distribution.\n\nIn fact, as we shall see in more detail in Chapter 4, the central limit theorem of proba-\nbility theory can be put in a Bayesian context to show:\n\n(\nθ − E(θ|y)√\n\nvar(θ|y)\n\n∣∣∣∣∣ y\n)\n→ N(0, 1).\n\nThis result is often used to justify approximating the posterior distribution with a normal\ndistribution. For the binomial parameter θ, the normal distribution is a more accurate\napproximation in practice if we transform θ to the logit scale; that is, performing inference\nfor log(θ/(1 − θ)) instead of θ itself, thus expanding the probability space from [0, 1] to\n(−∞,∞), which is more fitting for a normal approximation.\n\nConjugate prior distributions\n\nConjugacy is formally defined as follows. If F is a class of sampling distributions p(y|θ),\nand P is a class of prior distributions for θ, then the class P is conjugate for F if\n\np(θ|y) ∈ P for all p(·|θ) ∈ F and p(·) ∈ P .\n\nThis definition is formally vague since if we choose P as the class of all distributions, then\nP is always conjugate no matter what class of sampling distributions is used. We are most\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n36 2. SINGLE-PARAMETER MODELS\n\ninterested in natural conjugate prior families, which arise by taking P to be the set of all\ndensities having the same functional form as the likelihood.\n\nConjugate prior distributions have the practical advantage, in addition to computational\nconvenience, of being interpretable as additional data, as we have seen for the binomial\nexample and will also see for the normal and other standard models in Sections 2.5 and 2.6.\n\nNonconjugate prior distributions\n\nThe basic justification for the use of conjugate prior distributions is similar to that for using\nstandard models (such as binomial and normal) for the likelihood: it is easy to understand\nthe results, which can often be put in analytic form, they are often a good approximation,\nand they simplify computations. Also, they will be useful later as building blocks for more\ncomplicated models, including in many dimensions, where conjugacy is typically impossible.\nFor these reasons, conjugate models can be good starting points; for example, mixtures of\nconjugate families can sometimes be useful when simple conjugate distributions are not\nreasonable (see Exercise 2.4).\n\nAlthough they can make interpretations of posterior inferences less transparent and\ncomputation more difficult, nonconjugate prior distributions do not pose any new conceptual\nproblems. In practice, for complicated models, conjugate prior distributions may not even\nbe possible. Section 2.4 and Exercises 2.10 and 2.11 present examples of nonconjugate\ncomputation; a more extensive nonconjugate example, an analysis of a bioassay experiment,\nappears in Section 3.7.\n\nConjugate prior distributions, exponential families, and sufficient statistics\n\nWe close this section by relating conjugate families of distributions to the classical concepts\nof exponential families and sufficient statistics. Readers who are unfamiliar with these\nconcepts can skip ahead to the example with no loss.\n\nProbability distributions that belong to an exponential family have natural conjugate\nprior distributions, so we digress at this point to review the definition of exponential families;\nfor complete generality in this section, we allow data points yi and parameters θ to be\nmultidimensional. The class F is an exponential family if all its members have the form,\n\np(yi|θ) = f(yi)g(θ)e\nφ(θ)Tu(yi).\n\nThe factors φ(θ) and u(yi) are, in general, vectors of equal dimension to that of θ. The\nvector φ(θ) is called the ‘natural parameter’ of the family F . The likelihood corresponding\nto a sequence y = (y1, . . . , yn) of independent and identically distributed observations is\n\np(y|θ) =\n(\n\nn∏\n\ni=1\n\nf(yi)\n\n)\ng(θ)n exp\n\n(\nφ(θ)T\n\nn∑\n\ni=1\n\nu(yi)\n\n)\n.\n\nFor all n and y, this has a fixed form (as a function of θ):\n\np(y|θ) ∝ g(θ)neφ(θ)T t(y), where t(y) =\n\nn∑\n\ni=1\n\nu(yi).\n\nThe quantity t(y) is said to be a sufficient statistic for θ, because the likelihood for θ\ndepends on the data y only through the value of t(y). Sufficient statistics are useful in\nalgebraic manipulations of likelihoods and posterior distributions. If the prior density is\nspecified as\n\np(θ) ∝ g(θ)ηeφ(θ)T ν ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 37\n\nthen the posterior density is\n\np(θ|y) ∝ g(θ)η+neφ(θ)T (ν+t(y)),\n\nwhich shows that this choice of prior density is conjugate. It has been shown that, in general,\nthe exponential families are the only classes of distributions that have natural conjugate\nprior distributions, since, apart from certain irregular cases, the only distributions having\na fixed number of sufficient statistics for all n are of the exponential type. We have already\ndiscussed the binomial distribution, where for the likelihood p(y|θ, n) = Bin(y|n, θ) with n\nknown, the conjugate prior distributions on θ are beta distributions. It is left as an exercise\nto show that the binomial is an exponential family with natural parameter logit(θ).\n\nExample. Probability of a girl birth given placenta previa\nAs a specific example of a factor that may influence the sex ratio, we consider the\nmaternal condition placenta previa, an unusual condition of pregnancy in which the\nplacenta is implanted low in the uterus, obstructing the fetus from a normal vaginal\ndelivery. An early study concerning the sex of placenta previa births in Germany found\nthat of a total of 980 births, 437 were female. How much evidence does this provide\nfor the claim that the proportion of female births in the population of placenta previa\nbirths is less than 0.485, the proportion of female births in the general population?\n\nAnalysis using a uniform prior distribution. Under a uniform prior distribution for\nthe probability of a girl birth, the posterior distribution is Beta(438, 544). Exact\nsummaries of the posterior distribution can be obtained from the properties of the\nbeta distribution (Appendix A): the posterior mean of θ is 0.446 and the posterior\nstandard deviation is 0.016. Exact posterior quantiles can be obtained using numerical\nintegration of the beta density, which in practice we perform by a computer function\ncall; the median is 0.446 and the central 95% posterior interval is [0.415, 0.477]. This\n95% posterior interval matches, to three decimal places, the interval that would be\nobtained by using a normal approximation with the calculated posterior mean and\nstandard deviation. Further discussion of the approximate normality of the posterior\ndistribution is given in Chapter 4.\nIn many situations it is not feasible to perform calculations on the posterior density\nfunction directly. In such cases it can be particularly useful to use simulation from the\nposterior distribution to obtain inferences. The first histogram in Figure 2.3 shows the\ndistribution of 1000 draws from the Beta(438, 544) posterior distribution. An estimate\nof the 95% posterior interval, obtained by taking the 25th and 976th of the 1000\nordered draws, is [0.415, 0.476], and the median of the 1000 draws from the posterior\ndistribution is 0.446. The sample mean and standard deviation of the 1000 draws are\n0.445 and 0.016, almost identical to the exact results. A normal approximation to the\n95% posterior interval is [0.445 ± 1.96 · 0.016] = [0.414, 0.476]. Because of the large\nsample and the fact that the distribution of θ is concentrated away from zero and one,\nthe normal approximation works well in this example.\nAs already noted, when estimating a proportion, the normal approximation is gener-\nally improved by applying it to the logit transform, log( θ\n\n1−θ ), which transforms the\nparameter space from the unit interval to the real line. The second histogram in Figure\n2.3 shows the distribution of the transformed draws. The estimated posterior mean\nand standard deviation on the logit scale based on 1000 draws are −0.220 and 0.065.\nA normal approximation to the 95% posterior interval for θ is obtained by inverting\nthe 95% interval on the logit scale [−0.220± 1.96 · 0.065], which yields [0.414, 0.477]\non the original scale. The improvement from using the logit scale is most noticeable\nwhen the sample size is small or the distribution of θ includes values near zero or one.\nIn any real data analysis, it is important to keep the applied context in mind. The pa-\nrameter of interest in this example is traditionally expressed as the ‘sex ratio,’ (1−θ)/θ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n38 2. SINGLE-PARAMETER MODELS\n\nFigure 2.3 Draws from the posterior distribution of (a) the probability of female birth, θ; (b) the\nlogit transform, logit(θ); (c) the male-to-female sex ratio, φ = (1− θ)/θ.\n\nParameters of the Summaries of the\nprior distribution posterior distribution\n\nPosterior 95% posterior\nα\n\nα+β\nα+ β median of θ interval for θ\n\n0.500 2 0.446 [0.415, 0.477]\n0.485 2 0.446 [0.415, 0.477]\n0.485 5 0.446 [0.415, 0.477]\n0.485 10 0.446 [0.415, 0.477]\n0.485 20 0.447 [0.416, 0.478]\n0.485 100 0.450 [0.420, 0.479]\n0.485 200 0.453 [0.424, 0.481]\n\nTable 2.1 Summaries of the posterior distribution of θ, the probability of a girl birth given placenta\nprevia, under a variety of conjugate prior distributions.\n\nthe ratio of male to female births. The posterior distribution of the ratio is illustrated\nin the third histogram. The posterior median of the sex ratio is 1.24, and the 95%\nposterior interval is [1.10, 1.41]. The posterior distribution is concentrated on values\nfar above the usual European-race sex ratio of 1.06, implying that the probability of\na female birth given placenta previa is less than in the general population.\n\nAnalysis using different conjugate prior distributions. The sensitivity of posterior\ninference about θ to the proposed prior distribution is exhibited in Table 2.1. The\nfirst row corresponds to the uniform prior distribution, α=1, β=1, and subsequent\nrows of the table use prior distributions that are increasingly concentrated around\n0.485, the proportion of female births in the general population. The first column\nshows the prior mean for θ, and the second column indexes the amount of prior\ninformation, as measured by α+ β; recall that α+ β − 2 is, in some sense, equivalent\nto the number of prior observations. Posterior inferences based on a large sample are\nnot particularly sensitive to the prior distribution. Only at the bottom of the table,\nwhere the prior distribution contains information equivalent to 100 or 200 births, are\nthe posterior intervals pulled noticeably toward the prior distribution, and even then,\nthe 95% posterior intervals still exclude the prior mean.\n\nAnalysis using a nonconjugate prior distribution. As an alternative to the conjugate\nbeta family for this problem, we might prefer a prior distribution that is centered\naround 0.485 but is flat far away from this value to admit the possibility that the\ntruth is far away. The piecewise linear prior density in Figure 2.4a is an example\nof a prior distribution of this form; 40% of the probability mass is outside the inter-\nval [0.385, 0.585]. This prior distribution has mean 0.493 and standard deviation 0.21,\nsimilar to the standard deviation of a beta distribution with α+β = 5. The unnormal-\nized posterior distribution is obtained at a grid of θ values, (0.000, 0.001, . . . , 1.000),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 39\n\nFigure 2.4 (a) Prior density for θ in an example nonconjugate analysis of birth ratio example; (b)\nhistogram of 1000 draws from a discrete approximation to the posterior density. Figures are plotted\non different scales.\n\nby multiplying the prior density and the binomial likelihood at each point. Poste-\nrior simulations can be obtained by normalizing the distribution on the discrete grid\nof θ values. Figure 2.4b is a histogram of 1000 draws from the discrete posterior\ndistribution. The posterior median is 0.448, and the 95% central posterior interval is\n[0.419, 0.480]. Because the prior distribution is overwhelmed by the data, these results\nmatch those in Table 2.1 based on beta distributions. In taking the grid approach, it\nis important to avoid grids that are too coarse and distort a significant portion of the\nposterior mass.\n\n2.5 Normal distribution with known variance\n\nThe normal distribution is fundamental to most statistical modeling. The central limit\ntheorem helps to justify using the normal likelihood in many statistical problems, as an\napproximation to a less analytically convenient actual likelihood. Also, as we shall see in\nlater chapters, even when the normal distribution does not itself provide a good model fit,\nit can be useful as a component of a more complicated model involving t or finite mixture\ndistributions. For now, we simply work through the Bayesian results assuming the normal\nmodel is appropriate. We derive results first for a single data point and then for the general\ncase of many data points.\n\nLikelihood of one data point\n\nAs the simplest first case, consider a single scalar observation y from a normal distribution\nparameterized by a mean θ and variance σ2, where for this initial development we assume\nthat σ2 is known. The sampling distribution is\n\np(y|θ) = 1√\n2πσ\n\ne−\n1\n\n2σ2 (y−θ)2.\n\nConjugate prior and posterior distributions\n\nConsidered as a function of θ, the likelihood is an exponential of a quadratic form in θ, so\nthe family of conjugate prior densities looks like\n\np(θ) = eAθ\n2+Bθ+C .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n40 2. SINGLE-PARAMETER MODELS\n\nWe parameterize this family as\n\np(θ) ∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n)\n;\n\nthat is, θ ∼ N(µ0, τ\n2\n0 ), with hyperparameters µ0 and τ20 . As usual in this preliminary\n\ndevelopment, we assume that the hyperparameters are known.\nThe conjugate prior density implies that the posterior distribution for θ is the exponential\n\nof a quadratic form and thus normal, but some algebra is required to reveal its specific\nform. In the posterior density, all variables except θ are regarded as constants, giving the\nconditional density,\n\np(θ|y) ∝ exp\n\n(\n−1\n\n2\n\n(\n(y − θ)2\nσ2\n\n+\n(θ − µ0)\n\n2\n\nτ20\n\n))\n.\n\nExpanding the exponents, collecting terms and then completing the square in θ (see Exercise\n2.14(a) for details) gives\n\np(θ|y) ∝ exp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\n, (2.9)\n\nthat is, θ|y ∼ N(µ1, τ\n2\n1 ), where\n\nµ1 =\n\n1\nτ2\n0\nµ0 +\n\n1\nσ2 y\n\n1\nτ2\n0\n+ 1\n\nσ2\n\nand\n1\n\nτ21\n=\n\n1\n\nτ20\n+\n\n1\n\nσ2\n. (2.10)\n\nPrecisions of the prior and posterior distributions. In manipulating normal distributions,\nthe inverse of the variance plays a prominent role and is called the precision. The algebra\nabove demonstrates that for normal data and normal prior distribution (each with known\nprecision), the posterior precision equals the prior precision plus the data precision.\n\nThere are several different ways of interpreting the form of the posterior mean, µ1. In\n(2.10), the posterior mean is expressed as a weighted average of the prior mean and the\nobserved value, y, with weights proportional to the precisions. Alternatively, we can express\nµ1 as the prior mean adjusted toward the observed y,\n\nµ1 = µ0 + (y − µ0)\nτ20\n\nσ2 + τ20\n,\n\nor as the data ‘shrunk’ toward the prior mean,\n\nµ1 = y − (y − µ0)\nσ2\n\nσ2 + τ20\n.\n\nEach formulation represents the posterior mean as a compromise between the prior mean\nand the observed value.\n\nAt the extremes, the posterior mean equals the prior mean or the observed data:\n\nµ1 = µ0 if y = µ0 or τ20 = 0;\n\nµ1 = y if y = µ0 or σ2 = 0.\n\nIf τ20 = 0, the prior distribution is infinitely more precise than the data, and so the posterior\nand prior distributions are identical and concentrated at the value µ0. If σ2 = 0, the data\nare perfectly precise, and the posterior distribution is concentrated at the observed value,\ny. If y = µ0, the prior and data means coincide, and the posterior mean must also fall at\nthis point.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\n\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, ỹ, p(ỹ|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ỹ|y) =\n\n∫\np(ỹ|θ)p(θ|y)dθ\n\n∝\n∫\n\nexp\n\n(\n− 1\n\n2σ2\n(ỹ − θ)2\n\n)\nexp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\ndθ.\n\nThe first line above holds because the distribution of the future observation, ỹ, given θ,\ndoes not depend on the past data, y. We can determine the distribution of ỹ more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (ỹ, θ); hence ỹ and θ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of ỹ is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(ỹ|θ) = θ and var(ỹ|θ) = σ2, along\nwith identities (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|θ, y)|y) = E(θ|y) = µ1,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|θ, y)|y) + var(E(ỹ|θ, y)|y)\n= E(σ2|y) + var(θ|y)\n= σ2 + τ21 .\n\nThus, the posterior predictive distribution of ỹ has mean equal to the posterior mean of θ\nand two components of variance: the predictive variance σ2 from the model and the variance\nτ21 due to posterior uncertainty in θ.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (y1, . . . , yn) is available. Proceeding formally, the posterior density is\n\np(θ|y) ∝ p(θ)p(y|θ)\n\n= p(θ)\n\nn∏\n\ni=1\n\np(yi|θ)\n\n∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n) n∏\n\ni=1\n\nexp\n\n(\n− 1\n\n2σ2\n(yi − θ)2\n\n)\n\n∝ exp\n\n(\n−1\n\n2\n\n(\n1\n\nτ20\n(θ − µ0)\n\n2 +\n1\n\nσ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n))\n\n.\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = 1\n\nn\n\n∑\ni yi; that is, y is a sufficient statistic\n\nin this model. In fact, since y|θ, σ2 ∼ N(θ, σ2/n), the results derived for the single normal\nobservation apply immediately (treating y as the single observation) to give\n\np(θ|y1 . . . , yn) = p(θ|y) = N(θ|µn, τ2n), (2.11)\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, y, p(y|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ily) = / p(Gl0)p(6ly)a6\n\nx few(-daa-0\") an (-y0-mr)an\n\nThe first line above holds because the distribution of the future observation, y, given 6,\ndoes not depend on the past data, y. We can determine the distribution of y more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (g, 8); hence 7 and @ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of y is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(g|@) = 6 and var(g|9) = 07, along\nwith identities (2.7) and (2.8):\n\nE(gly) = E(E(g|@, y)ly) = Ely) = 11,\nand\n\nvar(yly) = E(var(g|@,y)|y) + var(E(g/6, y)|y)\nE(o?|y) + var(4|y)\n\nor +77.\n\nThus, the posterior predictive distribution of y has mean equal to the posterior mean of 0\nand two components of variance: the predictive variance a? from the model and the variance\n7? due to posterior uncertainty in 0.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (yi,---, Yn) is available. Proceeding formally, the posterior density is\n\nPly) x p(O)p(yl@)\n\nR\noO)\n%\nso}\n“~\nwo |\naN\nOw]\nsS\n|\n=\nS\n“—\"\nbo\n+4\nSI —\nil =\nmn\nS\n|\nS\niw)\nNe\nNe\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = + 2, wii that is, 7 is a sufficient statistic\nin this model. In fact, since 9|9,07 ~ N(0,07/n), the results derived for the single normal\nobservation apply immediately (treating 7 as the single observation) to give\n\nP(O\\y1 -- Yn) = PO) = N(O|kins Tr): (2.11)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n42 2. SINGLE-PARAMETER MODELS\n\nwhere\n\nµn =\n\n1\nτ2\n0\nµ0 +\n\nn\nσ2 y\n\n1\nτ2\n0\n+ n\n\nσ2\n\nand\n1\n\nτ2n\n=\n\n1\n\nτ20\n+\n\nn\n\nσ2\n. (2.12)\n\nIncidentally, the same result is obtained by adding information for the data y1, y2, . . . , yn\none point at a time, using the posterior distribution at each step as the prior distribution\nfor the next (see Exercise 2.14(c)).\n\nIn the expressions for the posterior mean and variance, the prior precision, 1/τ20 , and\nthe data precision, n/σ2, play equivalent roles, so if n is large, the posterior distribution\nis largely determined by σ2 and the sample value y. For example, if τ20 = σ2, then the\nprior distribution has the same weight as one extra observation with the value µ0. More\nspecifically, as τ0 →∞ with n fixed, or as n→∞ with τ20 fixed, we have:\n\np(θ|y) ≈ N(θ|y, σ2/n), (2.13)\n\nwhich is, in practice, a good approximation whenever prior beliefs are relatively diffuse over\nthe range of θ where the likelihood is substantial.\n\n2.6 Other standard single-parameter models\n\nRecall that, in general, the posterior density, p(θ|y), has no closed-form expression; the\nnormalizing constant, p(y), is often especially difficult to compute due to the integral (1.3).\nMuch formal Bayesian analysis concentrates on situations where closed forms are available;\nsuch models are sometimes unrealistic, but their analysis often provides a useful starting\npoint when it comes to constructing more realistic models.\n\nThe standard distributions—binomial, normal, Poisson, and exponential—have natural\nderivations from simple probability models. As we have already discussed, the binomial\ndistribution is motivated from counting exchangeable outcomes, and the normal distribu-\ntion applies to a random variable that is the sum of many exchangeable or independent\nterms. We will also have occasion to apply the normal distribution to the logarithm of all-\npositive data, which would naturally apply to observations that are modeled as the product\nof many independent multiplicative factors. The Poisson and exponential distributions arise\nas the number of counts and the waiting times, respectively, for events modeled as occur-\nring exchangeably in all time intervals; that is, independently in time, with a constant rate\nof occurrence. We will generally construct realistic probability models for more compli-\ncated outcomes by combinations of these basic distributions. For example, in Section 22.2,\nwe model the reaction times of schizophrenic patients in a psychological experiment as a\nbinomial mixture of normal distributions on the logarithmic scale.\n\nEach of these standard models has an associated family of conjugate prior distributions,\nwhich we discuss in turn.\n\nNormal distribution with known mean but unknown variance\n\nThe normal model with known mean θ and unknown variance is an important example,\nnot necessarily for its direct applied value, but as a building block for more complicated,\nuseful models, most immediately the normal distribution with unknown mean and variance,\nwhich we cover in Section 3.2. In addition, the normal distribution with known mean but\nunknown variance provides an introductory example of the estimation of a scale parameter.\n\nFor p(y|θ, σ2) = N(y|θ, σ2), with θ known and σ2 unknown, the likelihood for a vector\ny of n independent and identically distributed observations is\n\np(y|σ2) ∝ σ−n exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43\n\n= (σ2)−n/2 exp\n(\n− n\n\n2σ2\nv\n)\n.\n\nThe sufficient statistic is\n\nv =\n1\n\nn\n\nn∑\n\ni=1\n\n(yi − θ)2.\n\nThe corresponding conjugate prior density is the inverse-gamma,\n\np(σ2) ∝ (σ2)−(α+1)e−β/σ\n2\n\n,\n\nwhich has hyperparameters (α, β). A convenient parameterization is as a scaled inverse-χ2\n\ndistribution with scale σ2\n0 and ν0 degrees of freedom (see Appendix A); that is, the prior\n\ndistribution of σ2 is taken to be the distribution of σ2\n0ν0/X , where X is a χ2\n\nν0 random\nvariable. We use the convenient but nonstandard notation, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0).\n\nThe resulting posterior density for σ2 is\n\np(σ2|y) ∝ p(σ2)p(y|σ2)\n\n∝\n(\nσ2\n0\n\nσ2\n\n)ν0/2+1\n\nexp\n\n(\n−ν0σ\n\n2\n0\n\n2σ2\n\n)\n· (σ2)−n/2 exp\n\n(\n−n\n2\n\nv\n\nσ2\n\n)\n\n∝ (σ2)−((n+ν0)/2+1) exp\n\n(\n− 1\n\n2σ2\n(ν0σ\n\n2\n0 + nv)\n\n)\n.\n\nThus,\n\nσ2|y ∼ Inv-χ2\n\n(\nν0 + n,\n\nν0σ\n2\n0 + nv\n\nν0 + n\n\n)\n,\n\nwhich is a scaled inverse-χ2 distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to ν0 observations with average squared deviation σ2\n\n0 .\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate θ, then the probability\ndistribution of a single observation y is\n\np(y|θ) = θye−θ\n\ny!\n, for y = 0, 1, 2, . . . ,\n\nand for a vector y = (y1, . . . , yn) of independent and identically distributed observations,\nthe likelihood is\n\np(y|θ) =\n\nn∏\n\ni=1\n\n1\n\nyi!\nθyie−θ\n\n∝ θt(y)e−nθ,\n\nwhere t(y) =\n∑n\ni=1 yi is the sufficient statistic. We can rewrite the likelihood in exponential\n\nfamily form as\n\np(y|θ) ∝ e−nθet(y) log θ,\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43,\nThe sufficient statistic is\n\nThe corresponding conjugate prior density is the inverse-gamma,\npo?) x (0?) De Ble\",\n\nwhich has hyperparameters (a, 3). A convenient parameterization is as a scaled inverse-y?\ndistribution with scale 2 and vo degrees of freedom (see Appendix A); that is, the prior\ndistribution of 0? is taken to be the distribution of o§vo/X, where X is a x7, random\nvariable. We use the convenient but nonstandard notation, 0? ~ Inv-x?(vo, 08).\n\nThe resulting posterior density for a? is\n\np(o|y) x p(o)p(y|o”)\n2 Vo /2+1 2\n90 Y009 2\\—n/2 nv\n(3) exp ( pa) (a) exp (Fa)\n\n—Un+V 1\nx (a?) (( + 0) /2+1) exp (— spate? + nv) .\n\nThus,\n\nYoo + nu\nYo +n ,\n\no* ly ~ Inv-x? (» +n,\n\nwhich is a scaled inverse-? distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to vp observations with average squared deviation o@.\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate 0, then the probability\ndistribution of a single observation y is\n\nWe?\np(ylO) = Tn for y =0,1,2,...,\n\nand for a vector y = (y1,...,Yn) of independent and identically distributed observations,\nthe likelihood is\n\nnm\n\nLoy\np(y) = [[—e%e~®\n\nja Yt\n\n« Gy e—nF\n\nwhere t(y) = >>\", y; is the sufficient statistic. We can rewrite the likelihood in exponential\nfamily form as\n\np(y|0) x e Mel lee?\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n44 2. SINGLE-PARAMETER MODELS\n\nrevealing that the natural parameter is φ(θ) = log θ, and the natural conjugate prior distri-\nbution is\n\np(θ) ∝ (e−θ)ηeν log θ,\n\nindexed by hyperparameters (η, ν). To put this argument another way, the likelihood is of\nthe form θae−bθ, and so the conjugate prior density must be of the form p(θ) ∝ θAe−Bθ. In\na more conventional parameterization,\n\np(θ) ∝ e−βθθα−1,\n\nwhich is a gamma density with parameters α and β, Gamma(α, β); see Appendix A. Com-\nparing p(y|θ) and p(θ) reveals that the prior density is, in some sense, equivalent to a total\ncount of α− 1 in β prior observations. With this conjugate prior distribution, the posterior\ndistribution is\n\nθ|y ∼ Gamma(α + ny, β + n).\n\nThe negative binomial distribution. With conjugate families, the known form of the prior\nand posterior densities can be used to find the marginal distribution, p(y), using the formula\n\np(y) =\np(y|θ)p(θ)\np(θ|y) .\n\nFor instance, the Poisson model for a single observation, y, has prior predictive distribution\n\np(y) =\nPoisson(y|θ)Gamma(θ|α, β)\nGamma(θ|α+ y, 1 + β)\n\n=\nΓ(α+ y)βα\n\nΓ(α)y!(1 + β)α+y\n,\n\nwhich reduces to\n\np(y) =\n\n(\nα+ y − 1\n\ny\n\n)(\nβ\n\nβ + 1\n\n)α(\n1\n\nβ + 1\n\n)y\n,\n\nwhich is known as the negative binomial density:\n\ny ∼ Neg-bin(α, β).\n\nThe above derivation shows that the negative binomial distribution is a mixture of Poisson\ndistributions with rates, θ, that follow the gamma distribution:\n\nNeg-bin(y|α, β) =\n∫\n\nPoisson(y|θ)Gamma(θ|α, β)dθ.\n\nWe return to the negative binomial distribution in Section 17.2 as a robust alternative to\nthe Poisson distribution.\n\nPoisson model parameterized in terms of rate and exposure\n\nIn many applications, it is convenient to extend the Poisson model for data points y1, . . . , yn\nto the form\n\nyi ∼ Poisson(xiθ), (2.14)\n\nwhere the values xi are known positive values of an explanatory variable, x, and θ is the\nunknown parameter of interest. In epidemiology, the parameter θ is often called the rate,\nand xi is called the exposure of the ith unit. This model is not exchangeable in the yi’s but\nis exchangeable in the pairs (x, y)i. The likelihood for θ in the extended Poisson model is\n\np(y|θ) ∝ θ(\n∑n\n\ni=1\nyi)e−(\n\n∑n\n\ni=1\nxi)θ\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 45\n\n(ignoring factors that do not depend on θ), and so the gamma distribution for θ is conjugate.\nWith prior distribution\n\nθ ∼ Gamma(α, β),\n\nthe resulting posterior distribution is\n\nθ|y ∼ Gamma\n\n(\nα+\n\nn∑\n\ni=1\n\nyi, β +\nn∑\n\ni=1\n\nxi\n\n)\n. (2.15)\n\nEstimating a rate from Poisson data: an idealized example\nSuppose that causes of death are reviewed in detail for a city in the United States for a\nsingle year. It is found that 3 persons, out of a population of 200,000, died of asthma,\ngiving a crude estimated asthma mortality rate in the city of 1.5 cases per 100,000\npersons per year. A Poisson sampling model is often used for epidemiological data of\nthis form. The Poisson model derives from an assumption of exchangeability among\nall small intervals of exposure. Under the Poisson model, the sampling distribution\nof y, the number of deaths in a city of 200,000 in one year, may be expressed as\nPoisson(2.0θ), where θ represents the true underlying long-term asthma mortality rate\nin our city (measured in cases per 100,000 persons per year). In the above notation,\ny = 3 is a single observation with exposure x = 2.0 (since θ is defined in units of\n100,000 people) and unknown rate θ. We can use knowledge about asthma mortality\nrates around the world to construct a prior distribution for θ and then combine the\ndatum y = 3 with that prior distribution to obtain a posterior distribution.\n\nSetting up a prior distribution. What is a sensible prior distribution for θ? Reviews\nof asthma mortality rates around the world suggest that mortality rates above 1.5\nper 100,000 people are rare in Western countries, with typical asthma mortality rates\naround 0.6 per 100,000. Trial-and-error exploration of the properties of the gamma dis-\ntribution, the conjugate prior family for this problem, reveals that a Gamma(3.0, 5.0)\ndensity provides a plausible prior density for the asthma mortality rate in this example\nif we assume exchangeability between this city and other cities and this year and other\nyears. The mean of this prior distribution is 0.6 (with a mode of 0.4), and 97.5% of\nthe mass of the density lies below 1.44. In practice, specifying a prior mean sets the\nratio of the two gamma parameters, and then the shape parameter can be altered by\ntrial and error to match the prior knowledge about the tail of the distribution.\n\nPosterior distribution. The result in (2.15) shows that the posterior distribution\nof θ for a Gamma(α, β) prior distribution is Gamma(α + y, β + x) in this case.\nWith the prior distribution and data described, the posterior distribution for θ is\nGamma(6.0, 7.0), which has mean 0.86—substantial shrinkage has occurred toward\nthe prior distribution. A histogram of 1000 draws from the posterior distribution for\nθ is shown as Figure 2.5a. For example, the posterior probability that the long-term\ndeath rate from asthma in our city is more than 1.0 per 100,000 per year, computed\nfrom the gamma posterior density, is 0.30.\n\nPosterior distribution with additional data. To consider the effect of additional data,\nsuppose that ten years of data are obtained for the city in our example, instead of just\none, and it is found that the mortality rate of 1.5 per 100,000 is maintained; we find\ny = 30 deaths over 10 years. Assuming the population is constant at 200,000, and\nassuming the outcomes in the ten years are independent with constant long-term rate\nθ, the posterior distribution of θ is then Gamma(33.0, 25.0); Figure 2.5b displays 1000\ndraws from this distribution. The posterior distribution is much more concentrated\nthan before, and it still lies between the prior distribution and the data. After ten\nyears of data, the posterior mean of θ is 1.32, and the posterior probability that θ\nexceeds 1.0 is 0.93.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n46 2. SINGLE-PARAMETER MODELS\n\nFigure 2.5 Posterior density for θ, the asthma mortality rate in cases per 100,000 persons per year,\nwith a Gamma(3.0, 5.0) prior distribution: (a) given y = 3 deaths out of 200,000 persons; (b) given\ny = 30 deaths in 10 years for a constant population of 200,000. The histograms appear jagged\nbecause they are constructed from only 1000 random draws from the posterior distribution in each\ncase.\n\nExponential model\n\nThe exponential distribution is commonly used to model ‘waiting times’ and other continu-\nous, positive, real-valued random variables, often measured on a time scale. The sampling\ndistribution of an outcome y, given parameter θ, is\n\np(y|θ) = θ exp(−yθ), for y > 0,\n\nand θ = 1/E(y|θ) is called the ‘rate.’ Mathematically, the exponential is a special case of the\ngamma distribution with the parameters (α, β) = (1, θ). In this case, however, it is being\nused as a sampling distribution for an outcome y, not a prior distribution for a parameter\nθ, as in the Poisson example.\n\nThe exponential distribution has a ‘memoryless’ property that makes it a natural model\nfor survival or lifetime data; the probability that an object survives an additional length of\ntime t is independent of the time elapsed to this point: Pr(y>t+s | y>s, θ) = Pr(y>t | θ) for\nany s, t. The conjugate prior distribution for the exponential parameter θ, as for the Poisson\nmean, is Gamma(θ|α, β) with corresponding posterior distribution Gamma(θ|α+1, β+y).\nThe sampling distribution of n independent exponential observations, y = (y1, . . . , yn), with\nconstant rate θ is\n\np(y|θ) = θn exp(−nyθ), for y ≥ 0,\n\nwhich when viewed as the likelihood of θ, for fixed y, is proportional to a Gamma(n+1, ny)\ndensity. Thus the Gamma(α, β) prior distribution for θ can be viewed as α−1 exponential\nobservations with total waiting time β (see Exercise 2.19).\n\n2.7 Example: informative prior distribution for cancer rates\n\nAt the end of Section 2.4, we considered the effect of the prior distribution on inference\ngiven a fixed quantity of data. Here, in contrast, we consider a large set of inferences, each\nbased on different data but with a common prior distribution. In addition to illustrating\nthe role of the prior distribution, this example introduces hierarchical modeling, to which\nwe return in Chapter 5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 47\n\nFigure 2.6 The counties of the United States with the highest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Why are most of the shaded counties in\nthe middle of the country? See Section 2.7 for discussion.\n\nFigure 2.7 The counties of the United States with the lowest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Surprisingly, the pattern is somewhat\nsimilar to the map of the highest rates, shown in Figure 2.6.\n\nA puzzling pattern in a map\n\nFigure 2.6 shows the counties in the United States with the highest kidney cancer death\nrates during the 1980s.1 The most noticeable pattern in the map is that many of the\ncounties in the Great Plains in the middle of the country, but relatively few counties near\nthe coasts, are shaded.\n\nWhen shown the map, people come up with many theories to explain the dispropor-\ntionate shading in the Great Plains: perhaps the air or the water is polluted, or the people\ntend not to seek medical care so the cancers get detected too late to treat, or perhaps their\ndiet is unhealthy . . . These conjectures may all be true but they are not actually needed\nto explain the patterns in Figure 2.6. To see this, look at Figure 2.7, which plots the 10%\nof counties with the lowest kidney cancer death rates. These are also mostly in the middle\nof the country. So now we need to explain why these areas have the lowest, as well as the\nhighest, rates.\n\n1The rates are age-adjusted and restricted to white males, issues which need not concern us here.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n48 2. SINGLE-PARAMETER MODELS\n\nThe issue is sample size. Consider a county of population 1000. Kidney cancer is a\nrare disease, and, in any ten-year period, a county of 1000 will probably have zero kidney\ncancer deaths, so that it will be tied for the lowest rate in the country and will be shaded\nin Figure 2.7. However, there is a chance the county will have one kidney cancer death\nduring the decade. If so, it will have a rate of 1 per 10,000 per year, which is high enough\nto put it in the top 10% so that it will be shaded in Figure 2.6. The Great Plains has many\nlow-population counties, and so it is overrepresented in both maps. There is no evidence\nfrom these maps that cancer rates are particularly high there.\n\nBayesian inference for the cancer death rates\n\nThe misleading patterns in the maps of raw rates suggest that a model-based approach to\nestimating the true underlying rates might be helpful. In particular, it is natural to estimate\nthe underlying cancer death rate in each county j using the model\n\nyj ∼ Poisson(10njθj), (2.16)\n\nwhere yj is the number of kidney cancer deaths in county j from 1980–1989, nj is the\npopulation of the county, and θj is the underlying rate in units of deaths per person per\nyear. In this notation, the maps in Figures 2.6 and 2.7 are plotting the raw rates,\n\nyj\n10nj\n\n.\n\n(Here we are ignoring the age-standardization, although a generalization of the model to\nallow for this would be possible.)\n\nThis model differs from (2.14) in that θj varies between counties, so that (2.16) is a\nseparate model for each of the counties in the U.S. We use the subscript j (rather than i)\nin (2.16) to emphasize that these are separate parameters, each being estimated from its\nown data. Were we performing inference for just one of the counties, we would simply write\ny ∼ Poisson(10nθ).\n\nTo perform Bayesian inference, we need a prior distribution for the unknown rate θj .\nFor convenience we use a gamma distribution, which is conjugate to the Poisson. As we\nshall discuss later, a gamma distribution with parameters α = 20 and β = 430,000 is a\nreasonable prior distribution for underlying kidney cancer death rates in the counties of\nthe U.S. during this period. This prior distribution has a mean of α\n\nβ = 4.65 × 10−5 and\n\nstandard deviation\n√\nα\nβ = 1.04× 10−5.\n\nThe posterior distribution of θj is then,\n\nθj |yj ∼ Gamma(20 + yj , 430,000+ 10nj),\n\nwhich has mean and variance,\n\nE(θj |yj) =\n20 + yj\n\n430,000 + 10nj\n\nvar(θj |yj) =\n20 + yj\n\n(430,000 + 10nj)2\n.\n\nThe posterior mean can be viewed as a weighted average of the raw rate,\nyj\n\n10nj\n, and the\n\nprior mean, αβ = 4.65× 10−5. (For a similar calculation, see Exercise 2.5.)\n\nRelative importance of the local data and the prior distribution\n\nInference for a small county. The relative weighting of prior information and data depends\non the population size nj . For example, consider a small county with nj = 1000:\n\n• For this county, if yj = 0, then the raw death rate is 0 but the posterior mean is\n20\n\n440,000 = 4.55× 10−5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 49\n\nFigure 2.8 (a) Kidney cancer death rates yj/(10nj) vs. population size nj. (b) Replotted on the\nscale of log10 population to see the data more clearly. The patterns come from the discreteness of\nthe data (nj = 0, 1, 2, . . .).\n\n• If yj = 1, then the raw death rate is 1 per 1000 per 10 years, or 10−4 per person-year\n(about twice as high as the national mean), but the posterior mean is only 21\n\n440,000 =\n\n4.77× 10−5.\n\n• If yj = 2, then the raw death rate is an extremely high 2 × 10−4 per person-year, but\nthe posterior mean is still only 22\n\n440,000 = 5.00× 10−5.\n\nWith such a small population size, the data are dominated by the prior distribution.\n\nBut how likely, a priori, is it that yj will equal 0, 1, 2, and so forth, for this county with\nnj = 1000? This is determined by the predictive distribution, the marginal distribution\nof yj , averaging over the prior distribution of θj . As discussed in Section 2.6, the Poisson\nmodel with gamma prior distribution has a negative binomial predictive distribution:\n\nyj ∼ Neg-bin\n\n(\nα,\n\nβ\n\n10nj\n\n)\n.\n\nIt is perhaps even simpler to simulate directly the predictive distribution of yj as follows:\n(1) draw 500 (say) values of θj from the Gamma(20, 430,000) distribution; (2) for each of\nthese, draw one value yj from the Poisson distribution with parameter 10,000 θj. Of 500\nsimulations of yj produced in this way, 319 were 0’s, 141 were 1’s, 33 were 2’s, and 5 were\n3’s.\n\nInference for a large county. Now consider a large county with nj = 1 million. How\nmany cancer deaths yj might we expect to see in a ten-year period? Again we can use\nthe Gamma(20, 430,000) and Poisson(107 θj) distributions to simulate 500 values yj from\nthe predictive distribution. Doing this we found a median of 473 and a 50% interval of\n[393, 545]. The raw death rate in such a county is then as likely or not to fall between\n3.93× 10−5 and 5.45× 10−5.\n\nWhat about the Bayesianly estimated or ‘Bayes-adjusted’ death rate? For example, if\nyj takes on the low value of 393, then the raw death rate is 3.93× 10−5 and the posterior\nmean of θj is 20+393\n\n107+430,000 = 3.96× 10−5, and if yj = 545, then the raw rate is 5.45× 10−5\n\nand the posterior mean is 5.41 × 10−5. In this large county, the data dominate the prior\ndistribution.\n\nComparing counties of different sizes. In the Poisson model (2.16), the variance of\nyj\n\n10nj\n\nis inversely proportional to the exposure parameter nj , which can thus be considered a\n‘sample size’ for county j. Figure 2.8 shows how the raw kidney cancer death rates vary by\npopulation. The extremely high and extremely low rates are all in low-population counties.\nBy comparison, Figure 2.9a shows that the Bayes-estimated rates are much less variable.\nFinally, Figure 2.9b displays 50% interval estimates for a sample of counties (chosen because\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n50 2. SINGLE-PARAMETER MODELS\n\nFigure 2.9 (a) Bayes-estimated posterior mean kidney cancer death rates, E(θj |yj) = 20+yj\n\n430,000+10nj\n\nvs. logarithm of population size nj , the 3071 counties in the U.S. (b) Posterior medians and 50%\nintervals for θj for a sample of 100 counties j. The scales on the y-axes differ from the plots in\nFigure 2.8b.\n\nit would be hard to display all 3071 in a single plot). The smaller counties supply less\ninformation and thus have wider posterior intervals.\n\nConstructing a prior distribution\n\nWe now step back and discuss where we got the Gamma(20, 430,000) prior distribution for\nthe underlying rates. As we discussed when introducing the model, we picked the gamma\ndistribution for mathematical convenience. We now explain how the two parameters α, β\ncan be estimated from data to match the distribution of the observed cancer death rates\nyj\n\n10nj\n. It might seem inappropriate to use the data to set the prior distribution, but we\n\nview this as a useful approximation to our preferred approach of hierarchical modeling\n(introduced in Chapter 5), in which distributional parameters such as α, β in this example\nare treated as unknowns to be estimated.\n\nUnder the model, the observed count yj for any county j comes from the predictive dis-\n\ntribution, p(yj) =\n∫\np(yj |θj)p(θj)dθj , which in this case is Neg-bin(α, β\n\n10nj\n). From Appendix\n\nA, we can find the mean and variance of this distribution:\n\nE(yj) = 10nj\nα\n\nβ\n\nvar(yj) = 10nj\nα\n\nβ\n+ (10nj)\n\n2 α\n\nβ2\n. (2.17)\n\nThese can also be derived directly using the mean and variance formulas (1.8) and (1.9);\nsee Exercise 2.6.\n\nMatching the observed mean and variance to their expectations and solving for α and β\nyields the parameters of the prior distribution. The actual computation is more complicated\nbecause we must deal with the age adjustment and it also is more efficient to work with the\nmean and variance of the rates\n\nyj\n10nj\n\n:\n\nE\n\n(\nyj\n\n10nj\n\n)\n=\n\nα\n\nβ\n\nvar\n\n(\nyj\n\n10nj\n\n)\n=\n\n1\n\n10nj\n\nα\n\nβ\n+\n\nα\n\nβ2\n. (2.18)\n\nAfter dealing with the age adjustments, we equate the observed and theoretical moments,\nsetting the mean of the values of\n\nyj\n10nj\n\nto α\nβ and setting the variance of the values of\n\nyj\n10nj\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 51\n\nFigure 2.10 Empirical distribution of the age-adjusted kidney cancer death rates,\nyj\n\n10nj\n, for the 3071\n\ncounties in the U.S., along with the Gamma(20, 430,000) prior distribution for the underlying cancer\nrates θj.\n\nto E\n(\n\n1\n10nj\n\n)\nα\nβ + α\n\nβ2 , using the sample average of the values 1\n10nj\n\nin place of E\n(\n\n1\n10nj\n\n)\nin that\n\nlast expression.\nFigure 2.10 shows the empirical distribution of the raw cancer rates, along with the\n\nestimated Gamma(20, 430,000) prior distribution for the underlying cancer rates θj . The\ndistribution of the raw rates is much broader, which makes sense since they include the\nPoisson variability as well as the variation between counties.\n\nOur prior distribution is reasonable in this example, but this method of constructing\nit—by matching moments—is somewhat sloppy and can be difficult to apply in general. In\nChapter 5, we discuss how to estimate this and other prior distributions in a more direct\nBayesian manner, using hierarchical models.\n\nA more important way this model could be improved is by including information at the\ncounty level that could predict variation in the cancer rates. This would move the model\ntoward a hierarchical Poisson regression of the sort discussed in Chapter 16.\n\n2.8 Noninformative prior distributions\n\nWhen prior distributions have no population basis, they can be difficult to construct, and\nthere has long been a desire for prior distributions that can be guaranteed to play a minimal\nrole in the posterior distribution. Such distributions are sometimes called ‘reference prior\ndistributions,’ and the prior density is described as vague, flat, diffuse or noninformative.\nThe rationale for using noninformative prior distributions is often said to be ‘to let the\ndata speak for themselves,’ so that inferences are unaffected by information external to the\ncurrent data.\n\nA related idea is the weakly informative prior distribution, which contains some informa-\ntion—enough to ‘regularize’ the posterior distribution, that is, to keep it roughly within rea-\nsonable bounds—but without attempting to fully capture one’s scientific knowledge about\nthe underlying parameter.\n\nProper and improper prior distributions\n\nWe return to the problem of estimating the mean θ of a normal model with known variance\nσ2, with a N(µ0, τ\n\n2\n0 ) prior distribution on θ. If the prior precision, 1/τ20 , is small relative to\n\nthe data precision, n/σ2, then the posterior distribution is approximately as if τ20 =∞:\n\np(θ|y) ≈ N(θ|y, σ2/n).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(θ) is proportional to a constant for θ ∈ (−∞,∞). Such a distribution is\nnot strictly possible, since the integral of the assumed p(θ) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(θ) proper if it\ndoes not depend on data and integrates to 1. (If p(θ) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-χ2 prior distri-\nbution. If the prior degrees of freedom, ν0, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if ν0 = 0:\n\np(σ2|y) ≈ Inv-χ2(σ2|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for σ2 as p(σ2) ∝ 1/σ2, which is improper, having an infinite integral over the range\n(0,∞).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, θ). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(θ|y) ∝ p(y|θ)p(θ).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,∫\np(θ|y)dθ is finite for all y. Posterior distributions obtained from improper prior distri-\n\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis as approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\nφ = h(θ). By transformation of variables, the prior density p(θ) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on φ:\n\np(φ) = p(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣ = p(θ)|h′(θ)|−1. (2.19)\n\nJeffreys’ general principle is that any rule for determining the prior density p(θ) should\nyield an equivalent result if applied to the transformed parameter; that is, p(φ) computed\nby determining p(θ) and applying (2.19) should match the distribution that is obtained by\ndetermining p(φ) directly using the transformed model, p(y, φ) = p(φ)p(y|φ).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(θ) ∝ [J(θ)]1/2,\nwhere J(θ) is the Fisher information for θ:\n\nJ(θ) = E\n\n((\nd log p(y|θ)\n\ndθ\n\n)2\n∣∣∣∣∣ θ\n)\n\n= −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n. (2.20)\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(9) is proportional to a constant for 6 € (—oo, 00). Such a distribution is\nnot strictly possible, since the integral of the assumed p(@) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(0) proper if it\ndoes not depend on data and integrates to 1. (If p(@) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-y? prior distri-\nbution. If the prior degrees of freedom, vo, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if vp = 0:\n\np(o\"|y) © Inv-x?(o*|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for 0? as p(a?) « 1/07, which is improper, having an infinite integral over the range\n(0, co).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, @). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(y) x p(y|O)p(9).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,\n{p(Oly)d@ is finite for all y. Posterior distributions obtained from improper prior distri-\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis aS approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\n@ = h(@). By transformation of variables, the prior density p(@) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on ¢:\n\ndo\n\ndd| —\nJeffreys’ general principle is that any rule for determining the prior density p(@) should\nyield an equivalent result if applied to the transformed parameter; that is, p(¢@) computed\nby determining p(@) and applying (2.19) should match the distribution that is obtained by\ndetermining p(#) directly using the transformed model, p(y, ¢) = p(¢)p(yl¢).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(6) « [J(6)]\nwhere J(@) is the Fisher information for 0:\n\n_ dlog p(y|9) \\”\nsay = (et\n\nrd) = v(0) p()|h!(a)|—2. (2.19)\n\n1/2\n?\n\ndé?\n\n) =-5 (eeu) 0). (2.20)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\n\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(φ) at θ =\nh−1(φ):\n\nJ(φ) = −E\n(\nd2 log p(y|φ)\n\ndφ2\n\n)\n\n= −E\n(\nd2 log p(y|θ=h−1(φ))\n\ndθ2\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n)\n\n= J(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n\n;\n\nthus, J(φ)1/2 = J(θ)1/2\n∣∣∣ dθdφ\n∣∣∣, as required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter θ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ∼ Bin(n, θ), which has log-likelihood\n\nlog p(y|θ) = constant + y log θ + (n− y) log(1− θ).\n\nRoutine evaluation of the second derivative and substitution of E(y|θ) = nθ yields the\nFisher information:\n\nJ(θ) = −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n\n=\nn\n\nθ(1 − θ) .\n\nJeffreys’ prior density is then p(θ) ∝ θ−1/2(1 − θ)−1/2, which is a Beta(12 ,\n1\n2 ) density. By\n\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\nθ ∼ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(θ)) ∝ constant (see\nExercise 2.7), which corresponds to the improper Beta(0, 0) density on θ. In practice,\nthe difference between these alternatives is often small, since to get from θ ∼ Beta(0, 0)\nto θ ∼ Beta(1, 1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0, 0) prior distribution—if\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y− θ|θ) is a function that is free of θ and y, say,\nf(u), where u = y − θ, then y − θ is a pivotal quantity, and θ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for θ\nwould give f(y−θ) for the posterior distribution, p(y−θ|y). That is, under the posterior\ndistribution, y − θ should still be a pivotal quantity, whose distribution is free of both\nθ and y. Under this condition, using Bayes’ rule, p(y−θ|y) ∝ p(θ)p(y−θ|θ), thereby\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(¢) at 0 =\nh-*(¢):\n@ log p(yl¢)\n= —K a\nH(0) Ger\n_ _p{ Plogpyl@=h\\(6)) | d0 |?\ndo? do\ndo |\n= J(0)|/—| ;\n0)\n\nagp |> 2 required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter @ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nthus, J(¢)!/2 = J(@)!/2 \\\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ~ Bin(n, 6), which has log-likelihood\nlog p(y|@) = constant + y log 6 + (n — y) log(1 — @).\nRoutine evaluation of the second derivative and substitution of E(y|0) = n@ yields the\n\nFisher information: 5 (v0)\nd° log p(y|@ n\n0) = —E | ——~——_] 0 ) = —_~..\n10) = 8 ee) = aaa\n\nJeffreys’ prior density is then p(@) « 6~'/2(1 — 0)~1/?, which is a Beta(4, 4) density. By\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\n@ ~ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(@)) o constant (see\nExercise 2.7), which corresponds to the improper Beta(0,0) density on @. In practice,\nthe difference between these alternatives is often small, since to get from 0 ~ Beta(0, 0)\nto @ ~ Beta(1,1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0,0) prior distribution—if\n\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y—0|@) is a function that is free of 0 and y, say,\nf(u), where u = y — 0, then y — 6 is a pivotal quantity, and @ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for 0\nwould give f(y—6@) for the posterior distribution, p(y—9|y). That is, under the posterior\ndistribution, y — 0 should still be a pivotal quantity, whose distribution is free of both\n6 and y. Under this condition, using Bayes’ rule, p(y—9@ly) « p(@)p(y—6|@), thereby\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n54 2. SINGLE-PARAMETER MODELS\n\nimplying that the noninformative prior density is uniform on θ; that is, p(θ) ∝ constant\nover the range (−∞,∞).\n\n2. If the density of y is such that p(yθ |θ) is a function that is free of θ and y—say, g(u), where\nu = y\n\nθ—then u = y\nθ is a pivotal quantity and θ is called a pure scale parameter. In such\n\na case, it is reasonable that a noninformative prior distribution for θ would give g(yθ )\nfor the posterior distribution, p(yθ |y). By transformation of variables, the conditional\ndistribution of y given θ can be expressed in terms of the distribution of u given θ,\n\np(y|θ) = 1\n\nθ\np(u|θ),\n\nand similarly,\n\np(θ|y) = y\n\nθ2\np(u|y).\n\nAfter letting both p(u|θ) and p(u|y) equal g(u), we have the identity p(θ|y) = y\nθ p(y|θ).\n\nThus, in this case, the reference prior distribution is p(θ) ∝ 1\nθ or, equivalently, p(log θ) ∝ 1\n\nor p(θ2) ∝ 1\nθ2 .\n\nThis approach, in which the sampling distribution of the pivot is used as its posterior\ndistribution, can be applied to sufficient statistics in more complicated examples, such as\nhierarchical normal models.\n\nEven these principles can be misleading in some problems, in the critical sense of suggest-\ning prior distributions that can lead to improper posterior distributions. For example, the\nuniform prior density does not work for the logarithm of a hierarchical variance parameter,\nas we discuss in Section 5.4.\n\nDifficulties with noninformative prior distributions\n\nThe search for noninformative priors has several problems, including:\n\n1. Searching for a prior distribution that is always vague seems misguided: if the likelihood\nis truly dominant in a given problem, then the choice among a range of relatively flat\nprior densities cannot matter. Establishing a particular specification as the reference\nprior distribution seems to encourage its automatic, and possibly inappropriate, use.\n\n2. For many problems, there is no clear choice for a vague prior distribution, since a density\nthat is flat or uniform in one parameterization will not be in another. This is the\nessential difficulty with Laplace’s principle of insufficient reason—on what scale should\nthe principle apply? For example, the ‘reasonable’ prior density on the normal mean θ\nabove is uniform, while for σ2, the density p(σ2) ∝ 1/σ2 seems reasonable. However, if\nwe define φ = log σ2, then the prior density on φ is\n\np(φ) = p(σ2)\n\n∣∣∣∣\ndσ2\n\ndφ\n\n∣∣∣∣ ∝\n1\n\nσ2\nσ2 = 1;\n\nthat is, uniform on φ = log σ2. With discrete distributions, there is the analogous\ndifficulty of deciding how to subdivide outcomes into ‘atoms’ of equal probability.\n\n3. Further difficulties arise when averaging over a set of competing models that have im-\nproper prior distributions, as we discuss in Section 7.3.\n\nNevertheless, noninformative and reference prior densities are often useful when it does\nnot seem to be worth the effort to quantify one’s real prior knowledge as a probability\ndistribution, as long as one is willing to perform the mathematical work to check that\nthe posterior density is proper and to determine the sensitivity of posterior inferences to\nmodeling assumptions of convenience.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.9. WEAKLY INFORMATIVE PRIOR DISTRIBUTIONS 55\n\n2.9 Weakly informative prior distributions\n\nWe characterize a prior distribution as weakly informative if it is proper but is set up so that\nthe information it does provide is intentionally weaker than whatever actual prior knowledge\nis available. We will discuss this further in the context of a specific example, but in general\nany problem has some natural constraints that would allow a weakly informative model.\nFor example, for regression models on the logarithmic or logistic scale, with predictors that\nare binary or scaled to have standard deviation 1, we can be sure for most applications that\neffect sizes will be less than 10, given that a difference of 10 on the log scale changes the\nexpected value by a factor of exp(10) = 20,000, and on the logit scale shifts a probability\nof logit−1(−5) = 0.01 to logit−1(5) = 0.99.\n\nRather than trying to model complete ignorance, we prefer in most problems to use\nweakly informative prior distributions that include a small amount of real-world information,\nenough to ensure that the posterior distribution makes sense. For example, in the sex\nratio example from Sections 2.1 and 2.4, one could use a prior distribution concentrated\nbetween 0.4 and 0.6, for example N(0.5, 0.12) or, to keep the mathematical convenience of\nconjugacy, Beta(20, 20).2 In the general problem of estimating a normal mean from Section\n2.5, a N(0, A2) prior distribution is weakly informative, with A set to some large value that\ndepends on the context of the problem.\n\nIn almost every real problem, the data analyst will have more information than can\nbe conveniently included in the statistical model. This is an issue with the likelihood as\nwell as the prior distribution. In practice, there is always compromise for a number of\nreasons: to describe the model more conveniently; because it may be difficult to express\nknowledge accurately in probabilistic form; to simplify computations; or perhaps to avoid\nusing a possibly unreliable source of information. Except for the last reason, these are all\narguments for convenience and are best justified by the claim that the answer would not\nhave changed much had we been more accurate. If so few data are available that the choice\nof noninformative prior distribution makes a difference, one should put relevant information\ninto the prior distribution, perhaps using a hierarchical model, as we discuss in Chapter 5.\nWe return to the issue of accuracy vs. convenience in likelihoods and prior distributions in\nthe examples of the later chapters.\n\nConstructing a weakly informative prior distribution\n\nOne might argue that virtually all statistical models are weakly informative: a model always\nconveys some information, if only in its choice of inputs and the functional form of how\nthey are combined, but it is not possible or perhaps even desirable to encode all of one’s\nprior beliefs about a subject into a set of probability distributions. With that in mind, we\noffer two principles for setting up weakly informative priors, going at the problem from two\ndifferent directions:\n\n• Start with some version of a noninformative prior distribution and then add enough\ninformation so that inferences are constrained to be reasonable.\n\n• Start with a strong, highly informative prior and broaden it to account for uncertainty\nin one’s prior beliefs and in the applicability of any historically based prior distribution\nto new data.\n\nNeither of these approaches is pure. In the first case, it can happen that the purportedly\nnoninformative prior distribution used as a starting point is in fact too strong. For example,\nif a U(0, 1) prior distribution is assigned to the probability of some rare disease, then in\nthe presence of weak data the probability can be grossly overestimated (suppose y = 0\n\n2A quick R calculation, pbeta(.6,20,20) - pbeta(.4,20,20), reveals that 80% of the probability mass\nin the Beta(20, 20) falls between 0.4 and 0.6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n56 2. SINGLE-PARAMETER MODELS\n\nincidences out of n = 100 cases, and the true prevalence is known to be less than 1 in\n10,000), and an appropriate weakly informative prior will be such that the posterior in this\ncase will be concentrated in that low range. In the second case, a prior distribution that is\nbelieved to be strongly informative may in fact be too weak along some direction. This is\nnot to say that priors should be made more precise whenever posterior inferences are vague;\nin many cases, our best strategy is simply to acknowledge whatever posterior uncertainty\nwe have. But we should not feel constrained by default noninformative models when we\nhave substantive prior knowledge available.\n\nThere are settings, however, when it can be recommended to not use relevant informa-\ntion, even when it could clearly improve posterior inferences. The concern here is often\nexpressed in terms of fairness and encoded mathematically as a symmetry principle, that\nthe prior distribution should not pull inferences in any predetermined direction. For exam-\nple, consider an experimenter studying an effect that she is fairly sure is positive; perhaps\nher prior distribution is N(0.5, 0.5) on some appropriate scale. Such an assumption might\nbe pefectly reasonable given current scientific information but seems potentially risky if it\nis part of the analysis of an experiment designed to test the scientist’s theory. If anything,\none might want a prior distribution that leans against an experimenter’s hypothesis in order\nto require a higher standard of proof.\n\nUltimately, such concerns can and should be subsumed into decision analysis and some\nsort of model of the entire scientific process, trading off the gains of early identification of\nlarge and real effects against the losses entailed in overestimating the magnitudes of effects\nand overreacting to patterns that could be attributed to chance. In the meantime, though,\nwe know that statistical inferences are taken as evidence of effects, and as guides to future\ndecision making, and for this purpose it can make sense to require models to have certain\nconstraints such as symmetry about 0 for the prior distribution of a single treatment effect.\n\n2.10 Bibliographic note\n\nA fascinating detailed account of the early development of the idea of ‘inverse probability’\n(Bayesian inference) is provided in the book by Stigler (1986), on which our brief accounts\nof Bayes’ and Laplace’s solutions to the problem of estimating an unknown proportion are\nbased. Bayes’ famous 1763 essay in the Philosophical Transactions of the Royal Society of\nLondon has been reprinted as Bayes (1763); see also Laplace (1785, 1810).\n\nIntroductory textbooks providing complementary discussions of the simple models cov-\nered in this chapter were listed at the end of Chapter 1. In particular, Box and Tiao (1973)\nprovide a detailed treatment of Bayesian analysis with the normal model and also discuss\nhighest posterior density regions in some detail. The theory of conjugate prior distributions\nwas developed in detail by Raiffa and Schlaifer (1961). An interesting account of inference\nfor prediction, which also includes extensive details of particular probability models and\nconjugate prior analyses, appears in Aitchison and Dunsmore (1975).\n\nLiu et al. (2013) discuss how to efficiently compute highest posterior density intervals\nusing simulations.\n\nNoninformative and reference prior distributions have been studied by many researchers.\nJeffreys (1961) and Hartigan (1964) discuss invariance principles for noninformative prior\ndistributions. Chapter 1 of Box and Tiao (1973) presents a straightforward and practically\noriented discussion, a brief but detailed survey is given by Berger (1985), and the article by\nBernardo (1979) is accompanied by a wide-ranging discussion. Bernardo and Smith (1994)\ngive an extensive treatment of this topic along with many other matters relevant to the\nconstruction of prior distributions. Barnard (1985) discusses the relation between pivotal\nquantities and noninformative Bayesian inference. Kass and Wasserman (1996) provide a\nreview of many approaches for establishing noninformative prior densities based on Jeffreys’\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 57\n\nrule, and they also discuss the problems that may arise from uncritical use of purportedly\nnoninformative prior specifications. Dawid, Stone, and Zidek (1973) discuss some difficulties\nthat can arise with noninformative prior distributions; also see Jaynes (1980).\n\nKerman (2011) discusses noninformative and informative conjugate prior distributions\nfor the binomial and Poisson models.\n\nJaynes (1983) discusses in several places the idea of objectively constructing prior dis-\ntributions based on invariance principles and maximum entropy. Appendix A of Bretthorst\n(1988) outlines an objective Bayesian approach to assigning prior distributions, as applied\nto the problem of estimating the parameters of a sinusoid from time series data. More\ndiscussions of maximum entropy models appear in Jaynes (1982), Skilling (1989), and Gull\n(1989a); see Titterington (1984) and Donoho et al. (1992) for other views.\n\nFor more on weakly informative prior distributions, see Gelman (2006a) and Gelman,\nJakulin, et al. (2008). Gelman (2004b) discusses connections between parameterization and\nBayesian modeling. Greenland (2001) discusses informative prior distributions in epidemi-\nology.\n\nThe data for the placenta previa example come from a study from 1922 reported in\nJames (1987). For more on the challenges of estimating sex ratios from small samples,\nsee Gelman and Weakliem (2009). The Bayesian analysis of age-adjusted kidney cancer\ndeath rates in Section 2.7 is adapted from Manton et al. (1989); see also Gelman and Nolan\n(2002a) for more on this particular example and Bernardinelli, Clayton, and Montomoli\n(1995) for a general discussion of prior distributions for disease mapping. Gelman and\nPrice (1999) discuss artifacts in maps of parameter estimates, and Louis (1984), Shen and\nLouis (1998), and Louis and Shen (1999) analyze the general problem of estimation of\nensembles of parameters, a topic to which we return in Chapter 5.\n\n2.11 Exercises\n\n1. Posterior inference: suppose you have a Beta(4, 4) prior distribution on the probability θ\nthat a coin will yield a ‘head’ when spun in a specified manner. The coin is independently\nspun ten times, and ‘heads’ appear fewer than 3 times. You are not told how many heads\nwere seen, only that the number is less than 3. Calculate your exact posterior density\n(up to a proportionality constant) for θ and sketch it.\n\n2. Predictive distributions: consider two coins, C1 and C2, with the following characteristics:\nPr(heads|C1) = 0.6 and Pr(heads|C2) = 0.4. Choose one of the coins at random and\nimagine spinning it repeatedly. Given that the first two spins from the chosen coin are\ntails, what is the expectation of the number of additional spins until a head shows up?\n\n3. Predictive distributions: let y be the number of 6’s in 1000 rolls of a fair die.\n\n(a) Sketch the approximate distribution of y, based on the normal approximation.\n\n(b) Using the normal distribution table, give approximate 5%, 25%, 50%, 75%, and 95%\npoints for the distribution of y.\n\n4. Predictive distributions: let y be the number of 6’s in 1000 independent rolls of a par-\nticular real die, which may be unfair. Let θ be the probability that the die lands on ‘6.’\nSuppose your prior distribution for θ is as follows:\n\nPr(θ = 1/12) = 0.25,\n\nPr(θ = 1/6) = 0.5,\n\nPr(θ = 1/4) = 0.25.\n\n(a) Using the normal approximation for the conditional distributions, p(y|θ), sketch your\napproximate prior predictive distribution for y.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n58 2. SINGLE-PARAMETER MODELS\n\n(b) Give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution of y. (Be\ncareful here: y does not have a normal distribution, but you can still use the normal\ndistribution as part of your analysis.)\n\n5. Posterior distribution as a compromise between prior information and data: let y be the\nnumber of heads in n spins of a coin, whose probability of heads is θ.\n\n(a) If your prior distribution for θ is uniform on the range [0, 1], derive your prior predictive\ndistribution for y,\n\nPr(y = k) =\n\n∫ 1\n\n0\n\nPr(y = k|θ)dθ,\n\nfor each k = 0, 1, . . . , n.\n\n(b) Suppose you assign a Beta(α, β) prior distribution for θ, and then you observe y heads\nout of n spins. Show algebraically that your posterior mean of θ always lies between\nyour prior mean, α\n\nα+β , and the observed relative frequency of heads, yn .\n\n(c) Show that, if the prior distribution on θ is uniform, the posterior variance of θ is\nalways less than the prior variance.\n\n(d) Give an example of a Beta(α, β) prior distribution and data y, n, in which the posterior\nvariance of θ is higher than the prior variance.\n\n6. Predictive distributions: Derive the mean and variance (2.17) of the negative binomial\npredictive distribution for the cancer rate example, using the mean and variance formulas\n(1.8) and (1.9).\n\n7. Noninformative prior densities:\n\n(a) For the binomial likelihood, y ∼ Bin(n, θ), show that p(θ) ∝ θ−1(1 − θ)−1 is the\nuniform prior distribution for the natural parameter of the exponential family.\n\n(b) Show that if y = 0 or n, the resulting posterior distribution is improper.\n\n8. Normal distribution with unknown mean: a random sample of n students is drawn\nfrom a large population, and their weights are measured. The average weight of the n\nsampled students is y = 150 pounds. Assume the weights in the population are normally\ndistributed with unknown mean θ and known standard deviation 20 pounds. Suppose\nyour prior distribution for θ is normal with mean 180 and standard deviation 40.\n\n(a) Give your posterior distribution for θ. (Your answer will be a function of n.)\n\n(b) A new student is sampled at random from the same population and has a weight of\nỹ pounds. Give a posterior predictive distribution for ỹ. (Your answer will still be a\nfunction of n.)\n\n(c) For n = 10, give a 95% posterior interval for θ and a 95% posterior predictive interval\nfor ỹ.\n\n(d) Do the same for n = 100.\n\n9. Setting parameters for a beta prior distribution: suppose your prior distribution for θ,\nthe proportion of Californians who support the death penalty, is beta with mean 0.6 and\nstandard deviation 0.3.\n\n(a) Determine the parameters α and β of your prior distribution. Sketch the prior density\nfunction.\n\n(b) A random sample of 1000 Californians is taken, and 65% support the death penalty.\nWhat are your posterior mean and variance for θ? Draw the posterior density function.\n\n(c) Examine the sensitivity of the posterior distribution to different prior means and\nwidths including a non-informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 59\n\nYear Fatal Passenger Death\naccidents deaths rate\n\n1976 24 734 0.19\n1977 25 516 0.12\n1978 31 754 0.15\n1979 31 877 0.16\n1980 22 814 0.14\n1981 21 362 0.06\n1982 26 764 0.13\n1983 20 809 0.13\n1984 16 223 0.03\n1985 22 1066 0.15\n\nTable 2.2 Worldwide airline fatalities, 1976–1985. Death rate is passenger deaths per 100 million\npassenger miles. Source: Statistical Abstract of the United States.\n\n10. Discrete sample spaces: suppose there are N cable cars in San Francisco, numbered\nsequentially from 1 to N . You see a cable car at random; it is numbered 203. You wish\nto estimate N . (See Goodman, 1952, for a discussion and references to several versions of\nthis problem, and Jeffreys, 1961, Lee, 1989, and Jaynes, 2003, for Bayesian treatments.)\n\n(a) Assume your prior distribution on N is geometric with mean 100; that is,\n\np(N) = (1/100)(99/100)N−1, for N = 1, 2, . . . .\n\nWhat is your posterior distribution for N?\n\n(b) What are the posterior mean and standard deviation of N? (Sum the infinite series\nanalytically or approximate them on the computer.)\n\n(c) Choose a reasonable ‘noninformative’ prior distribution for N and give the resulting\nposterior distribution, mean, and standard deviation for N .\n\n11. Computing with a nonconjugate single-parameter model: suppose y1, . . . , y5 are inde-\npendent samples from a Cauchy distribution with unknown center θ and known scale 1:\np(yi|θ) ∝ 1/(1 + (yi − θ)2). Assume, for simplicity, that the prior distribution for θ is\nuniform on [0, 100]. Given the observations (y1, . . . , y5) = (43, 44, 45, 46.5, 47.5):\n\n(a) Compute the unnormalized posterior density function, p(θ)p(y|θ), on a grid of points\nθ = 0, 1\n\nm ,\n2\nm , . . . , 100, for some large integerm. Using the grid approximation, compute\n\nand plot the normalized posterior density function, p(θ|y), as a function of θ.\n\n(b) Sample 1000 draws of θ from the posterior density and plot a histogram of the draws.\n\n(c) Use the 1000 samples of θ to obtain 1000 samples from the predictive distribution of\na future observation, y6, and plot a histogram of the predictive draws.\n\n12. Jeffreys’ prior distributions: suppose y|θ ∼ Poisson(θ). Find Jeffreys’ prior density for θ,\nand then find α and β for which the Gamma(α, β) density is a close match to Jeffreys’\ndensity.\n\n13. Discrete data: Table 2.2 gives the number of fatal accidents and deaths on scheduled\nairline flights per year over a ten-year period. We use these data as a numerical example\nfor fitting discrete data models.\n\n(a) Assume that the numbers of fatal accidents in each year are independent with a\nPoisson(θ) distribution. Set a prior distribution for θ and determine the posterior\ndistribution based on the data from 1976 through 1985. Under this model, give a 95%\npredictive interval for the number of fatal accidents in 1986. You can use the normal\napproximation to the gamma and Poisson or compute using simulation.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n60 2. SINGLE-PARAMETER MODELS\n\n(b) Assume that the numbers of fatal accidents in each year follow independent Poisson\ndistributions with a constant rate and an exposure in each year proportional to the\nnumber of passenger miles flown. Set a prior distribution for θ and determine the\nposterior distribution based on the data for 1976–1985. (Estimate the number of\npassenger miles flown in each year by dividing the appropriate columns of Table 2.2\nand ignoring round-off errors.) Give a 95% predictive interval for the number of fatal\naccidents in 1986 under the assumption that 8 × 1011 passenger miles are flown that\nyear.\n\n(c) Repeat (a) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(d) Repeat (b) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(e) In which of the cases (a)–(d) above does the Poisson model seem more or less rea-\nsonable? Why? Discuss based on general principles, without specific reference to the\nnumbers in Table 2.2.\n\nIncidentally, in 1986, there were 22 fatal accidents, 546 passenger deaths, and a death\nrate of 0.06 per 100 million miles flown. We return to this example in Exercises 3.12,\n6.2, 6.3, and 8.14.\n\n14. Algebra of the normal model:\n\n(a) Fill in the steps to derive (2.9)–(2.10), and (2.11)–(2.12).\n\n(b) Derive (2.11) and (2.12) by starting with a N(µ0, τ\n2\n0 ) prior distribution and adding\n\ndata points one at a time, using the posterior distribution at each step as the prior\ndistribution for the next.\n\n15. Beta distribution: assume the result, from standard advanced calculus, that\n\n∫ 1\n\n0\n\nuα−1(1− u)β−1du =\nΓ(α)Γ(β)\n\nΓ(α + β)\n.\n\nIf Z has a beta distribution with parameters α and β, find E[Zm(1− Z)n] for any non-\nnegative integers m and n. Hence derive the mean and variance of Z.\n\n16. Beta-binomial distribution and Bayes’ prior distribution: suppose y has a binomial dis-\ntribution for given n and unknown parameter θ, where the prior distribution of θ is\nBeta(α, β).\n\n(a) Find p(y), the marginal distribution of y, for y = 0, . . . , n (unconditional on θ). This\ndiscrete distribution is known as the beta-binomial, for obvious reasons.\n\n(b) Show that if the beta-binomial probability is constant in y, then the prior distribution\nhas to have α = β = 1.\n\n17. Posterior intervals: unlike the central posterior interval, the highest posterior interval\nis not invariant to transformation. For example, suppose that, given σ2, the quantity\nnv/σ2 is distributed as χ2\n\nn, and that σ has the (improper) noninformative prior density\np(σ) ∝ σ−1, σ > 0.\n\n(a) Prove that the corresponding prior density for σ2 is p(σ2) ∝ σ−2.\n\n(b) Show that the 95% highest posterior density region for σ2 is not the same as the region\nobtained by squaring the endpoints of a posterior interval for σ.\n\n18. Poisson model: derive the gamma posterior distribution (2.15) for the Poisson model\nparameterized in terms of rate and exposure with conjugate prior distribution.\n\n19. Exponential model with conjugate prior distribution:\n\n(a) Show that if y|θ is exponentially distributed with rate θ, then the gamma prior dis-\ntribution is conjugate for inferences about θ given an independent and identically\ndistributed sample of y values.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 61\n\n(b) Show that the equivalent prior specification for the mean, φ = 1/θ, is inverse-gamma.\n(That is, derive the latter density function.)\n\n(c) The length of life of a light bulb manufactured by a certain process has an exponential\ndistribution with unknown rate θ. Suppose the prior distribution for θ is a gamma\ndistribution with coefficient of variation 0.5. (The coefficient of variation is defined\nas the standard deviation divided by the mean.) A random sample of light bulbs is\nto be tested and the lifetime of each obtained. If the coefficient of variation of the\ndistribution of θ is to be reduced to 0.1, how many light bulbs need to be tested?\n\n(d) In part (c), if the coefficient of variation refers to φ instead of θ, how would your\nanswer be changed?\n\n20. Censored and uncensored data in the exponential model:\n\n(a) Suppose y|θ is exponentially distributed with rate θ, and the marginal (prior) distri-\nbution of θ is Gamma(α, β). Suppose we observe that y ≥ 100, but do not observe\nthe exact value of y. What is the posterior distribution, p(θ|y≥100), as a function of\nα and β? Write down the posterior mean and variance of θ.\n\n(b) In the above problem, suppose that we are now told that y is exactly 100. Now what\nare the posterior mean and variance of θ?\n\n(c) Explain why the posterior variance of θ is higher in part (b) even though more in-\nformation has been observed. Why does this not contradict identity (2.8) on page\n32?\n\n21. Simple hierarchical modeling:\nThe file pew research center june elect wknd data.dta3 has data from Pew Research\nCenter polls taken during the 2008 election campaign. You can read these data into R\nusing the read.dta() function (after first loading the foreign package into R).\nYour task is to estimate the percentage of the (adult) population in each state (excluding\nAlaska, Hawaii, and the District of Columbia) who label themselves as ‘very liberal,’\nfollowing the general procedure that was used in Section 2.7 to estimate cancer rates,\nbut using the binomial and beta rather than Poisson and gamma distributions. But you\ndo not need to make maps; it will be enough to make scatterplots, plotting the estimate\nvs. Barack Obama’s vote share in 2008 (data available at 2008ElectionResult.csv,\nreadable in R using read.csv()).\nMake the following four graphs on a single page:\n\n• Graph proportion very liberal among the survey respondents in each state vs. Obama\nvote share—that is, a scatterplot using the two-letter state abbreviations (see state.abb()\nin R).\n\n• Graph the Bayes posterior mean in each state vs. Obama vote share.\n\n• Repeat graphs (a) and (b) using the number of respondents in the state on the x-axis.\n\nThis exercise has four challenges: first, manipulating the data in order to get the totals\nby state; second, estimating the parameters of the prior distribution; third, doing the\nBayesian analysis by state; and fourth, making the graphs.\n\n22. Prior distributions:\nA (hypothetical) study is performed to estimate the effect of a simple training program\non basketball free-throw shooting. A random sample of 100 college students is recruited\ninto the study. Each student first shoots 100 free-throws to establish a baseline success\nprobability. Each student then takes 50 practice shots each day for a month. At the end\nof that time, he or she takes 100 shots for a final measurement. Let θ be the average\nimprovement in success probability.\nGive three prior distributions for θ (explaining each in a sentence):\n\n3For data for this and other exercises, go to http://www.stat.columbia.edu/∼gelman/book/.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n62 2. SINGLE-PARAMETER MODELS\n\n(a) A noninformative prior,\n\n(b) A subjective prior based on your best knowledge, and\n\n(c) A weakly informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 3\n\nIntroduction to multiparameter models\n\nVirtually every practical problem in statistics involves more than one unknown or unob-\nservable quantity. It is in dealing with such problems that the simple conceptual framework\nof the Bayesian approach reveals its principal advantages over other methods of inference.\nAlthough a problem can include several parameters of interest, conclusions will often be\ndrawn about one, or only a few, parameters at a time. In this case, the ultimate aim of a\nBayesian analysis is to obtain the marginal posterior distribution of the particular param-\neters of interest. In principle, the route to achieving this aim is clear: we first require the\njoint posterior distribution of all unknowns, and then we integrate this distribution over the\nunknowns that are not of immediate interest to obtain the desired marginal distribution.\nOr equivalently, using simulation, we draw samples from the joint posterior distribution\nand then look at the parameters of interest and ignore the values of the other unknowns.\nIn many problems there is no interest in making inferences about many of the unknown\nparameters, although they are required in order to construct a realistic model. Parameters\nof this kind are often called nuisance parameters. A classic example is the scale of the\nrandom errors in a measurement problem.\n\nWe begin this chapter with a general treatment of nuisance parameters and then cover\nthe normal distribution with unknown mean and variance in Section 3.2. Sections 3.4\nand 3.5 present inference for the multinomial and multivariate normal distributions—the\nsimplest models for discrete and continuous multivariate data, respectively. The chapter\nconcludes with an analysis of a nonconjugate logistic regression model, using numerical\ncomputation of the posterior density on a grid.\n\n3.1 Averaging over ‘nuisance parameters’\n\nTo express the ideas of joint and marginal posterior distributions mathematically, suppose\nθ has two parts, each of which can be a vector, θ = (θ1, θ2), and further suppose that we\nare only interested (at least for the moment) in inference for θ1, so θ2 may be considered a\n‘nuisance’ parameter. For instance, in the simple example,\n\ny|µ, σ2 ∼ N(µ, σ2),\n\nin which both µ (=‘θ1’) and σ\n2 (=‘θ2’) are unknown, interest commonly centers on µ.\n\nWe seek the conditional distribution of the parameter of interest given the observed\ndata; in this case, p(θ1|y). This is derived from the joint posterior density,\n\np(θ1, θ2|y) ∝ p(y|θ1, θ2)p(θ1, θ2),\n\nby averaging over θ2:\n\np(θ1|y) =\n∫\np(θ1, θ2|y)dθ2.\n\n63\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(θ1|y) =\n∫\np(θ1|θ2, y)p(θ2|y)dθ2, (3.1)\n\nwhich shows that the posterior distribution of interest, p(θ1|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, θ2, where p(θ2|y) is a weighting\nfunction for the different possible values of θ2. The weights depend on the posterior density\nof θ2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters θ2 can be interpreted generally; for example, θ2 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing θ2 from\nits marginal posterior distribution and then θ1 from its conditional posterior distribution,\ngiven the drawn value of θ2. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(µ, σ2);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for µ and σ, assuming prior\nindependence of location and scale parameters, is uniform on (µ, log σ) or, equivalently,\n\np(µ, σ2) ∝ (σ2)−1.\n\nThe joint posterior distribution, p(µ, σ2|y)\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/σ2:\n\np(µ, σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − µ)2\n)\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\n[\nn∑\n\ni=1\n\n(yi − y)2 + n(y − µ)2\n])\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\n, (3.2)\n\nwhere\n\ns2 =\n1\n\nn− 1\n\nn∑\n\ni=1\n\n(yi − y)2\n\nis the sample variance of the yi’s. The sufficient statistics are y and s2.\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(Orly) = / p(61|62, y)p(O2|y) dbo, (3.1)\n\nwhich shows that the posterior distribution of interest, p(@i|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, 02, where p(02|y) is a weighting\nfunction for the different possible values of 62. The weights depend on the posterior density\nof #2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters 62 can be interpreted generally; for example, 62 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing 62 from\nits marginal posterior distribution and then 0; from its conditional posterior distribution,\ngiven the drawn value of 02. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(, 07);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for w and o, assuming prior\nindependence of location and scale parameters, is uniform on (y,logo) or, equivalently,\n\np(u,o7) x (a7) *.\n\nThe joint posterior distribution, p(u,07|y)\n\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/c?:\n\nan 1<\nP(u,o*|y) oo\" * exp (-2: Sen -n]\n\nII\nq\n3\ni)\n)\ntal\nue)\n—“~\nto\nPai\nbo\nS\n3\n—\n=\n|\nSl\niw)\n+\n=\nRad]\n|\n=\ni)\n|\nn___”\n\n(3.2)\n\nII\nQ\n3\nbo\nco\ntal\nue)\na ™~\n|\n| H\n=\n3\n|\n—\n“—\nD\niw)\n+\n3\n—\nKa]\n|\n=\n“—\n7\nQe\n\nwhere\n\nis the sample variance of the y;’s. The sufficient statistics are 7 and s?.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(µ|σ2, y), and then the marginal posterior density, p(σ2|y). To determine\nthe posterior distribution of µ, given σ2, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nµ|σ2, y ∼ N(y, σ2/n). (3.3)\n\nThe marginal posterior distribution, p(σ2|y)\n\nTo determine p(σ2|y), we must average the joint distribution (3.2) over µ:\n\np(σ2|y) ∝\n∫\nσ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\ndµ.\n\nIntegrating this expression over µ requires evaluating the integral exp\n(\n− 1\n\n2σ2n(y − µ)2\n)\n,\n\nwhich is a simple normal integral; thus,\n\np(σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n(n−1)s2\n\n)√\n2πσ2/n\n\n∝ (σ2)−(n+1)/2 exp\n\n(\n− (n− 1)s2\n\n2σ2\n\n)\n, (3.4)\n\nwhich is a scaled inverse-χ2 density:\n\nσ2|y ∼ Inv-χ2(n− 1, s2). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(µ, σ2|y) = p(µ|σ2, y)p(σ2|y).\n\nThis marginal posterior distribution for σ2 has a remarkable similarity to the analogous\nsampling theory result: conditional on σ2 (and µ), the distribution of the appropriately\n\nscaled sufficient statistic, (n−1)s2\n\nσ2 , is χ2\nn−1. Considering our derivation of the reference prior\n\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw σ2 from (3.5),\nthen draw µ from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of µ\n\nThe population mean, µ, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of µ, which can be obtained by\nintegrating σ2 out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of µ can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-χ2 distribution for the variance, σ2. We can derive the marginal\nposterior density for µ by integrating the joint posterior density over σ2:\n\np(µ|y) =\n∫ ∞\n\n0\n\np(µ, σ2|y)dσ2.\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\nThe conditional posterior distribution, p(u|o?, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(u|o”, y), and then the marginal posterior density, p(a?|y). To determine\nthe posterior distribution of 4, given 07, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nulo?,y ~ NG,o7/n). (3.3)\n\nThe marginal posterior distribution, p(o?|y)\n\nTo determine p(o?|y), we must average the joint distribution (3.2) over p:\n2 —n-2 1 2 = 2\nploly) x fo\" exp | —sal(n—1s° +n — 4)\"] } du.\n\nIntegrating this expression over fz requires evaluating the integral exp (-sin(y — L)’),\nwhich is a simple normal integral; thus,\n\npoly) oo\" Pexp (—o5(n—1)s*) VBra%]n\n\n20?\n\nx (0?) TY? exp (-“S*) ; (3.4)\n\nwhich is a scaled inverse-y? density:\no*|y ~ Inv-x?(n — 1, s”). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(u,0?|y) = p(ulo?, y)p(o?|y).\n\nThis marginal posterior distribution for ¢? has a remarkable similarity to the analogous\nsampling theory result: conditional on o? (and jy), the distribution of the appropriately\nscaled sufficient statistic, (nas , is x2_,. Considering our derivation of the reference prior\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw o? from (3.5),\nthen draw py from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of ju\n\nThe population mean, ju, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of yu, which can be obtained by\nintegrating 0? out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of 4: can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-x? distribution for the variance, 07. We can derive the marginal\n\nposterior density for 4 by integrating the joint posterior density over o?:\n\nP(Hly) = [ p(b, 07 |y)do”.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\n\nz =\nA\n\n2σ2\n, where A = (n− 1)s2 + n(µ− y)2,\n\nand recognizing that the result is an unnormalized gamma integral:\n\np(µ|y) ∝ A−n/2\n∫ ∞\n\n0\n\nz(n−2)/2 exp(−z)dz\n\n∝ [(n− 1)s2 + n(µ− y)2]−n/2\n\n∝\n[\n1 +\n\nn(µ− y)2\n(n− 1)s2\n\n]−n/2\n.\n\nThis is the tn−1(y, s\n2/n) density (see Appendix A).\n\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (µ, log σ), the posterior distribution of µ has the form\n\nµ− y\ns/\n√\nn\n\n∣∣∣∣ y ∼ tn−1,\n\nwhere tn−1 denotes the standard t density (location 0, scale 1) with n−1 degrees of freedom.\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|µ, σ2), the following relation holds:\n\ny − µ\ns/\n√\nn\n\n∣∣∣∣µ, σ2 ∼ tn−1.\n\nThe sampling distribution of the pivotal quantity (y − µ)/(s/√n) does not depend on the\nnuisance parameter σ2, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, ỹ, can be written as a mixture,\np(ỹ|y) =\n\n∫∫\np(ỹ|µ, σ2, y)p(µ, σ2|y)dµdσ2. The first of the two factors in the integral is just\n\nthe normal distribution for the future observation given the values of (µ, σ2), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw µ, σ2 from\ntheir joint posterior distribution and then simulate ỹ ∼ N(µ, σ2).\n\nIn fact, the posterior predictive distribution of ỹ is a t distribution with location y,\nscale (1 + 1\n\nn )\n1/2s, and n − 1 degrees of freedom. This analytic form is obtained using the\n\nsame techniques as in the derivation of the posterior distribution of µ. Specifically, the\ndistribution can be obtained by integrating out the parameters µ, σ2 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(ỹ|σ2, y) =\n\n∫\np(ỹ|µ, σ2, y)p(µ|σ2, y)dµ leads to p(ỹ|σ2, y) = N(ỹ|y, (1 + 1\n\nn )σ\n2),\n\nwhich is the same, up to a changed scale factor, as the distribution of µ|σ2, y.\n\nExample. Estimating the speed of light\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\nz= AL where A = (n —1)s? + n(pp— 9)?\n202 >) ?\n\nand recognizing that the result is an unnormalized gamma integral:\n\nply) 2 Av? [0-2 exp(—a)de\n10)\nx [(n—1)s? +n(u— 92?\n\n_\\94—n/2\nnu 9)?”\n«x |1+———> .\n(n — 1)s?\nThis is the t,_1(Y, s?/n) density (see Appendix A).\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (js, log a), the posterior distribution of y has the form\n\nMay\nwhere t,,_1 denotes the standard ¢ density (location 0, scale 1) with n—1 degrees of freedom.\n\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|u, 07), the following relation holds:\n\nyYy~ tn—-1;\n\nYH\n\ns/vn\\ i?\n\nThe sampling distribution of the pivotal quantity (y — )/(s/./n) does not depend on the\nnuisance parameter o7, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\n2 ty.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, y, can be written as a mixture,\nply) = f{vG\\u, 0? y)p(u, 07 |y)dudo?. The first of the two factors in the integral is just\nthe normal distribution for the future observation given the values of (1,07), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw 4,0? from\ntheir joint posterior distribution and then simulate 7 ~ N(j, 07).\n\nIn fact, the posterior predictive distribution of y is a t distribution with location ¥,\nscale (1 + 4)1/ 2s, and n — 1 degrees of freedom. This analytic form is obtained using the\nsame techniques as in the derivation of the posterior distribution of jz. Specifically, the\ndistribution can be obtained by integrating out the parameters j1,07 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(glo”, y) = [p(glu,07, y)p(ulo?, yd leads to p(glo?, y) = N(gly, (1+ 4)o”),\nwhich is the same, up to a changed scale factor, as the distribution of pu\\o?, y.\n\nExample. Estimating the speed of light\n\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\n\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\n\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\n\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.3. NORMAL DATA WITH A CONJUGATE PRIOR DISTRIBUTION 67\n\nFigure 3.1 Histogram of Simon Newcomb’s measurements for estimating the speed of light, from\nStigler (1977). The data are recorded as deviations from 24,800 nanoseconds.\n\nµ and variance σ2. The main substantive goal is posterior inference for µ. The outlying\nmeasurements do not fit the normal model; we discuss Bayesian methods for measur-\ning the lack of fit for these data in Section 6.3. The mean of the 66 measurements is\ny = 26.2, and the sample standard deviation is s = 10.8. Assuming the noninformative\nprior distribution p(µ, σ2) ∝ (σ2)−1, a 95% central posterior interval for µ is obtained\nfrom the t65 marginal posterior distribution of µ as y ± 1.997s/\n\n√\n66 = [23.6, 28.8].\n\nThe posterior interval can also be obtained by simulation. Following the factorization\nof the posterior distribution given by (3.5) and (3.3), we first draw a random value of\nσ2 ∼ Inv-χ2(65, s2) as 65s2 divided by a random draw from the χ2\n\n65 distribution (see\nAppendix A). Then given this value of σ2, we draw µ from its conditional posterior\ndistribution, N(26.2, σ2/66). Based on 1000 simulated values of (µ, σ2), we estimate\nthe posterior median of µ to be 26.2 and a 95% central posterior interval for µ to be\n[23.6, 28.9], close to the analytically calculated interval.\nIncidentally, based on the currently accepted value of the speed of light, the ‘true\nvalue’ for µ in Newcomb’s experiment is 33.0, which falls outside our 95% interval.\nThis reinforces the fact that posterior inferences are only as good as the model and\nthe experiment that produced the data.\n\n3.3 Normal data with a conjugate prior distribution\n\nA family of conjugate prior distributions\n\nA first step toward a more general model is to assume a conjugate prior distribution for\nthe two-parameter univariate normal sampling model in place of the noninformative prior\ndistribution just considered. The form of the likelihood displayed in (3.2) and the subse-\nquent discussion shows that the conjugate prior density must also have the product form\np(σ2)p(µ|σ2), where the marginal distribution of σ2 is scaled inverse-χ2 and the conditional\ndistribution of µ given σ2 is normal (so that marginally µ has a t distribution). A convenient\nparameterization is given by the following specification:\n\nµ|σ2 ∼ N(µ0, σ\n2/κ0)\n\nσ2 ∼ Inv-χ2(ν0, σ\n2\n0),\n\nwhich corresponds to the joint prior density\n\np(µ, σ2) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ0 − µ)2]\n\n)\n. (3.6)\n\nWe label this the N-Inv-χ2(µ, σ2|µ0, σ\n2\n0/κ0; ν0, σ\n\n2\n0) density; its four parameters can be iden-\n\ntified as the location and scale of µ and the degrees of freedom and scale of σ2, respectively.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n68 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThe appearance of σ2 in the conditional distribution of µ|σ2 means that µ and σ2 are\nnecessarily dependent in their joint conjugate prior density: for example, if σ2 is large, then\na high-variance prior distribution is induced on µ. This dependence is notable, considering\nthat conjugate prior distributions are used largely for convenience. Upon reflection, however,\nit often makes sense for the prior variance of the mean to be tied to σ2, which is the sampling\nvariance of the observation y. In this way, prior belief about µ is calibrated by the scale of\nmeasurement of y and is equivalent to κ0 prior measurements on this scale.\n\nThe joint posterior distribution, p(µ, σ2|y)\n\nMultiplying the prior density (3.6) by the normal likelihood yields the posterior density\n\np(µ, σ2|y) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ− µ0)\n\n2]\n\n)\n×\n\n× (σ2)−n/2 exp\n\n(\n− 1\n\n2σ2\n[(n− 1)s2 + n(y − µ)2]\n\n)\n(3.7)\n\n= N-Inv-χ2(µ, σ2|µn, σ2\nn/κn; νn, σ\n\n2\nn),\n\nwhere, after some algebra (see Exercise 3.9), it can be shown that\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nνnσ\n2\nn = ν0σ\n\n2\n0 + (n− 1)s2 +\n\nκ0n\n\nκ0 + n\n(y − µ0)\n\n2.\n\nThe parameters of the posterior distribution combine the prior information and the infor-\nmation contained in the data. For example µn is a weighted average of the prior mean and\nthe sample mean, with weights determined by the relative precision of the two pieces of\ninformation. The posterior degrees of freedom, νn, is the prior degrees of freedom plus the\nsample size. The posterior sum of squares, νnσ\n\n2\nn, combines the prior sum of squares, the\n\nsample sum of squares, and the additional uncertainty conveyed by the difference between\nthe sample mean and the prior mean.\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nThe conditional posterior density of µ, given σ2, is proportional to the joint posterior density\n(3.7) with σ2 held constant,\n\nµ|σ2, y ∼ N(µn, σ\n2/κn)\n\n= N\n\n( κ0\n\nσ2µ0 +\nn\nσ2 y\n\nκ0\n\nσ2 + n\nσ2\n\n,\n1\n\nκ0\n\nσ2 + n\nσ2\n\n)\n, (3.8)\n\nwhich agrees, as it must, with the analysis in Section 2.5 of µ with σ considered fixed.\n\nThe marginal posterior distribution, p(σ2|y)\n\nThe marginal posterior density of σ2, from (3.7), is scaled inverse-χ2:\n\nσ2|y ∼ Inv-χ2(νn, σ\n2\nn). (3.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.4. MULTINOMIAL MODEL FOR CATEGORICAL DATA 69\n\nSampling from the joint posterior distribution\n\nTo sample from the joint posterior distribution, just as in the previous section, we first draw\nσ2 from its marginal posterior distribution (3.9), then draw µ from its normal conditional\nposterior distribution (3.8), using the simulated value of σ2.\n\nAnalytic form of the marginal posterior distribution of µ\n\nIntegration of the joint posterior density with respect to σ2, in a precisely analogous way\nto that used in the previous section, shows that the marginal posterior density for µ is\n\np(µ|y) ∝\n(\n1 +\n\nκn(µ− µn)2\nνnσ2\n\nn\n\n)−(νn+1)/2\n\n= tνn(µ|µn, σ2\nn/κn).\n\n3.4 Multinomial model for categorical data\n\nThe binomial distribution that was emphasized in Chapter 2 can be generalized to allow\nmore than two possible outcomes. The multinomial sampling distribution is used to describe\ndata for which each observation is one of k possible outcomes. If y is the vector of counts\nof the number of observations of each outcome, then\n\np(y|θ) ∝\nk∏\n\nj=1\n\nθ\nyj\nj ,\n\nwhere the sum of the probabilities,\n∑k\n\nj=1 θj , is 1. The distribution is typically thought of as\n\nimplicitly conditioning on the number of observations,\n∑k\nj=1 yj = n. The conjugate prior\n\ndistribution is a multivariate generalization of the beta distribution known as the Dirichlet,\n\np(θ|α) ∝\nk∏\n\nj=1\n\nθ\nαj−1\nj ,\n\nwhere the distribution is restricted to nonnegative θj ’s with\n∑k\nj=1 θj = 1; see Appendix\n\nA for details. The resulting posterior distribution for the θj ’s is Dirichlet with parameters\nαj + yj .\n\nThe prior distribution expressed on the scale of α is mathematically equivalent to a\nlikelihood resulting from\n\n∑k\nj=1(αj−1) observations with αj−1 observations of the jth out-\n\ncome category. As in the binomial there are several plausible noninformative Dirichlet prior\ndistributions. A uniform density is obtained by setting αj = 1 for all j; this distribution\n\nassigns equal density to any vector θ satisfying\n∑k\nj=1 θj = 1. Setting αj = 0 for all j results\n\nin an improper prior distribution that is uniform in the log(θj)’s. The resulting posterior\ndistribution is proper if there is at least one observation in each of the k categories, so that\neach component of y is positive. The bibliographic note at the end of this chapter points\nto other suggested noninformative prior distributions for the multinomial model.\n\nExample. Pre-election polling\nFor a simple example of a multinomial model, we consider a sample survey question\nwith three possible responses. In late October, 1988, a survey was conducted by CBS\nNews of 1447 adults in the United States to find out their preferences in the upcoming\npresidential election. Out of 1447 persons, y1 = 727 supported George Bush, y2 = 583\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n70 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.2 Histogram of values of (θ1− θ2) for 1000 simulations from the posterior distribution for\nthe election polling example.\n\nsupported Michael Dukakis, and y3 = 137 supported other candidates or expressed no\nopinion. Assuming no other information on the respondents, the 1447 observations\nare exchangeable. If we also assume simple random sampling (that is, 1447 names\n‘drawn out of a hat’), then the data (y1, y2, y3) follow a multinomial distribution, with\nparameters (θ1, θ2, θ3), the proportions of Bush supporters, Dukakis supporters, and\nthose with no opinion in the survey population. An estimand of interest is θ1 − θ2,\nthe population difference in support for the two major candidates.\nWith a noninformative uniform prior distribution on θ, α1=α2=α3=1, the posterior\ndistribution for (θ1, θ2, θ3) is Dirichlet(728, 584, 138). We could compute the posterior\ndistribution of θ1 − θ2 by integration, but it is simpler just to draw 1000 points\n(θ1, θ2, θ3) from the posterior Dirichlet distribution and then compute θ1 − θ2 for\neach. The result is displayed in Figure 3.2. All of the 1000 simulations had θ1 > θ2;\nthus, the estimated posterior probability that Bush had more support than Dukakis\nin the survey population is over 99.9%.\nIn fact, the CBS survey does not use independent random sampling but rather uses a\nvariant of a stratified sampling plan. We discuss an improved analysis of this survey,\nusing some knowledge of the sampling scheme, in Section 8.3 (see Table 8.2 on page\n207).\n\nIn complicated problems—for example, analyzing the results of many survey questions\nsimultaneously—the number of multinomial categories, and thus parameters, becomes so\nlarge that it is hard to usefully analyze a dataset of moderate size without additional\nstructure in the model. Formally, additional information can enter the analysis through\nthe prior distribution or the sampling model. An informative prior distribution might be\nused to improve inference in complicated problems, using the ideas of hierarchical modeling\nintroduced in Chapter 5. Alternatively, loglinear models can be used to impose structure on\nmultinomial parameters that result from cross-classifying several survey questions; Section\n16.7 provides details and an example.\n\n3.5 Multivariate normal model with known variance\n\nHere we give a somewhat formal account of the distributional results of Bayesian inference\nfor the parameters of a multivariate normal distribution. In many ways, these results\nparallel those already given for the univariate normal model, but there are some important\nnew aspects that play a major role in the analysis of linear models, which is the central\nactivity of much applied statistical work (see Chapters 5, 14, and 15). This section can be\nviewed at this point as reference material for future chapters.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\n\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|µ,Σ ∼ N(µ,Σ), (3.10)\n\nwhere µ is a (column) vector of length d and Σ is a d×d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\np(y|µ,Σ) ∝ |Σ|−1/2 exp\n\n(\n−1\n\n2\n(y − µ)TΣ−1(y − µ)\n\n)\n,\n\nand for a sample of n independent and identically distributed observations, y1, . . . , yn, is\n\np(y1, . . . , yn|µ,Σ) ∝ |Σ|−n/2 exp\n(\n−1\n\n2\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n)\n. (3.11)\n\nConjugate analysis\n\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known Σ.\n\nConjugate prior distribution for µ with known Σ. The log-likelihood is a quadratic form\nin µ, and therefore the conjugate prior distribution for µ is the multivariate normal distri-\nbution, which we parameterize as µ ∼ N(µ0,Λ0).\n\nPosterior distribution for µ with known Σ. The posterior distribution of µ is\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n\n(\n(µ− µ0)\n\nTΛ−1\n0 (µ− µ0) +\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n))\n\n,\n\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n(µ− µn)TΛ−1\n\nn (µ− µn)\n)\n\n= N(µ|µn,Λn),\n\nwhere\n\nµn = (Λ−1\n0 + nΣ−1)−1(Λ−1\n\n0 µ0 + nΣ−1y)\n\nΛ−1\nn = Λ−1\n\n0 + nΣ−1. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nΣ−1 and Λ−1\n\n0 , respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of µ with known Σ. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, µ(1) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\nµn and variance matrix equal to the appropriate submatrix of Λn. Also, the conditional\nposterior distribution of a subset µ(1) given the values of a second subset µ(2) is multivariate\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|w, &~ N(w,%), (3.10)\n\nwhere yu is a (column) vector of length d and © is a dx d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\npul.) x [E12 ep (—S y= w= 1)),\n\nand for a sample of n independent and identically distributed observations, y1,..., Yn, 1S\n\n_n 1X _\nPCY «+++ Yn|ts B) oc [Z|-\"/? exp (-4 Yow w= Mn 0))- (3.11)\n\ni=l\n\nConjugate analysis\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known ™.\n\nConjugate prior distribution for w with known X. The log-likelihood is a quadratic form\nin ys, and therefore the conjugate prior distribution for yz is the multivariate normal distri-\nbution, which we parameterize as pu ~ N({uo, Ao).\n\nPosterior distribution for with known %. The posterior distribution of ju is\n1 _ . _\np(uly, &) x exp (-} (\\: — po)” Ng (= po) + S3(yi = BPE yi - »))\ni=1\n\nwhich is an exponential of a quadratic form in jz. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\nplays) 0 exp (—5(00~ nn) A\" — sn)\n\n= N(u| én, An),\nwhere\nHin = (Ag* + n¥7\")7*(Ag to + nE~\"y)\nAyt = Apt+nzc7. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nu~! and Ag 1 respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of 1 with known %. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, 1“) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\n[in and variance matrix equal to the appropriate submatrix of A,. Also, the conditional\nposterior distribution of a subset p“) given the values of a second subset “?) is multivariate\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n72 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nnormal. If we write superscripts in parentheses to indicate appropriate subvectors and\nsubmatrices, then\n\nµ(1)|µ(2), y ∼ N\n(\nµ(1)\nn + β1|2(µ(2) − µ(2)\n\nn ),Λ1|2\n)\n, (3.13)\n\nwhere the regression coefficients β1|2 and conditional variance matrix Λ1|2 are defined by\n\nβ1|2 = Λ(12)\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ1|2 = Λ(11)\nn − Λ(12)\n\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ(21)\nn .\n\nPosterior predictive distribution for new data. We now work out the analytic form of the\nposterior predictive distribution for a new observation ỹ ∼ N(µ,Σ). As with the univariate\nnormal, we first note that the joint distribution, p(ỹ, µ|y) = N(ỹ|µ,Σ)N(µ|µn,Λn), is the\nexponential of a quadratic form in (ỹ, µ); hence (ỹ, µ) have a joint normal posterior distri-\nbution, and so the marginal posterior distribution of ỹ is (multivariate) normal. We are\nstill assuming the variance matrix Σ is known. As in the univariate case, we can determine\nthe posterior mean and variance of ỹ using (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|µ, y)|y)\n= E(µ|y) = µn,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|µ, y)|y) + var(E(ỹ|µ, y)|y)\n= E(Σ|y) + var(µ|y) = Σ + Λn.\n\nTo sample from the posterior distribution or the posterior predictive distribution, re-\nfer to Appendix A for a method of generating random draws from a multivariate normal\ndistribution with specified mean and variance matrix.\n\nNoninformative prior density for µ. A noninformative uniform prior density for µ is p(µ) ∝\nconstant, obtained in the limit as the prior precision tends to zero in the sense |Λ−1\n\n0 | → 0;\nin the limit of infinite prior variance (zero prior precision), the prior mean is irrelevant.\nThough this choice of prior density does not combine with the likelihood to form a proper\njoint probability model for µ and y, the posterior density obtained by applying Bayes’ rule\nis a proper posterior density. The posterior density is proportional to the likelihood (3.11)\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant terms yields the posterior distribution for µ, given the uniform prior density,\nas µ|Σ, y ∼ N(y,Σ/n).\n\n3.6 Multivariate normal with unknown mean and variance\n\nConjugate inverse-Wishart family of prior distributions\n\nRecall that the conjugate distribution for the univariate normal with unknown mean and\nvariance is the normal-inverse-χ2 distribution (3.6). We can use the inverse-Wishart dis-\ntribution, a multivariate generalization of the scaled inverse-χ2, to describe the prior dis-\ntribution of the matrix Σ. The conjugate prior distribution for (µ,Σ), the normal-inverse-\nWishart, is conveniently parameterized in terms of hyperparameters (µ0,Λ0/κ0; ν0,Λ0):\n\nΣ ∼ Inv-Wishartν0(Λ\n−1\n0 )\n\nµ|Σ ∼ N(µ0,Σ/κ0),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.6. MULTIVARIATE NORMAL WITH UNKNOWN MEAN AND VARIANCE 73\n\nwhich corresponds to the joint prior density\n\np(µ,Σ)∝|Σ|−((ν0+d)/2+1) exp\n\n(\n−1\n\n2\ntr(Λ0Σ\n\n−1)− κ0\n2\n(µ− µ0)\n\nTΣ−1(µ− µ0)\n\n)\n.\n\nThe parameters ν0 and Λ0 describe the degrees of freedom and the scale matrix for the\ninverse-Wishart distribution on Σ. The remaining parameters are the prior mean, µ0, and\nthe number of prior measurements, κ0, on the Σ scale. Multiplying the prior density by the\nnormal likelihood results in a posterior density of the same family with parameters\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nΛn = Λ0 + S +\nκ0n\n\nκ0 + n\n(y − µ0)(y − µ0)\n\nT ,\n\nwhere S is the sum of squares matrix about the sample mean,\n\nS =\n\nn∑\n\ni=1\n\n(yi − y)(yi − y)T .\n\nOther results from the univariate normal easily generalize to the multivariate case. The\nmarginal posterior distribution of µ is multivariate tνn−d+1(µn,Λn/(κn(νn − d+ 1))). The\nposterior predictive distribution of a new observation ỹ is also multivariate t with an ad-\nditional factor of κn+1 in the numerator of the scale matrix. Samples from the joint\nposterior distribution of (µ,Σ) are easily obtained using the following procedure: first,\ndraw Σ|y ∼ Inv-Wishartνn(Λ\n\n−1\nn ), then draw µ|Σ, y ∼ N(µn,Σ/κn). See Appendix A for\n\ndrawing from inverse-Wishart and multivariate normal distributions. To draw from the\nposterior predictive distribution of a new observation, draw ỹ|µ,Σ, y ∼ N(µ,Σ), given the\nalready drawn values of µ and Σ.\n\nDifferent noninformative prior distributions\n\nInverse-Wishart with d + 1 degrees of freedom. Setting Σ ∼ Inv-Wishartd+1(I) has the\nappealing feature that each of the correlations in Σ has, marginally, a uniform prior distri-\nbution. (The joint distribution is not uniform, however, because of the constraint that the\ncorrelation matrix be positive definite.)\n\nInverse-Wishart with d−1 degrees of freedom. Another proposed noninformative prior\ndistribution is the multivariate Jeffreys prior density,\n\np(µ,Σ) ∝ |Σ|−(d+1)/2,\n\nwhich is the limit of the conjugate prior density as κ0 → 0, ν0 → −1, |Λ0| → 0. The\ncorresponding posterior distribution can be written as\n\nΣ|y ∼ Inv-Wishartn−1(S\n−1)\n\nµ|Σ, y ∼ N(y,Σ/n).\n\nResults for the marginal distribution of µ and the posterior predictive distribution of ỹ,\nassuming that the posterior distribution is proper, follow from the previous paragraph. For\nexample, the marginal posterior distribution of µ is multivariate tn−d(y, S/(n(n− d))).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n74 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nDose, xi Number of Number of\n(log g/ml) animals, ni deaths, yi\n\n−0.86 5 0\n−0.30 5 1\n−0.05 5 3\n0.73 5 5\n\nTable 3.1: Bioassay data from Racine et al. (1986).\n\nScaled inverse-Wishart model\n\nWhen modeling covariance matrices it can help to extend the inverse-Wishart model by\nmultiplying by a set of scale parameters that can be modeled separately. This gives flexibility\nin modeling and allows one to set up a uniform or weak prior distribution on correlations\nwithout overly constraining the variance parameters. The scaled inverse-Wishart model for\nΣ has the form,\n\nΣ = Diag(ξ)ΣηDiag(ξ),\n\nwhere Ση is given an inverse-Wishart prior distribution (one choice is Inv-Wishartd+1(I), so\nthat the marginal distributions of the correlations are uniform) and then the scale param-\neters ξ can be given weakly informative priors themselves. We discuss further in Section\n15.4 in the context of varying-intercept, varying-slope hierarchical regression models.\n\n3.7 Example: analysis of a bioassay experiment\n\nBeyond the normal distribution, few multiparameter sampling models allow simple explicit\ncalculation of posterior distributions. Data analysis for such models is possible using the\ncomputational methods described in Part III of this book. Here we present an example\nof a nonconjugate model for a bioassay experiment, drawn from the literature on applied\nBayesian statistics. The model is a two-parameter example from the broad class of general-\nized linear models to be considered more thoroughly in Chapter 16. We use a particularly\nsimple simulation approach, approximating the posterior distribution by a discrete distri-\nbution supported on a two-dimensional grid of points, that provides sufficiently accurate\ninferences for this two-parameter example.\n\nThe scientific problem and the data\n\nIn the development of drugs and other chemical compounds, acute toxicity tests or bioassay\nexperiments are commonly performed on animals. Such experiments proceed by adminis-\ntering various dose levels of the compound to batches of animals. The animals’ responses\nare typically characterized by a dichotomous outcome: for example, alive or dead, tumor\nor no tumor. An experiment of this kind gives rise to data of the form\n\n(xi, ni, yi); i = 1, . . . , k,\n\nwhere xi represents the ith of k dose levels (often measured on a logarithmic scale) given\nto ni animals, of which yi subsequently respond with positive outcome. An example of real\ndata from such an experiment is shown in Table 3.1: twenty animals were tested, five at\neach of four dose levels.\n\nModeling the dose–response relation\n\nGiven what we have seen so far, we must model the outcomes of the five animals within\neach group i as exchangeable, and it seems reasonable to model them as independent with\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 75\n\nequal probabilities, which implies that the data points yi are binomially distributed:\n\nyi|θi ∼ Bin(ni, θi),\n\nwhere θi is the probability of death for animals given dose xi. (An example of a situation\nin which independence and the binomial model would not be appropriate is if the deaths\nwere caused by a contagious disease.) For this experiment, it is also reasonable to treat the\noutcomes in the four groups as independent of each other, given the parameters θ1, . . . , θ4.\n\nThe simplest analysis would treat the four parameters θi as exchangeable in their prior\ndistribution, perhaps using a noninformative density such as p(θ1, . . . , θ4) ∝ 1, in which case\nthe parameters θi would have independent beta posterior distributions. The exchangeable\nprior model for the θi parameters has a serious flaw, however; we know the dose level xi\nfor each group i, and one would expect the probability of death to vary systematically as a\nfunction of dose.\n\nThe simplest model of the dose–response relation—that is, the relation of θi to xi—is\nlinear: θi = α + βxi. Unfortunately, this model has the flaw that at low or high doses,\nxi approaches ±∞ (recall that the dose is measured on the log scale), whereas θi, being a\nprobability, must be constrained to lie between 0 and 1. The standard solution is to use a\ntransformation of the θ’s, such as the logistic, in the dose–response relation:\n\nlogit(θi) = α+ βxi, (3.14)\n\nwhere logit(θi) = log(θi/(1 − θi)) as defined in (1.10). This is called a logistic regression\nmodel.\n\nThe likelihood\n\nUnder the model (3.14), we can write the sampling distribution, or likelihood, for each\ngroup i in terms of the parameters α and β as\n\np(yi|α, β, ni, xi) ∝ [logit−1(α+ βxi)]\nyi [1− logit−1(α+ βxi)]\n\nni−yi .\n\nThe model is characterized by the parameters α and β, whose joint posterior distribution\nis\n\np(α, β|y, n, x) ∝ p(α, β|n, x)p(y|α, β, n, x) (3.15)\n\n∝ p(α, β)\n\nk∏\n\ni=1\n\np(yi|α, β, ni, xi).\n\nWe consider the sample sizes ni and dose levels xi as fixed for this analysis and suppress\nthe conditioning on (n, x) in subsequent notation.\n\nThe prior distribution\n\nWe present an analysis based on a prior distribution for (α, β) that is independent and locally\nuniform in the two parameters; that is, p(α, β) ∝ 1. In practice, we might use a uniform\nprior distribution if we really have no prior knowledge about the parameters, or if we want to\npresent a simple analysis of this experiment alone. If the analysis using the noninformative\nprior distribution is insufficiently precise, we may consider using other sources of substantive\ninformation (for example, from other bioassay experiments) to construct an informative\nprior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n76 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.3 (a) Contour plot for the posterior density of the parameters in the bioassay example.\nContour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of 1000 draws\nfrom the posterior distribution.\n\nA rough estimate of the parameters\n\nWe will compute the joint posterior distribution (3.15) at a grid of points (α, β), but before\ndoing so, it is a good idea to get a rough estimate of (α, β) so we know where to look. To\nobtain the rough estimate, we use existing software to perform a logistic regression; that\nis, finding the maximum likelihood estimate of (α, β) in (3.15) for the four data points in\n\nTable 3.1. The estimate is (α̂, β̂) = (0.8, 7.7), with standard errors of 1.0 and 4.9 for α and\nβ, respectively.\n\nObtaining a contour plot of the joint posterior density\n\nWe are now ready to compute the posterior density at a grid of points (α, β). After some\nexperimentation, we use the range (α, β) ∈ [−5, 10]× [−10, 40], which captures almost all\nthe mass of the posterior distribution. The resulting contour plot appears in Figure 3.3a;\na general justification for setting the lowest contour level at 0.05 for two-dimensional plots\nappears in Section 4.1.\n\nSampling from the joint posterior distribution\n\nHaving computed the unnormalized posterior density at a grid of values that cover the\neffective range of (α, β), we can normalize by approximating the distribution as a step\nfunction over the grid and setting the total probability in the grid to 1. We sample 1000\nrandom draws (αs, βs) from the posterior distribution using the following procedure.\n\n1. Compute the marginal posterior distribution of α by numerically summing over β in the\ndiscrete distribution computed on the grid of Figure 3.3a.\n\n2. For s = 1, . . . , 1000:\n\n(a) Draw αs from the discretely computed p(α|y); this can be viewed as a discrete version\nof the inverse cdf method described in Section 1.9.\n\n(b) Draw βs from the discrete conditional distribution, p(β|α, y), given the just-sampled\nvalue of α.\n\n(c) For each of the sampled α and β, add a uniform random jitter centered at zero with\na width equal to the spacing of the sampling grid. This gives the simulation draws a\ncontinuous distribution.\n\nThe 1000 draws (αs, βs) are displayed on a scatterplot in Figure 3.3b. The scale of the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 77\n\nFigure 3.4 Histogram of the draws from the posterior distribution of the LD50 (on the scale of log\ndose in g/ml) in the bioassay example, conditional on the parameter β being positive.\n\nplot, which is the same as the scale of Figure 3.3a, has been set large enough that all the\n1000 draws would fit on the graph.\n\nThere are a number of practical considerations when applying this two-dimensional grid\napproximation. There can be difficulty finding the correct location and scale for the grid\npoints. A grid that is defined on too small an area may miss important features of the\nposterior distribution that fall outside the grid. A grid defined on a large area with wide\nintervals between points can miss important features that fall between the grid points. It\nis also important to avoid overflow and underflow operations when computing the poste-\nrior distribution. It is usually a good idea to compute the logarithm of the unnormalized\nposterior distribution and subtract off the maximum value before exponentiating. This\ncreates an unnormalized discrete approximation with maximum value 1, which can then be\nnormalized (by setting the total probability in the grid to 1).\n\nThe posterior distribution of the LD50\n\nA parameter of common interest in bioassay studies is the LD50—the dose level at which\nthe probability of death is 50%. In our logistic model, a 50% survival rate means\n\nLD50: E\n\n(\nyi\nni\n\n)\n= logit−1(α+ βxi) = 0.5;\n\nthus, α + βxi = logit(0.5) = 0, and the LD50 is xi = −α/β. Computing the posterior\ndistribution of any summaries in the Bayesian approach is straightforward, as discussed at\nthe end of Section 1.9. Given what we have done so far, simulating the posterior distribution\nof the LD50 is trivial: we just compute −α/β for the 1000 draws of (α, β) pictured in Figure\n3.3b.\n\nDifficulties with the LD50 parameterization if the drug is beneficial. In the context of this\nexample, LD50 is a meaningless concept if β ≤ 0, in which case increasing the dose does not\ncause the probability of death to increase. If we were certain that the drug could not cause\nthe tumor rate to decrease, we should constrain the parameter space to exclude values of β\nless than 0. However, it seems more reasonable here to allow the possibility of β ≤ 0 and\njust note that LD50 is hard to interpret in this case.\n\nWe summarize the inference on the LD50 scale by reporting two results: (1) the posterior\nprobability that β > 0—that is, that the drug is harmful—and (2) the posterior distribution\nfor the LD50 conditional on β > 0. All of the 1000 simulation draws had positive values of\nβ, so the posterior probability that β > 0 is roughly estimated to exceed 0.999. We compute\nthe LD50 for the simulation draws with positive values of β (which happen to be all 1000\ndraws for this example); a histogram is displayed in Figure 3.4. This example illustrates that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n78 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe marginal posterior mean is not always a good summary of inference about a parameter.\nWe are not, in general, interested in the posterior mean of the LD50, because the posterior\nmean includes the cases in which the dose–response relation is negative.\n\n3.8 Summary of elementary modeling and computation\n\nThe lack of multiparameter models permitting easy calculation of posterior distributions is\nnot a major practical handicap for three main reasons. First, when there are few parame-\nters, posterior inference in nonconjugate multiparameter models can be obtained by simple\nsimulation methods, as we have seen in the bioassay example. Second, sophisticated models\ncan often be represented in a hierarchical or conditional manner, as we shall see in Chapter\n5, for which effective computational strategies are available (as we discuss in general in Part\nIII). Finally, as we discuss in Chapter 4, we can often apply a normal approximation to\nthe posterior distribution, and therefore the conjugate structure of the normal model can\nplay an important role in practice, well beyond its application to explicitly normal sampling\nmodels.\n\nOur successful analysis of the bioassay example suggests the following strategy for com-\nputation of simple Bayesian posterior distributions. What follows is not truly a general ap-\nproach, but it summarizes what we have done so far and foreshadows the general methods—\nbased on successive approximations—presented in Part III.\n\n1. Write the likelihood part of the model, p(y|θ), ignoring any factors that are free of θ.\n\n2. Write the posterior density, p(θ|y) ∝ p(θ)p(y|θ). If prior information is well-formulated,\ninclude it in p(θ). Otherwise use a weakly informative prior distribution or temporarily\nset p(θ) ∝ constant, with the understanding that the prior density can be altered later\nto include additional information or structure.\n\n3. Create a crude estimate of the parameters, θ, for use as a starting point and a comparison\nto the computation in the next step.\n\n4. Draw simulations θ1, . . . , θS , from the posterior distribution. Use the sample draws to\ncompute the posterior density of any functions of θ that may be of interest.\n\n5. If any predictive quantities, ỹ, are of interest, simulate ỹ1, . . . , ỹS by drawing each ỹs\n\nfrom the sampling distribution conditional on the drawn value θs, p(ỹ|θs). In Chapter\n6, we discuss how to use posterior simulations of θ and ỹ to check the fit of the model to\ndata and substantive knowledge.\n\nFor nonconjugate models, step 4 above can be difficult. Various methods have been\ndeveloped to draw posterior simulations in complicated models, as we discuss in Part III.\nOccasionally, high-dimensional problems can be solved by combining analytical and nu-\nmerical simulation methods. If θ has only one or two components, it is possible to draw\nsimulations by computing on a grid, as we illustrated in the previous section for the bioassay\nexample.\n\n3.9 Bibliographic note\n\nChapter 2 of Box and Tiao (1973) thoroughly treats the univariate and multivariate normal\ndistribution problems and also some related problems such as estimating the difference\nbetween two means and the ratio between two variances. At the time that book was\nwritten, computer simulation methods were much less convenient than they are now, and\nso Box and Tiao, and other Bayesian authors of the period, restricted their attention to\nconjugate families and devoted much effort to deriving analytic forms of marginal posterior\ndensities.\n\nMany textbooks on multivariate analysis discuss the unique mathematical features of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 79\n\nSurvey Bush Dukakis No opinion/other Total\n\npre-debate 294 307 38 639\npost-debate 288 332 19 639\n\nTable 3.2 Number of respondents in each preference category from ABC News pre- and post-debate\nsurveys in 1988.\n\nthe multivariate normal distribution, such as the property that all marginal and conditional\ndistributions of components of a multivariate normal vector are normal; for example, see\nMardia, Kent, and Bibby (1979).\n\nSimon Newcomb’s data, along with a discussion of his experiment, appear in Stigler\n(1977).\n\nThe multinomial model and corresponding informative and noninformative prior distri-\nbutions are discussed by Good (1965) and Fienberg (1977); also see the bibliographic note\non loglinear models at the end of Chapter 16.\n\nThe data and model for the bioassay example appear in Racine et al. (1986), an article\nthat presents several examples of simple Bayesian analyses that have been useful in the\npharmaceutical industry.\n\n3.10 Exercises\n\n1. Binomial and multinomial models: suppose data (y1, . . . , yJ) follow a multinomial distri-\nbution with parameters (θ1, . . . , θJ). Also suppose that θ = (θ1, . . . , θJ ) has a Dirichlet\nprior distribution. Let α = θ1\n\nθ1+θ2\n.\n\n(a) Write the marginal posterior distribution for α.\n\n(b) Show that this distribution is identical to the posterior distribution for α obtained by\ntreating y1 as an observation from the binomial distribution with probability α and\nsample size y1 + y2, ignoring the data y3, . . . , yJ .\n\nThis result justifies the application of the binomial distribution to multinomial problems\nwhen we are only interested in two of the categories; for example, see the next problem.\n\n2. Comparison of two multinomial observations: on September 25, 1988, the evening of a\npresidential campaign debate, ABC News conducted a survey of registered voters in the\nUnited States; 639 persons were polled before the debate, and 639 different persons were\npolled after. The results are displayed in Table 3.2. Assume the surveys are independent\nsimple random samples from the population of registered voters. Model the data with\ntwo different multinomial distributions. For j = 1, 2, let αj be the proportion of voters\nwho preferred Bush, out of those who had a preference for either Bush or Dukakis at\nthe time of survey j. Plot a histogram of the posterior density for α2 − α1. What is the\nposterior probability that there was a shift toward Bush?\n\n3. Estimation from two independent experiments: an experiment was performed on the\neffects of magnetic fields on the flow of calcium out of chicken brains. Two groups\nof chickens were involved: a control group of 32 chickens and an exposed group of 36\nchickens. One measurement was taken on each chicken, and the purpose of the experiment\nwas to measure the average flow µc in untreated (control) chickens and the average flow\nµt in treated chickens. The 32 measurements on the control group had a sample mean of\n1.013 and a sample standard deviation of 0.24. The 36 measurements on the treatment\ngroup had a sample mean of 1.173 and a sample standard deviation of 0.20.\n\n(a) Assuming the control measurements were taken at random from a normal distribution\nwith mean µc and variance σ2\n\nc , what is the posterior distribution of µc? Similarly, use\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n80 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe treatment group measurements to determine the marginal posterior distribution\nof µt. Assume a uniform prior distribution on (µc, µt, log σc, log σt).\n\n(b) What is the posterior distribution for the difference, µt − µc? To get this, you may\nsample from the independent t distributions you obtained in part (a) above. Plot a\nhistogram of your samples and give an approximate 95% posterior interval for µt−µc.\n\nThe problem of estimating two normal means with unknown ratio of variances is called\nthe Behrens–Fisher problem.\n\n4. Inference for a 2× 2 table: an experiment was performed to estimate the effect of beta-\nblockers on mortality of cardiac patients. A group of patients were randomly assigned\nto treatment and control groups: out of 674 patients receiving the control, 39 died, and\nout of 680 receiving the treatment, 22 died. Assume that the outcomes are independent\nand binomially distributed, with probabilities of death of p0 and p1 under the control\nand treatment, respectively. We return to this example in Section 5.6.\n\n(a) Set up a noninformative prior distribution on (p0, p1) and obtain posterior simulations.\n\n(b) Summarize the posterior distribution for the odds ratio, (p1/(1− p1))/(p0/(1− p0)).\n(c) Discuss the sensitivity of your inference to your choice of noninformative prior density.\n\n5. Rounded data: it is a common problem for measurements to be observed in rounded\nform (for a review, see Heitjan, 1989). For a simple example, suppose we weigh an\nobject five times and measure weights, rounded to the nearest pound, of 10, 10, 12, 11,\n9. Assume the unrounded measurements are normally distributed with a noninformative\nprior distribution on the mean µ and variance σ2.\n\n(a) Give the posterior distribution for (µ, σ2) obtained by pretending that the observations\nare exact unrounded measurements.\n\n(b) Give the correct posterior distribution for (µ, σ2) treating the measurements as rounded.\n\n(c) How do the incorrect and correct posterior distributions differ? Compare means,\nvariances, and contour plots.\n\n(d) Let z = (z1, . . . , z5) be the original, unrounded measurements corresponding to the five\nobservations above. Draw simulations from the posterior distribution of z. Compute\nthe posterior mean of (z1 − z2)2.\n\n6. Binomial with unknown probability and sample size: some of the difficulties with setting\nprior distributions in multiparameter models can be illustrated with the simple binomial\ndistribution. Consider data y1, . . . , yn modeled as independent Bin(N, θ), with both N\nand θ unknown. Defining a convenient family of prior distributions on (N, θ) is difficult,\npartly because of the discreteness of N .\nRaftery (1988) considers a hierarchical approach based on assigning the parameter N\na Poisson distribution with unknown mean µ. To define a prior distribution on (θ,N),\nRaftery defines λ = µθ and specifies a prior distribution on (λ, θ). The prior distribution\nis specified in terms of λ rather than µ because ‘it would seem easier to formulate prior\ninformation about λ, the unconditional expectation of the observations, than about µ,\nthe mean of the unobserved quantity N .’\n\n(a) A suggested noninformative prior distribution is p(λ, θ) ∝ λ−1. What is a motivation\nfor this noninformative distribution? Is the distribution improper? Transform to\ndetermine p(N, θ).\n\n(b) The Bayesian method is illustrated on counts of waterbuck obtained by remote pho-\ntography on five separate days in Kruger Park in South Africa. The counts were\n53, 57, 66, 67, and 72. Perform the Bayesian analysis on these data and display a\nscatterplot of posterior simulations of (N, θ). What is the posterior probability that\nN > 100?\n\n(c) Why not simply use a Poisson with fixed µ as a prior distribution for N?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 81\n\nType of Bike Counts of bicycles/other vehicles\nstreet route?\n\nResidential yes 16/58, 9/90, 10/48, 13/57, 19/103,\n20/57, 18/86, 17/112, 35/273, 55/64\n\nResidential no 12/113, 1/18, 2/14, 4/44, 9/208,\n7/67, 9/29, 8/154\n\nFairly busy yes 8/29, 35/415, 31/425, 19/42, 38/180,\n47/675, 44/620, 44/437, 29/47, 18/462\n\nFairly busy no 10/557, 43/1258, 5/499, 14/601, 58/1163,\n15/700, 0/90, 47/1093, 51/1459, 32/1086\n\nBusy yes 60/1545, 51/1499, 58/1598, 59/503, 53/407,\n68/1494, 68/1558, 60/1706, 71/476, 63/752\n\nBusy no 8/1248, 9/1246, 6/1596, 9/1765, 19/1290,\n61/2498, 31/2346, 75/3101, 14/1918, 25/2318\n\nTable 3.3 Counts of bicycles and other vehicles in one hour in each of 10 city blocks in each of\nsix categories. (The data for two of the residential blocks were lost.) For example, the first block\nhad 16 bicycles and 58 other vehicles, the second had 9 bicycles and 90 other vehicles, and so on.\nStreets were classified as ‘residential,’ ‘fairly busy,’ or ‘busy’ before the data were gathered.\n\n7. Poisson and binomial distributions: a student sits on a street corner for an hour and\nrecords the number of bicycles b and the number of other vehicles v that go by. Two\nmodels are considered:\n\n• The outcomes b and v have independent Poisson distributions, with unknown means\nθb and θv.\n\n• The outcome b has a binomial distribution, with unknown probability p and sample\nsize b+ v.\n\nShow that the two models have the same likelihood if we define p = θb\nθb+θv\n\n.\n\n8. Analysis of proportions: a survey was done of bicycle and other vehicular traffic in the\nneighborhood of the campus of the University of California, Berkeley, in the spring of\n1993. Sixty city blocks were selected at random; each block was observed for one hour,\nand the numbers of bicycles and other vehicles traveling along that block were recorded.\nThe sampling was stratified into six types of city blocks: busy, fairly busy, and residential\nstreets, with and without bike routes, with ten blocks measured in each stratum. Table\n3.3 displays the number of bicycles and other vehicles recorded in the study. For this\nproblem, restrict your attention to the first four rows of the table: the data on residential\nstreets.\n\n(a) Let y1, . . . , y10 and z1, . . . , z8 be the observed proportion of traffic that was on bicycles\nin the residential streets with bike lanes and with no bike lanes, respectively (so\ny1 = 16/(16 + 58) and z1 = 12/(12 + 113), for example). Set up a model so that the\nyi’s are independent and identically distributed given parameters θy and the zi’s are\nindependent and identically distributed given parameters θz.\n\n(b) Set up a prior distribution that is independent in θy and θz.\n\n(c) Determine the posterior distribution for the parameters in your model and draw 1000\nsimulations from the posterior distribution. (Hint: θy and θz are independent in the\nposterior distribution, so they can be simulated independently.)\n\n(d) Let µy = E(yi|θy) be the mean of the distribution of the yi’s; µy will be a function of\nθy. Similarly, define µz. Using your posterior simulations from (c), plot a histogram of\nthe posterior simulations of µy − µz, the expected difference in proportions in bicycle\ntraffic on residential streets with and without bike lanes.\n\nWe return to this example in Exercise 5.13.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n82 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\n9. Conjugate normal model: suppose y is an independent and identically distributed sam-\nple of size n from the distribution N(µ, σ2), where the prior distribution for (µ, σ2) is\nN-Inv-χ2(µ, σ2|µ0, σ\n\n2\n0/κ0; ν0, σ\n\n2\n0); that is, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0) and µ|σ2 ∼ N(µ0, σ\n\n2/κ0).\nThe posterior distribution, p(µ, σ2|y), is also normal-inverse-χ2; derive explicitly its pa-\nrameters in terms of the prior parameters and the sufficient statistics of the data.\n\n10. Comparison of normal variances: for j = 1, 2, suppose that\n\nyj1, . . . , yjnj |µj , σ2\nj ∼ iid N(µj , σ\n\n2\nj ),\n\np(µj , σ\n2\nj ) ∝ σ−2\n\nj ,\n\nand (µ1, σ\n2\n1) are independent of (µ2, σ\n\n2\n2) in the prior distribution. Show that the posterior\n\ndistribution of (s21/s\n2\n2)/(σ\n\n2\n1/σ\n\n2\n2) is F with (n1−1) and (n2−1) degrees of freedom. (Hint:\n\nto show the required form of the posterior density, you do not need to carry along all the\nnormalizing constants.)\n\n11. Computation: in the bioassay example, replace the uniform prior density by a joint nor-\nmal prior distribution on (α, β), with α ∼ N(0, 22), β ∼ N(10, 102), and corr(α, β)=0.5.\n\n(a) Repeat all the computations and plots of Section 3.7 with this new prior distribution.\n\n(b) Check that your contour plot and scatterplot look like a compromise between the prior\ndistribution and the likelihood (as displayed in Figure 3.3).\n\n(c) Discuss the effect of this hypothetical prior information on the conclusions in the\napplied context.\n\n12. Poisson regression model: expand the model of Exercise 2.13(a) by assuming that the\nnumber of fatal accidents in year t follows a Poisson distribution with mean α+ βt. You\nwill estimate α and β, following the example of the analysis in Section 3.7.\n\n(a) Discuss various choices for a ‘noninformative’ prior for (α, β). Choose one.\n\n(b) Discuss what would be a realistic informative prior distribution for (α, β). Sketch its\ncontours and then put it aside. Do parts (c)–(h) of this problem using your noninfor-\nmative prior distribution from (a).\n\n(c) Write the posterior density for (α, β). What are the sufficient statistics?\n\n(d) Check that the posterior density is proper.\n\n(e) Calculate crude estimates and uncertainties for (α, β) using linear regression.\n\n(f) Plot the contours and take 1000 draws from the joint posterior density of (α, β).\n\n(g) Using your samples of (α, β), plot a histogram of the posterior density for the expected\nnumber of fatal accidents in 1986, α+ 1986β.\n\n(h) Create simulation draws and obtain a 95% predictive interval for the number of fatal\naccidents in 1986.\n\n(i) How does your hypothetical informative prior distribution in (b) differ from the pos-\nterior distribution in (f) and (g), obtained from the noninformative prior distribution\nand the data? If they disagree, discuss.\n\n13. Multivariate normal model: derive equations (3.12) by completing the square in vector-\nmatrix notation.\n\n14. Improper prior and proper posterior distributions: prove that the posterior density (3.15)\nfor the bioassay example has a finite integral over the range (α, β) ∈ (−∞,∞)×(−∞,∞).\n\n15. Joint distributions: The autoregressive time-series model y1, y2, . . . with mean level 0,\nautocorrelation 0.8, residual standard deviation 1, and normal errors can be written as\n(yt|yt−1, yt−2, . . .) ∼ N(0.8yt−1, 1) for all t.\n\n(a) Prove that the distribution of yt, given the observations at all other integer time points\nt, depends only on yt−1 and yt+1.\n\n(b) What is the distribution of yt given yt−1 and yt+1?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 4\n\nAsymptotics and connections to non-Bayesian\n\napproaches\n\nWe have seen that many simple Bayesian analyses based on noninformative prior distribu-\ntions give similar results to standard non-Bayesian approaches (for example, the posterior t\ninterval for the normal mean with unknown variance). The extent to which a noninforma-\ntive prior distribution can be justified as an objective assumption depends on the amount\nof information available in the data: in the simple cases discussed in Chapters 2 and 3,\nit was clear that as the sample size n increases, the influence of the prior distribution on\nposterior inferences decreases. These ideas, sometimes referred to as asymptotic theory,\nbecause they refer to properties that hold in the limit as n becomes large, will be reviewed\nin the present chapter, along with some more explicit discussion of the connections between\nBayesian and non-Bayesian methods. The large-sample results are not actually necessary\nfor performing Bayesian data analysis but are often useful as approximations and as tools\nfor understanding.\n\nWe begin this chapter with a discussion of the various uses of the normal approximation\nto the posterior distribution. Theorems about consistency and normality of the posterior\ndistribution in large samples are outlined in Section 4.2, followed by several counterexamples\nin Section 4.3; proofs of the theorems are sketched in Appendix B. Finally, we discuss how\nthe methods of frequentist statistics can be used to evaluate the properties of Bayesian\ninferences.\n\n4.1 Normal approximations to the posterior distribution\n\nNormal approximation to the joint posterior distribution\n\nIf the posterior distribution p(θ|y) is unimodal and roughly symmetric, it can be convenient\nto approximate it by a normal distribution; that is, the logarithm of the posterior density\nis approximated by a quadratic function of θ.\n\nHere we consider a quadratic approximation to the log-posterior density that is centered\nat the posterior mode (which in general is easy to compute using off-the-shelf optimization\nroutines); in Chapter 13 we discuss more elaborate approximations which can be effective\nin settings where simple mode-based approximations fail.\n\nA Taylor series expansion of log p(θ|y) centered at the posterior mode, θ̂ (where θ can\n\nbe a vector and θ̂ is assumed to be in the interior of the parameter space), gives\n\nlog p(θ|y) = log p(θ̂|y) + 1\n\n2\n(θ − θ̂)T\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n(θ − θ̂) + · · · , (4.1)\n\nwhere the linear term in the expansion is zero because the log-posterior density has zero\nderivative at its mode. As we discuss in Section 4.2, the remainder terms of higher order fade\nin importance relative to the quadratic term when θ is close to θ̂ and n is large. Considering\n\n83\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n84 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of θ, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\np(θ|y) ≈ N(θ̂, [I(θ̂)]−1), (4.2)\n\nwhere I(θ) is the observed information,\n\nI(θ) = − d2\n\ndθ2\nlog p(θ|y).\n\nIf the mode, θ̂, is in the interior of parameter space, then the matrix I(θ̂) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1, . . . , yn be independent observations from a N(µ, σ2) distribution, and,\nfor simplicity, we assume a uniform prior density for (µ, log σ). We set up a normal\napproximation to the posterior distribution of (µ, log σ), which has the virtue of re-\nstricting σ to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\nlog p(µ, log σ|y) = constant− n log σ − 1\n\n2σ2\n((n− 1)s2 + n(y − µ)2).\n\nThe first derivatives are\n\nd\n\ndµ\nlog p(µ, log σ|y) =\n\nn(y − µ)\nσ2\n\n,\n\nd\n\nd(log σ)\nlog p(µ, log σ|y) = −n+\n\n(n− 1)s2 + n(y − µ)2\nσ2\n\n,\n\nfrom which the posterior mode is readily obtained as\n\n(µ̂, log σ̂) =\n\n(\ny, log\n\n(√\nn− 1\n\nn\ns\n\n))\n.\n\nThe second derivatives of the log posterior density are\n\nd2\n\ndµ2\nlog p(µ, log σ|y) = − n\n\nσ2\n\nd2\n\ndµd(log σ)\nlog p(µ, log σ|y) = −2ny − µ\n\nσ2\n\nd2\n\nd(log σ)2\nlog p(µ, log σ|y) = − 2\n\nσ2\n((n− 1)s2 + n(y − µ)2).\n\nThe matrix of second derivatives at the mode is then\n\n(\n−n/σ̂2 0\n\n0 −2n\n\n)\n. From (4.2),\n\nthe posterior distribution can be approximated as\n\np(µ, log σ|y) ≈ N\n\n((\nµ\n\nlog σ\n\n) ∣∣∣∣\n(\n\ny\nlog σ̂\n\n)\n,\n\n(\nσ̂2/n 0\n0 1/(2n)\n\n))\n.\n\nIf we had instead constructed the normal approximation in terms of p(µ, σ2), the sec-\nond derivative matrix would be multiplied by the Jacobian of the transformation from\nlog σ to σ2 and the mode would change slightly, to σ̃2 = n\n\nn+2 σ̂\n2. The two components,\n\n(µ, σ2), would still be independent in their approximate posterior distribution, and\np(σ2|y) ≈ N(σ2|σ̃2, 2σ̃4/(n+ 2)).\n\n84\n\nThis electronic edition is for non-commercial purposes only.\n\n4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of 0, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\nply) = N(8, [L(8)|~*), (4.2)\n\nwhere I(0) is the observed information,\n\n2\n\nd\nI(0) =— 762 log p(4|y).\n\nIf the mode, 6, is in the interior of parameter space, then the matrix I (6) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\n\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1,-.-,Ym be independent observations from a N(,07) distribution, and,\nfor simplicity, we assume a uniform prior density for (u,loga). We set up a normal\napproximation to the posterior distribution of (~,loga), which has the virtue of re-\nstricting o to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\n—((n— 1s? +n(9 — )?).\n\nlog p(y, log oly) = constant — nlog a — 5\no\n\nThe first derivatives are\n\nd ny = LL\nda log p(u,logaly) = mow)\nd (n —1)s* + n(¥ — pw)?\nl | —\nTogo) og p(t, log oly) n+ 53 ;\n\nfrom which the posterior mode is readily obtained as\n\nvue (ne (=).\n\nThe second derivatives of the log posterior density are\n\n2\n\nd n\nae srt logely) = —Ta\na? y-u\n—— | I = -2\nTud(logo) og p(t, log aly) 13\nd? 2 ne:\nTloga)2 SPH log aly) = ~aa((n— 1s + n(y — 1)”).\n. ar . —n/6* 0\nThe matrix of second derivatives at the mode is then 6 on | From (4.2),\n\nthe posterior distribution can be approximated as\n\nP(L, log oly) ~n(( logo ) ( lone ).( -_ 1/(2n) ))\n\nIf we had instead constructed the normal approximation in terms of p(j1,07), the sec-\n\nond derivative matrix would be multiplied by the Jacobian of the transformation from\n\nlog o to o? and the mode would change slightly, to ¢? = age. The two components,\n\n(u,07), would still be independent in their approximate posterior distribution, and\np(o?|y) © N(o?|a?, 264/(n + 2)).\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.1. NORMAL APPROXIMATIONS TO THE POSTERIOR DISTRIBUTION 85\n\nInterpretation of the posterior density function relative to its maximum\n\nIn addition to its direct use as an approximation, the multivariate normal distribution pro-\nvides a benchmark for interpreting the posterior density function and contour plots. In the d-\ndimensional normal distribution, the logarithm of the density function is a constant plus a χ2\n\nd\n\ndistribution divided by −2. For example, the 95th percentile of the χ2\n10 density is 18.31, so if\n\na problem has d = 10 parameters, then approximately 95% of the posterior probability mass\nis associated with the values of θ for which p(θ|y) is no less than exp(−18.31/2) = 1.1×10−4\n\ntimes the density at the mode. Similarly, with d = 2 parameters, approximately 95% of the\nposterior mass corresponds to densities above exp(−5.99/2) = 0.05, relative to the density\nat the mode. In a two-dimensional contour plot of a posterior density (for example, Figure\n3.3a), the 0.05 contour line thus includes approximately 95% of the probability mass.\n\nSummarizing posterior distributions by point estimates and standard errors\n\nThe asymptotic theory outlined in Section 4.2 shows that if n is large enough, a posterior\ndistribution can be approximated by a normal distribution. In many areas of application, a\nstandard inferential summary is the 95% interval obtained by computing a point estimate,\nθ̂, such as the maximum likelihood estimate (which is the posterior mode under a uniform\nprior density), plus or minus two standard errors, with the standard error estimated from\n\nthe information at the estimate, I(θ̂). A different asymptotic argument justifies the non-\nBayesian, frequentist interpretation of this summary, but in many simple situations both\ninterpretations hold. It is difficult to give general guidelines on when the normal approxi-\nmation is likely to be adequate in practice. From the Bayesian point of view, the accuracy\nin any given example can be directly determined by inspecting the posterior distribution.\n\nIn many cases, convergence to normality of the posterior distribution for a parameter\nθ can be dramatically improved by transformation. If φ is a continuous transformation\nof θ, then both p(φ|y) and p(θ|y) approach normal distributions, but the closeness of the\napproximation for finite n can vary substantially with the transformation chosen.\n\nData reduction and summary statistics\n\nUnder the normal approximation, the posterior distribution is summarized by its mode, θ̂,\nand the curvature of the posterior density, I(θ̂); that is, asymptotically, these are sufficient\nstatistics. In the examples at the end of the next chapter, we shall see that it can be\nconvenient to summarize ‘local-level’ or ‘individual-level’ data from a number of sources by\ntheir normal-theory sufficient statistics. This approach using summary statistics allows the\nrelatively easy application of hierarchical modeling techniques to improve each individual\nestimate. For example, in Section 5.5, each of a set of eight experiments is summarized by\na point estimate and a standard error estimated from an earlier linear regression analysis.\nUsing summary statistics is clearly most reasonable when posterior distributions are close\nto normal; the approach can otherwise discard important information and lead to erroneous\ninferences.\n\nLower-dimensional normal approximations\n\nFor a finite sample size n, the normal approximation is typically more accurate for condi-\ntional and marginal distributions of components of θ than for the full joint distribution. For\nexample, if a joint distribution is multivariate normal, all its margins are normal, but the\nconverse is not true. Determining the marginal distribution of a component of θ is equiva-\nlent to averaging over all the other components of θ, and averaging a family of distributions\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n86 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nFigure 4.1 (a) Contour plot of the normal approximation to the posterior distribution of the pa-\nrameters in the bioassay example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the\nmode. Compare to Figure 3.3a. (b) Scatterplot of 1000 draws from the normal approximation to\nthe posterior distribution. Compare to Figure 3.3b.\n\ngenerally brings them closer to normality, by the same logic that underlies the central limit\ntheorem.\n\nThe normal approximation for the posterior distribution of a low-dimensional θ is often\nperfectly acceptable, especially after appropriate transformation. If θ is high-dimensional,\ntwo situations commonly arise. First, the marginal distributions of many individual com-\nponents of θ can be approximately normal; inference about any one of these parameters,\ntaken individually, can then be well summarized by a point estimate and a standard error.\nSecond, it is possible that θ can be partitioned into two subvectors, θ = (θ1, θ2), for which\np(θ2|y) is not necessarily close to normal, but p(θ1|θ2, y) is, perhaps with mean and variance\nthat are functions of θ2. The approach of approximation using conditional distributions is\noften useful, and we consider it more systematically in Section 13.5. Lower-dimensional\napproximations are increasingly popular, for example in computation for latent Gaussian\nmodels.\n\nFinally, approximations based on the normal distribution are often useful for debugging\na computer program or checking a more elaborate method for approximating the posterior\ndistribution.\n\nExample. Bioassay experiment (continued)\nWe illustrate the normal approximation for the model and data from the bioassay\nexperiment of Section 3.7. The sample size in this experiment is relatively small, only\ntwenty animals in all, and we find that the normal approximation is close to the exact\nposterior distribution but with important differences.\n\nThe normal approximation to the joint posterior distribution of (α, β). To begin, we\ncompute the mode of the posterior distribution (using a logistic regression program)\nand the normal approximation (4.2) evaluated at the mode. The posterior mode of\n(α, β) is the same as the maximum likelihood estimate because we have assumed a\nuniform prior density for (α, β). Figure 4.1 shows a contour plot of the bivariate normal\napproximation and a scatterplot of 1000 draws from this approximate distribution.\nThe plots resemble the plots of the actual posterior distribution in Figure 3.3 but\nwithout the skewness in the upper right corner of the earlier plots. The effect of\nthe skewness is apparent when comparing the mean of the normal approximation,\n(α, β) = (0.8, 7.7), to the mean of the actual posterior distribution, (α, β) = (1.4, 11.9),\ncomputed from the simulations displayed in Figure 3.3b.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.2. LARGE-SAMPLE THEORY 87\n\nFigure 4.2 (a) Histogram of the simulations of LD50, conditional on β > 0, in the bioassay example\nbased on the normal approximation p(α, β|y). The wide tails of the histogram correspond to values\nof β close to 0. Omitted from this histogram are five simulation draws with values of LD50 less\nthan −2 and four draws with values greater than 2; the extreme tails are truncated to make the\nhistogram visible. The values of LD50 for the 950 simulation draws corresponding to β > 0 had a\nrange of [−12.4, 5.4]. Compare to Figure 3.4. (b) Histogram of the central 95% of the distribution.\n\nThe posterior distribution for the LD50 using the normal approximation on (α, β).\nFlaws of the normal approximation. The same set of 1000 draws from the normal\napproximation can be used to estimate the probability that β is positive and the\nposterior distribution of the LD50, conditional on β being positive. Out of the 1000\nsimulation draws, 950 had positive values of β, yielding the estimate Pr(β > 0) = 0.95,\na different result than from the exact distribution, where Pr(β > 0) > 0.999. Con-\ntinuing with the analysis based on the normal approximation, we compute the LD50\nas −α/β for each of the 950 draws with β > 0; Figure 4.2a presents a histogram of\nthe LD50 values, excluding some extreme values in both tails. (If the entire range of\nthe simulations were included, the shape of the distribution would be nearly impos-\nsible to see.) To get a better picture of the center of the distribution, we display in\nFigure 4.2b a histogram of the middle 95% of the 950 simulation draws of the LD50.\nThe histograms are centered in approximately the same place as Figure 3.4 but with\nsubstantially more variation, due to the possibility that β is close to zero.\nIn summary, posterior inferences based on the normal approximation here are roughly\nsimilar to the exact results, but because of the small sample, the actual joint posterior\ndistribution is substantially more skewed than the large-sample approximation, and\nthe posterior distribution of the LD50 actually has much shorter tails than implied by\nusing the joint normal approximation. Whether or not these differences imply that\nthe normal approximation is inadequate for practical use in this example depends on\nthe ultimate aim of the analysis.\n\n4.2 Large-sample theory\n\nTo understand why the normal approximation is often reasonable, we review some theory\nof how the posterior distribution behaves as the amount of data, from some fixed sampling\ndistribution, increases.\n\nNotation and mathematical setup\n\nThe basic tool of large sample Bayesian inference is asymptotic normality of the posterior\ndistribution: as more and more data arrive from the same underlying process, the posterior\ndistribution of the parameter vector approaches multivariate normality, even if the true\ndistribution of the data is not within the parametric family under consideration. Mathe-\nmatically, the results apply most directly to observations y1, . . . , yn that are independent\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n88 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\noutcomes sampled from a common distribution, f(y). In many situations, the notion of a\n‘true’ underlying distribution, f(y), for the data is difficult to interpret, but it is necessary\nin order to develop the asymptotic theory. Suppose the data are modeled by a parametric\nfamily, p(y|θ), with a prior distribution p(θ). In general, the data points yi and the parame-\nter θ can be vectors. If the true data distribution is included in the parametric family—that\nis, if f(y) = p(y|θ0) for some θ0—then, in addition to asymptotic normality, the property\nof consistency holds: the posterior distribution converges to a point mass at the true pa-\nrameter value, θ0, as n→∞. When the true distribution is not included in the parametric\nfamily, there is no longer a true value θ0, but its role in the theoretical result is replaced by\na value θ0 that makes the model distribution, p(y|θ), closest to the true distribution, f(y),\nin a technical sense involving Kullback-Leibler divergence, as is explained in Appendix B.\n\nIn discussing the large-sample properties of posterior distributions, the concept of Fisher\ninformation, J(θ), introduced as (2.20) in Section 2.8 in the context of Jeffreys’ prior dis-\ntributions, plays an important role.\n\nAsymptotic normality and consistency\n\nThe fundamental mathematical result given in Appendix B shows that, under some regu-\nlarity conditions (notably that the likelihood is a continuous function of θ and that θ0 is\nnot on the boundary of the parameter space), as n → ∞, the posterior distribution of θ\napproaches normality with mean θ0 and variance (nJ(θ0))\n\n−1. At its simplest level, this\nresult can be understood in terms of the Taylor series expansion (4.1) of the log posterior\ndensity centered about the posterior mode. A preliminary result shows that the posterior\nmode is consistent for θ0, so that as n → ∞, the mass of the posterior distribution p(θ|y)\nbecomes concentrated in smaller and smaller neighborhoods of θ0, and the distance |θ̂− θ0|\napproaches zero.\n\nFurthermore, we can rewrite the coefficient of the quadratic term in (4.1):\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n=\n\n[\nd2\n\ndθ2\nlog p(θ)\n\n]\n\nθ=θ̂\n\n+\n\nn∑\n\ni=1\n\n[\nd2\n\ndθ2\nlog p(yi|θ)\n\n]\n\nθ=θ̂\n\n.\n\nThis coefficient is a single term for the prior plus the sum of n likelihood terms, each of\nwhose expected value under the true sampling distribution of yi, p(y|θ0), is approximately\n\n−J(θ0), as long as θ̂ is close to θ0 (we are assuming now that f(y) = p(y|θ0) for some θ0).\nTherefore, for large n, the curvature of the log posterior density can be approximated by\nthe Fisher information, evaluated at either θ̂ or θ0 (where only the former is available in\npractice).\n\nIn summary, in the limit of large n, in the context of a specified family of models,\nthe posterior mode, θ̂, approaches θ0, and the curvature (the observed information or the\n\nnegative of the coefficient of the second term in the Taylor expansion) approaches nJ(θ̂) or\nnJ(θ0). In addition, as n → ∞, the likelihood dominates the prior distribution, so we can\njust use the likelihood alone to obtain the mode and curvature for the normal approximation.\nMore precise statements of the theorems and outlines of proofs appear in Appendix B.\n\nLikelihood dominating the prior distribution\n\nThe asymptotic results formalize the notion that the importance of the prior distribution\ndiminishes as the sample size increases. One consequence of this result is that in problems\nwith large sample sizes we need not work especially hard to formulate a prior distribution\nthat accurately reflects all available information. When sample sizes are small, the prior\ndistribution is a critical part of the model specification.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.3. COUNTEREXAMPLES TO THE THEOREMS 89\n\n4.3 Counterexamples to the theorems\n\nA good way to understand the limitations of the large-sample results is to consider cases in\nwhich the theorems fail. The normal distribution is usually helpful as a starting approxi-\nmation, but one must examine deviations, especially with unusual parameter spaces and in\nthe extremes of the distribution. The counterexamples to the asymptotic theorems gener-\nally correspond to situations in which the prior distribution has an impact on the posterior\ninference, even in the limit of infinite sample sizes.\n\nUnderidentified models and nonidentified parameters. The model is underidentified given\ndata y if the likelihood, p(y|θ), is equal for a range of values of θ. This may also be called\na flat likelihood (although that term is sometimes also used for likelihoods for parameters\nthat are only weakly identified by the data—so the likelihood function is not strictly equal\nfor a range of values, only almost so). Under such a model, there is no single point θ0 to\nwhich the posterior distribution can converge.\n\nFor example, consider the model,\n(\nu\nv\n\n)\n∼ N\n\n((\n0\n0\n\n)\n,\n\n(\n1 ρ\nρ 1\n\n))\n,\n\nin which only one of u or v is observed from each pair (u, v). Here, the parameter ρ is\nnonidentified. The data supply no information about ρ, so the posterior distribution of ρ is\nthe same as its prior distribution, no matter how large the dataset is.\n\nThe only solution to a problem of nonidentified or underidentified parameters is to\nrecognize that the problem exists and, if there is a desire to estimate these parameters\nmore precisely, gather further information that can enable the parameters to be estimated\n(either from future data collection or from external information that can inform a prior\ndistribution).\n\nNumber of parameters increasing with sample size. In complicated problems, there can\nbe large numbers of parameters, and then we need to distinguish between different types\nof asymptotics. If, as n increases, the model changes so that the number of parameters\nincreases as well, then the simple results outlined in Sections 4.1 and 4.2, which assume a\nfixed model class p(yi|θ), do not apply. For example, sometimes a parameter is assigned\nfor each sampling unit in a study; for example, yi ∼ N(θi, σ\n\n2). The parameters θi generally\ncannot be estimated consistently unless the amount of data collected from each sampling\nunit increases along with the number of units. In nonparametric models such as Gaussian\nprocesses (see Chapter 21) there can be a new latent parameter corresponding to each data\npoint.\n\nAs with underidentified parameters, the posterior distribution for θi will not converge to\na point mass if new data do not bring enough information about θi. Here, the posterior dis-\ntribution will not in general converge to a point in the expanding parameter space (reflecting\nthe increasing dimensionality of θ), and its projection into any fixed space—for example,\nthe marginal posterior distribution of any particular θi—will not necessarily converge to a\npoint either.\n\nAliasing. Aliasing is a special case of underidentified parameters in which the same likeli-\nhood function repeats at a discrete set of points. For example, consider the following normal\nmixture model with independent and identically distributed data y1, . . . , yn and parameter\nvector θ = (µ1, µ2, σ\n\n2\n1 , σ\n\n2\n2 , λ):\n\np(yi|µ1, µ2, σ\n2\n1 , σ\n\n2\n2 , λ)=λ\n\n1√\n2π σ1\n\ne\n− 1\n\n2σ2\n1\n\n(yi−µ1)\n2\n\n+ (1 − λ) 1√\n2π σ2\n\ne\n− 1\n\n2σ2\n2\n\n(yi−µ2)\n2\n\n.\n\nIf we interchange each of (µ1, µ2) and (σ2\n1 , σ\n\n2\n2), and replace λ by (1 − λ), the likelihood of\n\nthe data remains the same. The posterior distribution of this model generally has at least\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n90 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ntwo modes and consists of a (50%, 50%) mixture of two distributions that are mirror images\nof each other; it does not converge to a single point no matter how large the dataset is.\n\nIn general, the problem of aliasing is eliminated by restricting the parameter space so\nthat no duplication appears; in the above example, the aliasing can be removed by restricting\nµ1 to be less than or equal to µ2.\n\nUnbounded likelihoods. If the likelihood function is unbounded, then there might be no\nposterior mode within the parameter space, invalidating both the consistency results and\nthe normal approximation. For example, consider the previous normal mixture model; for\nsimplicity, assume that λ is known (and not equal to 0 or 1). If we set µ1 = yi for any\narbitrary yi, and let σ2\n\n1 → 0, then the likelihood approaches infinity. As n → ∞, the\nnumber of modes of the likelihood increases. If the prior distribution is uniform on σ2\n\n1 and\nσ2\n2 in the region near zero, there will be likewise an increasing number of posterior modes,\n\nwith no corresponding normal approximations. A prior distribution proportional to σ−2\n1 σ−2\n\n2\n\njust makes things worse because this puts more probability near zero, causing the posterior\ndistribution to explode even faster at zero.\n\nIn general, this problem should arise rarely in practice, because the poles of an un-\nbounded likelihood correspond to unrealistic conditions in a model. The problem can be\nsolved by restricting to a plausible set of distributions. When the problem occurs for\nvariance components near zero, it can be resolved in various ways, such as using a prior\ndistribution that declines to zero at the boundary or by assigning an informative prior\ndistribution to the ratio of the variance parameters.\n\nImproper posterior distributions. If the unnormalized posterior density, obtained by multi-\nplying the likelihood by a ‘formal’ prior density representing an improper prior distribution,\nintegrates to infinity, then the asymptotic results, which rely on probabilities summing to\n1, do not follow. An improper posterior distribution cannot occur except with an improper\nprior distribution.\n\nA simple example arises from combining a Beta(0, 0) prior distribution for a binomial\nproportion with data consisting of n successes and 0 failures. More subtle examples, with\nhierarchical binomial and normal models, are discussed in Sections 5.3 and 5.4.\n\nThe solution to this problem is clear. An improper prior distribution is only a convenient\napproximation, and if it does not give rise to a proper posterior distribution then the sought\nconvenience is lost. In this case a proper prior distribution is needed, or at least an improper\nprior density that when combined with the likelihood has a finite integral.\n\nPrior distributions that exclude the point of convergence. If p(θ0) = 0 for a discrete param-\neter space, or if p(θ) = 0 in a neighborhood about θ0 for a continuous parameter space, then\nthe convergence results, which are based on the likelihood dominating the prior distribution,\ndo not hold. The solution is to give positive probability density in the prior distribution to\nall values of θ that are even remotely plausible.\n\nConvergence to the edge of parameter space. If θ0 is on the boundary of the parameter\nspace, then the Taylor series expansion must be truncated in some directions, and the\nnormal distribution will not necessarily be appropriate, even in the limit.\n\nFor example, consider the model, yi ∼ N(θ, 1), with the restriction θ ≥ 0. Suppose that\nthe model is accurate, with θ = 0 as the true value. The posterior distribution for θ is\nnormal, centered at y, truncated to be positive. The shape of the posterior distribution for\nθ, in the limit as n → ∞, is half of a normal distribution, centered about 0, truncated to\nbe positive.\n\nFor another example, consider the same assumed model, but now suppose that the true\nθ is −1, a value outside the assumed parameter space. The limiting posterior distribution\nfor θ has a sharp spike at 0 with no resemblance to a normal distribution at all. The\nsolution in practice is to recognize the difficulties of applying the normal approximation if\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.4. FREQUENCY EVALUATIONS OF BAYESIAN INFERENCES 91\n\none is interested in parameter values near the edge of parameter space. More important,\none should give positive prior probability density to all values of θ that are even remotely\npossible, or in the neighborhood of remotely possible values.\n\nTails of the distribution. The normal approximation can hold for essentially all the mass\nof the posterior distribution but still not be accurate in the tails. For example, suppose\np(θ|y) is proportional to e−c|θ| as |θ| → ∞, for some constant c; by comparison, the normal\n\ndensity is proportional to e−cθ\n2\n\n. The distribution function still converges to normality, but\nfor any finite sample size n the approximation fails far out in the tail. As another example,\nconsider any parameter that is constrained to be positive. For any finite sample size, the\nnormal approximation will admit the possibility of the parameter being negative, because\nthe approximation is simply not appropriate at that point in the tail of the distribution,\nbut that point becomes farther and farther in the tail as n increases.\n\n4.4 Frequency evaluations of Bayesian inferences\n\nJust as the Bayesian paradigm can be seen to justify simple ‘classical’ techniques, the\nmethods of frequentist statistics provide a useful approach for evaluating the properties of\nBayesian inferences—their operating characteristics—when these are regarded as embedded\nin a sequence of repeated samples. We have already used this notion in discussing the ideas\nof consistency and asymptotic normality. The notion of stable estimation, which says that\nfor a fixed model, the posterior distribution approaches a point as more data arrive—\nleading, in the limit, to inferential certainty—is based on the idea of repeated sampling.\nIt is certainly appealing that if the hypothesized family of probability models contains the\ntrue distribution (and assigns it a nonzero prior density), then as more information about\nθ arrives, the posterior distribution converges to the true value of θ.\n\nLarge-sample correspondence\n\nSuppose that the normal approximation (4.2) for the posterior distribution of θ holds; then\nwe can transform to the standard multivariate normal:\n\n[I(θ̂)]1/2(θ − θ̂) | y ∼ N(0, I), (4.3)\n\nwhere θ̂ is the posterior mode and [I(θ̂)]1/2 is any matrix square root of I(θ̂). In addition,\n\nθ̂ → θ0, and so we could just as well write the approximation in terms of I(θ0). If the true\ndata distribution is included in the class of models, so that f(y) ≡ p(y|θ) for some θ, then\nin repeated sampling with fixed θ, in the limit n→∞, it can be proved that\n\n[I(θ̂)]1/2(θ − θ̂) | θ ∼ N(0, I), (4.4)\n\na result from classical statistical theory that is generally proved for θ̂ equal to the maximum\nlikelihood estimate but is easily extended to the case with θ̂ equal to the posterior mode.\nThese results mean that, for any function of (θ− θ̂), the posterior distribution derived from\n(4.3) is asymptotically the same as the repeated sampling distribution derived from (4.4).\nThus, for example, a 95% central posterior interval for θ will cover the true value 95% of\nthe time under repeated sampling with any fixed true θ.\n\nPoint estimation, consistency, and efficiency\n\nIn the Bayesian framework, obtaining an ‘estimate’ of θ makes most sense in large samples\nwhen the posterior mode, θ̂, is the obvious center of the posterior distribution of θ and the\nuncertainty conveyed by nI(θ̂) is so small as to be practically unimportant. More generally,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n92 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nhowever, in smaller samples, it is inappropriate to summarize inference about θ by one\nvalue, especially when the posterior distribution of θ is more variable or even asymmetric.\nFormally, by incorporating loss functions in a decision-theoretic context (see Section 9.1\nand Exercise 9.6), one can define optimal point estimates; for the purposes of Bayesian data\nanalysis, however, we believe that representation of the full posterior distribution (as, for\nexample, with 50% and 95% central posterior intervals) is more useful. In many problems,\nespecially with large samples, a point estimate and its estimated standard error are adequate\nto summarize a posterior inference, but we interpret the estimate as an inferential summary,\nnot as the solution to a decision problem. In any case, the large-sample frequency properties\nof any estimate can be evaluated, without consideration of whether the estimate was derived\nfrom a Bayesian analysis.\n\nA point estimate is said to be consistent in the sampling theory sense if, as samples\nget larger, it converges to the true value of the parameter that it is asserted to estimate.\nThus, if f(y) ≡ p(y|θ0), then a point estimate θ̂ of θ is consistent if its sampling distribution\nconverges to a point mass at θ0 as the data sample size n increases (that is, considering\n\nθ̂ as a function of y, which is a random variable conditional on θ0). A closely related\n\nconcept is asymptotic unbiasedness, where (E(θ̂|θ0) − θ0)/sd(θ̂|θ0) converges to 0 (once\n\nagain, considering θ̂(y) as a random variable whose distribution is determined by p(y|θ0)).\nWhen the truth is included in the family of models being fitted, the posterior mode θ̂, and\nalso the posterior mean and median, are consistent and asymptotically unbiased under mild\nregularity conditions.\n\nA point estimate θ̂ is said to be efficient if there exists no other function of y that\nestimates θ with lower mean squared error, that is, if the expression E((θ̂ − θ0)2|θ0) is at\n\nits optimal, lowest value. More generally, the efficiency of θ̂ is the optimal mean squared\nerror divided by the mean squared error of θ̂. An estimate is asymptotically efficient if its\nefficiency approaches 1 as the sample size n → ∞. Under mild regularity conditions, the\ncenter of the posterior distribution (defined, for example, by the posterior mean, median,\nor mode) is asymptotically efficient.\n\nConfidence coverage\n\nIf a region C(y) includes θ0 at least 100(1 − α)% of the time (given any value of θ0) in\nrepeated samples, then C(y) is called a 100(1− α)% confidence region for the parameter\nθ. The word ‘confidence’ is carefully chosen to distinguish such intervals from probability\nintervals and to convey the following behavioral meaning: if one chooses α to be small\nenough (for example, 0.05 or 0.01), then since confidence regions cover the truth in at least\n(1 − α) of their applications, one should be confident in each application that the truth\nis within the region and therefore act as if it is. We saw previously that asymptotically a\n100(1− α)% central posterior interval for θ has the property that, in repeated samples of\ny, 100(1− α)% of the intervals include the value θ0.\n\n4.5 Bayesian interpretations of other statistical methods\n\nWe consider three levels at which Bayesian statistical methods can be compared with other\nmethods. First, as we have already indicated, Bayesian methods are often similar to other\nstatistical approaches in problems involving large samples from a fixed probability model.\nSecond, even for small samples, many statistical methods can be considered as approxi-\nmations to Bayesian inferences based on particular prior distributions; as a way of under-\nstanding a statistical procedure, it is often useful to determine the implicit underlying prior\ndistribution. Third, some methods from classical statistics (notably hypothesis testing)\ncan give results that differ greatly from those given by Bayesian methods. In this section,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 93\n\nwe briefly consider several statistical concepts—point and interval estimation, likelihood\ninference, unbiased estimation, frequency coverage of confidence intervals, hypothesis test-\ning, multiple comparisons, nonparametric methods, and the jackknife and bootstrap—and\ndiscuss their relation to Bayesian methods.\n\nOne way to develop possible models is to examine the interpretation of crude data-\nanalytic procedures as approximations to Bayesian inference under specific models. For\nexample, a widely used technique in sample surveys is ratio estimation, in which, for exam-\nple, given data from a simple random sample, one estimates R = y/x by yobs/xobs, in the\nnotation of Chapter 8. It can be shown that this estimate corresponds to a summary of\na Bayesian posterior inference given independent observations yi|xi ∼ N(Rxi, σ\n\n2xi) and a\nnoninformative prior distribution. Ratio estimates can be useful in a wide variety of cases\nin which this model does not hold, but when the data deviate greatly from this model, the\nratio estimate generally is not appropriate.\n\nFor another example, standard methods of selecting regression predictors, based on\n‘statistical significance,’ correspond roughly to Bayesian analyses under exchangeable prior\ndistributions on the coefficients in which the prior distribution of each coefficient is a mixture\nof a peak at zero and a widely spread distribution, as we discuss further in Section 14.6. We\nbelieve that understanding this correspondence suggests when such models can be usefully\napplied and how they can be improved. Often, in fact, such procedures can be improved\nby including additional information, for example, in problems involving large numbers of\npredictors, by clustering regression coefficients that are likely to be similar into batches.\n\nMaximum likelihood and other point estimates\n\nFrom the perspective of Bayesian data analysis, we can often interpret classical point esti-\nmates as exact or approximate posterior summaries based on some implicit full probability\nmodel. In the limit of large sample size, in fact, we can use asymptotic theory to con-\nstruct a theoretical Bayesian justification for classical maximum likelihood inference. In the\nlimit (assuming regularity conditions), the maximum likelihood estimate, θ̂, is a sufficient\nstatistic—and so is the posterior mode, mean, or median. That is, for large enough n,\nthe maximum likelihood estimate (or any of the other summaries) supplies essentially all\nthe information about θ available from the data. The asymptotic irrelevance of the prior\ndistribution can be taken to justify the use of convenient noninformative prior models.\n\nIn repeated sampling with θ = θ0,\n\np(θ̂(y)|θ=θ0) ≈ N(θ̂(y)|θ0, (nJ(θ0))−1);\n\nthat is, the sampling distribution of θ̂(y) is approximately normal with mean θ0 and precision\n\nnJ(θ0), where for clarity we emphasize that θ̂ is a function of y. Assuming that the prior\ndistribution is locally uniform (or continuous and nonzero) near the true θ, the simple\nanalysis of the normal mean (Section 3.5) shows that the posterior Bayesian inference is\n\np(θ|θ̂) ≈ N(θ|θ̂, (nJ(θ̂))−1).\n\nThis result appears directly from the asymptotic normality theorem, but deriving it indi-\nrectly through Bayesian inference given θ̂ gives insight into a Bayesian rationale for classical\nasymptotic inference based on point estimates and standard errors.\n\nFor finite n, the above approach is inefficient or wasteful of information to the extent\nthat θ̂ is not a sufficient statistic. When the number of parameters is large, the consistency\nresult is often not helpful, and noninformative prior distributions are hard to justify. As\ndiscussed in Chapter 5, hierarchical models are preferable when dealing with a large number\nof parameters since then their common distribution can be estimated from data. In addi-\ntion, any method of inference based on the likelihood alone can be improved if real prior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n94 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ninformation is available that is strong enough to contribute substantially to that contained\nin the likelihood function.\n\nUnbiased estimates\n\nSome non-Bayesian statistical methods place great emphasis on unbiasedness as a desirable\nprinciple of estimation, and it is intuitively appealing that, over repeated sampling, the mean\n(or perhaps the median) of a parameter estimate should be equal to its true value. Formally,\n\nan estimate θ̂(y) is called unbiased if E(θ̂(y)|θ) = θ for any value of θ, where this expectation\nis taken over the data distribution, p(y|θ). From a Bayesian perspective, the principle of\nunbiasedness is reasonable in the limit of large samples (see page 92) but otherwise is\npotentially misleading. The major difficulties arise when there are many parameters to be\nestimated and our knowledge or partial knowledge of some of these parameters is clearly\nrelevant to the estimation of others. Requiring unbiased estimates will often lead to relevant\ninformation being ignored (as we discuss with hierarchical models in Chapter 5). In sampling\ntheory terms, minimizing bias will often lead to counterproductive increases in variance.\n\nOne general problem with unbiasedness (and point estimation in general) is that it is\noften not possible to estimate several parameters at once in an even approximately unbiased\nmanner. For example, unbiased estimates of θ1, . . . , θJ yield an upwardly biased estimate\nof the variance of the θj ’s (except in the trivial case in which the θj’s are known exactly).\n\nAnother problem with the principle of unbiasedness arises when treating a future ob-\nservable value as a parameter in prediction problems.\n\nExample. Prediction using regression\nConsider the problem of estimating θ, the height of an adult daughter, given y, her\nmother’s height. For simplicity, assume that the heights of mothers and daughters\nare jointly normally distributed, with known equal means of 160 centimeters, equal\nvariances, and a known correlation of 0.5. Conditioning on the known value of y (in\nother words, using Bayesian inference), the posterior mean of θ is\n\nE(θ|y) = 160 + 0.5(y − 160). (4.5)\n\nThe posterior mean is not, however, an unbiased estimate of θ, in the sense of repeated\nsampling of y given a fixed θ. Given the daughter’s height, θ, the mother’s height, y,\nhas mean E(y|θ) = 160+0.5(θ− 160). Thus, under repeated sampling of y given fixed\nθ, the posterior mean (4.5) has expectation 160+ 0.25(θ− 160) and is biased towards\nthe grand mean of 160. In contrast, the estimate\n\nθ̂ = 160 + 2(y − 160)\n\nis unbiased under repeated sampling of y, conditional on θ. Unfortunately, the esti-\nmate θ̂ makes no sense for values of y not equal to 160; for example, if a mother is 10\ncentimeters taller than average, it estimates her daughter to be 20 centimeters taller\nthan average!\nIn this simple example, in which θ has an accepted population distribution, a sensible\nnon-Bayesian statistician would not use the unbiased estimate θ̂; instead, this problem\nwould be classified as ‘prediction’ rather than ‘estimation,’ and procedures would not\nbe evaluated conditional on the random variable θ. The example illustrates, however,\nthe limitations of unbiasedness as a general principle: it requires unknown quantities to\nbe characterized either as ‘parameters’ or ‘predictions,’ with different implications for\nestimation but no clear substantive distinction. Chapter 5 considers similar situations\nin which the population distribution of θ must be estimated from data rather than\nconditioning on a particular value.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 95\n\nThe important principle illustrated by the example is that of regression to the mean:\nfor any given mother, the expected value of her daughter’s height lies between her\nmother’s height and the population mean. This principle was fundamental to the\noriginal use of the term ‘regression’ for this type of analysis by Galton in the late\n19th century. In many ways, Bayesian analysis can be seen as a logical extension of\nthe principle of regression to the mean, ensuring that proper weighting is made of\ninformation from different sources.\n\nConfidence intervals\n\nEven in small samples, Bayesian (1−α) posterior intervals often have close to (1−α) con-\nfidence coverage under repeated samples conditional on θ. But there are some confidence\nintervals, derived purely from sampling-theory arguments, that differ considerably from\nBayesian probability intervals. From our perspective these intervals are of doubtful value.\nFor example, many authors have shown that a general theory based on unconditional behav-\nior can lead to clearly counterintuitive results, for example, the possibilities of confidence\nintervals with zero or infinite length. A simple example is the confidence interval that is\nempty 5% of the time and contains all of the real line 95% of the time: this always contains\nthe true value (of any real-valued parameter) in 95% of repeated samples. Such examples\ndo not imply that there is no value in the concept of confidence coverage but rather show\nthat coverage alone is not a sufficient basis on which to form reasonable inferences.\n\nHypothesis testing\n\nThe perspective of this book has little role for the non-Bayesian concept of hypothesis\ntests, especially where these relate to point null hypotheses of the form θ = θ0. In order\nfor a Bayesian analysis to yield a nonzero probability for a point null hypothesis, it must\nbegin with a nonzero prior probability for that hypothesis; in the case of a continuous\nparameter, such a prior distribution (comprising a discrete mass, of say 0.5, at θ0 mixed\nwith a continuous density elsewhere) usually seems contrived. In fact, most of the difficulties\nin interpreting hypothesis tests arise from the artificial dichotomy that is required between\nθ = θ0 and θ 6= θ0. Difficulties related to this dichotomy are widely acknowledged from all\nperspectives on statistical inference. In problems involving a continuous parameter θ (say\nthe difference between two means), the hypothesis that θ is exactly zero is rarely reasonable,\nand it is of more interest to estimate a posterior distribution or a corresponding interval\nestimate of θ. For a continuous parameter θ, the question ‘Does θ equal 0?’ can generally\nbe rephrased more usefully as ‘What is the posterior distribution for θ?’\n\nIn various simple one-sided hypothesis tests, conventional p-values may correspond with\nposterior probabilities under noninformative prior distributions. For example, suppose we\nobserve y = 1 from the model y ∼ N(θ, 1), with a uniform prior density on θ. One cannot\n‘reject the hypothesis’ that θ = 0: the one-sided p-value is 0.16 and the two-sided p-value\nis 0.32, both greater than the conventionally accepted cutoff value of 0.05 for ‘statistical\nsignificance.’ On the other hand, the posterior probability that θ > 0 is 84%, which is a\nmore satisfactory and informative conclusion than the dichotomous verdict ‘reject’ or ‘do\nnot reject.’\n\nIn contrast to the problem of making inference about a parameter within a particular\nmodel, we do find a form of hypothesis test to be useful when assessing the goodness of fit of\na probability model. In the Bayesian framework, it is useful to check a model by comparing\nobserved data to possible predictive outcomes, as we discuss in detail in Chapter 6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n96 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nMultiple comparisons and multilevel modeling\n\nConsider a problem with independent measurements, yj ∼ N(θj , 1), on each of J parameters,\nin which the goal is to detect differences among and ordering of the continuous parame-\nters θj . Several competing multiple comparisons procedures have been derived in classical\nstatistics, with rules about when various θj ’s can be declared ‘significantly different.’ In\nthe Bayesian approach, the parameters have a joint posterior distribution. One can com-\npute the posterior probability of each of the J ! orderings if desired. If there is posterior\nuncertainty in the ordering, several permutations will have substantial probabilities, which\nis a more reasonable conclusion than producing a list of θj ’s that can be declared different\n(with the false implication that other θj ’s may be exactly equal). With J large, the exact\nordering is probably not important, and it might be more reasonable to give a posterior\nmedian and interval estimate of the quantile of each θj in the population.\n\nWe prefer to handle multiple comparisons problems using hierarchical models, as we\nshall illustrate in a comparison of treatment effects in eight schools in Section 5.5 (see also\nExercise 5.3). Hierarchical modeling automatically partially pools estimates of different θj ’s\ntoward each other when there is little evidence for real variation. As a result, this Bayesian\nprocedure automatically addresses the key concern of classical multiple comparisons analy-\nsis, which is the possibility of finding large differences as a byproduct of searching through\nso many possibilities. For example, in the educational testing example, the eight schools\ngive 8 · 7/2 = 28 possible comparisons, and none turn out to be close to ‘statistically sig-\nnificant’ (in the sense that zero is contained within the 95% intervals for all the differences\nin effects between pairs of schools), which makes sense since the between-school variation\n(the parameter τ in that model) is estimated to be low.\n\nNonparametric methods, permutation tests, jackknife, bootstrap\n\nMany non-Bayesian methods have been developed that avoid complete probability models,\neven at the sampling level. It is difficult to evaluate many of these from a Bayesian point\nof view. For instance, hypothesis tests for comparing medians based on ranks do not have\ndirect counterparts in Bayesian inference; therefore it is hard to interpret the resulting es-\ntimates and p-values from a Bayesian point of view (for example, as posterior expectations,\nintervals, or probabilities for parameters or predictions of interest). In complicated prob-\nlems, there is often a degree of arbitrariness in the procedures used; for example there is\ngenerally no clear method for constructing a nonparametric inference or an estimator to\njackknife/bootstrap in hypothetical replications. Without a specified probability model,\nit is difficult to see how to test the assumptions underlying a particular nonparametric\nmethod. In such problems, we find it more satisfactory to construct a joint probability\ndistribution and check it against the data (as in Chapter 6) than to construct an estimator\nand evaluate its frequency properties. Nonparametric methods are useful to us as tools for\ndata summary and description that can help us to construct models or help us evaluate\ninferences from a completely different perspective.\n\nFrom a different direction, one might well say that Bayesian methods involve arbitrary\nchoices of models and are difficult to evaluate because in practice there will always be\nimportant aspects of a model that are impossible to check. Our purpose here is not to\ndismiss or disparage classical nonparametric methods but rather to put them in a Bayesian\ncontext to the extent this is possible.\n\nSome nonparametric methods such as permutation tests for experiments and sampling-\ntheory inference for surveys turn out to give similar results in simple problems to Bayesian\ninferences with noninformative prior distributions, if the Bayesian model is constructed\nto fit the data reasonably well. Such simple problems include balanced designs with no\nmissing data and surveys based on simple random sampling. When estimating several pa-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.6. BIBLIOGRAPHIC NOTE 97\n\nrameters at once or including explanatory variables in the analysis (using methods such as\nthe analysis of covariance or regression) or prior information on the parameters, the permu-\ntation/sampling theory methods give no direct answer, and this often provides considerable\npractical incentive to move to a model-based Bayesian approach.\n\nExample. The Wilcoxon rank test\nAnother connection can be made by interpreting nonparametric methods in terms\nof implicit models. For example, the Wilcoxon rank test for comparing two samples\n(y1, . . . , yny ) and (z1, . . . , znz) proceeds by first ranking each of the points in the com-\nbined data from 1 to n = ny +nz, then computing the difference between the average\nranks of the y’s and z’s, and finally computing the p-value of this difference by com-\nparing to a tabulated reference distribution calculated based on the assumption of\nrandom assignment of the n ranks. This can be formulated as a nonlinear transfor-\nmation that replaces each data point by its rank in the combined data, followed by\na comparison of the mean values of the two transformed samples. Even more clear\nwould be to transform the ranks 1, 2, . . . , n to quantiles 1\n\n2n ,\n3\n2n , . . . ,\n\n2n−1\n2n , so that the\n\ndifference between the two means can be interpreted as an average distance in the scale\nof the quantiles of the combined distribution. From the Central Limit Theorem, the\nmean difference is approximately normally distributed, and so classical normal-theory\nconfidence intervals can be interpreted as Bayesian posterior probability statements,\nas discussed at the beginning of this section.\nWe see two major advantages of expressing rank tests as approximate Bayesian infer-\nences. First, the Bayesian framework is more flexible than rank testing for handling\nthe complications that arise, for example, from additional information such as regres-\nsion predictors or from complications such as censored or truncated data. Second,\nsetting up the problem in terms of a nonlinear transformation reveals the general-\nity of the model-based approach—we are free to use any transformation that might\nbe appropriate for the problem, perhaps now treating the combined quantiles as a\nconvenient default choice.\n\n4.6 Bibliographic note\n\nRelatively little has been written on the practical implications of asymptotic theory for\nBayesian analysis. The overview by Edwards, Lindman, and Savage (1963) remains one of\nthe best and includes a detailed discussion of the principle of ‘stable estimation’ or when\nprior information can be satisfactorily approximated by a uniform density function. Much\nmore has been written comparing Bayesian and non-Bayesian approaches to inference, and\nwe have largely ignored the extensive philosophical and logical debates on this subject.\nSome good sources on the topic from the Bayesian point of view include Lindley (1958),\nPratt (1965), and Berger and Wolpert (1984). Jaynes (1976) discusses some disadvantages\nof non-Bayesian methods compared to a particular Bayesian approach.\n\nIn Appendix B we provide references to the asymptotic normality theory. The coun-\nterexamples presented in Section 4.3 have arisen, in various forms, in our own applied\nresearch. Berzuini et al. (1997) discuss Bayesian inference for sequential data problems, in\nwhich the posterior distribution changes as data arrive, thus approaching the asymptotic\nresults dynamically.\n\nAn example of the use of the normal approximation with small samples is provided by\nRubin and Schenker (1987), who approximate the posterior distribution of the logit of the\nbinomial parameter in a real application and evaluate the frequentist operating character-\nistics of their procedure; see also Agresti and Coull (1998). Clogg et al. (1991) provide\nadditional discussion of this approach in a more complicated setting.\n\nMorris (1983) and Rubin (1984) discuss, from two different standpoints, the concept\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n98 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nof evaluating Bayesian procedures by examining long-run frequency properties (such as\ncoverage of 95% confidence intervals). An example of frequency evaluation of Bayesian\nprocedures in an applied problem is given by Zaslavsky (1993).\n\nKrantz (1999) discusses the strengths and weaknesses of p-values as used in statistical\ndata analysis in practice. Discussions of the role of p-values in Bayesian inference appear\nin Bayarri and Berger (1998, 2000). Earlier work on the Bayesian analysis of hypothesis\ntesting and the problems of interpreting conventional p-values is provided by Berger and\nSellke (1987), which contains a lively discussion and many further references. Gelman\n(2008a) and discussants provide a more recent airing of arguments for and against Bayesian\nstatistics. Gelman (2006b) compares Bayesian inference and the more generalized approach\nknown as belief functions (Dempster, 1967, 1968) using a simple toy example.\n\nGreenland and Poole (2013) and Gelman (2013a) present some more recent discussions\nof the relevance of classical p-values in Bayesian inference.\n\nA simple and pragmatic discussion of the need to consider Bayesian ideas in hypothesis\ntesting in a biostatistical context is given by Browner and Newman (1987), and further dis-\ncussion of the role of Bayesian thinking in medical statistics appears in Goodman (1999a, b)\nand Sterne and Smith (2001). Gelman and Tuerlinckx (2000), Efron and Tibshirani (2002),\nand Gelman, Hill, and Yajima (2012) give a Bayesian perspective on multiple comparisons\nin the context of hierarchical modeling.\n\nStigler (1983) discusses the similarity between Bayesian inference and regression predic-\ntion that we mention in our critique of unbiasedness in Section 4.5; Stigler (1986) discusses\nGalton’s use of regression.\n\nSequential monitoring and analysis of clinical trials in medical research is an important\narea of practical application that has been dominated by frequentist thinking but has re-\ncently seen considerable discussion of the merits of a Bayesian approach; recent reviews\nand examples are provided by Freedman, Spiegelhalter, and Parmar (1994), Parmar et al.\n(2001), and Vail et al. (2001). Thall, Simon, and Estey (1995) consider frequency properties\nof Bayesian analyses of sequential trials. More references on sequential designs appear in\nthe bibliographic note at the end of Chapter 8.\n\nThe non-Bayesian principles and methods mentioned in Section 4.5 are covered in many\nbooks, for example, Lehmann (1983, 1986), Cox and Hinkley (1974), Hastie and Tibshi-\nrani (1990), and Efron and Tibshirani (1993). The connection between ratio estimation\nand modeling alluded to in Section 4.5 is discussed by Brewer (1963), Royall (1970), and,\nfrom our Bayesian approach, Rubin (1987a, p. 46). Conover and Iman (1980) discuss the\nconnection between nonparametric tests and data transformations.\n\n4.7 Exercises\n\n1. Normal approximation: suppose that y1, . . . , y5 are independent samples from a Cauchy\ndistribution with unknown center θ and known scale 1: p(yi|θ) ∝ 1/(1 + (yi − θ)2).\nAssume that the prior distribution for θ is uniform on [0, 1]. Given the observations\n(y1, . . . , y5) = (−2,−1, 0, 1.5, 2.5):\n\n(a) Determine the derivative and the second derivative of the log posterior density.\n\n(b) Find the posterior mode of θ by iteratively solving the equation determined by setting\nthe derivative of the log-likelihood to zero.\n\n(c) Construct the normal approximation based on the second derivative of the log posterior\ndensity at the mode. Plot the approximate normal density and compare to the exact\ndensity as computed using the approach described in Exercise 2.11.\n\n2. Normal approximation: derive the analytic form of the information matrix and the nor-\nmal approximation variance for the bioassay example.\n\n3. Normal approximation to the marginal posterior distribution of an estimand: in the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.7. EXERCISES 99\n\nbioassay example, the normal approximation to the joint posterior distribution of (α, β)\nis obtained. The posterior distribution of any estimand, such as the LD50, can be ap-\nproximated by a normal distribution fit to its marginal posterior mode and the curvature\nof the marginal posterior density about the mode. This is sometimes called the ‘delta\nmethod.’ Expand the posterior distribution of the LD50, −α/β, as a Taylor series around\nthe posterior mode and thereby derive the asymptotic posterior median and standard\ndeviation. Compare to the histogram in Figure 4.2.\n\n4. Asymptotic normality: assuming the regularity conditions hold, we know that p(θ|y)\napproaches normality as n → ∞. In addition, if φ = f(θ) is any one-to-one continuous\ntransformation of θ, we can express the Bayesian inference in terms of φ and find that\np(φ|y) also approaches normality. But a nonlinear transformation of a normal distribution\nis no longer normal. How can both limiting normal distributions be valid?\n\n5. Approximate mean and variance:\n\n(a) Suppose x and y are independent normally distributed random variables, where x has\nmean 4 and standard deviation 1, and y has mean 3 and standard deviation 2. What\nare the mean and standard deviation of y/x? Compute this using simulation.\n\n(b) Suppose x and y are independent random variables, where x has mean 4 and standard\ndeviation 1, and y has mean 3 and standard deviation 2. What are the approximate\nmean and standard deviation of y/x? Determine this without using simulation.\n\n(c) What assumptions are required for the approximation in (b) to be reasonable?\n\n6. Statistical decision theory: a decision-theoretic approach to the estimation of an unknown\nparameter θ introduces the loss function L(θ, a) which, loosely speaking, gives the cost of\ndeciding that the parameter has the value a, when it is in fact equal to θ. The estimate\na can be chosen to minimize the posterior expected loss,\n\nE(L(a|y)) =\n∫\nL(θ, a)p(θ|y)dθ.\n\nThis optimal choice of a is called a Bayes estimate for the loss function L. Show that:\n\n(a) If L(θ, a) = (θ − a)2 (squared error loss), then the posterior mean, E(θ|y), if it exists,\nis the unique Bayes estimate of θ.\n\n(b) If L(θ, a) = |θ − a|, then any posterior median of θ is a Bayes estimate of θ.\n\n(c) If k0 and k1 are nonnegative numbers, not both zero, and\n\nL(θ, a) =\n\n{\nk0(θ − a) if θ ≥ a\nk1(a− θ) if θ < a,\n\nthen any k0\nk0+k1\n\nquantile of the posterior distribution p(θ|y) is a Bayes estimate of θ.\n\n7. Unbiasedness: prove that the Bayesian posterior mean, based on a proper prior distri-\nbution, cannot be an unbiased estimator except in degenerate problems (see Bickel and\nBlackwell, 1967, and Lehmann, 1983, p. 244).\n\n8. Regression to the mean: work through the details of the example of mother’s and daugh-\nter’s heights on page 94, illustrating with a sketch of the joint distribution and relevant\nconditional distributions.\n\n9. Point estimation: suppose a measurement y is recorded with a N(θ, σ2) sampling dis-\ntribution, with σ known exactly and θ known to lie in the interval [0, 1]. Consider two\npoint estimates of θ: (1) the maximum likelihood estimate, restricted to the range [0, 1],\nand (2) the posterior mean based on the assumption of a uniform prior distribution on\nθ. Show that if σ is large enough, estimate (1) has a higher mean squared error than\n(2) for any value of θ in [0, 1]. (The unrestricted maximum likelihood estimate has even\nhigher mean squared error.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n100 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n10. Non-Bayesian inference: replicate the analysis of the bioassay example in Section 3.7\nusing non-Bayesian inference. This problem does not have a unique answer, so be clear\non what methods you are using.\n\n(a) Construct an ‘estimator’ of (α, β); that is, a function whose input is a dataset, (x, n, y),\n\nand whose output is a point estimate (α̂, β̂). Compute the value of the estimate for\nthe data given in Table 3.1.\n\n(b) The bias and variance of this estimate are functions of the true values of the parameters\n(α, β) and also of the sampling distribution of the data, given α, β. Assuming the\nbinomial model, estimate the bias and variance of your estimator.\n\n(c) Create approximate 95% confidence intervals for α, β, and the LD50 based on asymp-\ntotic theory and the estimated bias and variance.\n\n(d) Does the inaccuracy of the normal approximation for the posterior distribution (com-\npare Figures 3.3 and 4.1) cast doubt on the coverage properties of your confidence\nintervals in (c)? If so, why?\n\n(e) Create approximate 95% confidence intervals for α, β, and the LD50 using the jack-\nknife or bootstrap (see Efron and Tibshirani, 1993).\n\n(f) Compare your 95% intervals for the LD50 in (c) and (e) to the posterior distribution\ndisplayed in Figure 3.4 and the posterior distribution based on the normal approxima-\ntion, displayed in 4.2b. Comment on the similarities and differences among the four\nintervals. Which do you prefer as an inferential summary about the LD50? Why?\n\n11. Bayesian interpretation of non-Bayesian estimates: consider the following estimation\nprocedure, which is based on classical hypothesis testing. A matched pairs experiment\nis done, and the differences y1, . . . , yn are recorded and modeled as independent draws\nfrom N(θ, σ2). For simplicity, assume σ2 is known. The parameter θ is estimated as the\naverage observed difference if it is ‘statistically significant’ and zero otherwise:\n\nθ̂ =\n\n{\ny if y ≥ 1.96σ/\n\n√\nn\n\n0 otherwise.\n\nCan this be interpreted, in some sense, as an approximate summary (for example, a\nposterior mean or mode) of a Bayesian inference under some prior distribution on θ?\n\n12. Bayesian interpretation of non-Bayesian estimates: repeat the above problem but with\nσ replaced by s, the sample standard deviation of y1, . . . , yn.\n\n13. Objections to Bayesian inference: discuss the criticism, ‘Bayesianism assumes: (a) Either\na weak or uniform prior [distribution], in which case why bother?, (b) Or a strong prior\n[distribution], in which case why collect new data?, (c) Or more realistically, something\nin between, in which case Bayesianism always seems to duck the issue’ (Ehrenberg, 1986).\nFeel free to use any of the examples covered so far to illustrate your points.\n\n14. Objectivity and subjectivity: discuss the statement, ‘People tend to believe results that\nsupport their preconceptions and disbelieve results that surprise them. Bayesian methods\nencourage this undisciplined mode of thinking.’\n\n15. Coverage of posterior intervals:\n\n(a) Consider a model with scalar parameter θ. Prove that, if you draw θ from the prior,\ndraw y|θ from the data model, then perform Bayesian inference for θ given y, that\nthere is a 50% probability that your 50% interval for θ contains the true value.\n\n(b) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is 1. What is the\ncoverage of the posterior 50% interval for θ? (You have to work this one out; it’s not\n50% or any other number you could just guess.)\n\n(c) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is θ0. Make a\nplot showing the coverage of the posterior 50% interval for θ, as a function of θ0.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 5\n\nHierarchical models\n\nMany statistical applications involve multiple parameters that can be regarded as related\nor connected in some way by the structure of the problem, implying that a joint probability\nmodel for these parameters should reflect their dependence. For example, in a study of the\neffectiveness of cardiac treatments, with the patients in hospital j having survival probability\nθj , it might be reasonable to expect that estimates of the θj ’s, which represent a sample of\nhospitals, should be related to each other. We shall see that this is achieved in a natural\nway if we use a prior distribution in which the θj ’s are viewed as a sample from a common\npopulation distribution. A key feature of such applications is that the observed data, yij ,\nwith units indexed by i within groups indexed by j, can be used to estimate aspects of\nthe population distribution of the θj ’s even though the values of θj are not themselves\nobserved. It is natural to model such a problem hierarchically, with observable outcomes\nmodeled conditionally on certain parameters, which themselves are given a probabilistic\nspecification in terms of further parameters, known as hyperparameters. Such hierarchical\nthinking helps in understanding multiparameter problems and also plays an important role\nin developing computational strategies.\n\nPerhaps even more important in practice is that simple nonhierarchical models are usu-\nally inappropriate for hierarchical data: with few parameters, they generally cannot fit large\ndatasets accurately, whereas with many parameters, they tend to ‘overfit’ such data in the\nsense of producing models that fit the existing data well but lead to inferior predictions for\nnew data. In contrast, hierarchical models can have enough parameters to fit the data well,\nwhile using a population distribution to structure some dependence into the parameters,\nthereby avoiding problems of overfitting. As we show in the examples in this chapter, it is\noften sensible to fit hierarchical models with more parameters than there are data points.\n\nIn Section 5.1, we consider the problem of constructing a prior distribution using hierar-\nchical principles but without fitting a formal probability model for the hierarchical structure.\nWe first consider the analysis of a single experiment, using historical data to create a prior\ndistribution, and then we consider a plausible prior distribution for the parameters of a set\nof experiments. The treatment in Section 5.1 is not fully Bayesian, because, for the purpose\nof simplicity in exposition, we work with a point estimate, rather than a complete joint\nposterior distribution, for the parameters of the population distribution (the hyperparam-\neters). In Section 5.2, we discuss how to construct a hierarchical prior distribution in the\ncontext of a fully Bayesian analysis. Sections 5.3–5.4 present a general approach to compu-\ntation with hierarchical models in conjugate families by combining analytical and numerical\nmethods. We defer details of the most general computational methods to Part III in order\nto explore immediately the important practical and conceptual advantages of hierarchical\nBayesian models. The chapter continues with two extended examples: a hierarchical model\nfor an educational testing experiment and a Bayesian treatment of the method of ‘meta-\nanalysis’ as used in medical research to combine the results of separate studies relating to\nthe same research question. We conclude with a discussion of weakly informative priors,\nwhich become important for hierarchical models fit to data from a small number of groups.\n\n101\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n102 5. HIERARCHICAL MODELS\n\nPrevious experiments:\n\n0/20 0/20 0/20 0/20 0/20 0/20 0/20 0/19 0/19 0/19\n0/19 0/18 0/18 0/17 1/20 1/20 1/20 1/20 1/19 1/19\n1/18 1/18 2/25 2/24 2/23 2/20 2/20 2/20 2/20 2/20\n2/20 1/10 5/49 2/19 5/46 3/27 2/17 7/49 7/47 3/20\n3/20 2/13 9/48 10/50 4/20 4/20 4/20 4/20 4/20 4/20\n4/20 10/48 4/19 4/19 4/19 5/22 11/46 12/49 5/20 5/20\n6/23 5/19 6/22 6/20 6/20 6/20 16/52 15/47 15/46 9/24\n\nCurrent experiment:\n4/14\n\nTable 5.1 Tumor incidence in historical control groups and current group of rats, from Tarone\n(1982). The table displays the values of\n\nyj\n\nnj\n: (number of rats with tumors)/(total number of rats).\n\n5.1 Constructing a parameterized prior distribution\n\nAnalyzing a single experiment in the context of historical data\n\nTo begin our description of hierarchical models, we consider the problem of estimating a\nparameter θ using data from a small experiment and a prior distribution constructed from\nsimilar previous (or historical) experiments. Mathematically, we will consider the current\nand historical experiments to be a random sample from a common population.\n\nExample. Estimating the risk of tumor in a group of rats\nIn the evaluation of drugs for possible clinical application, studies are routinely per-\nformed on rodents. For a particular study drawn from the statistical literature, sup-\npose the immediate aim is to estimate θ, the probability of tumor in a population of\nfemale laboratory rats of type ‘F344’ that receive a zero dose of the drug (a control\ngroup). The data show that 4 out of 14 rats developed endometrial stromal polyps (a\nkind of tumor). It is natural to assume a binomial model for the number of tumors,\ngiven θ. For convenience, we select a prior distribution for θ from the conjugate family,\nθ ∼ Beta(α, β).\n\nAnalysis with a fixed prior distribution. From historical data, suppose we knew that\nthe tumor probabilities θ among groups of female lab rats of type F344 follow an\napproximate beta distribution, with known mean and standard deviation. The tumor\nprobabilities θ vary because of differences in rats and experimental conditions among\nthe experiments. Referring to the expressions for the mean and variance of the beta\ndistribution (see Appendix A), we could find values for α, β that correspond to the\ngiven values for the mean and standard deviation. Then, assuming a Beta(α, β) prior\ndistribution for θ yields a Beta(α+ 4, β + 10) posterior distribution for θ.\n\nApproximate estimate of the population distribution using the historical data. Typ-\nically, the mean and standard deviation of underlying tumor risks are not available.\nRather, historical data are available on previous experiments on similar groups of rats.\nIn the rat tumor example, the historical data were in fact a set of observations of tu-\nmor incidence in 70 groups of rats (Table 5.1). In the jth historical experiment, let the\nnumber of rats with tumors be yj and the total number of rats be nj . We model the\nyj ’s as independent binomial data, given sample sizes nj and study-specific means θj .\nAssuming that the beta prior distribution with parameters (α, β) is a good description\nof the population distribution of the θj ’s in the historical experiments, we can display\nthe hierarchical model schematically as in Figure 5.1, with θ71 and y71 corresponding\nto the current experiment.\nThe observed sample mean and standard deviation of the 70 values\n\nyj\nnj\n\nare 0.136 and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.1. CONSTRUCTING A PARAMETERIZED PRIOR DISTRIBUTION 103\n\nα, β\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑✑✰\n\n✚\n✚\n\n✚\n✚\n\n✚\n✚\n\n✚✚❂\n\n�\n�\n\n�\n�\n\n�\n�✠\n\n✁\n✁\n✁\n\n✁\n✁\n✁☛ ❄\n\n❆\n❆\n❆\n❆\n❆\n❆❯\n\n❅\n❅\n❅\n❅\n❅\n❅❘\n\n❩\n❩\n❩\n❩\n❩\n❩\n❩❩⑦\n\n◗\n◗\n◗\n◗\n◗\n◗\n◗\n◗◗s\n\nθ1 θ2 θ3 . . . . . . . . . . . . θ70 θ71\n\n❄ ❄ ❄ ❄ ❄\ny1 y2 y3 . . . . . . . . . . . . y70 y71\n\nFigure 5.1: Structure of the hierarchical model for the rat tumor example.\n\n0.103. If we set the mean and standard deviation of the population distribution to\nthese values, we can solve for α and β—see (A.3) on page 585 in Appendix A. The\nresulting estimate for (α, β) is (1.4, 8.6). This is not a Bayesian calculation because\nit is not based on any specified full probability model. We present a better, fully\nBayesian approach to estimating (α, β) for this example in Section 5.3. The estimate\n(1.4, 8.6) is simply a starting point from which we can explore the idea of estimating\nthe parameters of the population distribution.\nUsing the simple estimate of the historical population distribution as a prior distribu-\ntion for the current experiment yields a Beta(5.4, 18.6) posterior distribution for θ71:\nthe posterior mean is 0.223, and the standard deviation is 0.083. The prior informa-\ntion has resulted in a posterior mean substantially lower than the crude proportion,\n4/14 = 0.286, because the weight of experience indicates that the number of tumors\nin the current experiment is unusually high.\nThese analyses require that the current tumor risk, θ71, and the 70 historical tumor\nrisks, θ1, . . . , θ70, be considered a random sample from a common distribution, an\nassumption that would be invalidated, for example, if it were known that the historical\nexperiments were all done in laboratory A but the current data were gathered in\nlaboratory B, or if time trends were relevant. In practice, a simple, although arbitrary,\nway of accounting for differences between the current and historical data is to inflate\nthe historical variance. For the beta model, inflating the historical variance means\ndecreasing (α+β) while holding α\n\nβ constant. Other systematic differences, such as a\ntime trend in tumor risks, can be incorporated in a more extensive model.\n\nHaving used the 70 historical experiments to form a prior distribution for θ71, we might\nnow like also to use this same prior distribution to obtain Bayesian inferences for the tumor\nprobabilities in the first 70 experiments, θ1, . . . , θ70. There are several logical and practical\nproblems with the approach of directly estimating a prior distribution from existing data:\n\n• If we wanted to use the estimated prior distribution for inference about the first 70\nexperiments, then the data would be used twice: first, all the results together are used to\nestimate the prior distribution, and then each experiment’s results are used to estimate\nits θ. This would seem to cause us to overestimate our precision.\n\n• The point estimate for α and β seems arbitrary, and using any point estimate for α and\nβ necessarily ignores some posterior uncertainty.\n\n• We can also make the opposite point: does it make sense to ‘estimate’ α and β at all?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n104 5. HIERARCHICAL MODELS\n\nThey are part of the ‘prior’ distribution: should they be known before the data are\ngathered, according to the logic of Bayesian inference?\n\nLogic of combining information\n\nDespite these problems, it clearly makes more sense to try to estimate the population\ndistribution from all the data, and thereby to help estimate each θj , than to estimate all 71\nvalues θj separately. Consider the following thought experiment about inference on two of\nthe parameters, θ26 and θ27, each corresponding to experiments with 2 observed tumors out\nof 20 rats. Suppose our prior distribution for both θ26 and θ27 is centered around 0.15; now\nsuppose that you were told after completing the data analysis that θ26 = 0.1 exactly. This\nshould influence your estimate of θ27; in fact, it would probably make you think that θ27\nis lower than you previously believed, since the data for the two parameters are identical,\nand the postulated value of 0.1 is lower than you previously expected for θ26 from the prior\ndistribution. Thus, θ26 and θ27 should be dependent in the posterior distribution, and they\nshould not be analyzed separately.\n\nWe retain the advantages of using the data to estimate prior parameters and eliminate\nall of the disadvantages just mentioned by putting a probability model on the entire set of\nparameters and experiments and then performing a Bayesian analysis on the joint distribu-\ntion of all the model parameters. A complete Bayesian analysis is described in Section 5.3.\nThe analysis using the data to estimate the prior parameters, which is sometimes called\nempirical Bayes, can be viewed as an approximation to the complete hierarchical Bayesian\nanalysis. We prefer to avoid the term ‘empirical Bayes’ because it misleadingly suggests\nthat the full Bayesian method, which we discuss here and use for the rest of the book, is\nnot ‘empirical.’\n\n5.2 Exchangeability and hierarchical models\n\nGeneralizing from the example of the previous section, consider a set of experiments j =\n1, . . . , J , in which experiment j has data (vector) yj and parameter (vector) θj , with like-\nlihood p(yj |θj). (Throughout this chapter we use the word ‘experiment’ for convenience,\nbut the methods can apply equally well to nonexperimental data.) Some of the parameters\nin different experiments may overlap; for example, each data vector yj may be a sample of\nobservations from a normal distribution with mean µj and common variance σ2, in which\ncase θj = (µj , σ\n\n2). In order to create a joint probability model for all the parameters θ, we\nuse the crucial idea of exchangeability introduced in Chapter 1 and used repeatedly since\nthen.\n\nExchangeability\n\nIf no information—other than the data y—is available to distinguish any of the θj ’s from any\nof the others, and no ordering or grouping of the parameters can be made, one must assume\nsymmetry among the parameters in their prior distribution. This symmetry is represented\nprobabilistically by exchangeability; the parameters (θ1, . . . , θJ) are exchangeable in their\njoint distribution if p(θ1, . . . , θJ) is invariant to permutations of the indexes (1, . . . , J). For\nexample, in the rat tumor problem, suppose we have no information to distinguish the 71\nexperiments, other than the sample sizes nj, which presumably are not related to the values\nof θj ; we therefore use an exchangeable model for the θj ’s.\n\nWe have already encountered the concept of exchangeability in constructing independent\nand identically distributed models for direct data. In practice, ignorance implies exchange-\nability. Generally, the less we know about a problem, the more confidently we can make\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 105\n\nclaims of exchangeability. (This is not, we hasten to add, a good reason to limit our knowl-\nedge of a problem before embarking on statistical analysis!) Consider the analogy to a\nroll of a die: we should initially assign equal probabilities to all six outcomes, but if we\nstudy the measurements of the die and weigh the die carefully, we might eventually notice\nimperfections, which might make us favor one outcome over the others and thus eliminate\nthe symmetry among the six outcomes.\n\nThe simplest form of an exchangeable distribution has each of the parameters θj as an\nindependent sample from a prior (or population) distribution governed by some unknown\nparameter vector φ; thus,\n\np(θ|φ) =\nJ∏\n\nj=1\n\np(θj |φ). (5.1)\n\nIn general, φ is unknown, so our distribution for θ must average over our uncertainty in φ:\n\np(θ) =\n\n∫ ( J∏\n\nj=1\n\np(θj |φ)\n)\np(φ)dφ, (5.2)\n\nThis form, the mixture of independent identical distributions, is usually all that we need to\ncapture exchangeability in practice.\n\nA related theoretical result, de Finetti’s theorem, to which we alluded in Section 1.2,\nstates that in the limit as J → ∞, any suitably well-behaved exchangeable distribution\non (θ1, . . . , θJ) can be expressed as a mixture of independent and identical distributions\nas in (5.2). The theorem does not hold when J is finite (see Exercises 5.1, 5.2, and 5.4).\nStatistically, the mixture model characterizes parameters θ as drawn from a common ‘su-\nperpopulation’ that is determined by the unknown hyperparameters, φ. We are already\nfamiliar with exchangeable models for data, y1, . . . , yn, in the form of likelihoods in which\nthe n observations are independent and identically distributed, given some parameter vector\nθ.\n\nAs a simple counterexample to the above mixture model, consider the probabilities of a\ngiven die landing on each of its six faces. The probabilities θ1, . . . , θ6 are exchangeable, but\nthe six parameters θj are constrained to sum to 1 and so cannot be modeled with a mixture\nof independent identical distributions; nonetheless, they can be modeled exchangeably.\n\nExample. Exchangeability and sampling\nThe following thought experiment illustrates the role of exchangeability in inference\nfrom random sampling. For simplicity, we use a nonhierarchical example with ex-\nchangeability at the level of y rather than θ.\nWe, the authors, have selected eight states out of the United States and recorded the\ndivorce rate per 1000 population in each state in 1981. Call these y1, . . . , y8. What\ncan you, the reader, say about y8, the divorce rate in the eighth state?\nSince you have no information to distinguish any of the eight states from the others,\nyou must model them exchangeably. You might use a beta distribution for the eight\nyj ’s, a logit normal, or some other prior distribution restricted to the range [0, 1].\nUnless you are familiar with divorce statistics in the United States, your distribution\non (y1, . . . , y8) should be fairly vague.\nWe now randomly sample seven states from these eight and tell you their divorce\nrates: 5.8, 6.6, 7.8, 5.6, 7.0, 7.1, 5.4, each in numbers of divorces per 1000 population\n(per year). Based primarily on the data, a reasonable posterior (predictive) distri-\nbution for the remaining value, y8, would probably be centered around 6.5 and have\nmost of its mass between 5.0 and 8.0. Changing the indexing does not change the\njoint distribution. If we relabel the remaining value to be any other yj the posterior\nestimate would be the same. yj are exchangeable but they are not independent as we\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n106 5. HIERARCHICAL MODELS\n\nassume that the divorce rate in the eighth unobserved state is probably similar to the\nobserved rates.\nSuppose initially we had given you the further prior information that the eight states\nare Mountain states: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah,\nand Wyoming, but selected in a random order; you still are not told which observed\nrate corresponds to which state. Now, before the seven data points were observed,\nthe eight divorce rates should still be modeled exchangeably. However, your prior\ndistribution (that is, before seeing the data), for the eight numbers should change:\nit seems reasonable to assume that Utah, with its large Mormon population, has a\nmuch lower divorce rate, and Nevada, with its liberal divorce laws, has a much higher\ndivorce rate, than the remaining six states. Perhaps, given your expectation of outliers\nin the distribution, your prior distribution should have wide tails. Given this extra\ninformation (the names of the eight states), when you see the seven observed values\nand note that the numbers are so close together, it might seem a reasonable guess that\nthe missing eighth state is Nevada or Utah. Therefore its value might be expected to\nbe much lower or much higher than the seven values observed. This might lead to a\nbimodal or trimodal posterior distribution to account for the two plausible scenarios.\nThe prior distribution on the eight values yj is still exchangeable, however, because\nyou have no information telling which state corresponds to which index number. (See\nExercise 5.6.)\nFinally, we tell you that the state not sampled (corresponding to y8) was Nevada.\nNow, even before seeing the seven observed values, you cannot assign an exchangeable\nprior distribution to the set of eight divorce rates, since you have information that\ndistinguishes y8 from the other seven numbers, here suspecting it is larger than any\nof the others. Once y1, . . . , y7 have been observed, a reasonable posterior distribution\nfor y8 plausibly should have most of its mass above the largest observed rate, that is,\np(y8 > max(y1, . . . , y7)|y1, . . . , y7) should be large.\nIncidentally, Nevada’s divorce rate in 1981 was 13.9 per 1000 population.\n\nExchangeability when additional information is available on the units\n\nOften observations are not fully exchangeable, but are partially or conditionally exchange-\nable:\n\n• If observations can be grouped, we may make hierarchical model, where each group has its\nown submodel, but the group properties are unknown. If we assume that group properties\nare exchangeable, we can use a common prior distribution for the group properties.\n\n• If yi has additional information xi so that yi are not exchangeable but (yi, xi) still are\nexchangeable, then we can make a joint model for (yi, xi) or a conditional model for\nyi|xi.\nIn the rat tumor example, yj were exchangeable as no additional knowledge was available\n\non experimental conditions. If we knew that specific batches of experiments were made in\ndifferent laboratories we could assume partial exchangeability and use two level hierarchical\nmodel to model variation within each laboratory and between laboratories.\n\nIn the divorce example, if we knew xj , the divorce rate in state j last year, for j =\n1, . . . , 8, but not which index corresponded to which state, then we would certainly be able\nto distinguish the eight values of yj, but the joint prior distribution p(xj , yj) would be the\nsame for each state. For states having the same last year divorce rates xj , we could use\ngrouping and assume partial exchangeability or if there are many possible values for xj (as\nwe would assume for divorce rates) we could assume conditional exchangeability and use xj\nas covariate in regression model.\n\nIn general, the usual way to model exchangeability with covariates is through con-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 107\n\nditional independence: p(θ1, . . . , θJ |x1, . . . , xJ ) =\n∫\n[\n∏J\nj=1 p(θj |φ, xj)]p(φ|x)dφ, with x =\n\n(x1, . . . , xJ ). In this way, exchangeable models become almost universally applicable, be-\ncause any information available to distinguish different units should be encoded in the x\nand y variables.\n\nIn the rat tumor example, we have already noted that the sample sizes nj are the only\navailable information to distinguish the different experiments. It does not seem likely that\nnj would be a useful variable for modeling tumor rates, but if one were interested, one\ncould create an exchangeable model for the J pairs (n, y)j . A natural first step would be\nto plot\n\nyj\nnj\n\nvs. nj to see any obvious relation that could be modeled. For example, perhaps\n\nsome studies j had larger sample sizes nj because the investigators correctly suspected rarer\nevents; that is, smaller θj and thus smaller expected values of\n\nyj\nnj\n. In fact, the plot of\n\nyj\nnj\n\nversus nj , not shown here, shows no apparent relation between the two variables.\n\nObjections to exchangeable models\n\nIn virtually any statistical application, it is natural to object to exchangeability on the\ngrounds that the units actually differ. For example, the 71 rat tumor experiments were\nperformed at different times, on different rats, and presumably in different laboratories.\nSuch information does not, however, invalidate exchangeability. That the experiments differ\nimplies that the θj ’s differ, but it might be perfectly acceptable to consider them as if drawn\nfrom a common distribution. In fact, with no information available to distinguish them, we\nhave no logical choice but to model the θj ’s exchangeably. Objecting to exchangeability for\nmodeling ignorance is no more reasonable than objecting to an independent and identically\ndistributed model for samples from a common population, objecting to regression models\nin general, or, for that matter, objecting to displaying points in a scatterplot without\nindividual labels. As with regression, the valid concern is not about exchangeability, but\nabout encoding relevant knowledge as explanatory variables where possible.\n\nThe full Bayesian treatment of the hierarchical model\n\nReturning to the problem of inference, the key ‘hierarchical’ part of these models is that\nφ is not known and thus has its own prior distribution, p(φ). The appropriate Bayesian\nposterior distribution is of the vector (φ, θ). The joint prior distribution is\n\np(φ, θ) = p(φ)p(θ|φ),\n\nand the joint posterior distribution is\n\np(φ, θ|y) ∝ p(φ, θ)p(y|φ, θ)\n= p(φ, θ)p(y|θ), (5.3)\n\nwith the latter simplification holding because the data distribution, p(y|φ, θ), depends only\non θ; the hyperparameters φ affect y only through θ. Previously, we assumed φ was known,\nwhich is unrealistic; now we include the uncertainty in φ in the model.\n\nThe hyperprior distribution\n\nIn order to create a joint probability distribution for (φ, θ), we must assign a prior distri-\nbution to φ. If little is known about φ, we can assign a diffuse prior distribution, but we\nmust be careful when using an improper prior density to check that the resulting poste-\nrior distribution is proper, and we should assess whether our conclusions are sensitive to\nthis simplifying assumption. In most real problems, one should have enough substantive\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n108 5. HIERARCHICAL MODELS\n\nknowledge about the parameters in φ at least to constrain the hyperparameters into a finite\nregion, if not to assign a substantive hyperprior distribution. As in nonhierarchical models,\nit is often practical to start with a simple, relatively noninformative, prior distribution on φ\nand seek to add more prior information if there remains too much variation in the posterior\ndistribution.\n\nIn the rat tumor example, the hyperparameters are (α, β), which determine the beta\ndistribution for θ. We illustrate one approach to constructing an appropriate hyperprior\ndistribution in the continuation of that example in the next section.\n\nPosterior predictive distributions\n\nHierarchical models are characterized both by hyperparameters, φ, in our notation, and\nparameters θ. There are two posterior predictive distributions that might be of interest to\nthe data analyst: (1) the distribution of future observations ỹ corresponding to an existing\nθj , or (2) the distribution of observations ỹ corresponding to future θj ’s drawn from the\n\nsame superpopulation. We label the future θj’s as θ̃. Both kinds of replications can be used\nto assess model adequacy, as we discuss in Chapter 6. In the rat tumor example, future\nobservations can be (1) additional rats from an existing experiment, or (2) results from a\nfuture experiment. In the former case, the posterior predictive draws ỹ are based on the\nposterior draws of θj for the existing experiment. In the latter case, one must first draw\n\nθ̃ for the new experiment from the population distribution, given the posterior draws of φ,\nand then draw ỹ given the simulated θ̃.\n\n5.3 Bayesian analysis of conjugate hierarchical models\n\nOur inferential strategy for hierarchical models follows the general approach to multiparam-\neter problems presented in Section 3.8 but is more difficult in practice because of the large\nnumber of parameters that commonly appear in a hierarchical model. In particular, we\ncannot generally plot the contours or display a scatterplot of the simulations from the joint\nposterior distribution of (θ, φ). With care, however, we can follow a similar simulation-based\napproach as before.\n\nIn this section, we present an approach that combines analytical and numerical methods\nto obtain simulations from the joint posterior distribution, p(θ, φ|y), for the beta-binomial\nmodel for the rat-tumor example, for which the population distribution, p(θ|φ), is conjugate\nto the likelihood, p(y|θ). For the many nonconjugate hierarchical models that arise in\npractice, more advanced computational methods, presented in Part III of this book, are\nnecessary. Even for more complicated problems, however, the approach using conjugate\ndistributions is useful for obtaining approximate estimates and starting points for more\naccurate computations.\n\nAnalytic derivation of conditional and marginal distributions\n\nWe first perform the following three steps analytically.\n\n1. Write the joint posterior density, p(θ, φ|y), in unnormalized form as a product of the\nhyperprior distribution p(φ), the population distribution p(θ|φ), and the likelihood p(y|θ).\n\n2. Determine analytically the conditional posterior density of θ given the hyperparameters\nφ; for fixed observed y, this is a function of φ, p(θ|φ, y).\n\n3. Estimate φ using the Bayesian paradigm; that is, obtain its marginal posterior distribu-\ntion, p(φ|y).\nThe first step is immediate, and the second step is easy for conjugate models because,\n\nconditional on φ, the population distribution for θ is just the independent and identically\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 109\n\ndistributed model (5.1), so that the conditional posterior density is a product of conjugate\nposterior densities for the components θj .\n\nThe third step can be performed by brute force by integrating the joint posterior distri-\nbution over θ:\n\np(φ|y) =\n∫\np(θ, φ|y)dθ. (5.4)\n\nFor many standard models, however, including the normal distribution, the marginal pos-\nterior distribution of φ can be computed algebraically using the conditional probability\nformula,\n\np(φ|y) = p(θ, φ|y)\np(θ|φ, y) . (5.5)\n\nThis expression is useful because the numerator is just the joint posterior distribution (5.3),\nand the denominator is the posterior distribution for θ if φ were known. The difficulty in\nusing (5.5), beyond a few standard conjugate models, is that the denominator, p(θ|φ, y),\nregarded as a function of both θ and φ for fixed y, has a normalizing factor that depends on\nφ as well as y. One must be careful with the proportionality ‘constant’ in Bayes’ theorem,\nespecially when using hierarchical models, to make sure it is actually constant. Exercise\n5.11 has an example of a nonconjugate model in which the integral (5.4) has no closed-form\nsolution so that (5.5) is no help.\n\nDrawing simulations from the posterior distribution\n\nThe following strategy is useful for simulating a draw from the joint posterior distribution,\np(θ, φ|y), for simple hierarchical models such as are considered in this chapter.\n\n1. Draw the vector of hyperparameters, φ, from its marginal posterior distribution, p(φ|y).\nIf φ is low-dimensional, the methods discussed in Chapter 3 can be used; for high-\ndimensional φ, more sophisticated methods such as described in Part III may be needed.\n\n2. Draw the parameter vector θ from its conditional posterior distribution, p(θ|φ, y), given\nthe drawn value of φ. For the examples we consider in this chapter, the factorization\np(θ|φ, y) =\n\n∏\nj p(θj |φ, y) holds, and so the components θj can be drawn independently,\n\none at a time.\n\n3. If desired, draw predictive values ỹ from the posterior predictive distribution given the\ndrawn θ. Depending on the problem, it might be necessary first to draw a new value θ̃,\ngiven φ, as discussed at the end of the previous section.\n\nAs usual, the above steps are performed L times in order to obtain a set of L draws. From\nthe joint posterior simulations of θ and ỹ, we can compute the posterior distribution of any\nestimand or predictive quantity of interest.\n\nApplication to the model for rat tumors\n\nWe now perform a full Bayesian analysis of the rat tumor experiments described in Section\n5.1. Once again, the data from experiments j = 1, . . . , J , J = 71, are assumed to follow\nindependent binomial distributions:\n\nyj ∼ Bin(nj , θj),\n\nwith the number of rats, nj, known. The parameters θj are assumed to be independent\nsamples from a beta distribution:\n\nθj ∼ Beta(α, β),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n110 5. HIERARCHICAL MODELS\n\nand we shall assign a noninformative hyperprior distribution to reflect our ignorance about\nthe unknown hyperparameters. As usual, the word ‘noninformative’ indicates our attitude\ntoward this part of the model and is not intended to imply that this particular distribution\nhas any special properties. If the hyperprior distribution turns out to be crucial for our\ninference, we should report this and if possible seek further substantive knowledge that\ncould be used to construct a more informative prior distribution. If we wish to assign an\nimproper prior distribution for the hyperparameters, (α, β), we must check that the poste-\nrior distribution is proper. We defer the choice of noninformative hyperprior distribution,\na relatively arbitrary and unimportant part of this particular analysis, until we inspect the\nintegrability of the posterior density.\n\nJoint, conditional, and marginal posterior distributions. We first perform the three steps\nfor determining the analytic form of the posterior distribution. The joint posterior distri-\nbution of all parameters is\n\np(θ, α, β|y) ∝ p(α, β)p(θ|α, β)p(y|θ, α, β)\n\n∝ p(α, β)\n\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\nθα−1\nj (1− θj)β−1\n\nJ∏\n\nj=1\n\nθ\nyj\nj (1− θj)nj−yj . (5.6)\n\nGiven (α, β), the components of θ have independent posterior densities that are of the form\nθAj (1− θj)B—that is, beta densities—and the joint density is\n\np(θ|α, β, y) =\nJ∏\n\nj=1\n\nΓ(α+β+nj)\n\nΓ(α+yj)Γ(β+nj−yj)\nθ\nα+yj−1\nj (1− θj)β+nj−yj−1. (5.7)\n\nWe can determine the marginal posterior distribution of (α, β) by substituting (5.6) and\n(5.7) into the conditional probability formula (5.5):\n\np(α, β|y) ∝ p(α, β)\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\n\nΓ(α+ yj)Γ(β + nj − yj)\nΓ(α+β + nj)\n\n. (5.8)\n\nThe product in equation (5.8) cannot be simplified analytically but is easy to compute for\nany specified values of (α, β) using a standard routine to compute the gamma function.\n\nChoosing a standard parameterization and setting up a ‘noninformative’ hyperprior dis-\ntribution. Because we have no immediately available information about the distribution\nof tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribu-\ntion for (α, β). Before assigning a hyperprior distribution, we reparameterize in terms\nof logit( α\n\nα+β ) = log(αβ ) and log(α+β), which are the logit of the mean and the logarithm\nof the ‘sample size’ in the beta population distribution for θ. It would seem reasonable to\nassign independent hyperprior distributions to the prior mean and ‘sample size,’ and we\nuse the logistic and logarithmic transformations to put each on a (−∞,∞) scale. Unfortu-\nnately, a uniform prior density on these newly transformed parameters yields an improper\nposterior density, with an infinite integral in the limit (α+β)→ ∞, and so this particular\nprior density cannot be used here.\n\nIn a problem such as this with a reasonably large amount of data, it is possible to set up a\n‘noninformative’ hyperprior density that is dominated by the likelihood and yields a proper\nposterior distribution. One reasonable choice of diffuse hyperprior density is uniform on\n( α\nα+β , (α+β)\n\n−1/2), which when multiplied by the appropriate Jacobian yields the following\ndensities on the original scale,\n\np(α, β) ∝ (α+β)−5/2, (5.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 111\n\nFigure 5.2 First try at a contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for\n\nthe rat tumor example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode.\n\nand on the natural transformed scale:\n\np\n\n(\nlog(\n\nα\n\nβ\n), log(α+β)\n\n)\n∝ αβ(α+β)−5/2. (5.10)\n\nSee Exercise 5.9 for a discussion of this prior density.\nWe could avoid the mathematical effort of checking the integrability of the posterior\n\ndensity if we were to use a proper hyperprior distribution. Another approach would be\ntentatively to use a flat hyperprior density, such as p( α\n\nα+β , α+β) ∝ 1, or even p(α, β) ∝ 1,\n\nand then compute the contours and simulations from the posterior density (as detailed\nbelow). The result would clearly show the posterior contours drifting off toward infinity,\nindicating that the posterior density is not integrable in that limit. The prior distribution\nwould then have to be altered to obtain an integrable posterior density.\n\nIncidentally, setting the prior distribution for (log(αβ ), log(α+β)) to uniform in a vague\n\nbut finite range, such as [−1010, 1010]× [−1010, 1010], would not be an acceptable solution\nfor this problem, as almost all the posterior mass in this case would be in the range of α\nand β near ‘infinity,’ which corresponds to a Beta(α, β) distribution with a variance of zero,\nmeaning that all the θj parameters would be essentially equal in the posterior distribution.\nWhen the likelihood is not integrable, setting a faraway finite cutoff to a uniform prior\ndensity does not necessarily eliminate the problem.\n\nComputing the marginal posterior density of the hyperparameters. Now that we have estab-\nlished a full probability model for data and parameters, we compute the marginal posterior\ndistribution of the hyperparameters. Figure 5.2 shows a contour plot of the unnormalized\nmarginal posterior density on a grid of values of (log(αβ ), log(α+β)). To create the plot, we\n\nfirst compute the logarithm of the density function (5.8) with prior density (5.9), multiply-\ning by the Jacobian to obtain the density p(log(αβ ), log(α+β)|y). We set a grid in the range\n\n(log(αβ ), log(α+β)) ∈ [−2.5,−1]× [1.5, 3], which is centered near our earlier point estimate\n\n(−1.8, 2.3) (that is, (α, β) = (1.4, 8.6)) and covers a factor of 4 in each parameter. Then, to\navoid computational overflows, we subtract the maximum value of the log density from each\npoint on the grid and exponentiate, yielding values of the unnormalized marginal posterior\ndensity.\n\nThe most obvious features of the contour plot are (1) the mode is not far from the\npoint estimate (as we would expect), and (2) important parts of the marginal posterior\ndistribution lie outside the range of the graph.\n\nWe recompute p(log(αβ ), log(α+ β)|y), this time in the range (log(αβ ), log(α+ β)) ∈\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n112 5. HIERARCHICAL MODELS\n\nFigure 5.3 (a) Contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for the rat tumor\n\nexample. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of\n1000 draws (log(α\n\nβ\n), log(α+β)) from the numerically computed marginal posterior density.\n\n[−2.3,−1.3]× [1, 5]. The resulting grid, shown in Figure 5.3a, displays essentially all of\nthe marginal posterior distribution. Figure 5.3b displays 1000 random draws from the\nnumerically computed posterior distribution. The graphs show that the marginal poste-\nrior distribution of the hyperparameters, under this transformation, is approximately sym-\nmetric about the mode, roughly (−1.75, 2.8). This corresponds to approximate values of\n(α, β) = (2.4, 14.0), which differs somewhat from the crude estimate obtained earlier.\n\nHaving computed the relative posterior density at a grid that covers the effective range\nof (α, β), we normalize by approximating the distribution as a step function over the grid\nand setting the total probability in the grid to 1.\n\nWe can then compute posterior moments based on the grid of (log(αβ ), log(α+β)); for\nexample,\n\nE(α|y) is estimated by\n∑\n\nlog(α\nβ ),log(α+β)\n\nα · p(log(α\nβ\n), log(α+β)|y).\n\nFrom the grid in Figure 5.3, we compute E(α|y) = 2.4 and E(β|y) = 14.3. This is close to the\nestimate based on the mode of Figure 5.3a, given above, because the posterior distribution is\napproximately symmetric on the scale of (log(αβ ), log(α+β)). A more important consequence\n\nof averaging over the grid is to account for the posterior uncertainty in (α, β), which is not\ncaptured in the point estimate.\n\nSampling from the joint posterior distribution of parameters and hyperparameters. We\ndraw 1000 random samples from the joint posterior distribution of (α, β, θ1, . . . , θJ ), as\nfollows.\n\n1. Simulate 1000 draws of (log(αβ ), log(α+β)) from their posterior distribution displayed\n\nin Figure 5.3, using the same discrete-grid sampling procedure used to draw (α, β) for\nFigure 3.3b in the bioassay example of Section 3.8.\n\n2. For l = 1, . . . , 1000:\n\n(a) Transform the lth draw of (log(αβ ), log(α+β)) to the scale (α, β) to yield a draw of\nthe hyperparameters from their marginal posterior distribution.\n\n(b) For each j = 1, . . . , J , sample θj from its conditional posterior distribution, θj |α, β, y ∼\nBeta(α+ yj, β + nj − yj).\n\nDisplaying the results. Figure 5.4 shows posterior medians and 95% intervals for the θj ’s,\ncomputed by simulation. The rates θj are shrunk from their sample point estimates,\n\nyj\nnj\n,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 113\n\nFigure 5.4 Posterior medians and 95% intervals of rat tumor rates, θj (plotted vs. observed tumor\nrates yj/nj), based on simulations from the joint posterior distribution. The 45◦ line corresponds\nto the unpooled estimates, θ̂i = yi/ni. The horizontal positions of the line have been jittered to\nreduce overlap.\n\ntowards the population distribution, with approximate mean 0.14; experiments with fewer\nobservations are shrunk more and have higher posterior variances. The results are superfi-\ncially similar to what would be obtained based on a point estimate of the hyperparameters,\nwhich makes sense in this example, because of the fairly large number of experiments.\nBut key differences remain, notably that posterior variability is higher in the full Bayesian\nanalysis, reflecting posterior uncertainty in the hyperparameters.\n\n5.4 Normal model with exchangeable parameters\n\nWe now present a full treatment of a simple hierarchical model based on the normal distribu-\ntion, in which observed data are normally distributed with a different mean for each ‘group’\nor ‘experiment,’ with known observation variance, and a normal population distribution\nfor the group means. This model is sometimes termed the one-way normal random-effects\nmodel with known data variance and is widely applicable, being an important special case\nof the hierarchical normal linear model, which we treat in some generality in Chapter 15.\nIn this section, we present a general treatment following the computational approach of\nSection 5.3. The following section presents a detailed example; those impatient with the\nalgebraic details may wish to look ahead at the example for motivation.\n\nThe data structure\n\nConsider J independent experiments, with experiment j estimating the parameter θj from\nnj independent normally distributed data points, yij , each with known error variance σ2;\nthat is,\n\nyij |θj ∼ N(θj , σ\n2), for i = 1, . . . , nj; j = 1, . . . , J. (5.11)\n\nUsing standard notation from the analysis of variance, we label the sample mean of each\ngroup j as\n\ny.j =\n1\n\nnj\n\nnj∑\n\ni=1\n\nyij\n\nwith sampling variance\n\nσ2\nj = σ2/nj.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n114 5. HIERARCHICAL MODELS\n\nWe can then write the likelihood for each θj using the sufficient statistics, y.j :\n\ny.j |θj ∼ N(θj , σ\n2\nj ), (5.12)\n\na notation that will prove useful later because of the flexibility in allowing a separate\nvariance σ2\n\nj for the mean of each group j. For the rest of this chapter, all expressions will\n\nbe implicitly conditional on the known values σ2\nj . The problem of estimating a set of means\n\nwith unknown variances will require some additional computational methods, presented in\nSections 11.6 and 13.6. Although rarely strictly true, the assumption of known variances\nat the sampling level of the model is often an adequate approximation.\n\nThe treatment of the model provided in this section is also appropriate for situations\nin which the variances differ for reasons other than the number of data points in the ex-\nperiment. In fact, the likelihood (5.12) can appear in much more general contexts than\nthat stated here. For example, if the group sizes nj are large enough, then the means y.j\nare approximately normally distributed, given θj , even when the data yij are not. Other\napplications where the actual likelihood is well approximated by (5.12) appear in the next\ntwo sections.\n\nConstructing a prior distribution from pragmatic considerations\n\nRather than considering immediately the problem of specifying a prior distribution for the\nparameter vector θ = (θ1, . . . , θJ ), let us consider what sorts of posterior estimates might\nbe reasonable for θ, given data (yij). A simple natural approach is to estimate θj by y.j, the\naverage outcome in experiment j. But what if, for example, there are J = 20 experiments\nwith only nj = 2 observations per experimental group, and the groups are 20 pairs of\nassays taken from the same strain of rat, under essentially identical conditions? The two\nobservations per group do not permit accurate estimates. Since the 20 groups are from the\nsame strain of rat, we might now prefer to estimate each θj by the pooled estimate,\n\ny.. =\n\n∑J\nj=1\n\n1\nσ2\nj\n\ny.j\n∑J\n\nj=1\n1\nσ2\nj\n\n. (5.13)\n\nTo decide which estimate to use, a traditional approach from classical statistics is to\nperform an analysis of variance F test for differences among means: if the J group means\nappear significantly variable, choose separate sample means, and if the variance between\nthe group means is not significantly greater than what could be explained by individual\nvariability within groups, use y... The theoretical analysis of variance table is as follows,\nwhere τ2 is the variance of θ1, . . . , θJ . For simplicity, we present the analysis of variance for\na balanced design in which nj = n and σ2\n\nj = σ2/n for all j.\n\ndf SS MS E(MS|σ2, τ )\n\nBetween groups J − 1\n∑\n\ni\n\n∑\nj\n(y.j − y..)\n\n2 SS/(J − 1) nτ 2 + σ2\n\nWithin groups J(n− 1)\n∑\n\ni\n\n∑\nj\n(yij − y.j)\n\n2 SS/(J(n− 1)) σ2\n\nTotal Jn− 1\n∑\n\ni\n\n∑\nj\n(yij − y..)\n\n2 SS/(Jn − 1)\n\nIn the classical random-effects analysis of variance, one computes the sum of squares (SS)\nand the mean square (MS) columns of the table and uses the ‘between’ and ‘within’ mean\nsquares to estimate τ . If the ratio of between to within mean squares is significantly greater\nthan 1, then the analysis of variance suggests separate estimates, θ̂j = y.j for each j. If\nthe ratio of mean squares is not ‘statistically significant,’ then the F test cannot ‘reject the\nhypothesis’ that τ = 0, and pooling is reasonable: θ̂j = y.., for all j. We discuss Bayesian\nanalysis of variance in Section 15.6 in the context of hierarchical regression models.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 115\n\nBut we are not forced to choose between complete pooling and none at all. An alternative\nis to use a weighted combination:\n\nθ̂j = λjy.j + (1− λj)y..,\n\nwhere λj is between 0 and 1.\nWhat kind of prior models produce these various posterior estimates?\n\n1. The unpooled estimate θ̂j = y.j is the posterior mean if the J values θj have independent\nuniform prior densities on (−∞,∞).\n\n2. The pooled estimate θ̂ = y.. is the posterior mean if the J values θj are restricted to be\nequal, with a uniform prior density on the common θ.\n\n3. The weighted combination is the posterior mean if the J values θj have independent and\nidentically distributed normal prior densities.\n\nAll three of these options are exchangeable in the θj ’s, and options 1 and 2 are special cases\nof option 3. No pooling corresponds to λj ≡ 1 for all j and an infinite prior variance for\nthe θj ’s, and complete pooling corresponds to λj ≡ 0 for all j and a zero prior variance for\nthe θj ’s.\n\nThe hierarchical model\n\nFor the convenience of conjugacy (more accurately, partial conjugacy), we assume that the\nparameters θj are drawn from a normal distribution with hyperparameters (µ, τ):\n\np(θ1, . . . , θJ |µ, τ) =\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2) (5.14)\n\np(θ1, . . . , θJ ) =\n\n∫ J∏\n\nj=1\n\n[\nN(θj |µ, τ2)\n\n]\np(µ, τ)d(µ, τ).\n\nThat is, the θj ’s are conditionally independent given (µ, τ). The hierarchical model also\npermits the interpretation of the θj ’s as a random sample from a shared population distri-\nbution, as illustrated in Figure 5.1 for the rat tumors.\n\nWe assign a noninformative uniform hyperprior distribution to µ, given τ :\n\np(µ, τ) = p(µ|τ)p(τ) ∝ p(τ). (5.15)\n\nThe uniform prior density for µ is generally reasonable for this problem; because the com-\nbined data from all J experiments are generally highly informative about µ, we can afford\nto be vague about its prior distribution. We defer discussion of the prior distribution of\nτ to later in the analysis, although relevant principles have already been discussed in the\ncontext of the rat tumor example. As usual, we first work out the answer conditional on\nthe hyperparameters and then consider their prior and posterior distributions.\n\nThe joint posterior distribution\n\nCombining the sampling model for the observable yij ’s and the prior distribution yields\nthe joint posterior distribution of all the parameters and hyperparameters, which we can\nexpress in terms of the sufficient statistics, y.j:\n\np(θ, µ, τ |y) ∝ p(µ, τ)p(θ|µ, τ)p(y|θ)\n\n∝ p(µ, τ)\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2)\nJ∏\n\nj=1\n\nN(y.j |θj , σ2\nj ), (5.16)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n116 5. HIERARCHICAL MODELS\n\nwhere we can ignore factors that depend only on y and the parameters σj , which are assumed\nknown for this analysis.\n\nThe conditional posterior distribution of the normal means, given the hyperparameters\n\nAs in the general hierarchical structure, the parameters θj are independent in the prior\ndistribution (given µ and τ) and appear in different factors in the likelihood (5.11); thus,\nthe conditional posterior distribution p(θ|µ, τ, y) factors into J components.\n\nConditional on the hyperparameters, we simply have J independent unknown normal\nmeans, given normal prior distributions, so we can use the methods of Section 2.5 inde-\npendently on each θj . The conditional posterior distributions for the θj’s are independent,\nand\n\nθj |µ, τ, y ∼ N(θ̂j , Vj),\n\nwhere\n\nθ̂j =\n\n1\nσ2\nj\n\ny.j +\n1\nτ2µ\n\n1\nσ2\nj\n\n+ 1\nτ2\n\nand Vj =\n1\n\n1\nσ2\nj\n\n+ 1\nτ2\n\n. (5.17)\n\nThe posterior mean is a precision-weighted average of the prior population mean and the\nsample mean of the jth group; these expressions for θ̂j and Vj are functions of µ and τ as\nwell as the data. The conditional posterior density for each θj given µ, τ is proper.\n\nThe marginal posterior distribution of the hyperparameters\n\nThe solution so far is only partial because it depends on the unknown µ and τ . The next step\nin our approach is a full Bayesian treatment for the hyperparameters. Section 5.3 mentions\nintegration or analytic computation as two approaches for obtaining p(µ, τ |y) from the joint\nposterior density p(θ, µ, τ |y). For the hierarchical normal model, we can simply consider\nthe information supplied by the data about the hyperparameters directly:\n\np(µ, τ |y) ∝ p(µ, τ)p(y|µ, τ).\n\nFor many problems, this decomposition is no help, because the ‘marginal likelihood’ factor,\np(y|µ, τ), cannot generally be written in closed form. For the normal distribution, however,\nthe marginal likelihood has a particularly simple form. The marginal distributions of the\ngroup means y.j , averaging over θ, are independent (but not identically distributed) normal:\n\ny.j |µ, τ ∼ N(µ, σ2\nj + τ2).\n\nThus we can write the marginal posterior density as\n\np(µ, τ |y) ∝ p(µ, τ)\nJ∏\n\nj=1\n\nN(y.j|µ, σ2\nj + τ2). (5.18)\n\nPosterior distribution of µ given τ . We could use (5.18) to compute directly the posterior\ndistribution p(µ, τ |y) as a function of two variables and proceed as in the rat tumor example.\nFor the normal model, however, we can further simplify by integrating over µ, leaving a\nsimple univariate numerical computation of p(τ |y). We factor the marginal posterior density\nof the hyperparameters as we did the prior density (5.15):\n\np(µ, τ |y) = p(µ|τ, y)p(τ |y). (5.19)\n\nThe first factor on the right side of (5.19) is just the posterior distribution of µ if τ were\nknown. From inspection of (5.18) with τ assumed known, and with a uniform conditional\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 117\n\nprior density p(µ|τ), the log posterior distribution is found to be quadratic in µ; thus,\np(µ|τ, y) must be normal. The mean and variance of this distribution can be obtained\nimmediately by considering the group means y.j as J independent estimates of µ with\nvariances (σ2\n\nj + τ2). Combining the data with the uniform prior density p(µ|τ) yields\n\nµ|τ, y ∼ N(µ̂, Vµ),\n\nwhere µ̂ is the precision-weighted average of the y.j-values, and V\n−1\nµ is the total precision:\n\nµ̂ =\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2 y.j\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2\n\nand V −1\nµ =\n\nJ∑\n\nj=1\n\n1\n\nσ2\nj + τ2\n\n. (5.20)\n\nThe result is a proper posterior density for µ, given τ .\n\nPosterior distribution of τ . We can now obtain the posterior distribution of τ analyti-\ncally from (5.19) and substitution of (5.18) and (5.20) for the numerator and denominator,\nrespectively:\n\np(τ |y) =\np(µ, τ |y)\np(µ|τ, y)\n\n∝\np(τ)\n\n∏J\nj=1 N(y.j |µ, σ2\n\nj + τ2)\n\nN(µ|µ̂, Vµ)\n.\n\nThis identity must hold for any value of µ (in other words, all the factors of µ must cancel\nwhen the expression is simplified); in particular, it holds if we set µ to µ̂, which makes\nevaluation of the expression simple:\n\np(τ |y) ∝\np(τ)\n\n∏J\nj=1 N(y.j |µ̂, σ2\n\nj + τ2)\n\nN(µ̂|µ̂, Vµ)\n\n∝ p(τ)V 1/2\nµ\n\nJ∏\n\nj=1\n\n(σ2\nj + τ2)−1/2 exp\n\n(\n−\n\n(y.j − µ̂)2\n2(σ2\n\nj + τ2)\n\n)\n, (5.21)\n\nwith µ̂ and Vµ defined in (5.20). Both expressions are functions of τ , which means that\np(τ |y) is a complicated function of τ .\n\nPrior distribution for τ . To complete our analysis, we must assign a prior distribution to\nτ . For convenience, we use a diffuse noninformative prior density for τ and hence must\nexamine the resulting posterior density to ensure it has a finite integral. For our illustrative\nanalysis, we use the uniform prior distribution, p(τ) ∝ 1. We leave it as an exercise to show\nmathematically that the uniform prior density for τ yields a proper posterior density and\nthat, in contrast, the seemingly reasonable ‘noninformative’ prior distribution for a variance\ncomponent, p(log τ) ∝ 1, yields an improper posterior distribution for τ . Alternatively, in\napplications it involves little extra effort to determine a ‘best guess’ and an upper bound\nfor the population variance τ , and a reasonable prior distribution can then be constructed\nfrom the scaled inverse-χ2 family (the natural choice for variance parameters), matching the\n‘best guess’ to the mean of the scaled inverse-χ2 density and the upper bound to an upper\npercentile such as the 99th. Once an initial analysis is performed using the noninformative\n‘uniform’ prior density, a sensitivity analysis with a more realistic prior distribution is often\ndesirable.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n118 5. HIERARCHICAL MODELS\n\nComputation\n\nFor this model, computation of the posterior distribution of θ is most conveniently performed\nvia simulation, following the factorization used above:\n\np(θ, µ, τ |y) = p(τ |y)p(µ|τ, y)p(θ|µ, τ, y).\n\nThe first step, simulating τ , is easily performed numerically using the inverse cdf method\n(see Section 1.9) on a grid of uniformly spaced values of τ , with p(τ |y) computed from\n(5.21). The second and third steps, simulating µ and then θ, can both be done easily by\nsampling from normal distributions, first (5.20) to obtain µ and then (5.17) to obtain the\nθj ’s independently.\n\nPosterior predictive distributions\n\nSampling from the posterior predictive distribution of new data, either from a current or\nnew batch, is straightforward given draws from the posterior distribution of the parameters.\nWe consider two scenarios: (1) future data ỹ from the current set of batches, with means\nθ = (θ1, . . . , θJ), and (2) future data ỹ from J̃ future batches, with means θ̃ = (θ̃1, . . . , θ̃J̃).\n\nIn the latter case, we must also specify the J̃ individual sample sizes ñj for the future\nbatches.\n\nTo obtain a draw from the posterior predictive distribution of new data ỹ from the\ncurrent batch of parameters, θ, first obtain a draw from p(θ, µ, τ |y) and then draw the\npredictive data ỹ from (5.11).\n\nTo obtain posterior predictive simulations of new data ỹ for J̃ new groups, perform the\nfollowing three steps: first, draw (µ, τ) from their posterior distribution; second, draw J̃\nnew parameters θ̃ = (θ̃1, . . . , θ̃J̃) from the population distribution p(θ̃j |µ, τ), which is the\npopulation, or prior, distribution for θ given the hyperparameters (equation (5.14)); and\nthird, draw ỹ given θ̃ from the data distribution (5.11).\n\nDifficulty with a natural non-Bayesian estimate of the hyperparameters\n\nTo see some advantages of our fully Bayesian approach, we compare it to an approximate\nmethod that is sometimes used based on a point estimate of µ and τ from the data. Unbiased\npoint estimates, derived from the analysis of variance presented earlier, are\n\nµ̂ = y..\n\nτ̂2 = (MSB −MSW )/n. (5.22)\n\nThe terms MSB and MSW are the ‘between’ and ‘within’ mean squares, respectively, from\nthe analysis of variance. In this alternative approach, inference for θ1, . . . , θJ is based on\nthe conditional posterior distribution, p(θ|µ̂, τ̂ ), given the point estimates.\n\nAs we saw in the rat tumor example of the previous section, the main problem with\nsubstituting point estimates for the hyperparameters is that it ignores our real uncertainty\nabout them. The resulting inference for θ cannot be interpreted as a Bayesian posterior\nsummary. In addition, the estimate τ̂2 in (5.22) has the flaw that it can be negative! The\nproblem of a negative estimate for a variance component can be avoided by setting τ̂2 to\nzero in the case that MSW exceeds MSB , but this creates new issues. Estimating τ2 = 0\nwhenever MSW > MSB seems too strong a claim: if MSW > MSB, then the sample size is\ntoo small for τ2 to be distinguished from zero, but this is not the same as saying we know\nthat τ2 = 0. The latter claim, made implicitly by the point estimate, implies that all the\ngroup means θj are absolutely identical, which leads to scientifically indefensible claims, as\nwe shall see in the example in the next section. It is possible to construct a point estimate\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 119\n\nof (µ, τ) to avoid this particular difficulty, but it would still have the problem, common to\nall point estimates, of ignoring uncertainty.\n\n5.5 Example: parallel experiments in eight schools\n\nWe illustrate the hierarchical normal model with a problem in which the Bayesian analysis\ngives conclusions that differ in important respects from other methods.\n\nA study was performed for the Educational Testing Service to analyze the effects of\nspecial coaching programs on test scores. Separate randomized experiments were performed\nto estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test-\nVerbal) in each of eight high schools. The outcome variable in each study was the score on\na special administration of the SAT-V, a standardized multiple choice test administered by\nthe Educational Testing Service and used to help colleges make admissions decisions; the\nscores can vary between 200 and 800, with mean about 500 and standard deviation about\n100. The SAT examinations are designed to be resistant to short-term efforts directed\nspecifically toward improving performance on the test; instead they are designed to reflect\nknowledge acquired and abilities developed over many years of education. Nevertheless,\neach of the eight schools in this study considered its short-term coaching program to be\nsuccessful at increasing SAT scores. Also, there was no prior reason to believe that any of\nthe eight programs was more effective than any other or that some were more similar in\neffect to each other than to any other.\n\nThe results of the experiments are summarized in Table 5.2. All students in the ex-\nperiments had already taken the PSAT (Preliminary SAT), and allowance was made for\ndifferences in the PSAT-M (Mathematics) and PSAT-V test scores between coached and\nuncoached students. In particular, in each school the estimated coaching effect and its\nstandard error were obtained by an analysis of covariance adjustment (that is, a linear\nregression was performed of SAT-V on treatment group, using PSAT-M and PSAT-V as\ncontrol variables) appropriate for a completely randomized experiment. A separate regres-\nsion was estimated for each school. Although not simple sample means (because of the\ncovariance adjustments), the estimated coaching effects, which we label yj , and their sam-\npling variances, σ2\n\nj , play the same role in our model as y.j and σ2\nj in the previous section.\n\nThe estimates yj are obtained by independent experiments and have approximately normal\nsampling distributions with sampling variances that are known, for all practical purposes,\nbecause the sample sizes in all of the eight experiments were relatively large, over thirty\nstudents in each school (recall the discussion of data reduction in Section 4.1). Incidentally,\nan increase of eight points on the SAT-V corresponds to about one more test item correct.\n\nInferences based on nonhierarchical models and their problems\n\nBefore fitting the hierarchical Bayesian model, we first consider two simpler nonhierarchical\nmethods—estimating the effects from the eight experiments independently, and complete\npooling—and discuss why neither of these approaches is adequate for this example.\n\nSeparate estimates. A cursory examination of Table 5.2 may at first suggest that some\ncoaching programs have moderate effects (in the range 18–28 points), most have small\neffects (0–12 points), and two have small negative effects; however, when we take note\nof the standard errors of these estimated effects, we see that it is difficult statistically\nto distinguish between any of the experiments. For example, treating each experiment\nseparately and applying the simple normal analysis in each yields 95% posterior intervals\nthat all overlap substantially.\n\nA pooled estimate. The general overlap in the posterior intervals based on independent\nanalyses suggests that all experiments might be estimating the same quantity. Under the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n120 5. HIERARCHICAL MODELS\n\nEstimated Standard error\ntreatment of effect\n\nSchool effect, yj estimate, σj\nA 28 15\nB 8 10\nC −3 16\nD 7 11\nE −1 9\nF 1 11\nG 18 10\nH 12 18\n\nTable 5.2 Observed effects of special preparation on SAT-V scores in eight randomized experiments.\nEstimates are based on separate analyses for the eight experiments.\n\nhypothesis that all experiments have the same effect and produce independent estimates\nof this common effect, we could treat the data in Table 5.2 as eight normally distributed\nobservations with known variances. With a noninformative prior distribution, the posterior\nmean for the common coaching effect in the schools is y.., as defined in equation (5.13) with\n\nyj in place of y.j . This pooled estimate is 7.7, and the posterior variance is (\n∑8\n\nj=1\n1\nσ2\nj\n\n)−1 =\n\n16.6 because the eight experiments are independent. Thus, we would estimate the common\neffect to be 7.7 points with standard error equal to\n\n√\n16.6 = 4.1, which would lead to the\n\n95% posterior interval [−0.5, 15.9], or approximately [8 ± 8]. Supporting this analysis, the\nclassical test of the hypothesis that all θj ’s are estimating the same quantity yields a χ2\n\nstatistic less than its degrees of freedom (seven, in this case):\n∑8\n\nj=1(yj − y..)2/σ2\ni = 4.6. To\n\nput it another way, the estimate τ̂2 from (5.22) is negative.\n\nWould it be possible to have one school’s observed effect be 28 just by chance, if the\ncoaching effects in all eight schools were really the same? To get a feeling for the natural\nvariation that we would expect across eight studies if this assumption were true, suppose\nthe estimated treatment effects are eight independent draws from a normal distribution\nwith mean 8 points and standard deviation 13 points (the square root of the mean of the\neight variances σ2\n\nj ). Then, based on the expected values of normal order statistics, we\nwould expect the largest observed value of yj to be about 26 points and the others, in\ndiminishing order, to be about 19, 14, 10, 6, 2, −3, and −9 points. These expected effect\nsizes are consistent with the set of observed effect sizes in Table 5.2. Thus, it would appear\nimprudent to believe that school A really has an effect as large as 28 points.\n\nDifficulties with the separate and pooled estimates. To see the problems with the two ex-\ntreme attitudes—the separate analyses that consider each θj separately, and the alternative\nview (a single common effect) that leads to the pooled estimate—consider θ1, the effect in\nschool A. The effect in school A is estimated as 28.4 with a standard error of 14.9 under\nthe separate analysis, versus a pooled estimate of 7.7 with a standard error of 4.1 under\nthe common-effect model. The separate analyses of the eight schools imply the following\nposterior statement: ‘the probability is 1\n\n2 that the true effect in A is more than 28.4,’ a\ndoubtful statement, considering the results for the other seven schools. On the other hand,\nthe pooled model implies the following statement: ‘the probability is 1\n\n2 that the true effect\nin A is less than 7.7,’ which, despite the non-significant χ2 test, seems an inaccurate sum-\nmary of our knowledge. The pooled model also implies the statement: ‘the probability is 1\n\n2\nthat the true effect in A is less than the true effect in C,’ which also is difficult to justify\ngiven the data in Table 5.2. As in the theoretical discussion of the previous section, neither\nestimate is fully satisfactory, and we would like a compromise that combines information\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 121\n\nFigure 5.5 Marginal posterior density, p(τ |y), for standard deviation of the population of school\neffects θj in the educational testing example.\n\nfrom all eight experiments without assuming all the θj ’s to be equal. The Bayesian analysis\nunder the hierarchical model provides exactly that.\n\nPosterior simulation under the hierarchical model\n\nConsequently, we compute the posterior distribution of θ1, . . . , θ8, based on the normal\nmodel presented in Section 5.4. (More discussion of the reasonableness of applying this\nmodel in this problem appears in Sections 6.5 and 17.4.) We draw from the posterior\ndistribution for the Bayesian model by simulating the random variables τ , µ, and θ, in that\norder, from their posterior distribution, as discussed at the end of the previous section. The\nsampling standard deviations, σj , are assumed known and equal to the values in Table 5.2,\nand we assume independent uniform prior densities on µ and τ .\n\nResults\n\nThe marginal posterior density function, p(τ |y) from (5.21), is plotted in Figure 5.5. Values\nof τ near zero are most plausible; zero is the most likely value, values of τ larger than 10\nare less than half as likely as τ = 0, and Pr(τ > 25) ≈ 0. Inference regarding the marginal\ndistributions of the other model parameters and the joint distribution are obtained from the\nsimulated values. Illustrations are provided in the discussion that follows this section. In\nthe normal hierarchical model, however, we learn a great deal by considering the conditional\nposterior distributions given τ (and averaged over µ).\n\nThe conditional posterior means E(θj |τ, y) (averaging over µ) are displayed as functions\nof τ in Figure 5.6; the vertical axis displays the scale for the θj ’s. Comparing Figure 5.6\nto Figure 5.5, which has the same scale on the horizontal axis, we see that for most of the\nlikely values of τ , the estimated effects are relatively close together; as τ becomes larger,\ncorresponding to more variability among schools, the estimates become more like the raw\nvalues in Table 5.2.\n\nThe lines in Figure 5.7 show the conditional standard deviations, sd(θj |τ, y), as a func-\ntion of τ . As τ increases, the population distribution allows the eight effects to be more\ndifferent from each other, and hence the posterior uncertainty in each individual θj increases,\napproaching the standard deviations in Table 5.2 in the limit of τ → ∞. (The posterior\nmeans and standard deviations for the components θj , given τ , are computed using the\nmean and variance formulas (2.7) and (2.8), averaging over µ; see Exercise 5.12.)\n\nThe general conclusion from an examination of Figures 5.5–5.7 is that an effect as large\nas 28.4 points in any school is unlikely. For the likely values of τ , the estimates in all\nschools are substantially less than 28 points. For example, even at τ = 10, the probability\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n122 5. HIERARCHICAL MODELS\n\nFigure 5.6 Conditional posterior means of treatment effects, E(θj |τ, y), as functions of the between-\nschool standard deviation τ , for the educational testing example. The line for school C crosses the\nlines for E and F because C has a higher measurement error (see Table 5.2) and its estimate is\ntherefore shrunk more strongly toward the overall mean in the Bayesian analysis.\n\nFigure 5.7 Conditional posterior standard deviations of treatment effects, sd(θj |τ, y), as functions\nof the between-school standard deviation τ , for the educational testing example.\n\nthat the effect in school A is less than 28 points is Φ[(28 − 14.5)/9.1] = 93%, where Φ is\nthe standard normal cumulative distribution function; the corresponding probabilities for\nthe effects being less than 28 points in the other schools are 99.5%, 99.2%, 98.5%, 99.96%,\n99.8%, 97%, and 98%.\n\nOf substantial importance, we do not obtain an accurate summary of the data if we\ncondition on the posterior mode of τ . The technique of conditioning on a modal value (for\nexample, the maximum likelihood estimate) of a hyperparameter such as τ is often used\nin practice (at least as an approximation), but it ignores the uncertainty conveyed by the\nposterior distribution of the hyperparameter. At τ = 0, the inference is that all experiments\nhave the same size effect, 7.7 points, and the same standard error, 4.1 points. Figures 5.5–\n5.7 certainly suggest that this answer represents too much pulling together of the estimates\nin the eight schools. The problem is especially acute in this example because the posterior\nmode of τ is on the boundary of its parameter space. A joint posterior modal estimate of\n(θ1, . . . , θJ , µ, τ) suffers from even worse problems in general.\n\nDiscussion\n\nTable 5.3 summarizes the 200 simulated effect estimates for all eight schools. In one sense,\nthese results are similar to the pooled 95% interval [8± 8], in that the eight Bayesian 95%\nintervals largely overlap and are median-centered between 5 and 10. In a second sense,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 123\n\nSchool Posterior quantiles\n2.5% 25% median 75% 97.5%\n\nA −2 7 10 16 31\nB −5 3 8 12 23\nC −11 2 7 11 19\nD −7 4 8 11 21\nE −9 1 5 10 18\nF −7 2 6 10 28\nG −1 7 10 15 26\nH −6 3 8 13 33\n\nTable 5.3: Summary of 200 simulations of the treatment effects in the eight schools.\n\nFigure 5.8 Histograms of two quantities of interest computed from the 200 simulation draws: (a)\nthe effect in school A, θ1; (b) the largest effect, max{θj}. The jaggedness of the histograms is just\nan artifact caused by sampling variability from using only 200 random draws.\n\nthe results in the table differ from the pooled estimate in a direction toward the eight\nindependent answers: the 95% Bayesian intervals are each almost twice as wide as the one\ncommon interval and suggest substantially greater probabilities of effects larger than 16\npoints, especially in school A, and greater probabilities of negative effects, especially in\nschool C. If greater precision were required in the posterior intervals, one could simulate\nmore simulation draws; we use only 200 draws here to illustrate that a small simulation\ngives adequate inference for many practical purposes.\n\nThe ordering of the effects in the eight schools as suggested by Table 5.3 is essentially the\nsame as would be obtained by the eight separate estimates. However, there are differences\nin the details; for example, the Bayesian probability that the effect in school A is as large\nas 28 points is less than 10%, which is substantially less than the 50% probability based on\nthe separate estimate for school A.\n\nAs an illustration of the simulation-based posterior results, 200 simulations of school\nA’s effect are shown in Figure 5.8a. Having simulated the parameter θ, it is easy to ask\nmore complicated questions of this model. For example, what is the posterior distribution\nof max{θj}, the effect of the most successful of the eight coaching programs? Figure 5.8b\ndisplays a histogram of 200 values from this posterior distribution and shows that only 22\ndraws are larger than 28.4; thus, Pr(max{θj} > 28.4) ≈ 22\n\n200 . Since Figure 5.8a gives the\nmarginal posterior distribution of the effect in school A, and Figure 5.8b gives the marginal\nposterior distribution of the largest effect no matter which school it is in, the latter figure has\nlarger values. For another example, we can estimate Pr(θ1 > θ3|y), the posterior probability\nthat the coaching program is more effective in school A than in school C, by the proportion\nof simulated draws of θ for which θ1 > θ3; the result is 141\n\n200 = 0.705.\nTo sum up, the Bayesian analysis of this example not only allows straightforward infer-\n\nences about many parameters that may be of interest, but the hierarchical model is flexible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n124 5. HIERARCHICAL MODELS\n\nStudy,\nRaw data\n\n(deaths/total)\nLog-\nodds,\n\nsd,\nPosterior quantiles of effect θj\n\nnormal approx. (on log-odds scale)\nj Control Treated yj σj 2.5% 25% median 75% 97.5%\n\n1 3/39 3/38 0.028 0.850 −0.57 −0.33 −0.24 −0.16 0.12\n2 14/116 7/114 −0.741 0.483 −0.64 −0.37 −0.28 −0.20 −0.00\n3 11/93 5/69 −0.541 0.565 −0.60 −0.35 −0.26 −0.18 0.05\n4 127/1520 102/1533 −0.246 0.138 −0.45 −0.31 −0.25 −0.19 −0.05\n5 27/365 28/355 0.069 0.281 −0.43 −0.28 −0.21 −0.11 0.15\n6 6/52 4/59 −0.584 0.676 −0.62 −0.35 −0.26 −0.18 0.05\n7 152/939 98/945 −0.512 0.139 −0.61 −0.43 −0.36 −0.28 −0.17\n8 48/471 60/632 −0.079 0.204 −0.43 −0.28 −0.21 −0.13 0.08\n9 37/282 25/278 −0.424 0.274 −0.58 −0.36 −0.28 −0.20 −0.02\n\n10 188/1921 138/1916 −0.335 0.117 −0.48 −0.35 −0.29 −0.23 −0.13\n11 52/583 64/873 −0.213 0.195 −0.48 −0.31 −0.24 −0.17 0.01\n12 47/266 45/263 −0.039 0.229 −0.43 −0.28 −0.21 −0.12 0.11\n13 16/293 9/291 −0.593 0.425 −0.63 −0.36 −0.28 −0.20 0.01\n14 45/883 57/858 0.282 0.205 −0.34 −0.22 −0.12 0.00 0.27\n15 31/147 25/154 −0.321 0.298 −0.56 −0.34 −0.26 −0.19 0.01\n16 38/213 33/207 −0.135 0.261 −0.48 −0.30 −0.23 −0.15 0.08\n17 12/122 28/251 0.141 0.364 −0.47 −0.29 −0.21 −0.12 0.17\n18 6/154 8/151 0.322 0.553 −0.51 −0.30 −0.23 −0.13 0.15\n19 3/134 6/174 0.444 0.717 −0.53 −0.31 −0.23 −0.14 0.15\n20 40/218 32/209 −0.218 0.260 −0.50 −0.32 −0.25 −0.17 0.04\n21 43/364 27/391 −0.591 0.257 −0.64 −0.40 −0.31 −0.23 −0.09\n22 39/674 22/680 −0.608 0.272 −0.65 −0.40 −0.31 −0.23 −0.07\n\nTable 5.4 Results of 22 clinical trials of beta-blockers for reducing mortality after myocardial infarc-\ntion, with empirical log-odds and approximate sampling variances. Data from Yusuf et al. (1985).\nPosterior quantiles of treatment effects are based on 5000 draws from a Bayesian hierarchical model\ndescribed here. Negative effects correspond to reduced probability of death under the treatment.\n\nenough to adapt to the data, thereby providing posterior inferences that account for the\npartial pooling as well as the uncertainty in the hyperparameters.\n\n5.6 Hierarchical modeling applied to a meta-analysis\n\nMeta-analysis is an increasingly popular and important process of summarizing and inte-\ngrating the findings of research studies in a particular area. As a method for combining\ninformation from several parallel data sources, meta-analysis is closely connected to hierar-\nchical modeling. In this section we consider a relatively simple application of hierarchical\nmodeling to a meta-analysis in medicine. We consider another meta-analysis problem in\nthe context of a decision problem in Section 9.2.\n\nThe data in our medical example are displayed in the first three columns of Table 5.4,\nwhich summarize mortality after myocardial infarction in 22 clinical trials, each consisting of\ntwo groups of heart attack patients randomly allocated to receive or not receive beta-blockers\n(a family of drugs that affect the central nervous system and can relax the heart muscles).\nMortality varies from 3% to 21% across the studies, most of which show a modest, though\nnot ‘statistically significant,’ benefit from the use of beta-blockers. The aim of a meta-\nanalysis is to provide a combined analysis of the studies that indicates the overall strength\nof the evidence for a beneficial effect of the treatment under study. Before proceeding to a\nformal meta-analysis, it is important to apply rigorous criteria in determining which studies\nare included. (This relates to concerns of ignorability in data collection for observational\nstudies, as discussed in Chapter 8.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 125\n\nDefining a parameter for each study\n\nIn the beta-blocker example, the meta-analysis involves data in the form of several 2 × 2\ntables. If clinical trial j (in the series to be considered for meta-analysis) involves the use\nof n0j subjects in the control group and n1j in the treatment group, giving rise to y0j and\ny1j deaths in control and treatment groups, respectively, then the usual sampling model\ninvolves two independent binomial distributions with probabilities of death p0j and p1j ,\nrespectively. Estimands of interest include the difference in probabilities, p1j − p0j , the\nprobability or risk ratio, p1j/p0j , and the odds ratio, ρj =\n\np1j\n1−p1j /\n\np0j\n1−p0j . For a number of\n\nreasons, including interpretability in a range of study designs (including case-control studies\nas well as clinical trials and cohort studies), and the fact that its posterior distribution is\nclose to normality even for relatively small sample sizes, we concentrate on inference for the\n(natural) logarithm of the odds ratio, which we label θj = log ρj .\n\nA normal approximation to the likelihood\n\nRelatively simple Bayesian meta-analysis is possible using the normal-theory results of the\nprevious sections if we summarize the results of each experiment j with an approximate\nnormal likelihood for the parameter θj . This is possible with a number of standard analytic\napproaches that produce a point estimate and standard errors, which can be regarded as\napproximating a normal mean and standard deviation. One approach is based on empirical\nlogits: for each study j, one can estimate θj by\n\nyj = log\n\n(\ny1j\n\nn1j − y1j\n\n)\n− log\n\n(\ny0j\n\nn0j − y0j\n\n)\n, (5.23)\n\nwith approximate sampling variance\n\nσ2\nj =\n\n1\n\ny1j\n+\n\n1\n\nn1j − y1j\n+\n\n1\n\ny0j\n+\n\n1\n\nn0j − y0j\n. (5.24)\n\nWe use the notation yj and σ2\nj to be consistent with our earlier expressions for the hier-\n\narchical normal model. There are various refinements of these estimates that improve the\nasymptotic normality of the sampling distributions involved (in particular, it is often rec-\nommended to add a fraction such as 0.5 to each of the four counts in the 2× 2 table), but\nwhenever study-specific sample sizes are moderately large, such details do not concern us.\n\nThe estimated log-odds ratios yj and their estimated standard errors σ2\nj are displayed\n\nas the fourth and fifth columns of Table 5.4. We use a hierarchical Bayesian analysis to\ncombine information from the 22 studies and gain improved estimates of each θj , along with\nestimates of the mean and variance of the effects over all studies.\n\nGoals of inference in meta-analysis\n\nDiscussions of meta-analysis are sometimes imprecise about the estimands of interest in the\nanalysis, especially when the primary focus is on testing the null hypothesis of no effect in\nany of the studies to be combined. Our focus is on estimating meaningful parameters, and\nfor this objective there appear to be three possibilities, accepting the overarching assumption\nthat the studies are comparable in some broad sense. The first possibility is that we view\nthe studies as identical replications of each other, in the sense we regard the individuals in\nall the studies as independent samples from a common population, with the same outcome\nmeasures and so on. A second possibility is that the studies are so different that the results\nof any one study provide no information about the results of any of the others. A third, more\ngeneral, possibility is that we regard the studies as exchangeable but not necessarily either\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n126 5. HIERARCHICAL MODELS\n\nidentical or completely unrelated; in other words we allow differences from study to study,\nbut such that the differences are not expected a priori to have predictable effects favoring\none study over another. As we have discussed in detail in this chapter, this third possibility\nrepresents a continuum between the two extremes, and it is this exchangeable model (with\nunknown hyperparameters characterizing the population distribution) that forms the basis\nof our Bayesian analysis.\n\nExchangeability does not dictate the form of the joint distribution of the study effects.\nIn what follows we adopt the convenient assumption of a normal distribution for the varying\nparameters; in practice it is important to check this assumption using some of the techniques\ndiscussed in Chapter 6.\n\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem\nin general, is the mean of the distribution of effect sizes, since this represents the overall\n‘average’ effect across all studies that could be regarded as exchangeable with the observed\nstudies. Other possible estimands are the effect size in any of the observed studies and the\neffect size in another, comparable (exchangeable) unobserved study.\n\nWhat if exchangeability is inappropriate?\n\nWhen assuming exchangeability we assume there are no important covariates that might\nform the basis of a more complex model, and this assumption (perhaps misguidedly) is\nwidely adopted in meta-analysis. What if other information (in addition to the data (n, y))\nis available to distinguish among the J studies in a meta-analysis, so that an exchangeable\nmodel is inappropriate? In this situation, we can expand the framework of the model to be\nexchangeable in the observed data and covariates, for example using a hierarchical regression\nmodel, as in Chapter 15, so as to estimate how the treatment effect behaves as a function\nof the covariates. The real aim might in general be to estimate a response surface so that\none could predict an effect based on known characteristics of a population and its exposure\nto risk.\n\nA hierarchical normal model\n\nA normal population distribution in conjunction with the approximate normal sampling\ndistribution of the study-specific effect estimates allows an analysis of the same form as\nused for the SAT coaching example in the previous section. Let yj represent generically the\npoint estimate of the effect θj in the jth study, obtained from (5.23), where j = 1, . . . , J .\nThe first stage of the hierarchical normal model assumes that\n\nyj |θj , σj ∼ N(θj , σ\n2\nj ),\n\nwhere σj represents the corresponding estimated standard error from (5.24), which is as-\nsumed known without error. The simplification of known variances has little effect here\nbecause, with the large sample sizes (more than 50 persons in each treatment group in\nnearly all of the studies in the beta-blocker example), the binomial variances in each study\nare precisely estimated. At the second stage of the hierarchy, we again use an exchangeable\nnormal prior distribution, with mean µ and standard deviation τ , which are unknown hy-\nperparameters. Finally, a hyperprior distribution is required for µ and τ . For this problem,\nit is reasonable to assume a noninformative or locally uniform prior density for µ, since\neven with a small number of studies (say 5 or 10), the combined data become relatively\ninformative about the center of the population distribution of effect sizes. As with the\nSAT coaching example, we also assume a locally uniform prior density for τ , essentially for\nconvenience, although it is easy to modify the analysis to include prior information.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 127\n\nPosterior quantiles\nEstimand 2.5% 25% median 75% 97.5%\n\nMean, µ −0.37 −0.29 −0.25 −0.20 −0.11\nStandard deviation, τ 0.02 0.08 0.13 0.18 0.31\n\nPredicted effect, θ̃j −0.58 −0.34 −0.25 −0.17 0.11\n\nTable 5.5 Summary of posterior inference for the overall mean and standard deviation of study\neffects, and for the predicted effect in a hypothetical future study, from the meta-analysis of the\nbeta-blocker trials in Table 5.4. All effects are on the log-odds scale.\n\nResults of the analysis and comparison to simpler methods\n\nThe analysis of our meta-analysis model now follows exactly the same methodology as in\nthe previous sections. First, a plot (not shown here) similar to Figure 5.5 shows that the\nmarginal posterior density of τ peaks at a nonzero value, although values near zero are\nclearly plausible, zero having a posterior density only about 25% lower than that at the\nmode. Posterior quantiles for the effects θj for the 22 studies on the logit scale are displayed\nas the last columns of Table 5.4.\n\nSince the posterior distribution of τ is concentrated around values that are small relative\nto the sampling standard deviations of the data (compare the posterior median of τ , 0.13,\nin Table 5.5 to the values of σj in the fourth column of Table 5.4), considerable shrinkage\nis evident in the Bayes estimates, especially for studies with low internal precision (for\nexample, studies 1, 6, and 18). The substantial degree of homogeneity between the studies\nis further reflected in the large reductions in posterior variance obtained when going from\nthe study-specific estimates to the Bayesian ones, which borrow strength from each other.\nUsing an approximate approach fixing τ would yield standard deviations that would be too\nsmall compared to the fully Bayesian ones.\n\nHistograms (not shown) of the simulated posterior densities for each of the individual\neffects exhibit skewness away from the central value of the overall mean, whereas the distri-\nbution of the overall mean has greater symmetry. The imprecise studies, such as 2 and 18,\nexhibit longer-tailed posterior distributions than the more precise ones, such as 7 and 14.\n\nIn meta-analysis, interest often focuses on the estimate of the overall mean effect, µ.\nSuperimposing the graphs (not shown here) of the conditional posterior mean and standard\ndeviation of µ given τ on the posterior density of τ reveals a small range in the plausible\nvalues of E(µ|τ, y), from about −0.26 to just over −0.24, but sd(µ|τ, y) varies by a factor\nof more than 2 across the plausible range of values of τ . The latter feature indicates\nthe importance of averaging over τ in order to account adequately for uncertainty in its\nestimation. In fact, the conditional posterior standard deviation, sd(µ|τ, y) has the value\n0.060 at τ = 0.13, whereas upon averaging over the posterior distribution for τ we find a\nvalue of sd(µ|y) = 0.071.\n\nTable 5.5 gives a summary of posterior inferences for the hyperparameters µ and τ and\nthe predicted effect, θ̃j , in a hypothetical future study. The approximate 95% highest pos-\nterior density interval for µ is [−0.37,−0.11], or [0.69, 0.90] when converted to the odds\nratio scale (that is, exponentiated). In contrast, the 95% posterior interval that results\nfrom complete pooling—that is, assuming τ = 0—is considerably narrower, [0.70, 0.85]. In\nthe original published discussion of these data, it was remarked that the latter seems an\n‘unusually narrow range of uncertainty.’ The hierarchical Bayesian analysis suggests that\nthis was due to the use of an inappropriate model that had the effect of claiming all the\nstudies were identical. In mathematical terms, complete pooling makes the assumption that\nthe parameter τ is exactly zero, whereas the data supply evidence that τ might be close\nto zero, but might also plausibly be as high as 0.3. A related concern is that commonly\nused analyses tend to place undue emphasis on inference for the overall mean effect. Un-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n128 5. HIERARCHICAL MODELS\n\ncertainty about the probable treatment effect in a particular population where a study has\nnot been performed (or indeed in a previously studied population but with a slightly mod-\nified treatment) might be more reasonably represented by inference for a new study effect,\nexchangeable with those for which studies have been performed, rather than for the overall\nmean. In this case, uncertainty is even greater, as exhibited in the ‘Predicted effect’ row of\nTable 5.5; uncertainty for an individual patient includes yet another component of varia-\ntion. In particular, with the beta-blocker data, there is just over 10% posterior probability\nthat the true effect, θ̃j , in a new study would be positive (corresponding to the treatment\nincreasing the probability of death in that study).\n\n5.7 Weakly informative priors for variance parameters\n\nA key element in the analyses above is the prior distribution for the scale parameter, τ .\nWe have used the uniform, but various other noninformative prior distributions have been\nsuggested in the Bayesian literature. It turns out that the choice of ‘noninformative’ prior\ndistribution can have a big effect on inferences, especially for problems where the number\nof groups J is small or the group-level variation τ is small.\n\nWe discuss the options here in the context of the normal model, but the principles apply\nto inferences for group-level variances more generally.\n\nConcepts relating to the choice of prior distribution\n\nImproper limit of a prior distribution. Improper prior densities can, but do not necessarily,\nlead to proper posterior distributions. To avoid confusion it is useful to define improper\ndistributions as particular limits of proper distributions. For the group-level variance pa-\nrameter, two commonly considered improper densities are uniform(0, A) on τ , as A → ∞,\nand inverse-gamma(ǫ, ǫ) on τ2, as ǫ→ 0.\n\nAs we shall see, the uniform(0, A) model yields a limiting proper posterior density as\nA→ ∞, as long as the number of groups J is at least 3. Thus, for a finite but sufficiently\nlarge A, inferences are not sensitive to the choice of A.\n\nIn contrast, the inverse-gamma(ǫ, ǫ) model does not have any proper limiting poste-\nrior distribution. As a result, posterior inferences are sensitive to ǫ—it cannot simply be\ncomfortably set to a low value such as 0.001.\n\nCalibration. Posterior inferences can be evaluated using the concept of calibration of the\nposterior mean, the Bayesian analogue to the classical notion of bias. For any parameter\nθ, if we label the posterior mean as θ̂ = E(θ|y), we can define the miscalibration of the\n\nposterior mean as E(θ|θ̂) − θ̂. If the prior distribution is true—that is, if the data are\nconstructed by first drawing θ from p(θ), then drawing y from p(y|θ)—then the posterior\n\nmean is automatically calibrated; that is, the miscalibration is 0 for all values of θ̂.\nTo restate: in classical bias analysis, we condition on the true θ and look at the distri-\n\nbution of the data-based estimate, θ̂. In a Bayesian calibration analysis, we condition on\nthe data y (and thus also on the estimate, θ̂) and look at the distribution of parameters θ\nthat could have produced these data.\n\nWhen considering improper models, the theory must be expanded, since it is impossible\nfor θ to be drawn from an unnormalized density. To evaluate calibration in this context,\nit is necessary to posit a ‘true prior distribution’ from which θ is drawn along with the\n‘inferential prior distribution’ that is used in the Bayesian inference.\n\nFor the hierarchical model for the 8 schools, we can consider the improper uniform\ndensity on τ as a limit of uniform prior densities on the range (0, A), with A → ∞. For\nany finite value of A, we can then see that the improper uniform density leads to inferences\nwith a positive miscalibration—that is, overestimates (on average) of τ .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 129\n\nWe demonstrate this miscalibration in two steps. First, suppose that both the true\nand inferential prior distributions for τ are uniform on (0, A). Then the miscalibration is\ntrivially zero. Now keep the true prior distribution at U(0, A) and let the inferential prior\n\ndistribution go to U(0,∞). This will necessarily increase θ̂ for any data y (since we are now\naveraging over values of θ in the range [A,∞)) without changing the true θ, thus causing\nthe average value of the miscalibration to become positive.\n\nClasses of noninformative and weakly informative prior distributions for hierarchical\nvariance parameters\n\nGeneral considerations. We view any noninformative or weakly informative prior distribu-\ntion as inherently provisional—after the model has been fit, one should look at the posterior\ndistribution and see if it makes sense. If the posterior distribution does not make sense,\nthis implies that additional prior knowledge is available that has not been included in the\nmodel, and that contradicts the assumptions of the prior distribution that has been used.\nIt is then appropriate to go back and alter the prior distribution to be more consistent with\nthis external knowledge.\n\nUniform prior distributions. We first consider uniform priors while recognizing that we\nmust be explicit about the scale on which the distribution is defined. Various choices have\nbeen proposed for modeling variance parameters. A uniform prior distribution on log τ\nwould seem natural—working with the logarithm of a parameter that must be positive—\nbut it results in an improper posterior distribution. An alternative would be to define the\nprior distribution on a compact set (e.g., in the range [−A,A] for some large value of A),\nbut then the posterior distribution would depend strongly on the lower bound −A of the\nprior support.\n\nThe problem arises because the marginal likelihood, p(y|τ)—after integrating over θ and\nµ in (5.16)—approaches a finite nonzero value as τ → 0. Thus, if the prior density for log τ\nis uniform, the posterior will have infinite mass integrating to the limit log τ → −∞. To put\nit another way, in a hierarchical model the data can never rule out a group-level variance\nof zero, and so the prior distribution cannot put an infinite mass in this area.\n\nAnother option is a uniform prior distribution on τ itself, which has a finite integral\nnear τ = 0 and thus avoids the above problem. We have generally used this noninformative\ndensity in our applied work (as illustrated in Section 5.5), but it has a slightly disagreeable\nmiscalibration toward positive values, with its infinite prior mass in the range τ → ∞.\nWith J = 1 or 2 groups, this actually results in an improper posterior density, essentially\nconcluding τ = ∞ and doing no pooling. In a sense this is reasonable behavior, since it\nwould seem difficult from the data alone to decide how much, if any, pooling should be\ndone with data from only one or two groups. However, from a Bayesian perspective it is\nawkward for the decision to be made ahead of time, as it were, with the data having no say\nin the matter. In addition, for small J , such as 4 or 5, we worry that the heavy right tail of\nthe posterior distribution would lead to overestimates of τ and thus result in pooling that\nis less than optimal for estimating the individual θj ’s.\n\nWe can interpret these improper uniform prior densities as limits of weakly informative\nconditionally conjugate priors. The uniform prior distribution on log τ is equivalent to\np(τ) ∝ τ−1 or p(τ2) ∝ τ−2, which has the form of an inverse-χ2 density with 0 degrees of\nfreedom and can be taken as a limit of proper inverse-gamma priors.\n\nThe uniform density on τ is equivalent to p(τ2) ∝ τ−1, an inverse-χ2 density with −1\ndegrees of freedom. This density cannot easily be seen as a limit of proper inverse-χ2\n\ndensities (since these must have positive degrees of freedom), but it can be interpreted as a\nlimit of the half-t family on τ , where the scale approaches ∞ (and any value of ν).\n\nAnother noninformative prior distribution sometimes proposed in the Bayesian literature\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n130 5. HIERARCHICAL MODELS\n\nis uniform on τ2. We do not recommend this, as it seems to have the miscalibration toward\nhigher values as described above, but more so, and also requires J ≥ 4 groups for a proper\nposterior distribution.\n\nInverse-gamma(ǫ, ǫ) prior distributions. The parameter τ in model (5.21) does not have\nany simple family of conjugate prior distributions because its marginal likelihood depends\nin a complex way on the data from all J groups. However, the inverse-gamma family\nis conditionally conjugate given the other parameters in the model: that is, if τ2 has an\ninverse-gamma prior distribution, then the conditional posterior distribution p(τ2 | θ, µ, y)\nis also inverse-gamma. The inverse-gamma(α, β) model for τ2 can also be expressed as an\ninverse-χ2 distribution with scale s2 = β\n\nα and degrees of freedom ν = 2α. The inverse-\nχ2 parameterization can be helpful in understanding the information underlying various\nchoices of proper prior distributions.\n\nThe inverse-gamma(ǫ, ǫ) prior distribution is an attempt at noninformativeness within\nthe conditionally conjugate family, with ǫ set to a low value such as 1 or 0.01 or 0.001.\nA difficulty of this prior distribution is that in the limit of ǫ → 0 it yields an improper\nposterior density, and thus ǫ must be set to a reasonable value. Unfortunately, for datasets\nin which low values of τ are possible, inferences become very sensitive to ǫ in this model,\nand the prior distribution hardly looks noninformative, as we illustrate in Figure 5.9.\n\nHalf-Cauchy prior distributions. We shall also consider the t family of distributions (actu-\nally, the half-t, since the scale parameter τ is constrained to be positive) as an alternative\nclass that includes normal and Cauchy as edge cases. We first considered the t model for\nthis problem because it can be expressed as a conditionally conjugate prior distribution for\nτ using a reparameterization.\n\nFor our purposes here, however, it is enough to recognize that the half-Cauchy can be a\nconvenient weakly informative family; the distribution has a broad peak at zero and a single\nscale parameter, which we shall label A to indicate that it could be set to some large value.\nIn the limit A → ∞ this becomes a uniform prior density on τ . Large but finite values of\nA represent prior distributions which we consider weakly informative because, even in the\ntail, they have a gentle slope (unlike, for example, a half-normal distribution) and can let\nthe data dominate if the likelihood is strong in that region. We shall consider half-Cauchy\nmodels for variance parameters which are estimated from a small number of groups (so that\ninferences are sensitive to the choice of weakly informative prior distribution).\n\nApplication to the 8-schools example\n\nWe demonstrate the properties of some proposed noninformative prior densities on the\neight-schools example of Section 5.5. Here, the parameters θ1, . . . , θ8 represent the relative\neffects of coaching programs in eight different schools, and τ represents the between-school\nstandard deviations of these effects. The effects are measured as points on the test, which\nwas scored from 200 to 800 with an average of about 500; thus the largest possible range of\neffects could be about 300 points, with a realistic upper limit on τ of 100, say.\n\nNoninformative prior distributions for the 8-schools problem. Figure 5.9 displays the pos-\nterior distributions for the 8-schools model resulting from three different choices of prior\ndistributions that are intended to be noninformative.\n\nThe leftmost histogram shows posterior inference for τ for the model with uniform prior\ndensity. The data show support for a range of values below τ = 20, with a slight tail after\nthat, reflecting the possibility of larger values, which are difficult to rule out given that the\nnumber of groups J is only 8—that is, not much more than the J = 3 required to ensure a\nproper posterior density with finite mass in the right tail.\n\nIn contrast, the middle histogram in Figure 5.9 shows the result with an inverse-\ngamma(1, 1) prior distribution for τ2. This new prior distribution leads to changed in-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 131\n\nFigure 5.9 Histograms of posterior simulations of the between-school standard deviation, τ ,\nfrom models with three different prior distributions: (a) uniform prior distribution on τ , (b)\ninverse-gamma(1, 1) prior distribution on τ 2, (c) inverse-gamma(0.001, 0.001) prior distribution\non τ 2. Overlain on each is the corresponding prior density function for τ . (For models (b) and\n(c), the density for τ is calculated using the gamma density function multiplied by the Jacobian of\nthe 1/τ 2 transformation.) In models (b) and (c), posterior inferences are strongly constrained by\nthe prior distribution.\n\nferences. In particular, the posterior mean and median of τ are lower, and shrinkage of the\nθj ’s is greater than in the previously fitted model with a uniform prior distribution on τ . To\nunderstand this, it helps to graph the prior distribution in the range for which the posterior\ndistribution is substantial. The graph shows that the prior distribution is concentrated in\nthe range [0.5, 5], a narrow zone in which the likelihood is close to flat compared to this prior\n(as we can see because the distribution of the posterior simulations of τ closely matches the\nprior distribution, p(τ)). By comparison, in the left graph, the uniform prior distribution\non τ seems closer to ‘noninformative’ for this problem, in the sense that it does not appear\nto be constraining the posterior inference.\n\nFinally, the rightmost histogram in Figure 5.9 shows the corresponding result with an\ninverse-gamma(0.001, 0.001) prior distribution for τ2. This prior distribution is even more\nsharply peaked near zero and further distorts posterior inferences, with the problem arising\nbecause the marginal likelihood for τ remains high near zero.\n\nIn this example, we do not consider a uniform prior density on log τ , which would yield\nan improper posterior density with a spike at τ = 0, like the rightmost graph in Figure 5.9\nbut more so. We also do not consider a uniform prior density on τ2, which would yield a\nposterior similar to the leftmost graph in Figure 5.9, but with a slightly higher right tail.\n\nThis example is a gratifying case in which the simplest approach—the uniform prior\ndensity on τ—seems to perform well. As detailed in Appendix C, this model is also straight-\nforward to program directly in R or Stan.\n\nThe appearance of the histograms and density plots in Figure 5.9 is crucially affected by\nthe choice to plot them on the scale of τ . If instead they were plotted on the scale of log τ ,\nthe inverse-gamma(0.001, 0.001) prior density would appear to be the flattest. However, the\ninverse-gamma(ǫ, ǫ) prior is not at all ‘noninformative’ for this problem since the resulting\nposterior distribution remains highly sensitive to the choice of ǫ. The hierarchical model\nlikelihood does not constrain log τ in the limit log τ → −∞, and so a prior distribution that\nis noninformative on the log scale will not work.\n\nWeakly informative prior distribution for the 3-schools problem\n\nThe uniform prior distribution seems fine for the 8-school analysis, but problems arise if the\nnumber of groups J is much smaller, in which case the data supply little information about\nthe group-level variance, and a noninformative prior distribution can lead to a posterior\ndistribution that is improper or is proper but unrealistically broad. We demonstrate by\nreanalyzing the 8-schools example using just the data from the first three of the schools.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n132 5. HIERARCHICAL MODELS\n\nFigure 5.10 Histograms of posterior simulations of the between-school standard deviation, τ , from\nmodels for the 3-schools data with two different prior distributions on τ : (a) uniform (0,∞), (b)\nhalf-Cauchy with scale 25, set as a weakly informative prior distribution given that τ was expected\nto be well below 100. The histograms are not on the same scales. Overlain on each histogram is\nthe corresponding prior density function. With only J = 3 groups, the noninformative uniform\nprior distribution is too weak, and the proper Cauchy distribution works better, without appearing\nto distort inferences in the area of high likelihood.\n\nFigure 5.10 displays the inferences for τ based on two different priors. First we continue\nwith the default uniform distribution that worked well with J = 8 (as seen in Figure 5.9).\nUnfortunately, as the left histogram of Figure 5.10 shows, the resulting posterior distribution\nfor the 3-schools dataset has an extremely long right tail, containing values of τ that are\ntoo high to be reasonable. This heavy tail is expected since J is so low (if J were any lower,\nthe right tail would have an infinite integral), and using this as a posterior distribution will\nhave the effect of underpooling the estimates of the school effects θj .\n\nThe right histogram of Figure 5.10 shows the posterior inference for τ resulting from\na half-Cauchy prior distribution with scale parameter A = 25 (a value chosen to be a bit\nhigher than we expect for the standard deviation of the underlying θj ’s in the context of\nthis educational testing example, so that the model will constrain τ only weakly). As the\nline on the graph shows, this prior distribution is high over the plausible range of τ < 50,\nfalling off gradually beyond this point. This prior distribution appears to perform well in\nthis example, reflecting the marginal likelihood for τ at its low end but removing much of\nthe unrealistic upper tail.\n\nThis half-Cauchy prior distribution would also perform well in the 8-schools problem;\nhowever it was unnecessary because the default uniform prior gave reasonable results. With\nonly 3 schools, we went to the trouble of using a weakly informative prior, a distribution\nthat was not intended to represent our actual prior state of knowledge about τ but rather\nto constrain the posterior distribution, to an extent allowed by the data.\n\n5.8 Bibliographic note\n\nThe early non-Bayesian work on shrinkage estimation of Stein (1955) and James and Stein\n(1960) was influential in the development of hierarchical normal models. Efron and Morris\n(1971, 1972) present subsequent theoretical work on the topic. Robbins (1955, 1964) con-\nstructs and justifies hierarchical methods from a decision-theoretic perspective. De Finetti’s\ntheorem is described by de Finetti (1974); Bernardo and Smith (1994) discuss its role in\nBayesian modeling. An early thorough development of the idea of Bayesian hierarchical\nmodeling is given by Good (1965).\n\nMosteller and Wallace (1964) analyzed a hierarchical Bayesian model using the negative\nbinomial distribution for counts of words in a study of authorship. Restricted to the limited\ncomputing power at the time, they used various approximations and point estimates for\nhyperparameters.\n\nOther historically influential papers on ‘empirical Bayes’ (or, in our terminology, hierar-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.8. BIBLIOGRAPHIC NOTE 133\n\nchical Bayes) include Hartley and Rao (1967), Laird and Ware (1982) on longitudinal mod-\neling, and Clayton and Kaldor (1987) and Breslow (1990) on epidemiology and biostatistics.\nMorris (1983) and Deely and Lindley (1981) explored the relation between Bayesian and\nnon-Bayesian ideas for these models.\n\nThe problem of estimating several normal means using an exchangeable hierarchical\nmodel was treated in a fully Bayesian framework by Hill (1965), Tiao and Tan (1965,\n1966), and Lindley (1971b). Box and Tiao (1973) present hierarchical normal models using\nslightly different notation from ours. They compare Bayesian and non-Bayesian methods\nand discuss the analysis of variance table in some detail. More references on hierarchical\nnormal models appear in the bibliographic note at the end of Chapter 15.\n\nThe past few decades have seen the publication of applied Bayesian analyses using hierar-\nchical models in a wide variety of application areas. For example, an important application\nof hierarchical models is ‘small-area estimation,’ in which estimates of population charac-\nteristics for local areas are improved by combining the data from each area with information\nfrom neighboring areas (with important early work from Fay and Herriot, 1979, Dempster\nand Raghunathan, 1987, and Mollie and Richardson, 1991). Other applications that have\nmotivated methodological development include measurement error problems in epidemiol-\nogy (for example, Richardson and Gilks, 1993), multiple comparisons in toxicology (Meng\nand Dempster, 1987), and education research (Bock, 1989). We provide references to a\nnumber of other applications in later chapters dealing with specific model types.\n\nHierarchical models can be viewed as a subclass of ‘graphical models,’ and this connec-\ntion has been elegantly exploited for Bayesian inference in the development of the computer\npackage Bugs, using techniques that will be explained in Chapter 11 (see also Appendix C);\nsee Thomas, Spiegelhalter, and Gilks (1992), and Spiegelhalter et al. (1994, 2003). Related\ndiscussion and theoretical work appears in Lauritzen and Spiegelhalter (1988), Pearl (1988),\nWermuth and Lauritzen (1990), and Normand and Tritchler (1992).\n\nThe rat tumor data were analyzed hierarchically by Tarone (1982) and Dempster, Sel-\nwyn, and Weeks (1983); our approach is close in spirit to the latter paper’s. Leonard (1972)\nand Novick, Lewis, and Jackson (1973) are early examples of hierarchical Bayesian analysis\nof binomial data.\n\nMuch of the material in Sections 5.4 and 5.5, along with much of Section 6.5, originally\nappeared in Rubin (1981a), which is an early example of an applied Bayesian analysis using\nsimulation techniques. For later work on the effects of coaching on Scholastic Aptitude Test\nscores, see Hansen (2004).\n\nThe weakly-informative half-Cauchy prior distribution for the 3-schools problem in Sec-\ntion 5.7 comes from Gelman (2006a). Polson and Scott (2012) provide a theoretical justifi-\ncation for this model.\n\nThe material of Section 5.6 is adapted from Carlin (1992), which contains several key\nreferences on meta-analysis; the original data for the example are from Yusuf et al. (1985);\na similar Bayesian analysis of these data under a slightly different model appears as an\nexample in Spiegelhalter et al. (1994, 2003). Thall et al. (2003) discuss hierarchical models\nfor medical treatments that vary across subtypes of a disease. More general treatments\nof meta-analysis from a Bayesian perspective are provided by DuMouchel (1990), Rubin\n(1989), Skene and Wakefield (1990), and Smith, Spiegelhalter, and Thomas (1995). An ex-\nample of a Bayesian meta-analysis appears in Dominici et al. (1999). DuMouchel and Harris\n(1983) present what is essentially a meta-analysis with covariates on the studies; this article\nis accompanied by some interesting discussion by prominent Bayesian and non-Bayesian\nstatisticians. Higgins and Whitehead (1996) discuss how to construct a prior distribution\nfor the group-level variance in a meta-analysis by considering it as an example from larger\npopulation of meta-analyses. Lau, Ioannidis, and Schmid (1997) provide practical advice\non meta-analysis.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n134 5. HIERARCHICAL MODELS\n\n5.9 Exercises\n\n1. Exchangeability with known model parameters: For each of the following three examples,\nanswer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and y2\nindependent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has one black ball and one white ball. We pick a ball y1 at random, put it back,\nand pick another ball y2 at random.\n\n(b) A box has one black ball and one white ball. We pick a ball y1 at random, we do not\nput it back, then we pick ball y2.\n\n(c) A box has a million black balls and a million white balls. We pick a ball y1 at random,\nwe do not put it back, then we pick ball y2 at random.\n\n2. Exchangeability with unknown model parameters: For each of the following three exam-\nples, answer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and\ny2 independent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, put it back, and pick another ball y2 at random.\n\n(b) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, we do not put it back, then we pick ball y2 at random.\n\n(c) Same as (b) but we know that there are many balls of each color in the box.\n\n3. Hierarchical models and multiple comparisons:\n\n(a) Reproduce the computations in Section 5.5 for the educational testing example. Use\nthe posterior simulations to estimate (i) for each school j, the probability that its\ncoaching program is the best of the eight; and (ii) for each pair of schools, j and k,\nthe probability that the coaching program in school j is better than that in school k.\n\n(b) Repeat (a), but for the simpler model with τ set to ∞ (that is, separate estimation\nfor the eight schools). In this case, the probabilities (ii) can be computed analytically.\n\n(c) Discuss how the answers in (a) and (b) differ.\n\n(d) In the model with τ set to 0, the probabilities (i) and (ii) have degenerate values; what\nare they?\n\n4. Exchangeable prior distributions: suppose it is known a priori that the 2J parameters\nθ1, . . . , θ2J are clustered into two groups, with exactly half being drawn from a N(1, 1)\ndistribution, and the other half being drawn from a N(−1, 1) distribution, but we have\nnot observed which parameters come from which distribution.\n\n(a) Are θ1, . . . , θ2J exchangeable under this prior distribution?\n\n(b) Show that this distribution cannot be written as a mixture of independent and iden-\ntically distributed components.\n\n(c) Why can we not simply take the limit as J → ∞ and get a counterexample to de\nFinetti’s theorem?\n\nSee Exercise 8.10 for a related problem.\n\n5. Mixtures of independent distributions: suppose the distribution of θ = (θ1, . . . , θJ) can\nbe written as a mixture of independent and identically distributed components:\n\np(θ) =\n\n∫ J∏\n\nj=1\n\np(θj |φ)p(φ)dφ.\n\nProve that the covariances cov(θi, θj) are all nonnegative.\n\n6. Exchangeable models:\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 135\n\n(a) In the divorce rate example of Section 5.2, set up a prior distribution for the values\ny1, . . . , y8 that allows for one low value (Utah) and one high value (Nevada), with\nindependent and identical distributions for the other six values. This prior distribution\nshould be exchangeable, because it is not known which of the eight states correspond\nto Utah and Nevada.\n\n(b) Determine the posterior distribution for y8 under this model given the observed values\nof y1, . . . , y7 given in the example. This posterior distribution should probably have\ntwo or three modes, corresponding to the possibilities that the missing state is Utah,\nNevada, or one of the other six.\n\n(c) Now consider the entire set of eight data points, including the value for y8 given at\nthe end of the example. Are these data consistent with the prior distribution you gave\nin part (a) above? In particular, did your prior distribution allow for the possibility\nthat the actual data have an outlier (Nevada) at the high end, but no outlier at the\nlow end?\n\n7. Continuous mixture models:\n\n(a) If y|θ ∼ Poisson(θ), and θ ∼ Gamma(α, β), then the marginal (prior predictive)\ndistribution of y is negative binomial with parameters α and β (or p = β/(1 + β)).\nUse the formulas (2.7) and (2.8) to derive the mean and variance of the negative\nbinomial.\n\n(b) In the normal model with unknown location and scale (µ, σ2), the noninformative\nprior density, p(µ, σ2) ∝ 1/σ2, results in a normal-inverse-χ2 posterior distribution for\n(µ, σ2). Marginally then\n\n√\nn(µ − y)/s has a posterior distribution that is tn−1. Use\n\n(2.7) and (2.8) to derive the first two moments of the latter distribution, stating the\nappropriate condition on n for existence of both moments.\n\n8. Discrete mixture models: if pm(θ), for m = 1, . . . ,M , are conjugate prior densities for\nthe sampling model y|θ, show that the class of finite mixture prior densities given by\n\np(θ) =\n\nM∑\n\nm=1\n\nλmpm(θ)\n\nis also a conjugate class, where the λm’s are nonnegative weights that sum to 1. This\ncan provide a useful extension of the natural conjugate prior family to more flexible\ndistributional forms. As an example, use the mixture form to create a bimodal prior\ndensity for a normal mean, that is thought to be near 1, with a standard deviation of\n0.5, but has a small probability of being near −1, with the same standard deviation. If\nthe variance of each observation y1, . . . , y10 is known to be 1, and their observed mean\nis y = −0.25, derive your posterior distribution for the mean, making a sketch of both\nprior and posterior densities. Be careful: the prior and posterior mixture proportions are\ndifferent.\n\n9. Noninformative hyperprior distributions: consider the hierarchical binomial model in\nSection 5.3. Improper posterior distributions are, in fact, a general problem with hier-\narchical models when a uniform prior distribution is specified for the logarithm of the\npopulation standard deviation of the exchangeable parameters. In the case of the beta\npopulation distribution, the prior variance is approximately (α+β)−1 (see Appendix A),\nand so a uniform distribution on log(α+β) is approximately uniform on the log standard\ndeviation. The resulting unnormalized posterior density (5.8) has an infinite integral\nin the limit as the population standard deviation approaches 0. We encountered the\nproblem again in Section 5.4 for the hierarchical normal model.\n\n(a) Show that, with a uniform prior density on (log(αβ ), log(α+β)), the unnormalized\nposterior density has an infinite integral.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n136 5. HIERARCHICAL MODELS\n\n(b) A simple way to avoid the impropriety is to assign a uniform prior distribution to the\nstandard deviation parameter itself, rather than its logarithm. For the beta population\ndistribution we are considering here, this is achieved approximately by assigning a uni-\nform prior distribution to (α+β)−1/2. Show that combining this with an independent\nuniform prior distribution on α\n\nα+β yields the prior density (5.10).\n\n(c) Show that the resulting posterior density (5.8) is proper as long as 0 < yj < nj for at\nleast one experiment j.\n\n10. Checking the integrability of the posterior distribution: consider the hierarchical normal\nmodel in Section 5.4.\n\n(a) If the hyperprior distribution is p(µ, τ) ∝ τ−1 (that is, p(µ, log τ) ∝ 1), show that the\nposterior density is improper.\n\n(b) If the hyperprior distribution is p(µ, τ) ∝ 1, show that the posterior density is proper\nif J > 2.\n\n(c) How would you analyze SAT coaching data if J = 2 (that is, data from only two\nschools)?\n\n11. Nonconjugate hierarchical models: suppose that in the rat tumor example, we wish to\nuse a normal population distribution on the log-odds scale: logit(θj) ∼ N(µ, τ2), for\nj = 1, . . . , J . As in Section 5.3, you will assign a noninformative prior distribution to the\nhyperparameters and perform a full Bayesian analysis.\n\n(a) Write the joint posterior density, p(θ, µ, τ |y).\n(b) Show that the integral (5.4) has no closed-form expression.\n\n(c) Why is expression (5.5) no help for this problem?\n\nIn practice, we can solve this problem by normal approximation, importance sampling,\nand Markov chain simulation, as described in Part III.\n\n12. Conditional posterior means and variances: derive analytic expressions for E(θj |τ, y) and\nvar(θj |τ, y) in the hierarchical normal model (and used in Figures 5.6 and 5.7). (Hint:\nuse (2.7) and (2.8), averaging over µ.)\n\n13. Hierarchical binomial model: Exercise 3.8 described a survey of bicycle traffic in Berkeley,\nCalifornia, with data displayed in Table 3.3. For this problem, restrict your attention to\nthe first two rows of the table: residential streets labeled as ‘bike routes,’ which we will\nuse to illustrate this computational exercise.\n\n(a) Set up a model for the data in Table 3.3 so that, for j = 1, . . . , 10, the observed number\nof bicycles at location j is binomial with unknown probability θj and sample size equal\nto the total number of vehicles (bicycles included) in that block. The parameter θj\ncan be interpreted as the underlying or ‘true’ proportion of traffic at location j that is\nbicycles. (See Exercise 3.8.) Assign a beta population distribution for the parameters\nθj and a noninformative hyperprior distribution as in the rat tumor example of Section\n5.3. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and draw simulations\nfrom the joint posterior distribution of the parameters and hyperparameters, as in\nSection 5.3.\n\n(c) Compare the posterior distributions of the parameters θj to the raw proportions,\n(number of bicycles / total number of vehicles) in location j. How do the inferences\nfrom the posterior distribution differ from the raw proportions?\n\n(d) Give a 95% posterior interval for the average underlying proportion of traffic that is\nbicycles.\n\n(e) A new city block is sampled at random and is a residential street with a bike route. In\nan hour of observation, 100 vehicles of all kinds go by. Give a 95% posterior interval\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 137\n\nfor the number of those vehicles that are bicycles. Discuss how much you trust this\ninterval in application.\n\n(f) Was the beta distribution for the θj ’s reasonable?\n\n14. Hierarchical Poisson model: consider the dataset in the previous problem, but suppose\nonly the total amount of traffic at each location is observed.\n\n(a) Set up a model in which the total number of vehicles observed at each location j\nfollows a Poisson distribution with parameter θj , the ‘true’ rate of traffic per hour at\nthat location. Assign a gamma population distribution for the parameters θj and a\nnoninformative hyperprior distribution. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and plot its contours.\nSimulate random draws from the posterior distribution of the hyperparameters and\nmake a scatterplot of the simulation draws.\n\n(c) Is the posterior density integrable? Answer analytically by examining the joint pos-\nterior density at the limits or empirically by examining the plots of the marginal\nposterior density above.\n\n(d) If the posterior density is not integrable, alter it and repeat the previous two steps.\n\n(e) Draw samples from the joint posterior distribution of the parameters and hyperpa-\nrameters, by analogy to the method used in the hierarchical binomial model.\n\n15. Meta-analysis: perform the computations for the meta-analysis data of Table 5.4.\n\n(a) Plot the posterior density of τ over an appropriate range that includes essentially all\nof the posterior density, analogous to Figure 5.5.\n\n(b) Produce graphs analogous to Figures 5.6 and 5.7 to display how the posterior means\nand standard deviations of the θj ’s depend on τ .\n\n(c) Produce a scatterplot of the crude effect estimates vs. the posterior median effect\nestimates of the 22 studies. Verify that the studies with smallest sample sizes are\npartially pooled the most toward the mean.\n\n(d) Draw simulations from the posterior distribution of a new treatment effect, θ̃j . Plot\na histogram of the simulations.\n\n(e) Given the simulations just obtained, draw simulated outcomes from replications of\na hypothetical new experiment with 100 persons in each of the treated and control\ngroups. Plot a histogram of the simulations of the crude estimated treatment effect\n(5.23) in the new experiment.\n\n16. Equivalent data: Suppose we wish to apply the inferences from the meta-analysis example\nin Section 5.6 to data on a new study with equal numbers of people in the control and\ntreatment groups. How large would the study have to be so that the prior and data were\nweighted equally in the posterior inference for that study?\n\n17. Informative prior distributions: Continuing the example from Exercise 2.22, consider a\n(hypothetical) study of a simple training program for basketball free-throw shooting. A\nrandom sample of 100 college students is recruited into the study. Each student first\nshoots 100 free-throws to establish a baseline success probability. Each student then\ntakes 50 practice shots each day for a month. At the end of that time, he or she takes\n100 shots for a final measurement.\nLet θi be the improvement in success probability for person i. For simplicity, assume the\nθi’s are normally distributed with mean µ and standard deviation σ.\nGive three joint prior distributions for µ, σ:\n\n(a) A noninformative prior distribution,\n\n(b) A subjective prior distribution based on your best knowledge, and\n\n(c) A weakly informative prior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart II: Fundamentals of Bayesian Data\n\nAnalysis\n\nFor most problems of applied Bayesian statistics, the data analyst must go beyond the\nsimple structure of prior distribution, likelihood, and posterior distribution. In Chapter 6,\nwe discuss methods of assessing the sensitivity of posterior inferences to model assumptions\nand checking the fit of a probability model to data and substantive information. Model\nchecking allows an escape from the tautological aspect of formal approaches to Bayesian\ninference, under which all conclusions are conditional on the truth of the posited model.\nChapter 7 considers evaluating and comparing models using predictive accuracy, adjusting\nfor the parameters being fit to the data. Chapter 8 outlines the role of study design and\nmethods of data collection in probability modeling, focusing on how to set up Bayesian\ninference for sample surveys, designed experiments, and observational studies; this chapter\ncontains some of the most conceptually distinctive and potentially difficult material in\nthe book. Chapter 9 discusses the use of Bayesian inference in applied decision analysis,\nillustrating with examples from social science, medicine, and public health. These four\nchapters explore the creative choices that are required, first to set up a Bayesian model in\na complex problem, then to perform the model checking and confidence building that is\ntypically necessary to make posterior inferences scientifically defensible, and finally to use\nthe inferences in decision making.\n\n139\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 6\n\nModel checking\n\n6.1 The place of model checking in applied Bayesian statistics\n\nOnce we have accomplished the first two steps of a Bayesian analysis—constructing a prob-\nability model and computing the posterior distribution of all estimands—we should not\nignore the relatively easy step of assessing the fit of the model to the data and to our\nsubstantive knowledge. It is difficult to include in a probability distribution all of one’s\nknowledge about a problem, and so it is wise to investigate what aspects of reality are not\ncaptured by the model.\n\nChecking the model is crucial to statistical analysis. Bayesian prior-to-posterior infer-\nences assume the whole structure of a probability model and can yield misleading inferences\nwhen the model is poor. A good Bayesian analysis, therefore, should include at least some\ncheck of the adequacy of the fit of the model to the data and the plausibility of the model\nfor the purposes for which the model will be used. This is sometimes discussed as a problem\nof sensitivity to the prior distribution, but in practice the likelihood model is typically just\nas suspect; throughout, we use ‘model’ to encompass the sampling distribution, the prior\ndistribution, any hierarchical structure, and issues such as which explanatory variables have\nbeen included in a regression.\n\nSensitivity analysis and model improvement\n\nIt is typically the case that more than one reasonable probability model can provide an\nadequate fit to the data in a scientific problem. The basic question of a sensitivity analysis\nis: how much do posterior inferences change when other reasonable probability models\nare used in place of the present model? Other reasonable models may differ substantially\nfrom the present model in the prior specification, the sampling distribution, or in what\ninformation is included (for example, predictor variables in a regression). It is possible that\nthe present model provides an adequate fit to the data, but that posterior inferences differ\nunder plausible alternative models.\n\nIn theory, both model checking and sensitivity analysis can be incorporated into the\nusual prior-to-posterior analysis. Under this perspective, model checking is done by set-\nting up a comprehensive joint distribution, such that any data that might be observed are\nplausible outcomes under the joint distribution. That is, this joint distribution is a mixture\nof all possible ‘true’ models or realities, incorporating all known substantive information.\nThe prior distribution in such a case incorporates prior beliefs about the likelihood of the\ncompeting realities and about the parameters of the constituent models. The posterior dis-\ntribution of such an exhaustive probability model automatically incorporates all ‘sensitivity\nanalysis’ but is still predicated on the truth of some member of the larger class of models.\n\nIn practice, however, setting up such a super-model to include all possibilities and all\nsubstantive knowledge is both conceptually impossible and computationally infeasible in all\nbut the simplest problems. It is thus necessary for us to examine our models in other ways\n\n141\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n142 6. MODEL CHECKING\n\nto see how they fail to fit reality and how sensitive the resulting posterior distributions are\nto arbitrary specifications.\n\nJudging model flaws by their practical implications\n\nWe do not like to ask, ‘Is our model true or false?’, since probability models in most\ndata analyses will not be perfectly true. Even the coin tosses and die rolls ubiquitous in\nprobability theory texts are not truly exchangeable. The more relevant question is, ‘Do the\nmodel’s deficiencies have a noticeable effect on the substantive inferences?’\n\nIn the examples of Chapter 5, the beta population distribution for the tumor rates and\nthe normal distribution for the eight school effects are both chosen partly for convenience.\nIn these examples, making convenient distributional assumptions turns out not to matter,\nin terms of the impact on the inferences of most interest. How to judge when assumptions\nof convenience can be made safely is a central task of Bayesian sensitivity analysis. Failures\nin the model lead to practical problems by creating clearly false inferences about estimands\nof interest.\n\n6.2 Do the inferences from the model make sense?\n\nIn any applied problem, there will be knowledge that is not included formally in either\nthe prior distribution or the likelihood, for reasons of convenience or objectivity. If the\nadditional information suggests that posterior inferences of interest are false, then this\nsuggests a potential for creating a more accurate probability model for the parameters and\ndata collection process. We illustrate with an example of a hierarchical regression model.\n\nExample. Evaluating election predictions by comparing to substantive po-\nlitical knowledge\nFigure 6.1 displays a forecast, made in early October, 1992, of the probability that Bill\nClinton would win each state in the U.S. presidential election that November. The\nestimates are posterior probabilities based on a hierarchical linear regression model.\nFor each state, the height of the shaded part of the box represents the estimated\nprobability that Clinton would win the state. Even before the election occurred, the\nforecasts for some of the states looked wrong; for example, from state polls, Clinton\nwas known in October to be much weaker in Texas and Florida than shown in the\nmap. This does not mean that the forecast is useless, but it is good to know where\nthe weak points are. Certainly, after the election, we can do an even better job of\ncriticizing the model and understanding its weaknesses. We return to this election\nforecasting example in Section 15.2 as an example of a hierarchical linear model.\n\nExternal validation\n\nMore formally, we can check a model by external validation using the model to make predic-\ntions about future data, and then collecting those data and comparing to their predictions.\nPosterior means should be correct on average, 50% intervals should contain the true values\nhalf the time, and so forth. We used external validation to check the empirical probability\nestimates in the record-linkage example in Section 1.7, and we apply the idea again to check\na toxicology model in Section 19.2. In the latter example, the external validation (see Figure\n19.10 on page 484) reveals a generally reasonable fit but with some notable discrepancies\nbetween predictions and external data. Often we need to check the model before obtaining\nnew data or waiting for the future to happen. In this chapter and the next, we discuss\nmethods which can approximate external validation using the data we already have.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 143\n\nFigure 6.1 Summary of a forecast of the 1992 U.S. presidential election performed one month\nbefore the election. For each state, the proportion of the box that is shaded represents the estimated\nprobability of Clinton winning the state; the width of the box is proportional to the number of\nelectoral votes for the state.\n\nChoices in defining the predictive quantities\n\nA single model can be used to make different predictions. For example, in the SAT example\nwe could consider a joint prediction for future data from the 8 schools in the study, p(ỹ|y),\na joint prediction for 8 new schools p(ỹi|y), i = 9, . . . , 16, or any other combination of new\nand existing schools. Other scenarios may have even more different choices in defining the\nfocus of predictions. For example, in analyses of sample surveys and designed experiments,\nit often makes sense to consider hypothetical replications of the experiment with a new\nrandomization of selection or treatment assignment, by analogy to classical randomization\ntests.\n\nSections 6.3 and 6.4 discuss posterior predictive checking, which use global summaries\nto check the joint posterior predictive distribution p(ỹ|y). At the end of Section 6.3 we\nbriefly discuss methods that combine inferences for local quantities to check marginal pre-\ndictive distributions p(ỹi|y), an idea that is related to cross-validation methods considered\nin Chapter 7.\n\n6.3 Posterior predictive checking\n\nIf the model fits, then replicated data generated under the model should look similar to\nobserved data. To put it another way, the observed data should look plausible under\nthe posterior predictive distribution. This is really a self-consistency check: an observed\ndiscrepancy can be due to model misfit or chance.\n\nOur basic technique for checking the fit of a model to data is to draw simulated values\nfrom the joint posterior predictive distribution of replicated data and compare these samples\nto the observed data. Any systematic differences between the simulations and the data\nindicate potential failings of the model.\n\nWe introduce posterior predictive checking with a simple example of an obviously poorly\nfitting model, and then in the rest of this section we lay out the key choices involved in pos-\nterior predictive checking. Sections 6.3 and 6.4 discuss numerical and graphical predictive\nchecks in more detail.\n\nExample. Comparing Newcomb’s speed of light measurements to the pos-\nterior predictive distribution\nSimon Newcomb’s 66 measurements on the speed of light are presented in Section 3.2.\nIn the absence of other information, in Section 3.2 we modeled the measurements as\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n144 6. MODEL CHECKING\n\nFigure 6.2 Twenty replications, yrep, of the speed of light data from the posterior predictive distri-\nbution, p(yrep|y); compare to observed data, y, in Figure 3.1. Each histogram displays the result\nof drawing 66 independent values ỹi from a common normal distribution with mean and variance\n(µ, σ2) drawn from the posterior distribution, p(µ, σ2|y), under the normal model.\n\nFigure 6.3 Smallest observation of Newcomb’s speed of light data (the vertical line at the left of the\ngraph), compared to the smallest observations from each of the 20 posterior predictive simulated\ndatasets displayed in Figure 6.2.\n\nN(µ, σ2), with a noninformative uniform prior distribution on (µ, log σ). However, the\nlowest of Newcomb’s measurements look like outliers compared to the rest of the data.\nCould the extreme measurements have reasonably come from a normal distribution?\nWe address this question by comparing the observed data to what we expect to be\nobserved under our posterior distribution. Figure 6.2 displays twenty histograms,\neach of which represents a single draw from the posterior predictive distribution of\nthe values in Newcomb’s experiment, obtained by first drawing (µ, σ2) from their\njoint posterior distribution, then drawing 66 values from a normal distribution with\nthis mean and variance. All these histograms look different from the histogram of\nactual data in Figure 3.1 on page 67. One way to measure the discrepancy is to\ncompare the smallest value in each hypothetical replicated dataset to Newcomb’s\nsmallest observation, −44. The histogram in Figure 6.3 shows the smallest observation\nin each of the 20 hypothetical replications; all are much larger than Newcomb’s smallest\nobservation, which is indicated by a vertical line on the graph. The normal model\nclearly does not capture the variation that Newcomb observed. A revised model\nmight use an asymmetric contaminated normal distribution or a symmetric long-tailed\ndistribution in place of the normal measurement model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 145\n\nMany other examples of posterior predictive checks appear throughout the book, includ-\ning the educational testing example in Section 6.5, linear regressions examples in Sections\n14.3 and 15.2, and a hierarchical mixture model in Section 22.2.\n\nFor many problems, it is useful to examine graphical comparisons of summaries of the\ndata to summaries from posterior predictive simulations, as in Figure 6.3. In cases with\nless blatant discrepancies than the outliers in the speed of light data, it is often also useful\nto measure the ‘statistical significance’ of the lack of fit, a notion we formalize here.\n\nNotation for replications\n\nLet y be the observed data and θ be the vector of parameters (including all the hyperpa-\nrameters if the model is hierarchical). To avoid confusion with the observed data, y, we\ndefine yrep as the replicated data that could have been observed, or, to think predictively, as\nthe data we would see tomorrow if the experiment that produced y today were replicated\nwith the same model and the same value of θ that produced the observed data.\n\nWe distinguish between yrep and ỹ, our general notation for predictive outcomes: ỹ is\nany future observable value or vector of observable quantities, whereas yrep is specifically a\nreplication just like y. For example, if the model has explanatory variables, x, they will be\nidentical for y and yrep, but ỹ may have its own explanatory variables, x̃.\n\nWe will work with the distribution of yrep given the current state of knowledge, that is,\nwith the posterior predictive distribution\n\np(yrep|y) =\n∫\np(yrep|θ)p(θ|y)dθ. (6.1)\n\nTest quantities\n\nWe measure the discrepancy between model and data by defining test quantities, the aspects\nof the data we wish to check. A test quantity, or discrepancy measure, T (y, θ), is a scalar\nsummary of parameters and data that is used as a standard when comparing data to\npredictive simulations. Test quantities play the role in Bayesian model checking that test\nstatistics play in classical testing. We use the notation T (y) for a test statistic, which is a\ntest quantity that depends only on data; in the Bayesian context, we can generalize test\nstatistics to allow dependence on the model parameters under their posterior distribution.\nThis can be useful in directly summarizing discrepancies between model and data. We\ndiscuss options for graphical test quantities in Section 6.4. The test quantities in this\nsection are usually functions of y or replicated data yrep. In the end of this section we\nbriefly discuss a different sort of test quantities used for calibration that are functions of\nboth yi and y\n\nrep\ni (or ỹi). In Chapter 7 we discuss measures of discrepancy between model\n\nand data, that is, measures of predictive accuracy that are also functions of both yi and\nyrepi (or ỹi).\n\nTail-area probabilities\n\nLack of fit of the data with respect to the posterior predictive distribution can be measured\nby the tail-area probability, or p-value, of the test quantity, and computed using posterior\nsimulations of (θ, yrep). We define the p-value mathematically, first for the familiar classical\ntest and then in the Bayesian context.\n\nClassical p-values. The classical p-value for the test statistic T (y) is\n\npC = Pr(T (yrep)≥T (y)|θ), (6.2)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n146 6. MODEL CHECKING\n\nwhere the probability is taken over the distribution of yrep with θ fixed. (The distribution of\nyrep given y and θ is the same as its distribution given θ alone","extracted_metadata":{"dc:format":["application/pdf; version=1.4"],"access_permission:can_print":["true"],"pdf:unmappedUnicodeCharsPerPage":["1","0","0","0","0","0","0","0","0","0","0","0","0","0","0","1","7","0","1","0","0","0","0","0","0","2","0","0","0","4","13","2","3","0","0","0","0","0","2","2","9","2","0","0","8","7","0","0","0","8","18","3","15","9","4","0","0","0","2","5","4","24","32","8","0","0","0","1","0","1","0","0","1","12","11","14","2","6","9","0","12","6","3","0","1","0","2","0","0","0","0","0","2","19","0","0","0","7","8","0","0","0","0","0","1","0","0","0","2","1","0","0","7","0","5","0","2","0","2","4","2","1","1","8","7","1","8","0","0","2","0","0","0","0","4","0","0","0","0","0","0","0","0","2","1","0","0","0","0","0","0","0","0","0","1"],"pdf:hasCollection":["false"],"xmp:CreatorTool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"access_permission:can_modify":["true"],"pdf:PDFVersion":["1.4"],"pdf:encrypted":["false"],"pdf:producer":["GPL Ghostscript 9.26"],"pdf:docinfo:title":["book.dvi"],"xmpMM:DocumentID":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"pdf:charsPerPage":["396","53","1436","1728","1697","1636","1492","528","3071","3140","1234","53","2742","3330","3069","2988","2256","2803","2648","2740","3465","3348","2958","2470","2340","3049","1963","2161","2318","3056","2205","2627","2963","3087","3225","3644","2929","2677","2494","1927","2281","2873","3093","3116","2331","2646","3530","2407","1793","1903","1918","2860","1541","1822","3183","1972","1401","2495","2382","1813","2158","2677","2369","2617","3359","3595","2953","2521","2429","2675","3013","177","2211","2155","2102","2430","2259","1865","2320","2463","1936","2317","2074","2392","2285","1981","2439","3093","2762","3198","2770","2935","2362","1684","3302","2463","2576","3133","3078","3559","2965","3399","3413","3082","3222","3652","3361","3382","2915","3107","3106","2884","2549","3091","3033","3268","2839","3066","2445","2758","2420","2181","1850","2903","2149","2374","2207","2834","3400","2905","2465","1749","2319","3019","2985","3110","3392","3275","3498","3496","2971","2867","3626","2513","2989","2931","2971","53","1343","53","2696","3022","2284","1753","2796"],"dcterms:created":["2020-04-27T08:01:52Z"],"pdf:docinfo:created":["2020-04-27T08:01:52Z"],"Content-Length":["35493143"],"pdf:docinfo:producer":["GPL Ghostscript 9.26"],"access_permission:can_print_degraded":["true"],"access_permission:fill_in_form":["true"],"dc:title":["book.dvi"],"resourceName":["Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf"],"access_permission:extract_content":["true"],"access_permission:modify_annotations":["true"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"xmp:About":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"xmp:ModifyDate":["2020-04-27T08:01:52Z"],"dcterms:modified":["2020-04-27T08:01:52Z"],"pdf:hasMarkedContent":["false"],"access_permission:assemble_document":["true"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"pdf:hasXFA":["false"],"xmp:CreateDate":["2020-04-27T08:01:52Z"],"access_permission:extract_for_accessibility":["true"],"pdf:docinfo:creator_tool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"xmpTPg:NPages":["677"],"pdf:hasXMP":["true"],"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"Content-Type":["application/pdf"],"pdf:docinfo:modified":["2020-04-27T08:01:52Z"]},"metadata_field_count":38,"attempts":1,"timestamp":1754064811.5302048,"platform":"Linux","python_version":"3.13.5"},{"file_path":"test_documents/pdfs/Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf","file_size":20812100,"file_type":"pdf","category":"large","framework":"extractous","iteration":0,"extraction_time":137.42073249816895,"startup_time":null,"peak_memory_mb":548.3671875,"avg_memory_mb":530.2359375,"peak_cpu_percent":29.0,"avg_cpu_percent":5.8,"total_io_mb":null,"status":"success","character_count":500000,"word_count":112307,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":112307,"sentence_count":7563,"paragraph_count":6926,"avg_word_length":3.38541675941838,"avg_sentence_length":13.423773634801005,"extraction_completeness":1.0,"text_coherence":0.30375980524749796,"noise_ratio":0.5307600000000001,"gibberish_ratio":0.01694915254237288,"flesch_reading_ease":70.7747961398366,"gunning_fog_index":10.59532347580288,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":true,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5120034464866704,"extracted_text":"\nAlgebra, Topology, Differential Calculus, and\nOptimization Theory\n\nFor Computer Science and Machine Learning\n\nJean Gallier and Jocelyn Quaintance\nDepartment of Computer and Information Science\n\nUniversity of Pennsylvania\nPhiladelphia, PA 19104, USA\ne-mail: jean@cis.upenn.edu\n\nc© Jean Gallier\n\nAugust 2, 2019\n\n\n\n2\n\n\n\n\n\nContents\n\nContents 3\n\n1 Introduction 17\n\n2 Groups, Rings, and Fields 19\n2.1 Groups, Subgroups, Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.2 Cyclic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.3 Rings and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nI Linear Algebra 43\n\n3 Vector Spaces, Bases, Linear Maps 45\n3.1 Motivations: Linear Combinations, Linear Independence, Rank . . . . . . . 45\n3.2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.3 Indexed Families; the Sum Notation\n\n∑\ni∈I ai . . . . . . . . . . . . . . . . . . 60\n\n3.4 Linear Independence, Subspaces . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.5 Bases of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.6 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n3.7 Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n3.8 Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.9 Linear Forms and the Dual Space . . . . . . . . . . . . . . . . . . . . . . . . 94\n3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n4 Matrices and Linear Maps 107\n4.1 Representation of Linear Maps by Matrices . . . . . . . . . . . . . . . . . . 107\n4.2 Composition of Linear Maps and Matrix Multiplication . . . . . . . . . . . 112\n4.3 Change of Basis Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n4.4 The Effect of a Change of Bases on Matrices . . . . . . . . . . . . . . . . . 120\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\n5 Haar Bases, Haar Wavelets, Hadamard Matrices 131\n\n3\n\n\n\n4 CONTENTS\n\n5.1 Introduction to Signal Compression Using Haar Wavelets . . . . . . . . . . 131\n5.2 Haar Matrices, Scaling Properties of Haar Wavelets . . . . . . . . . . . . . . 133\n5.3 Kronecker Product Construction of Haar Matrices . . . . . . . . . . . . . . 138\n5.4 Multiresolution Signal Analysis with Haar Bases . . . . . . . . . . . . . . . 140\n5.5 Haar Transform for Digital Images . . . . . . . . . . . . . . . . . . . . . . . 142\n5.6 Hadamard Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n5.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\n6 Direct Sums 155\n6.1 Sums, Direct Sums, Direct Products . . . . . . . . . . . . . . . . . . . . . . 155\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation . . . . . . . . . . . . . . 165\n6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n6.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n7 Determinants 181\n7.1 Permutations, Signature of a Permutation . . . . . . . . . . . . . . . . . . . 181\n7.2 Alternating Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n7.3 Definition of a Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n7.4 Inverse Matrices and Determinants . . . . . . . . . . . . . . . . . . . . . . . 197\n7.5 Systems of Linear Equations and Determinants . . . . . . . . . . . . . . . . 200\n7.6 Determinant of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.7 The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.8 Permanents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n7.10 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n7.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n\n8 Gaussian Elimination, LU, Cholesky, Echelon Form 219\n8.1 Motivating Example: Curve Interpolation . . . . . . . . . . . . . . . . . . . 219\n8.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n8.3 Elementary Matrices and Row Operations . . . . . . . . . . . . . . . . . . . 228\n8.4 LU -Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n8.5 PA = LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n8.6 Proof of Theorem 8.5 ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n8.7 Dealing with Roundoff Errors; Pivoting Strategies . . . . . . . . . . . . . . . 251\n8.8 Gaussian Elimination of Tridiagonal Matrices . . . . . . . . . . . . . . . . . 252\n8.9 SPD Matrices and the Cholesky Decomposition . . . . . . . . . . . . . . . . 254\n8.10 Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n8.11 RREF, Free Variables, Homogeneous Systems . . . . . . . . . . . . . . . . . 269\n8.12 Uniqueness of RREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n8.13 Solving Linear Systems Using RREF . . . . . . . . . . . . . . . . . . . . . . 274\n8.14 Elementary Matrices and Columns Operations . . . . . . . . . . . . . . . . 281\n\n\n\nCONTENTS 5\n\n8.15 Transvections and Dilatations ~ . . . . . . . . . . . . . . . . . . . . . . . . 282\n8.16 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n8.17 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n\n9 Vector Norms and Matrix Norms 301\n9.1 Normed Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n9.2 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n9.3 Subordinate Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n9.4 Inequalities Involving Subordinate Norms . . . . . . . . . . . . . . . . . . . 324\n9.5 Condition Numbers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 326\n9.6 An Application of Norms: Inconsistent Linear Systems . . . . . . . . . . . . 335\n9.7 Limits of Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . 336\n9.8 The Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n9.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n\n10 Iterative Methods for Solving Linear Systems 351\n10.1 Convergence of Sequences of Vectors and Matrices . . . . . . . . . . . . . . 351\n10.2 Convergence of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . 354\n10.3 Methods of Jacobi, Gauss–Seidel, and Relaxation . . . . . . . . . . . . . . . 356\n10.4 Convergence of the Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n10.5 Convergence Methods for Tridiagonal Matrices . . . . . . . . . . . . . . . . 367\n10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n10.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n\n11 The Dual Space and Duality 375\n11.1 The Dual Space E∗ and Linear Forms . . . . . . . . . . . . . . . . . . . . . 375\n11.2 Pairing and Duality Between E and E∗ . . . . . . . . . . . . . . . . . . . . 382\n11.3 The Duality Theorem and Some Consequences . . . . . . . . . . . . . . . . 387\n11.4 The Bidual and Canonical Pairings . . . . . . . . . . . . . . . . . . . . . . . 393\n11.5 Hyperplanes and Linear Forms . . . . . . . . . . . . . . . . . . . . . . . . . 395\n11.6 Transpose of a Linear Map and of a Matrix . . . . . . . . . . . . . . . . . . 396\n11.7 Properties of the Double Transpose . . . . . . . . . . . . . . . . . . . . . . . 403\n11.8 The Four Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . 405\n11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n11.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n\n12 Euclidean Spaces 413\n12.1 Inner Products, Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . . 413\n12.2 Orthogonality and Duality in Euclidean Spaces . . . . . . . . . . . . . . . . 422\n12.3 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n12.4 Existence and Construction of Orthonormal Bases . . . . . . . . . . . . . . 432\n12.5 Linear Isometries (Orthogonal Transformations) . . . . . . . . . . . . . . . . 439\n\n\n\n6 CONTENTS\n\n12.6 The Orthogonal Group, Orthogonal Matrices . . . . . . . . . . . . . . . . . 442\n12.7 The Rodrigues Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\n12.8 QR-Decomposition for Invertible Matrices . . . . . . . . . . . . . . . . . . . 447\n12.9 Some Applications of Euclidean Geometry . . . . . . . . . . . . . . . . . . . 452\n12.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\n12.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n\n13 QR-Decomposition for Arbitrary Matrices 467\n13.1 Orthogonal Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467\n13.2 QR-Decomposition Using Householder Matrices . . . . . . . . . . . . . . . . 472\n13.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n13.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n\n14 Hermitian Spaces 489\n14.1 Hermitian Spaces, Pre-Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 489\n14.2 Orthogonality, Duality, Adjoint of a Linear Map . . . . . . . . . . . . . . . 498\n14.3 Linear Isometries (Also Called Unitary Transformations) . . . . . . . . . . . 503\n14.4 The Unitary Group, Unitary Matrices . . . . . . . . . . . . . . . . . . . . . 505\n14.5 Hermitian Reflections and QR-Decomposition . . . . . . . . . . . . . . . . . 508\n14.6 Orthogonal Projections and Involutions . . . . . . . . . . . . . . . . . . . . 513\n14.7 Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523\n14.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n\n15 Eigenvectors and Eigenvalues 529\n15.1 Eigenvectors and Eigenvalues of a Linear Map . . . . . . . . . . . . . . . . . 529\n15.2 Reduction to Upper Triangular Form . . . . . . . . . . . . . . . . . . . . . . 537\n15.3 Location of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541\n15.4 Conditioning of Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . 544\n15.5 Eigenvalues of the Matrix Exponential . . . . . . . . . . . . . . . . . . . . . 547\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n15.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n\n16 Unit Quaternions and Rotations in SO(3) 561\n16.1 The Group SU(2) and the Skew Field H of Quaternions . . . . . . . . . . . 561\n16.2 Representation of Rotation in SO(3) By Quaternions in SU(2) . . . . . . . 563\n16.3 Matrix Representation of the Rotation rq . . . . . . . . . . . . . . . . . . . 568\n16.4 An Algorithm to Find a Quaternion Representing a Rotation . . . . . . . . 570\n16.5 The Exponential Map exp: su(2)→ SU(2) . . . . . . . . . . . . . . . . . . 573\n16.6 Quaternion Interpolation ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 575\n16.7 Nonexistence of a “Nice” Section from SO(3) to SU(2) . . . . . . . . . . . . 577\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n16.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n\n\n\nCONTENTS 7\n\n17 Spectral Theorems 583\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n17.2 Normal Linear Maps: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . 583\n17.3 Spectral Theorem for Normal Linear Maps . . . . . . . . . . . . . . . . . . . 589\n17.4 Self-Adjoint and Other Special Linear Maps . . . . . . . . . . . . . . . . . . 594\n17.5 Normal and Other Special Matrices . . . . . . . . . . . . . . . . . . . . . . . 600\n17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlacing . . . . . . . . . . . . . 603\n17.7 The Courant–Fischer Theorem; Perturbation Results . . . . . . . . . . . . . 608\n17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611\n17.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n\n18 Computing Eigenvalues and Eigenvectors 619\n18.1 The Basic QR Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n18.2 Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627\n18.3 Making the QR Method More Efficient Using Shifts . . . . . . . . . . . . . 633\n18.4 Krylov Subspaces; Arnoldi Iteration . . . . . . . . . . . . . . . . . . . . . . 638\n18.5 GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642\n18.6 The Hermitian Case; Lanczos Iteration . . . . . . . . . . . . . . . . . . . . . 643\n18.7 Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644\n18.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646\n18.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647\n\n19 Introduction to The Finite Elements Method 649\n19.1 A One-Dimensional Problem: Bending of a Beam . . . . . . . . . . . . . . . 649\n19.2 A Two-Dimensional Problem: An Elastic Membrane . . . . . . . . . . . . . 660\n19.3 Time-Dependent Boundary Problems . . . . . . . . . . . . . . . . . . . . . . 663\n\n20 Graphs and Graph Laplacians; Basic Facts 671\n20.1 Directed Graphs, Undirected Graphs, Weighted Graphs . . . . . . . . . . . 674\n20.2 Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 681\n20.3 Normalized Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . 685\n20.4 Graph Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . 689\n20.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691\n20.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692\n\n21 Spectral Graph Drawing 695\n21.1 Graph Drawing and Energy Minimization . . . . . . . . . . . . . . . . . . . 695\n21.2 Examples of Graph Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . 698\n21.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702\n\n22 Singular Value Decomposition and Polar Form 705\n22.1 Properties of f ∗ ◦ f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705\n22.2 Singular Value Decomposition for Square Matrices . . . . . . . . . . . . . . 709\n\n\n\n8 CONTENTS\n\n22.3 Polar Form for Square Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 712\n22.4 Singular Value Decomposition for Rectangular Matrices . . . . . . . . . . . 715\n22.5 Ky Fan Norms and Schatten Norms . . . . . . . . . . . . . . . . . . . . . . 718\n22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n22.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n\n23 Applications of SVD and Pseudo-Inverses 723\n23.1 Least Squares Problems and the Pseudo-Inverse . . . . . . . . . . . . . . . . 723\n23.2 Properties of the Pseudo-Inverse . . . . . . . . . . . . . . . . . . . . . . . . 730\n23.3 Data Compression and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . 735\n23.4 Principal Components Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . 737\n23.5 Best Affine Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\n23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752\n23.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\n\nII Affine and Projective Geometry 757\n\n24 Basics of Affine Geometry 759\n24.1 Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759\n24.2 Examples of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\n24.3 Chasles’s Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769\n24.4 Affine Combinations, Barycenters . . . . . . . . . . . . . . . . . . . . . . . . 770\n24.5 Affine Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775\n24.6 Affine Independence and Affine Frames . . . . . . . . . . . . . . . . . . . . . 781\n24.7 Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787\n24.8 Affine Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794\n24.9 Affine Geometry: A Glimpse . . . . . . . . . . . . . . . . . . . . . . . . . . 796\n24.10 Affine Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 800\n24.11 Intersection of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 802\n\n25 Embedding an Affine Space in a Vector Space 805\n25.1 The “Hat Construction,” or Homogenizing . . . . . . . . . . . . . . . . . . . 805\n25.2 Affine Frames of E and Bases of Ê . . . . . . . . . . . . . . . . . . . . . . . 812\n25.3 Another Construction of Ê . . . . . . . . . . . . . . . . . . . . . . . . . . . 815\n25.4 Extending Affine Maps to Linear Maps . . . . . . . . . . . . . . . . . . . . . 818\n\n26 Basics of Projective Geometry 823\n26.1 Why Projective Spaces? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823\n26.2 Projective Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828\n26.3 Projective Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n26.4 Projective Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836\n26.5 Projective Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850\n\n\n\nCONTENTS 9\n\n26.6 Finding a Homography Between Two Projective Frames . . . . . . . . . . . 856\n26.7 Affine Patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869\n26.8 Projective Completion of an Affine Space . . . . . . . . . . . . . . . . . . . 872\n26.9 Making Good Use of Hyperplanes at Infinity . . . . . . . . . . . . . . . . . 877\n26.10 The Cross-Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880\n26.11 Fixed Points of Homographies and Homologies . . . . . . . . . . . . . . . . 884\n26.12 Duality in Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 898\n26.13 Cross-Ratios of Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . 902\n26.14 Complexification of a Real Projective Space . . . . . . . . . . . . . . . . . . 904\n26.15 Similarity Structures on a Projective Space . . . . . . . . . . . . . . . . . . 906\n26.16 Some Applications of Projective Geometry . . . . . . . . . . . . . . . . . . . 915\n\nIII The Geometry of Bilinear Forms 921\n\n27 The Cartan–Dieudonné Theorem 923\n27.1 The Cartan–Dieudonné Theorem for Linear Isometries . . . . . . . . . . . . 923\n27.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 935\n27.3 Fixed Points of Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 937\n27.4 Affine Isometries and Fixed Points . . . . . . . . . . . . . . . . . . . . . . . 939\n27.5 The Cartan–Dieudonné Theorem for Affine Isometries . . . . . . . . . . . . 945\n\n28 Isometries of Hermitian Spaces 949\n28.1 The Cartan–Dieudonné Theorem, Hermitian Case . . . . . . . . . . . . . . . 949\n28.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 958\n\n29 The Geometry of Bilinear Forms; Witt’s Theorem 963\n29.1 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963\n29.2 Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 971\n29.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 975\n29.4 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 980\n29.5 Isometries Associated with Sesquilinear Forms . . . . . . . . . . . . . . . . . 982\n29.6 Totally Isotropic Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 986\n29.7 Witt Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992\n29.8 Symplectic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000\n29.9 Orthogonal Groups and the Cartan–Dieudonné Theorem . . . . . . . . . . . 1004\n29.10 Witt’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1011\n\nIV Algebra: PID’s, UFD’s, Noetherian Rings, Tensors,\nModules over a PID, Normal Forms 1017\n\n30 Polynomials, Ideals and PID’s 1019\n\n\n\n10 CONTENTS\n\n30.1 Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1019\n30.2 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1020\n30.3 Euclidean Division of Polynomials . . . . . . . . . . . . . . . . . . . . . . . 1026\n30.4 Ideals, PID’s, and Greatest Common Divisors . . . . . . . . . . . . . . . . . 1028\n30.5 Factorization and Irreducible Factors in K[X] . . . . . . . . . . . . . . . . . 1036\n30.6 Roots of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1040\n30.7 Polynomial Interpolation (Lagrange, Newton, Hermite) . . . . . . . . . . . . 1047\n\n31 Annihilating Polynomials; Primary Decomposition 1055\n31.1 Annihilating Polynomials and the Minimal Polynomial . . . . . . . . . . . . 1057\n31.2 Minimal Polynomials of Diagonalizable Linear Maps . . . . . . . . . . . . . 1059\n31.3 Commuting Families of Linear Maps . . . . . . . . . . . . . . . . . . . . . . 1062\n31.4 The Primary Decomposition Theorem . . . . . . . . . . . . . . . . . . . . . 1065\n31.5 Jordan Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1071\n31.6 Nilpotent Linear Maps and Jordan Form . . . . . . . . . . . . . . . . . . . . 1074\n31.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1080\n31.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1081\n\n32 UFD’s, Noetherian Rings, Hilbert’s Basis Theorem 1085\n32.1 Unique Factorization Domains (Factorial Rings) . . . . . . . . . . . . . . . . 1085\n32.2 The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . 1099\n32.3 Noetherian Rings and Hilbert’s Basis Theorem . . . . . . . . . . . . . . . . 1105\n32.4 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1109\n\n33 Tensor Algebras 1111\n33.1 Linear Algebra Preliminaries: Dual Spaces and Pairings . . . . . . . . . . . 1113\n33.2 Tensors Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1118\n33.3 Bases of Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1130\n33.4 Some Useful Isomorphisms for Tensor Products . . . . . . . . . . . . . . . . 1131\n33.5 Duality for Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . 1135\n33.6 Tensor Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1141\n33.7 Symmetric Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1148\n33.8 Bases of Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1152\n33.9 Some Useful Isomorphisms for Symmetric Powers . . . . . . . . . . . . . . . 1155\n33.10 Duality for Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . 1155\n33.11 Symmetric Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1159\n33.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1162\n\n34 Exterior Tensor Powers and Exterior Algebras 1165\n34.1 Exterior Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1165\n34.2 Bases of Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1170\n34.3 Some Useful Isomorphisms for Exterior Powers . . . . . . . . . . . . . . . . 1173\n34.4 Duality for Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1173\n\n\n\nCONTENTS 11\n\n34.5 Exterior Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1177\n34.6 The Hodge ∗-Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1181\n34.7 Left and Right Hooks ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1185\n34.8 Testing Decomposability ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 1195\n34.9 The Grassmann-Plücker’s Equations and Grassmannians ~ . . . . . . . . . 1198\n34.10 Vector-Valued Alternating Forms . . . . . . . . . . . . . . . . . . . . . . . . 1201\n34.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205\n\n35 Introduction to Modules; Modules over a PID 1207\n35.1 Modules over a Commutative Ring . . . . . . . . . . . . . . . . . . . . . . . 1207\n35.2 Finite Presentations of Modules . . . . . . . . . . . . . . . . . . . . . . . . . 1216\n35.3 Tensor Products of Modules over a Commutative Ring . . . . . . . . . . . . 1222\n35.4 Torsion Modules over a PID; Primary Decomposition . . . . . . . . . . . . . 1225\n35.5 Finitely Generated Modules over a PID . . . . . . . . . . . . . . . . . . . . 1231\n35.6 Extension of the Ring of Scalars . . . . . . . . . . . . . . . . . . . . . . . . 1247\n\n36 Normal Forms; The Rational Canonical Form 1253\n36.1 The Torsion Module Associated With An Endomorphism . . . . . . . . . . 1253\n36.2 The Rational Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . 1261\n36.3 The Rational Canonical Form, Second Version . . . . . . . . . . . . . . . . . 1268\n36.4 The Jordan Form Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . 1269\n36.5 The Smith Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1272\n\nV Topology, Differential Calculus 1285\n\n37 Topology 1287\n37.1 Metric Spaces and Normed Vector Spaces . . . . . . . . . . . . . . . . . . . 1287\n37.2 Topological Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1294\n37.3 Continuous Functions, Limits . . . . . . . . . . . . . . . . . . . . . . . . . . 1303\n37.4 Connected Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1311\n37.5 Compact Sets and Locally Compact Spaces . . . . . . . . . . . . . . . . . . 1320\n37.6 Second-Countable and Separable Spaces . . . . . . . . . . . . . . . . . . . . 1331\n37.7 Sequential Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1335\n37.8 Complete Metric Spaces and Compactness . . . . . . . . . . . . . . . . . . . 1341\n37.9 Completion of a Metric Space . . . . . . . . . . . . . . . . . . . . . . . . . . 1344\n37.10 The Contraction Mapping Theorem . . . . . . . . . . . . . . . . . . . . . . 1351\n37.11 Continuous Linear and Multilinear Maps . . . . . . . . . . . . . . . . . . . . 1355\n37.12 Completion of a Normed Vector Space . . . . . . . . . . . . . . . . . . . . . 1362\n37.13 Normed Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n37.14 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n\n38 A Detour On Fractals 1367\n\n\n\n12 CONTENTS\n\n38.1 Iterated Function Systems and Fractals . . . . . . . . . . . . . . . . . . . . 1367\n\n39 Differential Calculus 1375\n39.1 Directional Derivatives, Total Derivatives . . . . . . . . . . . . . . . . . . . 1375\n39.2 Jacobian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1389\n39.3 The Implicit and The Inverse Function Theorems . . . . . . . . . . . . . . . 1397\n39.4 Tangent Spaces and Differentials . . . . . . . . . . . . . . . . . . . . . . . . 1401\n39.5 Second-Order and Higher-Order Derivatives . . . . . . . . . . . . . . . . . . 1402\n39.6 Taylor’s formula, Faà di Bruno’s formula . . . . . . . . . . . . . . . . . . . . 1407\n39.7 Vector Fields, Covariant Derivatives, Lie Brackets . . . . . . . . . . . . . . . 1411\n39.8 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1413\n\nVI Preliminaries for Optimization Theory 1415\n\n40 Extrema of Real-Valued Functions 1417\n40.1 Local Extrema and Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . 1417\n40.2 Using Second Derivatives to Find Extrema . . . . . . . . . . . . . . . . . . . 1427\n40.3 Using Convexity to Find Extrema . . . . . . . . . . . . . . . . . . . . . . . 1430\n40.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1440\n\n41 Newton’s Method and Its Generalizations 1441\n41.1 Newton’s Method for Real Functions of a Real Argument . . . . . . . . . . 1441\n41.2 Generalizations of Newton’s Method . . . . . . . . . . . . . . . . . . . . . . 1442\n41.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1448\n\n42 Quadratic Optimization Problems 1449\n42.1 Quadratic Optimization: The Positive Definite Case . . . . . . . . . . . . . 1449\n42.2 Quadratic Optimization: The General Case . . . . . . . . . . . . . . . . . . 1458\n42.3 Maximizing a Quadratic Function on the Unit Sphere . . . . . . . . . . . . 1463\n42.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1468\n\n43 Schur Complements and Applications 1469\n43.1 Schur Complements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1469\n43.2 SPD Matrices and Schur Complements . . . . . . . . . . . . . . . . . . . . . 1472\n43.3 SP Semidefinite Matrices and Schur Complements . . . . . . . . . . . . . . 1473\n\nVII Linear Optimization 1475\n\n44 Convex Sets, Cones, H-Polyhedra 1477\n44.1 What is Linear Programming? . . . . . . . . . . . . . . . . . . . . . . . . . 1477\n44.2 Affine Subsets, Convex Sets, Hyperplanes, Half-Spaces . . . . . . . . . . . . 1479\n44.3 Cones, Polyhedral Cones, and H-Polyhedra . . . . . . . . . . . . . . . . . . 1482\n\n\n\nCONTENTS 13\n\n45 Linear Programs 1489\n45.1 Linear Programs, Feasible Solutions, Optimal Solutions . . . . . . . . . . . 1489\n45.2 Basic Feasible Solutions and Vertices . . . . . . . . . . . . . . . . . . . . . . 1495\n\n46 The Simplex Algorithm 1503\n46.1 The Idea Behind the Simplex Algorithm . . . . . . . . . . . . . . . . . . . . 1503\n46.2 The Simplex Algorithm in General . . . . . . . . . . . . . . . . . . . . . . . 1512\n46.3 How to Perform a Pivoting Step Efficiently . . . . . . . . . . . . . . . . . . 1519\n46.4 The Simplex Algorithm Using Tableaux . . . . . . . . . . . . . . . . . . . . 1523\n46.5 Computational Efficiency of the Simplex Method . . . . . . . . . . . . . . . 1532\n\n47 Linear Programming and Duality 1535\n47.1 Variants of the Farkas Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 1535\n47.2 The Duality Theorem in Linear Programming . . . . . . . . . . . . . . . . . 1540\n47.3 Complementary Slackness Conditions . . . . . . . . . . . . . . . . . . . . . 1548\n47.4 Duality for Linear Programs in Standard Form . . . . . . . . . . . . . . . . 1550\n47.5 The Dual Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 1553\n47.6 The Primal-Dual Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1558\n\nVIII NonLinear Optimization 1569\n\n48 Basics of Hilbert Spaces 1571\n48.1 The Projection Lemma, Duality . . . . . . . . . . . . . . . . . . . . . . . . 1571\n48.2 Farkas–Minkowski Lemma in Hilbert Spaces . . . . . . . . . . . . . . . . . . 1588\n\n49 General Results of Optimization Theory 1591\n49.1 Optimization Problems; Basic Terminology . . . . . . . . . . . . . . . . . . 1591\n49.2 Existence of Solutions of an Optimization Problem . . . . . . . . . . . . . . 1594\n49.3 Minima of Quadratic Functionals . . . . . . . . . . . . . . . . . . . . . . . . 1599\n49.4 Elliptic Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1605\n49.5 Iterative Methods for Unconstrained Problems . . . . . . . . . . . . . . . . 1608\n49.6 Gradient Descent Methods for Unconstrained Problems . . . . . . . . . . . 1612\n49.7 Convergence of Gradient Descent with Variable Stepsize . . . . . . . . . . . 1617\n49.8 Steepest Descent for an Arbitrary Norm . . . . . . . . . . . . . . . . . . . . 1622\n49.9 Newton’s Method For Finding a Minimum . . . . . . . . . . . . . . . . . . . 1624\n49.10 Conjugate Gradient Methods; Unconstrained Problems . . . . . . . . . . . . 1628\n49.11 Gradient Projection for Constrained Optimization . . . . . . . . . . . . . . 1640\n49.12 Penalty Methods for Constrained Optimization . . . . . . . . . . . . . . . . 1642\n49.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1644\n\n50 Introduction to Nonlinear Optimization 1647\n50.1 The Cone of Feasible Directions . . . . . . . . . . . . . . . . . . . . . . . . . 1647\n\n\n\n14 CONTENTS\n\n50.2 Active Constraints and Qualified Constraints . . . . . . . . . . . . . . . . . 1654\n50.3 The Karush–Kuhn–Tucker Conditions . . . . . . . . . . . . . . . . . . . . . 1660\n50.4 Equality Constrained Minimization . . . . . . . . . . . . . . . . . . . . . . . 1672\n50.5 Hard Margin Support Vector Machine; Version I . . . . . . . . . . . . . . . 1677\n50.6 Hard Margin Support Vector Machine; Version II . . . . . . . . . . . . . . . 1681\n50.7 Lagrangian Duality and Saddle Points . . . . . . . . . . . . . . . . . . . . . 1690\n50.8 Weak and Strong Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1699\n50.9 Handling Equality Constraints Explicitly . . . . . . . . . . . . . . . . . . . . 1707\n50.10 Dual of the Hard Margin Support Vector Machine . . . . . . . . . . . . . . 1710\n50.11 Conjugate Function and Legendre Dual Function . . . . . . . . . . . . . . . 1715\n50.12 Some Techniques to Obtain a More Useful Dual Program . . . . . . . . . . 1725\n50.13 Uzawa’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1729\n50.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1735\n\n51 Subgradients and Subdifferentials 1737\n51.1 Extended Real-Valued Convex Functions . . . . . . . . . . . . . . . . . . . . 1739\n51.2 Subgradients and Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . . 1748\n51.3 Basic Properties of Subgradients and Subdifferentials . . . . . . . . . . . . . 1760\n51.4 Additional Properties of Subdifferentials . . . . . . . . . . . . . . . . . . . . 1766\n51.5 The Minimum of a Proper Convex Function . . . . . . . . . . . . . . . . . . 1770\n51.6 Generalization of the Lagrangian Framework . . . . . . . . . . . . . . . . . 1776\n51.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1780\n\n52 Dual Ascent Methods; ADMM 1783\n52.1 Dual Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1785\n52.2 Augmented Lagrangians and the Method of Multipliers . . . . . . . . . . . . 1789\n52.3 ADMM: Alternating Direction Method of Multipliers . . . . . . . . . . . . . 1794\n52.4 Convergence of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1797\n52.5 Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806\n52.6 Some Applications of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . 1807\n52.7 Applications of ADMM to `1-Norm Problems . . . . . . . . . . . . . . . . . 1810\n52.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1815\n\nIX Applications to Machine Learning 1817\n\n53 Ridge Regression and Lasso Regression 1819\n53.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1819\n53.2 Lasso Regression (`1-Regularized Regression) . . . . . . . . . . . . . . . . . 1829\n53.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1835\n\n54 Positive Definite Kernels 1837\n54.1 Basic Properties of Positive Definite Kernels . . . . . . . . . . . . . . . . . . 1837\n\n\n\nCONTENTS 15\n\n54.2 Hilbert Space Representation of a Positive Kernel . . . . . . . . . . . . . . . 1848\n54.3 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1852\n54.4 ν-SV Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1855\n\n55 Soft Margin Support Vector Machines 1865\n55.1 Soft Margin Support Vector Machines; (SVMs1) . . . . . . . . . . . . . . . . 1868\n55.2 Soft Margin Support Vector Machines; (SVMs2) . . . . . . . . . . . . . . . . 1878\n55.3 Soft Margin Support Vector Machines; (SVMs2′) . . . . . . . . . . . . . . . 1885\n55.4 Soft Margin SVM; (SVMs3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1900\n55.5 Soft Margin Support Vector Machines; (SVMs4) . . . . . . . . . . . . . . . . 1903\n55.6 Soft Margin SVM; (SVMs5) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1911\n55.7 Summary and Comparison of the SVM Methods . . . . . . . . . . . . . . . 1914\n\nX Appendices 1927\n\nA Total Orthogonal Families in Hilbert Spaces 1929\nA.1 Total Orthogonal Families, Fourier Coefficients . . . . . . . . . . . . . . . . 1929\nA.2 The Hilbert Space `2(K) and the Riesz-Fischer Theorem . . . . . . . . . . . 1937\n\nB Zorn’s Lemma; Some Applications 1947\nB.1 Statement of Zorn’s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 1947\nB.2 Proof of the Existence of a Basis in a Vector Space . . . . . . . . . . . . . . 1948\nB.3 Existence of Maximal Proper Ideals . . . . . . . . . . . . . . . . . . . . . . 1949\n\nBibliography 1951\n\n\n\n16 CONTENTS\n\n16\n\nCONTENTS\n\n\n\n\nChapter 1\n\nIntroduction\n\n17\n\n\n\n18 CHAPTER 1. INTRODUCTION\n\n\n\nChapter 2\n\nGroups, Rings, and Fields\n\nIn the following four chapters, the basic algebraic structures (groups, rings, fields, vector\nspaces) are reviewed, with a major emphasis on vector spaces. Basic notions of linear alge-\nbra such as vector spaces, subspaces, linear combinations, linear independence, bases, quo-\ntient spaces, linear maps, matrices, change of bases, direct sums, linear forms, dual spaces,\nhyperplanes, transpose of a linear maps, are reviewed.\n\n2.1 Groups, Subgroups, Cosets\n\nThe set R of real numbers has two operations +: R × R → R (addition) and ∗ : R × R →\nR (multiplication) satisfying properties that make R into an abelian group under +, and\nR− {0} = R∗ into an abelian group under ∗. Recall the definition of a group.\n\nDefinition 2.1. A group is a set G equipped with a binary operation · : G × G → G that\nassociates an element a · b ∈ G to every pair of elements a, b ∈ G, and having the following\nproperties: · is associative, has an identity element e ∈ G, and every element in G is invertible\n(w.r.t. ·). More explicitly, this means that the following equations hold for all a, b, c ∈ G:\n\n(G1) a · (b · c) = (a · b) · c. (associativity);\n\n(G2) a · e = e · a = a. (identity);\n\n(G3) For every a ∈ G, there is some a−1 ∈ G such that a · a−1 = a−1 · a = e. (inverse).\n\nA group G is abelian (or commutative) if\n\na · b = b · a for all a, b ∈ G.\n\nA set M together with an operation · : M ×M → M and an element e satisfying only\nConditions (G1) and (G2) is called a monoid . For example, the set N = {0, 1, . . . , n, . . .} of\nnatural numbers is a (commutative) monoid under addition. However, it is not a group.\n\nSome examples of groups are given below.\n\n19\n\n\n\n20 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.1.\n\n1. The set Z = {. . . ,−n, . . . ,−1, 0, 1, . . . , n, . . .} of integers is an abelian group under\naddition, with identity element 0. However, Z∗ = Z − {0} is not a group under\nmultiplication.\n\n2. The set Q of rational numbers (fractions p/q with p, q ∈ Z and q 6= 0) is an abelian\ngroup under addition, with identity element 0. The set Q∗ = Q−{0} is also an abelian\ngroup under multiplication, with identity element 1.\n\n3. Given any nonempty set S, the set of bijections f : S → S, also called permutations\nof S, is a group under function composition (i.e., the multiplication of f and g is the\ncomposition g ◦ f), with identity element the identity function idS. This group is not\nabelian as soon as S has more than two elements. The permutation group of the set\nS = {1, . . . , n} is often denoted Sn and called the symmetric group on n elements.\n\n4. For any positive integer p ∈ N, define a relation on Z, denoted m ≡ n (mod p), as\nfollows:\n\nm ≡ n (mod p) iff m− n = kp for some k ∈ Z.\n\nThe reader will easily check that this is an equivalence relation, and, moreover, it is\ncompatible with respect to addition and multiplication, which means that if m1 ≡ n1\n\n(mod p) and m2 ≡ n2 (mod p), then m1 + m2 ≡ n1 + n2 (mod p) and m1m2 ≡ n1n2\n\n(mod p). Consequently, we can define an addition operation and a multiplication\noperation of the set of equivalence classes (mod p):\n\n[m] + [n] = [m+ n]\n\nand\n[m] · [n] = [mn].\n\nThe reader will easily check that addition of residue classes (mod p) induces an abelian\ngroup structure with [0] as zero. This group is denoted Z/pZ.\n\n5. The set of n×n invertible matrices with real (or complex) coefficients is a group under\nmatrix multiplication, with identity element the identity matrix In. This group is\ncalled the general linear group and is usually denoted by GL(n,R) (or GL(n,C)).\n\n6. The set of n × n invertible matrices A with real (or complex) coefficients such that\ndet(A) = 1 is a group under matrix multiplication, with identity element the identity\nmatrix In. This group is called the special linear group and is usually denoted by\nSL(n,R) (or SL(n,C)).\n\n7. The set of n× n matrices Q with real coefficients such that\n\nQQ> = Q>Q = In\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nwe have Q−1 = Q>. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n× n invertible matrices Q with real coefficients such that\n\nQQ> = Q>Q = In and det(Q) = 1\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nas in (6), we have Q−1 = Q>. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)–(8) are nonabelian for n ≥ 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a−1 of an element a ∈ G is denoted by −a.\n\nThe identity element of a group is unique. In fact, we can prove a more general fact:\n\nProposition 2.1. If a binary operation · : M ×M → M is associative and if e′ ∈ M is a\nleft identity and e′′ ∈M is a right identity, which means that\n\ne′ · a = a for all a ∈M (G2l)\n\nand\na · e′′ = a for all a ∈M, (G2r)\n\nthen e′ = e′′.\n\nProof. If we let a = e′′ in equation (G2l), we get\n\ne′ · e′′ = e′′,\n\nand if we let a = e′ in equation (G2r), we get\n\ne′ · e′′ = e′,\n\nand thus\ne′ = e′ · e′′ = e′′,\n\nas claimed.\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix /,,;\nwe have Q-! = Q'. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n x n invertible matrices Q with real coefficients such that\n\nQQ'=Q'Q=I, and det(Q)=1\n\nis a group under matrix multiplication, with identity element the identity matrix [,,;\nas in (6), we have Q-! = Q'. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)—(8) are nonabelian for n > 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a~! of an element a € G is denoted by —a.\n\nThe identity element of a group is unzque. In fact, we can prove a more general fact:\n\nProposition 2.1. /f a binary operation -: M x M + M is associative and if e’ € M is a\nleft identity and e” € M is a right identity, which means that\n\ne-a=a forall ae M (G21)\n\nand\na-e’=a forall ae M, (G2r)\n\nthen e' = e”.\nProof. If we let a = e” in equation (G21), we get\n/ \" 1\n\ne-e =e,\n\nand if we let a = e’ in equation (G2r), we get\n\nand thus\n\nas claimed. Oo\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n\n\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a ∈M has some\nleft inverse a′ ∈M and some right inverse a′′ ∈M , which means that\n\na′ · a = e (G3l)\n\nand\na · a′′ = e, (G3r)\n\nthen a′ = a′′.\n\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a′ · a) · a′′ = e · a′′ = a′′.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na′ · (a · a′′) = a′ · e = a′.\n\nHowever, since M is monoid, the operation · is associative, so\n\na′ = a′ · (a · a′′) = (a′ · a) · a′′ = a′′,\n\nas claimed.\n\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n(G2l) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G is a group\nof order n. If G is infinite, we say that G has infinite order . The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R, S ⊆ G, we let\n\nRS = {r · s | r ∈ R, s ∈ S}.\n\nIn particular, for any g ∈ G, if R = {g}, we write\n\ngS = {g · s | s ∈ S},\n\nand similarly, if S = {g}, we write\n\nRg = {r · g | r ∈ R}.\n\nFrom now on, we will drop the multiplication sign and write g1g2 for g1 · g2.\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a € M has some\nleft inverse a’ € M and some right inverse a\" € M, which means that\n\na-a=e (G31)\n\nand\na-a\" =e, (G3r)\n\nthen a’ =a\".\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a’-a) ql — e-q’ — a’.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na’-(a-a\")=d'-e=d’.\nHowever, since M is monoid, the operation - is associative, so\na’ =a'-(a-a\") =(a'-a)-a\" =a\",\nas claimed. O\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n\n(G21) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G' is a group\nof order n. If G is infinite, we say that G has infinite order. The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R,S' C G, we let\nRS ={r-s|reR,seS}.\nIn particular, for any g € G, if R = {g}, we write\ngS = {g-s|s € S},\nand similarly, if S = {g}, we write\n\nRg={r-g|reR}.\n\nFrom now on, we will drop the multiplication sign and write gg for gi - go.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 23\n\nDefinition 2.3. Let G be a group. For any g ∈ G, define Lg, the left translation by g, by\nLg(a) = ga, for all a ∈ G, and Rg, the right translation by g, by Rg(a) = ag, for all a ∈ G.\n\nThe following simple fact is often used.\n\nProposition 2.3. Given a group G, the translations Lg and Rg are bijections.\n\nProof. We show this for Lg, the proof for Rg being similar.\n\nIf Lg(a) = Lg(b), then ga = gb, and multiplying on the left by g−1, we get a = b, so Lg\ninjective. For any b ∈ G, we have Lg(g\n\n−1b) = gg−1b = b, so Lg is surjective. Therefore, Lg\nis bijective.\n\nDefinition 2.4. Given a group G, a subset H of G is a subgroup of G iff\n\n(1) The identity element e of G also belongs to H (e ∈ H);\n\n(2) For all h1, h2 ∈ H, we have h1h2 ∈ H;\n\n(3) For all h ∈ H, we have h−1 ∈ H.\n\nThe proof of the following proposition is left as an exercise.\n\nProposition 2.4. Given a group G, a subset H ⊆ G is a subgroup of G iff H is nonempty\nand whenever h1, h2 ∈ H, then h1h\n\n−1\n2 ∈ H.\n\nIf the group G is finite, then the following criterion can be used.\n\nProposition 2.5. Given a finite group G, a subset H ⊆ G is a subgroup of G iff\n\n(1) e ∈ H;\n\n(2) H is closed under multiplication.\n\nProof. We just have to prove that Condition (3) of Definition 2.4 holds. For any a ∈ H,\nsince the left translation La is bijective, its restriction to H is injective, and since H is finite,\nit is also bijective. Since e ∈ H, there is a unique b ∈ H such that La(b) = ab = e. However,\nif a−1 is the inverse of a in G, we also have La(a\n\n−1) = aa−1 = e, and by injectivity of La, we\nhave a−1 = b ∈ H.\n\nExample 2.2.\n\n1. For any integer n ∈ Z, the set\n\nnZ = {nk | k ∈ Z}\n\nis a subgroup of the group Z.\n\n\n\n24 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The set of matrices\n\nGL+(n,R) = {A ∈ GL(n,R) | det(A) > 0}\n\nis a subgroup of the group GL(n,R).\n\n3. The group SL(n,R) is a subgroup of the group GL(n,R).\n\n4. The group O(n) is a subgroup of the group GL(n,R).\n\n5. The group SO(n) is a subgroup of the group O(n), and a subgroup of the group\nSL(n,R).\n\n6. It is not hard to show that every 2× 2 rotation matrix R ∈ SO(2) can be written as\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n, with 0 ≤ θ < 2π.\n\nThen SO(2) can be considered as a subgroup of SO(3) by viewing the matrix\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\nas the matrix\n\nQ =\n\ncos θ − sin θ 0\nsin θ cos θ 0\n\n0 0 1\n\n .\n\n7. The set of 2× 2 upper-triangular matrices of the form(\na b\n0 c\n\n)\na, b, c ∈ R, a, c 6= 0\n\nis a subgroup of the group GL(2,R).\n\n8. The set V consisting of the four matrices(\n±1 0\n0 ±1\n\n)\nis a subgroup of the group GL(2,R) called the Klein four-group.\n\nDefinition 2.5. If H is a subgroup of G and g ∈ G is any element, the sets of the form\ngH are called left cosets of H in G and the sets of the form Hg are called right cosets of H\nin G. The left cosets (resp. right cosets) of H induce an equivalence relation ∼ defined as\nfollows: For all g1, g2 ∈ G,\n\ng1 ∼ g2 iff g1H = g2H\n\n(resp. g1 ∼ g2 iff Hg1 = Hg2). Obviously, ∼ is an equivalence relation.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 25\n\nNow, we claim the following fact:\n\nProposition 2.6. Given a group G and any subgroup H of G, we have g1H = g2H iff\ng−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H, for all g1, g2 ∈ G.\n\nProof. If we apply the bijection Lg−1\n2\n\nto both g1H and g2H we get Lg−1\n2\n\n(g1H) = g−1\n2 g1H\n\nand Lg−1\n2\n\n(g2H) = H, so g1H = g2H iff g−1\n2 g1H = H. If g−1\n\n2 g1H = H, since 1 ∈ H, we get\n\ng−1\n2 g1 ∈ H. Conversely, if g−1\n\n2 g1 ∈ H, since H is a group, the left translation Lg−1\n2 g1\n\nis a\n\nbijection of H, so g−1\n2 g1H = H. Thus, g−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H.\n\nIt follows that the equivalence class of an element g ∈ G is the coset gH (resp. Hg).\nSince Lg is a bijection between H and gH, the cosets gH all have the same cardinality. The\nmap Lg−1 ◦ Rg is a bijection between the left coset gH and the right coset Hg, so they also\nhave the same cardinality. Since the distinct cosets gH form a partition of G, we obtain the\nfollowing fact:\n\nProposition 2.7. (Lagrange) For any finite group G and any subgroup H of G, the order\nh of H divides the order n of G.\n\nDefinition 2.6. Given a finite group G and a subgroup H of G, if n = |G| and h = |H|,\nthen the ratio n/h is denoted by (G : H) and is called the index of H in G.\n\nThe index (G : H) is the number of left (and right) cosets of H in G. Proposition 2.7\ncan be stated as\n\n|G| = (G : H)|H|.\n\nThe set of left cosets of H in G (which, in general, is not a group) is denoted G/H.\nThe “points” of G/H are obtained by “collapsing” all the elements in a coset into a single\nelement.\n\nExample 2.3.\n\n1. Let n be any positive integer, and consider the subgroup nZ of Z (under addition).\nThe coset of 0 is the set {0}, and the coset of any nonzero integer m ∈ Z is\n\nm+ nZ = {m+ nk | k ∈ Z}.\n\nBy dividing m by n, we have m = nq + r for some unique r such that 0 ≤ r ≤ n− 1.\nBut then we see that r is the smallest positive element of the coset m + nZ. This\nimplies that there is a bijection betwen the cosets of the subgroup nZ of Z and the set\nof residues {0, 1, . . . , n− 1} modulo n, or equivalently a bijection with Z/nZ.\n\n\n\n26 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The cosets of SL(n,R) in GL(n,R) are the sets of matrices\n\nASL(n,R) = {AB | B ∈ SL(n,R)}, A ∈ GL(n,R).\n\nSince A is invertible, det(A) 6= 0, and we can write A = (det(A))1/n((det(A))−1/nA)\nif det(A) > 0 and A = (− det(A))1/n((− det(A))−1/nA) if det(A) < 0. But we have\n(det(A))−1/nA ∈ SL(n,R) if det(A) > 0 and −(− det(A))−1/nA ∈ SL(n,R) if det(A) <\n0, so the coset ASL(n,R) contains the matrix\n\n(det(A))1/nIn if det(A) > 0, −(− det(A))1/nIn if det(A) < 0.\n\nIt follows that there is a bijection between the cosets of SL(n,R) in GL(n,R) and R.\n\n3. The cosets of SO(n) in GL+(n,R) are the sets of matrices\n\nASO(n) = {AQ | Q ∈ SO(n)}, A ∈ GL+(n,R).\n\nIt can be shown (using the polar form for matrices) that there is a bijection between\nthe cosets of SO(n) in GL+(n,R) and the set of n × n symmetric, positive, definite\nmatrices; these are the symmetric matrices whose eigenvalues are strictly positive.\n\n4. The cosets of SO(2) in SO(3) are the sets of matrices\n\nQSO(2) = {QR | R ∈ SO(2)}, Q ∈ SO(3).\n\nThe group SO(3) moves the points on the sphere S2 in R3, namely for any x ∈ S2,\n\nx 7→ Qx for any rotation Q ∈ SO(3).\n\nHere,\nS2 = {(x, y, z) ∈ R3 | x2 + y2 + z2 = 1}.\n\nLet N = (0, 0, 1) be the north pole on the sphere S2. Then it is not hard to show that\nSO(2) is precisely the subgroup of SO(3) that leaves N fixed. As a consequence, all\nrotations QR in the coset QSO(2) map N to the same point QN ∈ S2, and it can be\nshown that there is a bijection between the cosets of SO(2) in SO(3) and the points\non S2. The surjectivity of this map has to do with the fact that the action of SO(3)\non S2 is transitive, which means that for any point x ∈ S2, there is some rotation\nQ ∈ SO(3) such that QN = x.\n\nIt is tempting to define a multiplication operation on left cosets (or right cosets) by\nsetting\n\n(g1H)(g2H) = (g1g2)H,\n\nbut this operation is not well defined in general, unless the subgroup H possesses a special\nproperty. In Example 2.3, it is possible to define multiplication of cosets in (1), but it is not\npossible in (2) and (3).\n\nThe property of the subgroup H that allows defining a multiplication operation on left\ncosets is typical of the kernels of group homomorphisms, so we are led to the following\ndefinition.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G′, a function ϕ : G→ G′ is a homomorphism\niff\n\nϕ(g1g2) = ϕ(g1)ϕ(g2), for all g1, g2 ∈ G.\n\nTaking g1 = g2 = e (in G), we see that\n\nϕ(e) = e′,\n\nand taking g1 = g and g2 = g−1, we see that\n\nϕ(g−1) = (ϕ(g))−1.\n\nExample 2.4.\n\n1. The map ϕ : Z→ Z/nZ given by ϕ(m) = m mod n for all m ∈ Z is a homomorphism.\n\n2. The map det : GL(n,R) → R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A,B. Similarly, the map det : O(n)→ R is a homomorphism.\n\nIf ϕ : G → G′ and ψ : G′ → G′′ are group homomorphisms, then ψ ◦ ϕ : G → G′′ is also\na homomorphism. If ϕ : G→ G′ is a homomorphism of groups, and if H ⊆ G, H ′ ⊆ G′ are\ntwo subgroups, then it is easily checked that\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H}\n\nis a subgroup of G′ and\nϕ−1(H ′) = {g ∈ G | ϕ(g) ∈ H ′}\n\nis a subgroup of G. In particular, when H ′ = {e′}, we obtain the kernel , Ker ϕ, of ϕ.\n\nDefinition 2.8. If ϕ : G → G′ is a homomorphism of groups, and if H ⊆ G is a subgroup\nof G, then the subgroup of G′,\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H},\n\nis called the image of H by ϕ, and the subgroup of G,\n\nKer ϕ = {g ∈ G | ϕ(g) = e′},\n\nis called the kernel of ϕ.\n\nExample 2.5.\n\n1. The kernel of the homomorphism ϕ : Z→ Z/nZ is nZ.\n\n2. The kernel of the homomorphism det : GL(n,R)→ R is SL(n,R). Similarly, the kernel\nof the homomorphism det : O(n)→ R is SO(n).\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G’, a function y: G > G’ is a homomorphism\niff\n(9192) = P(m)e(G2), for all gr, g2 € G.\n\nTaking g, = go =e (in G), we see that\n\n! we see that\n\nog\") = (v(g))*.\n\nand taking g; = g and g2 = g~\n\nExample 2.4.\n1. The map y: Z > Z/nZ given by y(m) = m mod n for all m € Z is a homomorphism.\n\n2. The map det: GL(n, R) — R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A, B. Similarly, the map det: O(n) > R is a homomorphism.\n\nIf y: G > G' and wy: G’ > G\"” are group homomorphisms, then yo y: G > G\" is also\na homomorphism. If y: G + G’ is a homomorphism of groups, and if H C G, H’ C G’ are\ntwo subgroups, then it is easily checked that\n\nIm H = 9(H) = {y(9) |g © A}\n\nis a subgroup of G’ and\neg (H')= {9 €G| lg) € H’}\nis a subgroup of G. In particular, when H’ = {e’}, we obtain the kernel, Ker y, of y.\n\nDefinition 2.8. If ¢: G > G’ is a homomorphism of groups, and if H C G is a subgroup\nof G, then the subgroup of G’,\n\nIm H = 9(H) = {y(9) |g € FH},\n\nis called the image of H by vy, and the subgroup of G,\n\nKer p= {9 €G | p(y) =e},\nis called the kernel of y.\nExample 2.5.\n1. The kernel of the homomorphism y: Z > Z/nZ is nZ.\n\n2. The kernel of the homomorphism det: GL(n, R) > Ris SL(n, R). Similarly, the kernel\nof the homomorphism det: O(n) > R is SO(n).\n\n\n\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. If ϕ : G→ G′ is a homomorphism of groups, then ϕ : G→ G′ is injective\niff Ker ϕ = {e}. (We also write Ker ϕ = (0).)\n\nProof. Assume ϕ is injective. Since ϕ(e) = e′, if ϕ(g) = e′, then ϕ(g) = ϕ(e), and by\ninjectivity of ϕ we must have g = e, so Ker ϕ = {e}.\n\nConversely, assume that Ker ϕ = {e}. If ϕ(g1) = ϕ(g2), then by multiplication on the\nleft by (ϕ(g1))−1 we get\n\ne′ = (ϕ(g1))−1ϕ(g1) = (ϕ(g1))−1ϕ(g2),\n\nand since ϕ is a homomorphism (ϕ(g1))−1 = ϕ(g−1\n1 ), so\n\ne′ = (ϕ(g1))−1ϕ(g2) = ϕ(g−1\n1 )ϕ(g2) = ϕ(g−1\n\n1 g2).\n\nThis shows that g−1\n1 g2 ∈ Ker ϕ, but since Ker ϕ = {e} we have g−1\n\n1 g2 = e, and thus g2 = g1,\nproving that ϕ is injective.\n\nDefinition 2.9. We say that a group homomorphism ϕ : G→ G′ is an isomorphism if there\nis a homomorphism ψ : G′ → G, so that\n\nψ ◦ ϕ = idG and ϕ ◦ ψ = idG′ . (†)\n\nIf ϕ is an isomorphism we say that the groups G and G′ are isomorphic. When G′ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nϕ : G→ G′ is an isomorphism, then the homomorphism ψ : G′ → G satisfying Condition (†)\nis unique. This homomorphism is denoted ϕ−1.\n\nThe left translations Lg and the right translations Rg are automorphisms of G.\n\nSuppose ϕ : G → G′ is a bijective homomorphism, and let ϕ−1 be the inverse of ϕ (as a\nfunction). Then for all a, b ∈ G, we have\n\nϕ(ϕ−1(a)ϕ−1(b)) = ϕ(ϕ−1(a))ϕ(ϕ−1(b)) = ab,\n\nand so\n\nϕ−1(ab) = ϕ−1(a)ϕ−1(b),\n\nwhich proves that ϕ−1 is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism ϕ : G→ G′ is an isomorphism.\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. Ifo: G > G’' is a homomorphism of groups, then py: G — G\" is injective\niff Ker y = {e}. (We also write Ker y = (0).)\n\nProof. Assume ¢p is injective. Since y(e) = e’, if y(g) = e’, then y(g) = y(e), and by\ninjectivity of ~ we must have g = e, so Ker y = {e}.\n\nConversely, assume that Ker y = {e}. If y(g1) = v(ge), then by multiplication on the\nleft by (y(g1))~* we get\n\ne’ = (9(m)) yg) = (¢(91)) *9(92),\n\nand since y is a homomorphism (y(g))~! = y(g,\"), so\n\ne' = (y(m1)) *v(g2) = v(97')9(92) = v(9r 92):\n\nThis shows that g>'g2 € Ker y, but since Ker y = {e} we have gj ‘go = e, and thus g = gu,\nproving that y is injective. L\n\nDefinition 2.9. We say that a group homomorphism y: G — G’ is an isomorphism if there\nis a homomorphism w: G’ + G, so that\n\nwop=idg and pow=idq@. (Tt)\n\nIf y is an isomorphism we say that the groups G and G’ are isomorphic. When G’ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nyp: G > G’ is an isomorphism, then the homomorphism ~: G’ + G satisfying Condition (T)\nis unique. This homomorphism is denoted y!.\n\nThe left translations L, and the right translations R, are automorphisms of G.\n\nSuppose vy: G > G’ is a bijective homomorphism, and let y~' be the inverse of y (as a\nfunction). Then for all a,b € G, we have\n\no(y ‘(a)y *(b)) = v(e*(a))e(y *(b)) = ab,\n\nand so\nyp *(ab) =p ‘(a)y (0),\n\nwhich proves that y~! is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism yp: G > G\" is an isomorphism.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\n\ngH = Hg, for all g ∈ G. (∗)\n\nis equivalent by multiplication on the right by g−1 to\n\ngHg−1 = H, for all g ∈ G,\n\nand the above is equivalent to\n\ngHg−1 ⊆ H, for all g ∈ G. (∗∗)\n\nThis is because gHg−1 ⊆ H implies H ⊆ g−1Hg, and this for all g ∈ G.\n\nProposition 2.10. Let ϕ : G → G′ be a group homomorphism. Then H = Ker ϕ satisfies\nProperty (∗∗), and thus Property (∗).\n\nProof. We have\n\nϕ(ghg−1) = ϕ(g)ϕ(h)ϕ(g−1) = ϕ(g)e′ϕ(g)−1 = ϕ(g)ϕ(g)−1 = e′,\n\nfor all h ∈ H = Ker ϕ and all g ∈ G. Thus, by definition of H = Ker ϕ, we have gHg−1 ⊆\nH.\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\n\ngNg−1 = N, for all g ∈ G.\n\nThis is denoted by N CG.\n\nProposition 2.10 shows that the kernel Ker ϕ of a homomorphism ϕ : G→ G′ is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R ∈ SO(2) and A ∈ SL(2,R) be the matrices\n\nR =\n\n(\n0 −1\n1 0\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nThen\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\nand we have\n\nARA−1 =\n\n(\n1 1\n0 1\n\n)(\n0 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n1 −1\n\n)\n,\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\ngH =Hg, forallg €G. (*)\nis equivalent by multiplication on the right by g~! to\ngHg ‘=H, forallg€G,\nand the above is equivalent to\ngHg' CH, forallg€G. (x)\nThis is because gHg~' C H implies H C g7!Hq, and this for all g € G.\n\nProposition 2.10. Let y: G — G’ be a group homomorphism. Then H = Ker y satisfies\nProperty (**), and thus Property (x).\n\nProof. We have\n\n/\n\ne(ghg*) = v(gelh)ye(g\") = v(ae'e(9)* = vla)e(g)* =e,\n\nfor all h € H = Ker y and all g € G. Thus, by definition of H = Ker y, we have gHg™! C\nH. O\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\ngNg |=N, forallg €G.\n\nThis is denoted by N dG.\n\nProposition 2.10 shows that the kernel Ker y of a homomorphism y: G > G\" is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R € SO(2) and A € SL(2,R) be the matrices\n\ne=(1o)) a= i)\n\nThen\n\nand we have\n\nae (00 G6 TG ov) Z)-G 5)\n\n\n\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA−1 /∈ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R ∈ SO(2) and Q ∈ SO(3) be the matrices\n\nR =\n\n0 −1 0\n1 0 0\n0 0 1\n\n , Q =\n\n1 0 0\n0 0 −1\n0 1 0\n\n .\n\nThen\n\nQ−1 = Q> =\n\n1 0 0\n0 0 1\n0 −1 0\n\n\nand we have\n\nQRQ−1 =\n\n1 0 0\n0 0 −1\n0 1 0\n\n0 −1 0\n1 0 0\n0 0 1\n\n1 0 0\n0 0 1\n0 −1 0\n\n =\n\n0 −1 0\n0 0 −1\n1 0 0\n\n1 0 0\n0 0 1\n0 −1 0\n\n\n=\n\n0 0 −1\n0 1 0\n1 0 0\n\n .\n\nObserve that QRQ−1 /∈ SO(2), so SO(2) is not a normal subgroup of SO(3).\n\nLet T and A ∈ GL(2,R) be the following matrices\n\nT =\n\n(\n1 1\n0 1\n\n)\n, A =\n\n(\n0 1\n1 0\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n0 1\n1 0\n\n)\n= A,\n\nand\n\nATA−1 =\n\n(\n0 1\n1 0\n\n)(\n1 1\n0 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n0 1\n1 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n1 0\n1 1\n\n)\n.\n\nThe matrix T is upper triangular, but ATA−1 is not, so the group of 2× 2 upper triangular\nmatrices is not a normal subgroup of GL(2,R).\n\nLet Q ∈ V and A ∈ GL(2,R) be the following matrices\n\nQ =\n\n(\n1 0\n0 −1\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA~! ¢ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R € SO(2) and Q € SO(3) be the matrices\n\n0 -1 O 10 0\nR={1 0 0], Q={0 0 -1\n0 0 1 01 0\nThen\n1 O 0\nQ't=Q'={0 0 1\n0 —l1 O\nand we have\n10 0 0 -1 0 1 O 0 0 -1 O 1 O 0\nQRQ‘'*={0 0 -1 1 0 O 0 0 1y}y=]0 0 -I1 0 0 1\n01 0 0 0 1 0 —-l1 O 1 0 0O 0 —-1 O\n00 -l\n=|{|0 1 O\n10 0\n\nObserve that QRQ7! ¢ SO(2), so SO(2) is not a normal subgroup of SO(3).\nLet T and A € GL(2,R) be the following matrices\n\n(0)\nAt= (9 )) =4\naera 0 CO a) =0 9):\n\nThe matrix T is upper triangular, but ATA! is not, so the group of 2 x 2 upper triangular\nmatrices is not a normal subgroup of GL(2, R).\n\nLet Q € V and A € GL(2,R) be the following matrices\n\na=(5 4). 4-0)\n\nWe have\n\nWe have\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\nand\n\nAQA−1 =\n\n(\n1 1\n0 1\n\n)(\n1 0\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n0 −1\n\n)\n.\n\nClearly AQA−1 /∈ V , which shows that the Klein four group is not a normal subgroup of\nGL(2,R).\n\nThe reader should check that the subgroups nZ, GL+(n,R), SL(n,R), and SO(n,R) as\na subgroup of O(n,R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ∼ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all g1, g2, g\n\n′\n1, g\n′\n2 ∈ G,\n\n(1) If g1N = g′1N and g2N = g′2N , then g1g2N = g′1g\n′\n2N , and\n\n(2) If g1N = g2N , then g−1\n1 N = g−1\n\n2 N .\n\nAs a consequence, we can define a group structure on the set G/ ∼ of equivalence classes\nmodulo ∼, by setting\n\n(g1N)(g2N) = (g1g2)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(g1N)(g2N) = (g1g2)N, g1, g2 ∈ G\n\nis denoted G/N , and called the quotient of G by N . The equivalence class gN of an element\ng ∈ G is also denoted g (or [g]). The map π : G→ G/N given by\n\nπ(g) = g = gN\n\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups ϕ : G→ G′, the groups G/Ker ϕ and\nIm ϕ = ϕ(G) are isomorphic.\n\nProof. Since ϕ is surjective onto its image, we may assume that ϕ is surjective, so that\nG′ = Im ϕ. We define a map ϕ : G/Ker ϕ→ G′ as follows:\n\nϕ(g) = ϕ(g), g ∈ G.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = gKer ϕ, and that it is a homomorphism. If g′ is another element in\nthe coset gKer ϕ, which means that g′ = gh for some h ∈ Kerϕ, then\n\nϕ(g′) = ϕ(gh) = ϕ(g)ϕ(h) = ϕ(g)e′ = ϕ(g),\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\n1 fl i\\fa 0\\fa 71). fa -1\\ fa -1\\_ 1 -2\n4os*=(5 i)(0 S)(o a)=(0 a) (0 a )=(0 =):\nClearly AQA~' ¢ V, which shows that the Klein four group is not a normal subgroup of\n\nGL(2, R).\n\nThe reader should check that the subgroups nZ, GL*(n,R), SL(n, R), and SO(n, R) as\na subgroup of O(n, R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ~ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all gi, 92, 95,95 € G,\n\n(1) IfgN =g,N and gN = 95N, then gigoN = gig5N, and\n(2) If g.N = goN, then g7'N = go N.\n\nAs a consequence, we can define a group structure on the set G/ ~ of equivalence classes\nmodulo ~, by setting\n\n(mN)(g2N) = (giga)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(mN)(gN) =(ng)N, 1,92€G\n\nis denoted G/N, and called the quotient of G by N. The equivalence class gN of an element\ng € G is also denoted g (or [g]). The map 7: G > G/N given by\n\n™(9)=9=9N\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups yp: G > G\", the groups G/Ker vy and\nIm y = ¢(G) are isomorphic.\n\nProof. Since y is surjective onto its image, we may assume that y is surjective, so that\nG’ = Im y. We define a map G: G/Ker y > G’ as follows:\n\nAM=v(9), GEG.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = g Ker y, and that it is a homomorphism. If g’ is another element in\nthe coset g Ker y, which means that g’ = gh for some h € Ker y, then\n\n9(9') = v(gh) = v(g)y(h) = vlg)e’ = ¥(9),\n\n\n\n\n32 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nsince ϕ(h) = e′ as h ∈ Ker ϕ. This shows that\n\nϕ(g′) = ϕ(g′) = ϕ(g) = ϕ(g),\n\nso the map ϕ is well defined. It is a homomorphism because\n\nϕ(gg′) = ϕ(gg′)\n\n= ϕ(gg′)\n\n= ϕ(g)ϕ(g′)\n\n= ϕ(g)ϕ(g′).\n\nThe map ϕ is injective because ϕ(g) = e′ iff ϕ(g) = e′ iff g ∈ Ker ϕ, iff g = e. The map ϕ\nis surjective because ϕ is surjective. Therefore ϕ is a bijective homomorphism, and thus an\nisomorphism, as claimed.\n\nProposition 2.11 is called the first isomorphism theorem.\n\nA useful way to construct groups is the direct product construction.\n\nDefinition 2.12. Given two groups G an H, we let G×H be the Cartestian product of the\nsets G and H with the multiplication operation · given by\n\n(g1, h1) · (g2, h2) = (g1g2, h1h2).\n\nIt is immediately verified that G×H is a group called the direct product of G and H.\n\nSimilarly, given any n groups G1, . . . , Gn, we can define the direct product G1× · · ·×Gn\n\nis a similar way.\n\nIf G is an abelian group and H1, . . . , Hn are subgroups of G, the situation is simpler.\nConsider the map\n\na : H1 × · · · ×Hn → G\n\ngiven by\na(h1, . . . , hn) = h1 + · · ·+ hn,\n\nusing + for the operation of the group G. It is easy to verify that a is a group homomorphism,\nso its image is a subgroup of G denoted by H1 + · · ·+Hn, and called the sum of the groups\nHi. The following proposition will be needed.\n\nProposition 2.12. Given an abelian group G, if H1 and H2 are any subgroups of G such\nthat H1 ∩H2 = {0}, then the map a is an isomorphism\n\na : H1 ×H2 → H1 +H2.\n\nProof. The map is surjective by definition, so we just have to check that it is injective. For\nthis, we show that Ker a = {(0, 0)}. We have a(a1, a2) = 0 iff a1 + a2 = 0 iff a1 = −a2. Since\na1 ∈ H1 and a2 ∈ H2, we see that a1, a2 ∈ H1 ∩H2 = {0}, so a1 = a2 = 0, which proves that\nKer a = {(0, 0)}.\n\n\n\n2.2. CYCLIC GROUPS 33\n\nUnder the conditions of Proposition 2.12, namely H1 ∩H2 = {0}, the group H1 + H2 is\ncalled the direct sum of H1 and H2; it is denoted by H1 ⊕H2, and we have an isomorphism\nH1 ×H2\n\n∼= H1 ⊕H2.\n\n2.2 Cyclic Groups\n\nGiven a group G with unit element 1, for any element g ∈ G and for any natural number\nn ∈ N, define gn as follows:\n\ng0 = 1\n\ngn+1 = g · gn.\n\nFor any integer n ∈ Z, we define gn by\n\ngn =\n\n{\ngn if n ≥ 0\n\n(g−1)(−n) if n < 0.\n\nThe following properties are easily verified:\n\ngi · gj = gi+j\n\n(gi)−1 = g−i\n\ngi · gj = gj · gi,\n\nfor all i, j ∈ Z.\n\nDefine the subset 〈g〉 of G by\n\n〈g〉 = {gn | n ∈ Z}.\n\nThe following proposition is left as an exercise.\n\nProposition 2.13. Given a group G, for any element g ∈ G, the set 〈g〉 is the smallest\nabelian subgroup of G containing g.\n\nDefinition 2.13. A group G is cyclic iff there is some element g ∈ G such that G = 〈g〉.\nAn element g ∈ G with this property is called a generator of G.\n\nThe Klein four group V of Example 2.2 is abelian, but not cyclic. This is because V has\nfour elements, but all the elements different from the identity have order 2.\n\nCyclic groups are quotients of Z. For this, we use a basic property of Z. Recall that for\nany n ∈ Z, we let nZ denote the set of multiples of n,\n\nnZ = {nk | k ∈ Z}.\n\n\n\n34 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.14. Every subgroup H of Z is of the form H = nZ for some n ∈ N.\n\nProof. If H is the trivial group {0}, then let n = 0. If H is nontrivial, for any nonzero element\nm ∈ H, we also have −m ∈ H and either m or −m is positive, so let n be the smallest\npositive integer in H. By Proposition 2.13, nZ is the smallest subgroup of H containing n.\nFor any m ∈ H with m 6= 0, we can write\n\nm = nq + r, with 0 ≤ r < n.\n\nNow, since nZ ⊆ H, we have nq ∈ H, and since m ∈ H, we get r = m− nq ∈ H. However,\n0 ≤ r < n, contradicting the minimality of n, so r = 0, and H = nZ.\n\nGiven any cyclic group G, for any generator g of G, we can define a mapping ϕ : Z→ G\nby ϕ(m) = gm. Since g generates G, this mapping is surjective. The mapping ϕ is clearly a\ngroup homomorphism, so let H = Kerϕ be its kernel. By a previous observation, H = nZ\nfor some n ∈ Z, so by the first homomorphism theorem, we obtain an isomorphism\n\nϕ : Z/nZ −→ G\n\nfrom the quotient group Z/nZ onto G. Obviously, if G has finite order, then |G| = n. In\nsummary, we have the following result.\n\nProposition 2.15. Every cyclic group G is either isomorphic to Z, or to Z/nZ, for some\nnatural number n > 0. In the first case, we say that G is an infinite cyclic group, and in the\nsecond case, we say that G is a cyclic group of order n.\n\nThe quotient group Z/nZ consists of the cosets m+nZ = {m+nk | k ∈ Z}, with m ∈ Z,\nthat is, of the equivalence classes of Z under the equivalence relation ≡ defined such that\n\nx ≡ y iff x− y ∈ nZ iff x ≡ y (mod n).\n\nWe also denote the equivalence class x + nZ of x by x, or if we want to be more precise by\n[x]n. The group operation is given by\n\nx+ y = x+ y.\n\nFor every x ∈ Z, there is a unique representative, x mod n (the nonnegative remainder of\nthe division of x by n) in the class x of x, such that 0 ≤ x mod n ≤ n − 1. For this\nreason, we often identity Z/nZ with the set {0, . . . , n−1}. To be more rigorous, we can give\n{0, . . . , n− 1} a group structure by defining +n such that\n\nx+n y = (x+ y) mod n.\n\nThen, it is easy to see that {0, . . . , n − 1} with the operation +n is a group with identity\nelement 0 isomorphic to Z/nZ.\n\n\n\n2.2. CYCLIC GROUPS 35\n\nWe can also define a multiplication operation · on Z/nZ as follows:\n\na · b = ab = ab mod n.\n\nThen, it is easy to check that · is abelian, associative, that 1 is an identity element for ·, and\nthat · is distributive on the left and on the right with respect to addition. This makes Z/nZ\ninto a commutative ring . We usually suppress the dot and write a b instead of a · b.\n\nProposition 2.16. Given any integer n ≥ 1, for any a ∈ Z, the residue class a ∈ Z/nZ is\ninvertible with respect to multiplication iff gcd(a, n) = 1.\n\nProof. If a has inverse b in Z/nZ, then a b = 1, which means that\n\nab ≡ 1 (mod n),\n\nthat is ab = 1 + nk for some k ∈ Z, which is the Bezout identity\n\nab− nk = 1\n\nand implies that gcd(a, n) = 1. Conversely, if gcd(a, n) = 1, then by Bezout’s identity there\nexist u, v ∈ Z such that\n\nau+ nv = 1,\n\nso au = 1− nv, that is,\n\nau ≡ 1 (mod n),\n\nwhich means that a u = 1, so a is invertible in Z/nZ.\n\nDefinition 2.14. The group (under multiplication) of invertible elements of the ring Z/nZ\nis denoted by (Z/nZ)∗. Note that this group is abelian and only defined if n ≥ 2.\n\nThe Euler ϕ-function plays an important role in the theory of the groups (Z/nZ)∗.\n\nDefinition 2.15. Given any positive integer n ≥ 1, the Euler ϕ-function (or Euler totient\nfunction) is defined such that ϕ(n) is the number of integers a, with 1 ≤ a ≤ n, which are\nrelatively prime to n; that is, with gcd(a, n) = 1.1\n\nThen, by Proposition 2.16, we see that the group (Z/nZ)∗ has order ϕ(n).\n\nFor n = 2, (Z/2Z)∗ = {1}, the trivial group. For n = 3, (Z/3Z)∗ = {1, 2}, and for\nn = 4, we have (Z/4Z)∗ = {1, 3}. Both groups are isomorphic to the group {−1, 1}. Since\ngcd(a, n) = 1 for every a ∈ {1, . . . , n − 1} iff n is prime, by Proposition 2.16 we see that\n(Z/nZ)∗ = Z/nZ− {0} iff n is prime.\n\n1We allow a = n to accomodate the special case n = 1.\n\n\n\n36 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2.3 Rings and Fields\n\nThe groups Z,Q,R, C, Z/nZ, and Mn(R) are more than abelian groups, they are also\ncommutative rings. Furthermore, Q, R, and C are fields. We now introduce rings and fields.\n\nDefinition 2.16. A ring is a set A equipped with two operations +: A × A → A (called\naddition) and ∗ : A× A→ A (called multiplication) having the following properties:\n\n(R1) A is an abelian group w.r.t. +;\n\n(R2) ∗ is associative and has an identity element 1 ∈ A;\n\n(R3) ∗ is distributive w.r.t. +.\n\nThe identity element for addition is denoted 0, and the additive inverse of a ∈ A is\ndenoted by −a. More explicitly, the axioms of a ring are the following equations which hold\nfor all a, b, c ∈ A:\n\na+ (b+ c) = (a+ b) + c (associativity of +) (2.1)\n\na+ b = b+ a (commutativity of +) (2.2)\n\na+ 0 = 0 + a = a (zero) (2.3)\n\na+ (−a) = (−a) + a = 0 (additive inverse) (2.4)\n\na ∗ (b ∗ c) = (a ∗ b) ∗ c (associativity of ∗) (2.5)\n\na ∗ 1 = 1 ∗ a = a (identity for ∗) (2.6)\n\n(a+ b) ∗ c = (a ∗ c) + (b ∗ c) (distributivity) (2.7)\n\na ∗ (b+ c) = (a ∗ b) + (a ∗ c) (distributivity) (2.8)\n\nThe ring A is commutative if\n\na ∗ b = b ∗ a for all a, b ∈ A.\n\nFrom (2.7) and (2.8), we easily obtain\n\na ∗ 0 = 0 ∗ a = 0 (2.9)\n\na ∗ (−b) = (−a) ∗ b = −(a ∗ b). (2.10)\n\nNote that (2.9) implies that if 1 = 0, then a = 0 for all a ∈ A, and thus, A = {0}. The\nring A = {0} is called the trivial ring . A ring for which 1 6= 0 is called nontrivial . The\nmultiplication a ∗ b of two elements a, b ∈ A is often denoted by ab.\n\nExample 2.6.\n\n1. The additive groups Z,Q,R,C, are commutative rings.\n\n\n\n2.3. RINGS AND FIELDS 37\n\n2. For any positive integer n ∈ N, the group Z/nZ is a group under addition. We can\nalso define a multiplication operation by\n\na · b = ab = ab mod n,\n\nfor all a, b ∈ Z. The reader will easily check that the ring axioms are satisfied, with 0\nas zero and 1 as multiplicative unit. The resulting ring is denoted by Z/nZ.2\n\n3. The group R[X] of polynomials in one variable with real coefficients is a ring under\nmultiplication of polynomials. It is a commutative ring.\n\n4. Let d be any positive integer. If d is not divisible by any integer of the form m2, with\nm ∈ N and m ≥ 2, then we say that d is square-free. For example, d = 1, 2, 3, 5, 6, 7, 10\nare square-free, but 4, 8, 9, 12 are not square-free. If d is any square-free integer and if\nd ≥ 2, then the set of real numbers\n\nZ[\n√\nd] = {a+ b\n\n√\nd ∈ R | a, b ∈ Z}\n\nis a commutative a ring. If z = a + b\n√\nd ∈ Z[\n\n√\nd], we write z = a − b\n\n√\nd. Note that\n\nzz = a2 − db2.\n\n5. Similarly, if d ≥ 1 is a positive square-free integer, then the set of complex numbers\n\nZ[\n√\n−d] = {a+ ib\n\n√\nd ∈ C | a, b ∈ Z}\n\nis a commutative ring. If z = a + ib\n√\nd ∈ Z[\n\n√\n−d], we write z = a− ib\n\n√\nd. Note that\n\nzz = a2 + db2. The case where d = 1 is a famous example that was investigated by\nGauss, and Z[\n\n√\n−1], also denoted Z[i], is called the ring of Gaussian integers .\n\n6. The group of n× n matrices Mn(R) is a ring under matrix multiplication. However, it\nis not a commutative ring.\n\n7. The group C(a, b) of continuous functions f : (a, b) → R is a ring under the operation\nf · g defined such that\n\n(f · g)(x) = f(x)g(x)\n\nfor all x ∈ (a, b).\n\nDefinition 2.17. Given a ring A, for any element a ∈ A, if there is some element b ∈ A\nsuch that b 6= 0 and ab = 0, then we say that a is a zero divisor . A ring A is an integral\ndomain (or an entire ring) if 0 6= 1, A is commutative, and ab = 0 implies that a = 0 or\nb = 0, for all a, b ∈ A. In other words, an integral domain is a nontrivial commutative ring\nwith no zero divisors besides 0.\n\n2The notation Zn is sometimes used instead of Z/nZ but it clashes with the notation for the n-adic\nintegers so we prefer not to use it.\n\n\n\n38 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.7.\n\n1. The rings Z,Q,R,C, are integral domains.\n\n2. The ring R[X] of polynomials in one variable with real coefficients is an integral domain.\n\n3. For any positive integer, n ∈ N, we have the ring Z/nZ. Observe that if n is composite,\nthen this ring has zero-divisors. For example, if n = 4, then we have\n\n2 · 2 ≡ 0 (mod 4).\n\nThe reader should prove that Z/nZ is an integral domain iff n is prime (use Proposition\n2.16).\n\n4. If d is a square-free positive integer and if d ≥ 2, the ring Z[\n√\nd] is an integral domain.\n\nSimilarly, if d ≥ 1 is a square-free positive integer, the ring Z[\n√\n−d] is an integral\n\ndomain. Finding the invertible elements of these rings is a very interesting problem.\n\n5. The ring of n× n matrices Mn(R) has zero divisors.\n\nA homomorphism between rings is a mapping preserving addition and multiplication\n(and 0 and 1).\n\nDefinition 2.18. Given two rings A and B, a homomorphism between A and B is a function\nh : A→ B satisfying the following conditions for all x, y ∈ A:\n\nh(x+ y) = h(x) + h(y)\n\nh(xy) = h(x)h(y)\n\nh(0) = 0\n\nh(1) = 1.\n\nActually, because B is a group under addition, h(0) = 0 follows from\n\nh(x+ y) = h(x) + h(y).\n\nExample 2.8.\n\n1. If A is a ring, for any integer n ∈ Z, for any a ∈ A, we define n · a by\n\nn · a = a+ · · ·+ a︸ ︷︷ ︸\nn\n\nif n ≥ 0 (with 0 · a = 0) and\nn · a = −(−n) · a\n\nif n < 0. Then, the map h : Z→ A given by\n\nh(n) = n · 1A\nis a ring homomorphism (where 1A is the multiplicative identity of A).\n\n\n\n2.3. RINGS AND FIELDS 39\n\n2. Given any real λ ∈ R, the evaluation map ηλ : R[X]→ R defined by\n\nηλ(f(X)) = f(λ)\n\nfor every polynomial f(X) ∈ R[X] is a ring homomorphism.\n\nDefinition 2.19. A ring homomorphism h : A → B is an isomorphism iff there is a ring\nhomomorphism g : B → A such that g ◦ f = idA and f ◦ g = idB. An isomorphism from a\nring to itself is called an automorphism.\n\nAs in the case of a group isomorphism, the homomorphism g is unique and denoted by\nh−1, and it is easy to show that a bijective ring homomorphism h : A→ B is an isomorphism.\n\nDefinition 2.20. Given a ring A, a subset A′ of A is a subring of A if A′ is a subgroup of\nA (under addition), is closed under multiplication, and contains 1.\n\nFor example, we have the following sequence in which every ring on the left of an inlcusion\nsign is a subring of the ring on the right of the inclusion sign:\n\nZ ⊆ Q ⊆ R ⊆ C.\n\nThe ring Z is a subring of both Z[\n√\nd] and Z[\n\n√\n−d], the ring Z[\n\n√\nd] is a subring of R and the\n\nring Z[\n√\n−d] is a subring of C.\n\nIf h : A→ B is a homomorphism of rings, then it is easy to show for any subring A′, the\nimage h(A′) is a subring of B, and for any subring B′ of B, the inverse image h−1(B′) is a\nsubring of A.\n\nAs for groups, the kernel of a ring homomorphism h : A→ B is defined by\n\nKer h = {a ∈ A | h(a) = 0}.\n\nJust as in the case of groups, we have the following criterion for the injectivity of a ring\nhomomorphism. The proof is identical to the proof for groups.\n\nProposition 2.17. If h : A → B is a homomorphism of rings, then h : A → B is injective\niff Ker h = {0}. (We also write Ker h = (0).)\n\nThe kernel of a ring homomorphism is an abelian subgroup of the additive group A, but\nin general it is not a subring of A, because it may not contain the multiplicative identity\nelement 1. However, it satisfies the following closure property under multiplication:\n\nab ∈ Ker h and ba ∈ Ker h for all a ∈ Ker h and all b ∈ A.\n\nThis is because if h(a) = 0, then for all b ∈ A we have\n\nh(ab) = h(a)h(b) = 0h(b) = 0 and h(ba) = h(b)h(a) = h(b)0 = 0.\n\n\n\n40 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.21. Given a ring A, an additive subgroup I of A satisfying the property below\n\nab ∈ I and ba ∈ I for all a ∈ I and all b ∈ A (∗ideal)\n\nis called a two-sided ideal . If A is a commutative ring, we simply say an ideal .\n\nIt turns out that for any ring A and any two-sided ideal I, the set A/I of additive cosets\na + I (with a ∈ A) is a ring called a quotient ring . Then we have the following analog of\nProposition 2.11, also called the first isomorphism theorem.\n\nProposition 2.18. Given a homomorphism of rings h : A → B, the rings A/Ker h and\nIm h = h(A) are isomorphic.\n\nA field is a commutative ring K for which K − {0} is a group under multiplication.\n\nDefinition 2.22. A set K is a field if it is a ring and the following properties hold:\n\n(F1) 0 6= 1;\n\n(F2) K∗ = K − {0} is a group w.r.t. ∗ (i.e., every a 6= 0 has an inverse w.r.t. ∗);\n\n(F3) ∗ is commutative.\n\nIf ∗ is not commutative but (F1) and (F2) hold, we say that we have a skew field (or\nnoncommutative field).\n\nNote that we are assuming that the operation ∗ of a field is commutative. This convention\nis not universally adopted, but since ∗ will be commutative for most fields we will encounter,\nwe may as well include this condition in the definition.\n\nExample 2.9.\n\n1. The rings Q, R, and C are fields.\n\n2. The set of (formal) fractions f(X)/g(X) of polynomials f(X), g(X) ∈ R[X], where\ng(X) is not the null polynomial, is a field.\n\n3. The ring C(a, b) of continuous functions f : (a, b) → R such that f(x) 6= 0 for all\nx ∈ (a, b) is a field.\n\n4. Using Proposition 2.16, it is easy to see that the ring Z/pZ is a field iff p is prime.\n\n5. If d is a square-free positive integer and if d ≥ 2, the set\n\nQ(\n√\nd) = {a+ b\n\n√\nd ∈ R | a, b ∈ Q}\n\nis a field. If z = a + b\n√\nd ∈ Q(\n\n√\nd) and z = a − b\n\n√\nd, then it is easy to check that if\n\nz 6= 0, then z−1 = z/(zz).\n\n\n\n2.3. RINGS AND FIELDS 41\n\n6. Similarly, If d ≥ 1 is a square-free positive integer, the set of complex numbers\n\nQ(\n√\n−d) = {a+ ib\n\n√\nd ∈ C | a, b ∈ Q}\n\nis a field. If z = a + ib\n√\nd ∈ Q(\n\n√\n−d) and z = a− ib\n\n√\nd, then it is easy to check that\n\nif z 6= 0, then z−1 = z/(zz).\n\nDefinition 2.23. A homomorphism h : K1 → K2 between two fields K1 and K2 is just a\nhomomorphism between the rings K1 and K2.\n\nHowever, because K∗1 and K∗2 are groups under multiplication, a homomorphism of fields\nmust be injective.\n\nProof. First, observe that for any x 6= 0,\n\n1 = h(1) = h(xx−1) = h(x)h(x−1)\n\nand\n1 = h(1) = h(x−1x) = h(x−1)h(x),\n\nso h(x) 6= 0 and\nh(x−1) = h(x)−1.\n\nBut then, if h(x) = 0, we must have x = 0. Consequently, h is injective.\n\nDefinition 2.24. A field homomorphism h : K1 → K2 is an isomorphism iff there is a\nhomomorphism g : K2 → K1 such that g ◦ f = idK1 and f ◦ g = idK2 . An isomorphism from\na field to itself is called an automorphism.\n\nThen, just as in the case of rings, g is unique and denoted by h−1, and a bijective field\nhomomorphism h : K1 → K2 is an isomorphism.\n\nDefinition 2.25. Since every homomorphism h : K1 → K2 between two fields is injective,\nthe image f(K1) of K1 is a subfield of K2. We say that K2 is an extension of K1.\n\nFor example, R is an extension of Q and C is an extension of R. The fields Q(\n√\nd) and\n\nQ(\n√\n−d) are extensions of Q, the field R is an extension of Q(\n\n√\nd) and the field C is an\n\nextension of Q(\n√\n−d).\n\nDefinition 2.26. A field K is said to be algebraically closed if every polynomial p(X) with\ncoefficients in K has some root in K; that is, there is some a ∈ K such that p(a) = 0.\n\nIt can be shown that every field K has some minimal extension Ω which is algebraically\nclosed, called an algebraic closure of K. For example, C is the algebraic closure of R. The\nalgebraic closure of Q is called the field of algebraic numbers . This field consists of all\ncomplex numbers that are zeros of a polynomial with coefficients in Q.\n\n\n\n42 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.27. Given a field K and an automorphism h : K → K of K, it is easy to check\nthat the set\n\nFix(h) = {a ∈ K | h(a) = a}\nof elements of K fixed by h is a subfield of K called the field fixed by h.\n\nFor example, if d ≥ 2 is square-free, then the map c : Q(\n√\nd)→ Q(\n\n√\nd) given by\n\nc(a+ b\n√\nd) = a− b\n\n√\nd\n\nis an automorphism of Q(\n√\nd), and Fix(c) = Q.\n\nIf K is a field, we have the ring homomorphism h : Z → K given by h(n) = n · 1. If h\nis injective, then K contains a copy of Z, and since it is a field, it contains a copy of Q. In\nthis case, we say that K has characteristic 0. If h is not injective, then h(Z) is a subring of\nK, and thus an integral domain, the kernel of h is a subgroup of Z, which by Proposition\n2.14 must be of the form pZ for some p ≥ 1. By the first isomorphism theorem, h(Z) is\nisomorphic to Z/pZ for some p ≥ 1. But then, p must be prime since Z/pZ is an integral\ndomain iff it is a field iff p is prime. The prime p is called the characteristic of K, and we\nalso says that K is of finite characteristic.\n\nDefinition 2.28. If K is a field, then either\n\n(1) n · 1 6= 0 for all integer n ≥ 1, in which case we say that K has characteristic 0, or\n\n(2) There is some smallest prime number p such that p · 1 = 0 called the characteristic of\nK, and we say K is of finite characteristic.\n\nA field K of characteristic 0 contains a copy of Q, thus is infinite. As we will see in\nSection 8.10, a finite field has nonzero characteristic p. However, there are infinite fields of\nnonzero characteristic.\n\n\n\nPart I\n\nLinear Algebra\n\n43\n\n\n\n\n\n\n\nChapter 3\n\nVector Spaces, Bases, Linear Maps\n\n3.1 Motivations: Linear Combinations, Linear Inde-\n\npendence and Rank\n\nIn linear optimization problems, we often encounter systems of linear equations. For example,\nconsider the problem of solving the following system of three linear equations in the three\nvariables x1, x2, x3 ∈ R:\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3.\n\nOne way to approach this problem is introduce the “vectors” u, v, w, and b, given by\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−2\n\n w =\n\n−1\n1\n−2\n\n b =\n\n1\n2\n3\n\n\nand to write our linear system as\n\nx1u+ x2v + x3w = b.\n\nIn the above equation, we used implicitly the fact that a vector z can be multiplied by a\nscalar λ ∈ R, where\n\nλz = λ\n\nz1\n\nz2\n\nz3\n\n =\n\nλz1\n\nλz2\n\nλz3\n\n ,\n\nand two vectors y and and z can be added, where\n\ny + z =\n\ny1\n\ny2\n\ny3\n\n+\n\nz1\n\nz2\n\nz3\n\n =\n\ny1 + z1\n\ny2 + z2\n\ny3 + z3\n\n .\n\n45\n\n\n\n46 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAlso, given a vector\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\nwe define the additive inverse −x of x (pronounced minus x) as\n\n−x =\n\n−x1\n\n−x2\n\n−x3\n\n .\n\nObserve that −x = (−1)x, the scalar multiplication of x by −1.\n\nThe set of all vectors with three components is denoted by R3×1. The reason for using\nthe notation R3×1 rather than the more conventional notation R3 is that the elements of\nR3×1 are column vectors ; they consist of three rows and a single column, which explains the\nsuperscript 3 × 1. On the other hand, R3 = R × R × R consists of all triples of the form\n(x1, x2, x3), with x1, x2, x3 ∈ R, and these are row vectors . However, there is an obvious\nbijection between R3×1 and R3 and they are usually identified. For the sake of clarity, in\nthis introduction, we will denote the set of column vectors with n components by Rn×1.\n\nAn expression such as\nx1u+ x2v + x3w\n\nwhere u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using\nthis notion, the problem of solving our linear system\n\nx1u+ x2v + x3w = b.\n\nis equivalent to determining whether b can be expressed as a linear combination of u, v, w.\n\nNow if the vectors u, v, w are linearly independent , which means that there is no triple\n(x1, x2, x3) 6= (0, 0, 0) such that\n\nx1u+ x2v + x3w = 03,\n\nit can be shown that every vector in R3×1 can be written as a linear combination of u, v, w.\nHere, 03 is the zero vector\n\n03 =\n\n0\n0\n0\n\n .\n\nIt is customary to abuse notation and to write 0 instead of 03. This rarely causes a problem\nbecause in most cases, whether 0 denotes the scalar zero or the zero vector can be inferred\nfrom the context.\n\nIn fact, every vector z ∈ R3×1 can be written in a unique way as a linear combination\n\nz = x1u+ x2v + x3w.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nz = x1u+ x2v + x3w = y1u+ y2v + y3w,\n\nthen by using our (linear!) operations on vectors, we get\n\n(y1 − x1)u+ (y2 − x2)v + (y3 − x3)w = 0,\n\nwhich implies that\ny1 − x1 = y2 − x2 = y3 − x3 = 0,\n\nby linear independence. Thus,\n\ny1 = x1, y2 = x2, y3 = x3,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\n\nx1u+ x2v + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nx1 = 1.4\n\nx2 = −0.4\n\nx3 = −0.4\n\nis the solution.\n\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u, v, w), called the determinant of\n(u, v, w), and to check that it is nonzero. In our case, it turns out that\n\ndet(u, v, w) =\n\n∣∣∣∣∣∣\n1 2 −1\n2 1 1\n1 −2 −2\n\n∣∣∣∣∣∣ = 15,\n\nwhich confirms that u, v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\nA =\n(\nu v w\n\n)\n=\n\n1 2 −1\n2 1 1\n1 −2 −2\n\n .\n\nIf we form the vector of unknowns\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nZz XU LQV + U3W = YU + You + Y3U,\n\nthen by using our (linear!) operations on vectors, we get\n(yi — t1)u + (yo — t2)u + (y3 — Z3)w = 0,\n\nwhich implies that\nYi — Ly = Yo — Lo = y3 — ®3 = O,\n\nby linear independence. Thus,\n\nYF, Yor, Y3>= %,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\nrut rv + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nLy = 1.4\nt= —0.4\nL3 = —0.4\n\nis the solution.\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u,v,w), called the determinant of\n(u,v, w), and to check that it is nonzero. In our case, it turns out that\n\n1 2 -!1\ndet(u,v,w) =|2 1 1)/=15,\n1 -2 -2\n\nwhich confirms that u,v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\n1 2 -1\nA=(u v w) = 2 1 1\n1 —2 —2\n\nIf we form the vector of unknowns\n\n\n\n\n48 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nthen our linear combination x1u+ x2v + x3w can be written in matrix form as\n\nx1u+ x2v + x3w =\n\n1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n ,\n\nso our linear system is expressed by1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n =\n\n1\n2\n3\n\n ,\n\nor more concisely as\nAx = b.\n\nNow what if the vectors u, v, w are linearly dependent? For example, if we consider the\nvectors\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−1\n\n w =\n\n−1\n1\n2\n\n ,\n\nwe see that\nu− v = w,\n\na nontrivial linear dependence. It can be verified that u and v are still linearly independent.\nNow for our problem\n\nx1u+ x2v + x3w = b\n\nit must be the case that b can be expressed as linear combination of u and v. However,\nit turns out that u, v, b are linearly independent (one way to see this is to compute the\ndeterminant det(u, v, b) = −6), so b cannot be expressed as a linear combination of u and v\nand thus, our system has no solution.\n\nIf we change the vector b to\n\nb =\n\n3\n3\n0\n\n ,\n\nthen\nb = u+ v,\n\nand so the system\nx1u+ x2v + x3w = b\n\nhas the solution\nx1 = 1, x2 = 1, x3 = 0.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK49\n\nActually, since w = u− v, the above system is equivalent to\n\n(x1 + x3)u+ (x2 − x3)v = b,\n\nand because u and v are linearly independent, the unique solution in x1 + x3 and x2 − x3 is\n\nx1 + x3 = 1\n\nx2 − x3 = 1,\n\nwhich yields an infinite number of solutions parameterized by x3, namely\n\nx1 = 1− x3\n\nx2 = 1 + x3.\n\nIn summary, a 3× 3 linear system may have a unique solution, no solution, or an infinite\nnumber of solutions, depending on the linear independence (and dependence) or the vectors\nu, v, w, b. This situation can be generalized to any n × n system, and even to any n × m\nsystem (n equations in m variables), as we will see later.\n\nThe point of view where our linear system is expressed in matrix form as Ax = b stresses\nthe fact that the map x 7→ Ax is a linear transformation. This means that\n\nA(λx) = λ(Ax)\n\nfor all x ∈ R3×1 and all λ ∈ R and that\n\nA(u+ v) = Au+ Av,\n\nfor all u, v ∈ R3×1. We can view the matrix A as a way of expressing a linear map from R3×1\n\nto R3×1 and solving the system Ax = b amounts to determining whether b belongs to the\nimage of this linear map.\n\nGiven a 3× 3 matrix\n\nA =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n ,\n\nwhose columns are three vectors denoted A1, A2, A3, and given any vector x = (x1, x2, x3),\nwe defined the product Ax as the linear combination\n\nAx = x1A\n1 + x2A\n\n2 + x3A\n3 =\n\na11x1 + a12x2 + a13x3\n\na21x1 + a22x2 + a23x3\n\na31x1 + a32x2 + a33x3\n\n .\n\nThe common pattern is that the ith coordinate of Ax is given by a certain kind of product\ncalled an inner product , of a row vector , the ith row of A, times the column vector x:\n\n(\nai1 ai2 ai3\n\n)\n·\n\nx1\n\nx2\n\nx3\n\n = ai1x1 + ai2x2 + ai3x3.\n\n\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn) ∈ Rn, their\ninner product denoted x · y, or 〈x, y〉, is the number\n\nx · y =\n(\nx1 x2 · · · xn\n\n)\n·\n\n\ny1\n\ny2\n...\nyn\n\n =\nn∑\ni=1\n\nxiyi.\n\nInner products play a very important role. First, we quantity\n\n‖x‖2 =\n√\nx · x = (x2\n\n1 + · · ·+ x2\nn)1/2\n\nis a generalization of the length of a vector, called the Euclidean norm, or `2-norm. Second,\nit can be shown that we have the inequality\n\n|x · y| ≤ ‖x‖ ‖y‖ ,\n\nso if x, y 6= 0, the ratio (x · y)/(‖x‖ ‖y‖) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if x · y = 0 then the vectors x and y make the angle π/2,\nthat is, they are orthogonal . The (square) matrices Q that preserve the inner product, in\nthe sense that 〈Qx,Qy〉 = 〈x, y〉 for all x, y ∈ Rn, also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m × n matrix consisting of n columns A1, . . . , An (in\nRm), and B is a n× p matrix consisting of p columns B1, . . . , Bp (in Rn) we can form the p\nvectors (in Rm)\n\nAB1, . . . , ABp.\n\nThese p vectors constitute the m × p matrix denoted AB, whose jth column is ABj. But\nwe know that the ith coordinate of ABj is the inner product of the ith row of A by the jth\ncolumn of B,\n\n(\nai1 ai2 · · · ain\n\n)\n·\n\n\nb1j\n\nb2j\n...\nbnj\n\n =\nn∑\nk=1\n\naikbkj.\n\nThus we have defined a multiplication operation on matrices, namely if A = (aik) is a m×n\nmatrix and if B = (bjk) if n× p matrix, then their product AB is the m× n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\n(AB)ij =\nn∑\nk=1\n\naikbkj.\n\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are two n×n\nmatrices, in general, AB 6= BA.\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors 7 = (#1,...,%n,) and y = (y1,.--,Yn) € R”, their\ninner product denoted x - y, or (x,y), is the number\n\nY1\ny n\nvey = (a TQ 7° In) * ° = So xiyi.\n. i=l\nYn\n\nInner products play a very important role. First, we quantity\nvp = Vee = (ap +--+ 27)'?\n\nis a generalization of the length of a vector, called the Euclidean norm, or (?-norm. Second,\nit can be shown that we have the inequality\n\nIz-y| < fall ly,\n\nso if x,y £0, the ratio (x - y)/(||2|| ||y||) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if «-y = 0 then the vectors x and y make the angle 7/2,\nthat is, they are orthogonal. The (square) matrices Q that preserve the inner product, in\nthe sense that (Qz, Qy) = (x,y) for all x,y € R\", also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m X n matrix consisting of n columns A!,...,A” (in\nR™), and B is an Xx p matrix consisting of p columns B',...,B? (in R\") we can form the p\nvectors (in R”)\n\nAB',..., ABP.\n\nThese p vectors constitute the m x p matrix denoted AB, whose jth column is AB’. But\nwe know that the ith coordinate of AB is the inner product of the ith row of A by the jth\ncolumn of B,\n\nbi;\nn\nbo; _\n(ait aig \"7° Gin) * . =) i¢dp;-\n, k=1\nDnj\n\nThus we have defined a multiplication operation on matrices, namely if A = (a;,) isamxn\nmatrix and if B = (b;,) if n x p matrix, then their product AB is the m x n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\nn\n\n(AB);; = Ss\" indy; -\n\nk=1\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are twon xn\nmatrices, in general, AB 4 BA.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK51\n\nSuppose that A is an n× n matrix and that we are trying to solve the linear system\n\nAx = b,\n\nwith b ∈ Rn. Suppose we can find an n× n matrix B such that\n\nBAi = ei, i = 1, . . . , n,\n\nwith ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith slot. If we form\nthe n× n matrix\n\nIn =\n\n\n\n1 0 0 · · · 0 0\n0 1 0 · · · 0 0\n0 0 1 · · · 0 0\n...\n\n...\n...\n\n. . .\n...\n\n...\n0 0 0 · · · 1 0\n0 0 0 · · · 0 1\n\n\n,\n\ncalled the identity matrix , whose ith column is ei, then the above is equivalent to\n\nBA = In.\n\nIf Ax = b, then multiplying both sides on the left by B, we get\n\nB(Ax) = Bb.\n\nBut is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have\n\nx = Bb.\n\nWe can verify that x = Bb is indeed a solution, because it can be shown that\n\nA(Bb) = (AB)b = Inb = b.\n\nWhat is not obvious is that BA = In implies AB = In, but this is indeed provable. The\nmatrix B is usually denoted A−1 and called the inverse of A. It can be shown that it is the\nunique matrix such that\n\nAA−1 = A−1A = In.\n\nIf a square matrix A has an inverse, then we say that it is invertible or nonsingular , otherwise\nwe say that it is singular . We will show later that a square matrix is invertible iff its columns\nare linearly independent iff its determinant is nonzero.\n\nIn summary, if A is a square invertible matrix, then the linear system Ax = b has the\nunique solution x = A−1b. In practice, this is not a good way to solve a linear system because\ncomputing A−1 is too expensive. A practical method for solving a linear system is Gaussian\nelimination, discussed in Chapter 8. Other practical methods for solving a linear system\n\n\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m × n matrix A = (akl), the n × m matrix A> = (a>ij) whose ith row is the\nith column of A, which means that a>ij = aji for i = 1, . . . , n and j = 1, . . . ,m, is called the\ntranspose of A. An n× n matrix Q such that\n\nQQ> = Q>Q = In\n\nis called an orthogonal matrix . Equivalently, the inverse Q−1 of an orthogonal matrix Q is\nequal to its transpose Q>. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m× n matrix A can be written as\n\nA = V ΣU>,\n\nwhere V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and Σ is an m×n\nmatrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ · · · ≥ σp,\nwhere p = min(m,n), called the singular values of A. The factorization A = V ΣU> is called\na singular decomposition of A, or SVD .\n\nThe SVD can be used to “solve” a linear system Ax = b where A is an m × n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m > n) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax − b. Legendre and Gauss used ‖Ax− b‖2\n\n2, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\nx+ of minimum Euclidean norm that minimizes ‖Ax− b‖2\n\n2. Furthermore, x+ is given by the\nexpression x+ = A+b, where A+ is the pseudo-inverse of A, and A+ can be computed from\nan SVD A = V ΣU> of A. Indeed, A+ = UΣ+V >, where Σ+ is the matrix obtained from Σ\nby replacing every positive singular value σi by its inverse σ−1\n\ni , leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ‖Ax− b‖2\n2, we\n\ncan add the penalty term K ‖x‖2\n2 (for some positive K > 0) to ‖Ax− b‖2\n\n2 and minimize the\nquantity ‖Ax− b‖2\n\n2 + K ‖x‖2\n2. This approach is called ridge regression. It turns out that\n\nthere is a unique minimizer x+ given by x+ = (A>A + KIn)−1A>b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K ‖x‖2\n2 by K ‖x‖1, where ‖x‖1 = |x1|+\n\n· · · + |xn| (the `1-norm of x). The remarkable fact is that the minimizers x of ‖Ax− b‖2\n2 +\n\nK ‖x‖1 tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m x n matrix A = (aq), the n x m matrix A‘ = (a,;) whose ith row is the\nith column of A, which means that aj, =a; fori =1,...,n and j =1,...,m, is called the\ntranspose of A. An n X n matrix Q such that\n\nQQ'=Q'Q=In\n\nis called an orthogonal matrix. Equivalently, the inverse Q~! of an orthogonal matrix Q is\nequal to its transpose Q'. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m x n matrix A can be written as\n\nA=VSUT,\n\nwhere V is an m xX m orthogonal matrix, U is an n x n orthogonal matrix, and © is anm xn\nmatrix whose only nonzero entries are nonnegative diagonal entries 0) > 02 > ++: > Op,\nwhere p = min(m, n), called the singular values of A. The factorization A = VXU' is called\na singular decomposition of A, or SVD.\n\nThe SVD can be used to “solve” a linear system Ax = 6 where A is an m x n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m >) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax — b. Legendre and Gauss used ||Aa — ||}, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\na+ of minimum Euclidean norm that minimizes || Ax — ||}. Furthermore, «+ is given by the\nexpression xt = Atb, where At is the pseudo-inverse of A, and At can be computed from\nan SVD A=VU' of A. Indeed, At = UXNt+V', where ¥*+ is the matrix obtained from Y\nby replacing every positive singular value o; by its inverse a; ', leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ||Ax — 6||5, we\ncan add the penalty term K ||z||3 (for some positive K > 0) to || Ax — ||} and minimize the\nquantity || Ax — b||} + K ||a||3. This approach is called ridge regression. It turns out that\nthere is a unique minimizer 2+ given by + = (A'A+ KI,)~1A'b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K |||]. by K ||a||,, where |Ja'||, = |a1| +\n--»4+|x,| (the ¢'-norm of «). The remarkable fact is that the minimizers « of || Ax — b||3 +\nK |x|, tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK53\n\nAnother important application of the SVD is principal component analysis (or PCA), an\nimportant tool in data analysis.\n\nYet another fruitful way of interpreting the resolution of the system Ax = b is to view\nthis problem as an intersection problem. Indeed, each of the equations\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3\n\ndefines a subset of R3 which is actually a plane. The first equation\n\nx1 + 2x2 − x3 = 1\n\ndefines the plane H1 passing through the three points (1, 0, 0), (0, 1/2, 0), (0, 0,−1), on the\ncoordinate axes, the second equation\n\n2x1 + x2 + x3 = 2\n\ndefines the plane H2 passing through the three points (1, 0, 0), (0, 2, 0), (0, 0, 2), on the coor-\ndinate axes, and the third equation\n\nx1 − 2x2 − 2x3 = 3\n\ndefines the plane H3 passing through the three points (3, 0, 0), (0,−3/2, 0), (0, 0,−3/2), on\nthe coordinate axes. See Figure 3.1.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\nx -2x -2x = 31 2 3\n\nFigure 3.1: The planes defined by the preceding linear equations.\n\n\n\n54 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nx -2x -2x = 31 2 3\n\n2x + x + x = 21 2 3\n\n2x + 2x - x = 11 2 3\n\n(1.4, -0.4, -0.4)\n\nFigure 3.2: The solution of the system is the point in common with each of the three planes.\n\nThe intersection Hi∩Hj of any two distinct planes Hi and Hj is a line, and the intersection\nH1 ∩H2 ∩H3 of the three planes consists of the single point (1.4,−0.4,−0.4), as illustrated\nin Figure 3.2.\n\nThe planes corresponding to the system\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − x2 + 2x3 = 3,\n\nare illustrated in Figure 3.3.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\n1 2 3\n\nx - x +2x = 31 2 3\n\nFigure 3.3: The planes defined by the equations x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, and\nx1 − x2 + 2x3 = 3.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK55\n\nThis system has no solution since there is no point simultaneously contained in all three\nplanes; see Figure 3.4.\n\n2x + 2x - x = 11 2 3\n\nx - x +2x = 31 2 3\n\n2x + x + x = 21 2 32x + x + x = 21 2 3\n\nFigure 3.4: The linear system x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, x1 − x2 + 2x3 = 3 has\nno solution.\n\nFinally, the planes corresponding to the system\n\nx1 + 2x2 − x3 = 3\n\n2x1 + x2 + x3 = 3\n\nx1 − x2 + 2x3 = 0,\n\nare illustrated in Figure 3.5.\n\n2x + 2x -  x = 3\n1\n\n1\n\n1\n\n2\n\n2 3\n\n3\n\n2x + x + x = 32 3\n\nx - x + 2x = 01 2 3\n\n1\n\nFigure 3.5: The planes defined by the equations x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, and\nx1 − x2 + 2x3 = 0.\n\n\n\n56 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis system has infinitely many solutions, given parametrically by (1 − x3, 1 + x3, x3).\nGeometrically, this is a line common to all three planes; see Figure 3.6.\n\n2x + 2x -  x = 3\n1 2 3\n\nx - x + 2x = 01 2 3\n\n12x + x + x = 32 3\n\nFigure 3.6: The linear system x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, x1 − x2 + 2x3 = 0 has\nthe red line common to all three planes.\n\nUnder the above interpretation, observe that we are focusing on the rows of the matrix\nA, rather than on its columns , as in the previous interpretations.\n\nAnother great example of a real-world problem where linear algebra proves to be very\neffective is the problem of data compression, that is, of representing a very large data set\nusing a much smaller amount of storage.\n\nTypically the data set is represented as an m× n matrix A where each row corresponds\nto an n-dimensional data point and typically, m ≥ n. In most applications, the data are not\nindependent so the rank of A is a lot smaller than min{m,n}, and the the goal of low-rank\ndecomposition is to factor A as the product of two matrices B and C, where B is a m × k\nmatrix and C is a k×n matrix, with k � min{m,n} (here,� means “much smaller than”):\n\nA\nm× n\n\n\n=\n\n\nB\n\nm× k\n\n\n C\n\nk × n\n\n\n\nNow it is generally too costly to find an exact factorization as above, so we look for a\nlow-rank matrix A′ which is a “good” approximation of A. In order to make this statement\nprecise, we need to define a mechanism to determine how close two matrices are. This can\nbe done using matrix norms , a notion discussed in Chapter 9. The norm of a matrix A is a\n\n\n\n3.2. VECTOR SPACES 57\n\nnonnegative real number ‖A‖ which behaves a lot like the absolute value |x| of a real number\nx. Then our goal is to find some low-rank matrix A′ that minimizes the norm\n\n‖A− A′‖2\n,\n\nover all matrices A′ of rank at most k, for some given k � min{m,n}.\nSome advantages of a low-rank approximation are:\n\n1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus\nless storage and fewer operations are needed to reconstruct A.\n\n2. Often, the process for obtaining the decomposition exposes the underlying structure of\nthe data. Thus, it may turn out that “most” of the significant data are concentrated\nalong some directions called principal directions .\n\nLow-rank decompositions of a set of data have a multitude of applications in engineering,\nincluding computer science (especially computer vision), statistics, and machine learning.\nAs we will see later in Chapter 23, the singular value decomposition (SVD) provides a very\nsatisfactory solution to the low-rank approximation problem. Still, in many cases, the data\nsets are so large that another ingredient is needed: randomization. However, as a first step,\nlinear algebra often yields a good initial solution.\n\nWe will now be more precise as to what kinds of operations are allowed on vectors. In\nthe early 1900, the notion of a vector space emerged as a convenient and unifying framework\nfor working with “linear” objects and we will discuss this notion in the next few sections.\n\n3.2 Vector Spaces\n\nFor every n ≥ 1, let Rn be the set of n-tuples x = (x1, . . . , xn). Addition can be extended to\nRn as follows:\n\n(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn).\n\nWe can also define an operation · : R× Rn → Rn as follows:\n\nλ · (x1, . . . , xn) = (λx1, . . . , λxn).\n\nThe resulting algebraic structure has some interesting properties, those of a vector space.\n\nHowever, keep in mind that vector spaces are not just algebraic\nobjects; they are also geometric objects.\n\nVector spaces are defined as follows.\n\n\n\n58 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nDefinition 3.1. Given a field K (with addition + and multiplication ∗), a vector space over\nK (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E\n(called vector addition),1 and · : K × E → E (called scalar multiplication) satisfying the\nfollowing conditions for all α, β ∈ K and all u, v ∈ E;\n\n(V0) E is an abelian group w.r.t. +, with identity element 0;2\n\n(V1) α · (u+ v) = (α · u) + (α · v);\n\n(V2) (α + β) · u = (α · u) + (β · u);\n\n(V3) (α ∗ β) · u = α · (β · u);\n\n(V4) 1 · u = u.\n\nIn (V3), ∗ denotes multiplication in the field K.\n\nGiven α ∈ K and v ∈ E, the element α · v is also denoted by αv. The field K is often\ncalled the field of scalars.\n\nUnless specified otherwise or unless we are dealing with several different fields, in the rest\nof this chapter, we assume that all K-vector spaces are defined with respect to a fixed field\nK. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field\nK will be the field R of reals.\n\nFrom (V0), a vector space always contains the null vector 0, and thus is nonempty.\nFrom (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From (V2), we get 0 · v = 0, and\n(−α) · v = −(α · v).\n\nAnother important consequence of the axioms is the following fact:\n\nProposition 3.1. For any u ∈ E and any λ ∈ K, if λ 6= 0 and λ · u = 0, then u = 0.\n\nProof. Indeed, since λ 6= 0, it has a multiplicative inverse λ−1, so from λ · u = 0, we get\n\nλ−1 · (λ · u) = λ−1 · 0.\n\nHowever, we just observed that λ−1 · 0 = 0, and from (V3) and (V4), we have\n\nλ−1 · (λ · u) = (λ−1λ) · u = 1 · u = u,\n\nand we deduce that u = 0.\n\n1The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E.\nIt is usually clear from the context which + is intended.\n\n2The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element\nof E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.\n\n\n\n3.2. VECTOR SPACES 59\n\nRemark: One may wonder whether axiom (V4) is really needed. Could it be derived from\nthe other axioms? The answer is no. For example, one can take E = Rn and define\n· : R× Rn → Rn by\n\nλ · (x1, . . . , xn) = (0, . . . , 0)\n\nfor all (x1, . . . , xn) ∈ Rn and all λ ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails.\nLess trivial examples can be given using the notion of a basis, which has not been defined\nyet.\n\nThe field K itself can be viewed as a vector space over itself, addition of vectors being\naddition in the field, and multiplication by a scalar being multiplication in the field.\n\nExample 3.1.\n\n1. The fields R and C are vector spaces over R.\n\n2. The groups Rn and Cn are vector spaces over R, with scalar multiplication given by\n\nλ(x1, . . . , xn) = (λx1, . . . , λxn),\n\nfor any λ ∈ R and with (x1, . . . , xn) ∈ Rn or (x1, . . . , xn) ∈ Cn, and Cn is a vector\nspace over C with scalar multiplication as above, but with λ ∈ C.\n\n3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector\nspace over R, and the ring C[X]n of polynomials of degree at most n with complex\ncoefficients is a vector space over C, with scalar multiplication λ ·P (X) of a polynomial\n\nP (X) = amX\nm + am−1X\n\nm−1 + · · ·+ a1X + a0\n\n(with ai ∈ R or ai ∈ C) by the scalar λ (in R or C), with m ≤ n, given by\n\nλ · P (X) = λamX\nm + λam−1X\n\nm−1 + · · ·+ λa1X + λa0.\n\n4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the\nring C[X] of all polynomials with complex coefficients is a vector space over C, with\nthe same scalar multiplication as above.\n\n5. The ring of n× n matrices Mn(R) is a vector space over R.\n\n6. The ring of m× n matrices Mm,n(R) is a vector space over R.\n\n7. The ring C(a, b) of continuous functions f : (a, b) → R is a vector space over R, with\nthe scalar multiplication λf of a function f : (a, b)→ R by a scalar λ ∈ R given by\n\n(λf)(x) = λf(x), for all x ∈ (a, b).\n\n\n\n60 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n8. A very important example of vector space is the set of linear maps between two vector\nspaces to be defined in Section 11.1. Here is an example that will prepare us for the\nvector space of linear maps. Let X be any nonempty set and let E be a vector space.\nThe set of all functions f : X → E can be made into a vector space as follows: Given\nany two functions f : X → E and g : X → E, let (f + g) : X → E be defined such that\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that\n\n(λf)(x) = λf(x)\n\nfor all x ∈ X. The axioms of a vector space are easily verified.\n\nLet E be a vector space. We would like to define the important notions of linear combi-\nnation and linear independence.\n\nBefore defining these notions, we need to discuss a strategic choice which, depending\nhow it is settled, may reduce or increase headaches in dealing with notions such as linear\ncombinations and linear dependence (or independence). The issue has to do with using sets\nof vectors versus sequences of vectors.\n\n3.3 Indexed Families; the Sum Notation\n∑\n\ni∈I ai\n\nOur experience tells us that it is preferable to use sequences of vectors ; even better, indexed\nfamilies of vectors. (We are not alone in having opted for sequences over sets, and we are in\ngood company; for example, Artin [7], Axler [10], and Lang [108] use sequences. Nevertheless,\nsome prominent authors such as Lax [112] use sets. We leave it to the reader to conduct a\nsurvey on this issue.)\n\nGiven a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈ An of elements\nfrom A, for some natural number n. The elements of a sequence need not be distinct and\nthe order is important. For example, (a1, a2, a1) and (a2, a1, a1) are two distinct sequences\nin A3. Their underlying set is {a1, a2}.\n\nWhat we just defined are finite sequences, which can also be viewed as functions from\n{1, 2, . . . , n} to the set A; the ith element of the sequence (a1, . . . , an) is the image of i under\nthe function. This viewpoint is fruitful, because it allows us to define (countably) infinite\nsequences as functions s : N → A. But then, why limit ourselves to ordered sets such as\n{1, . . . , n} or N as index sets?\n\nThe main role of the index set is to tag each element uniquely, and the order of the tags\nis not crucial, although convenient. Thus, it is natural to define the notion of indexed family.\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 61\n\nDefinition 3.2. Given a set A, an I-indexed family of elements of A, for short a family ,\nis a function a : I → A where I is any set viewed as an index set. Since the function a is\ndetermined by its graph\n\n{(i, a(i)) | i ∈ I},\nthe family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈ I}. For notational simplicity,\nwe write ai instead of a(i), and denote the family a = {(i, a(i)) | i ∈ I} by (ai)i∈I .\n\nFor example, if I = {r, g, b, y} and A = N, the set of pairs\n\na = {(r, 2), (g, 3), (b, 2), (y, 11)}\n\nis an indexed family. The element 2 appears twice in the family with the two distinct tags\nr and b.\n\nWhen the indexed set I is totally ordered, a family (ai)i∈I is often called an I-sequence.\nInterestingly, sets can be viewed as special cases of families. Indeed, a set A can be viewed\nas the A-indexed family {(a, a) | a ∈ I} corresponding to the identity function.\n\nRemark: An indexed family should not be confused with a multiset. Given any set A, a\nmultiset is a similar to a set, except that elements of A may occur more than once. For\nexample, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d} is a multiset. Each element appears\nwith a certain multiplicity, but the order of the elements does not matter. For example, a\nhas multiplicity 3. Formally, a multiset is a function s : A→ N, or equivalently a set of pairs\n{(a, i) | a ∈ A}. Thus, a multiset is an A-indexed family of elements from N, but not a\nN-indexed family, since distinct elements may have the same multiplicity (such as c an d in\nthe example above). An indexed family is a generalization of a sequence, but a multiset is a\ngeneralization of a set.\n\nWe also need to take care of an annoying technicality, which is to define sums of the\nform\n\n∑\ni∈I ai, where I is any finite index set and (ai)i∈I is a family of elements in some set\n\nA equiped with a binary operation +: A × A → A which is associative (Axiom (G1)) and\ncommutative. This will come up when we define linear combinations.\n\nThe issue is that the binary operation + only tells us how to compute a1 + a2 for two\nelements of A, but it does not tell us what is the sum of three of more elements. For example,\nhow should a1 + a2 + a3 be defined?\n\nWhat we have to do is to define a1+a2+a3 by using a sequence of steps each involving two\nelements, and there are two possible ways to do this: a1 + (a2 +a3) and (a1 +a2) +a3. If our\noperation + is not associative, these are different values. If it associative, then a1+(a2+a3) =\n(a1 + a2) + a3, but then there are still six possible permutations of the indices 1, 2, 3, and if\n+ is not commutative, these values are generally different. If our operation is commutative,\nthen all six permutations have the same value. Thus, if + is associative and commutative,\nit seems intuitively clear that a sum of the form\n\n∑\ni∈I ai does not depend on the order of the\n\noperations used to compute it.\n\n\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form∑\n\ni∈I ai are indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums\n∑\n\ni∈I ai, where I is a finite sequence of distinct natural numbers,\nsay I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥ 2, we denote the sequence (i2, . . . , im) by\nI − {i1}. We proceed by induction on the size m of I. Let∑\n\ni∈I\nai = ai1 , if m = 1,\n\n∑\ni∈I\n\nai = ai1 +\n\n( ∑\ni∈I−{i1}\n\nai\n\n)\n, if m > 1.\n\nFor example, if I = (1, 2, 3, 4), we have∑\ni∈I\n\nai = a1 + (a2 + (a3 + a4)).\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\n\na1 + (a2 + (a3 + a4)) 6= (a1 + a2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum\n∑\n\ni∈I ai should not depend on the grouping\nof the elements in I, as long as their order is preserved. For example, if I = (1, 2, 3, 4, 5),\nJ1 = (1, 2), and J2 = (3, 4, 5), we expect that\n\n∑\ni∈I\n\nai =\n\n(∑\nj∈J1\n\naj\n\n)\n+\n\n(∑\nj∈J2\n\naj\n\n)\n.\n\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: A × A → A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Ik1 , . . . , Ikp, for some nonempty sequence K =\n(k1, . . . , kp) of distinct natural numbers such that ki < kj implies that α < β for all α ∈ Iki\nand all β ∈ Ikj , for every sequence (ai)i∈I of elements in A, we have\n\n∑\nα∈I\n\naα =\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n.\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds trivially.\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form\nvier 4% ave indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums }7;-,@;, where J is a finite sequence of distinct natural numbers,\nsay I = (i1,...,4m). If I = (i1,...,im) with m > 2, we denote the sequence (i2,...,%m) by\nI — {t,}. We proceed by induction on the size m of I. Let\n\n) a,=a,, Wwm=1,\n\nie]\nLa=a +( S- a). ifm > 1.\nie] i€I—{ir}\n\nFor example, if J = (1,2,3,4), we have\nSai =a, + (ag + (a3 + a4)).\niel\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\nay + (a2 + (a3 + a4)) A (a1 + G2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum }?,., a; should not depend on the grouping\nof the elements in J, as long as their order is preserved. For example, if J = (1,2,3,4,5),\nJ, = (1,2), and Jy = (3,4,5), we expect that\n\nYa=(La)+ (La).\n\niel jel je de\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: Ax A-— A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Iy,,...,Ik,, for some nonempty sequence K =\n(ky,...,kp) of distinct natural numbers such that k; < k; implies that a < 6 for alla € Ix,\nand all 8 € Iy,, for every sequence (a;)ier of elements in A, we have\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and IJ;, = J, so the proposition holds trivially.\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 63\n\nNext, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial, so assume that\np ≥ 2 and write J = (k2, . . . , kp). There are two cases.\n\nCase 1. The sequence Ik1 has a single element, say β, which is the first element of I.\nIn this case, write C for the sequence obtained from I by deleting its first element β. By\ndefinition, ∑\n\nα∈I\naα = aβ +\n\n(∑\nα∈C\n\naα\n\n)\n,\n\nand ∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n= aβ +\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nSince |C| = n− 1, by the induction hypothesis, we have(∑\nα∈C\n\naα\n\n)\n=\n∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)\n,\n\nwhich yields our identity.\n\nCase 2. The sequence Ik1 has at least two elements. In this case, let β be the first element\nof I (and thus of Ik1), let I ′ be the sequence obtained from I by deleting its first element β,\nlet I ′k1\n\nbe the sequence obtained from Ik1 by deleting its first element β, and let I ′ki = Iki for\ni = 2, . . . , p. Recall that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I ′ has n − 1\nelements, so by the induction hypothesis applied to I ′ and the I ′ki , we get∑\n\nα∈I′\naα =\n\n∑\nk∈K\n\n(∑\nα∈I′k\n\naα\n\n)\n=\n\n(∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nIf we add the lefthand side to aβ, by definition we get∑\nα∈I\n\naα.\n\nIf we add the righthand side to aβ, using associativity and the definition of an indexed sum,\nwe get\n\naβ +\n\n((∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)))\n=\n\n(\naβ +\n\n(∑\nα∈I′k1\n\naα\n\n))\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n\n(∑\nα∈Ik1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n,\n\nas claimed.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 63\n\nNext, assume n > 1. If p = 1, then J,, = J and the formula is trivial, so assume that\np > 2 and write J = (ko,...,k,). There are two cases.\n\nCase 1. The sequence J;, has a single element, say 3, which is the first element of I.\nIn this case, write C' for the sequence obtained from I by deleting its first element 6. By\n\ndefinition,\nSi aa =agt+ (Su),\n\nael acC\nand\n\nHE) (E(S\"))\n\nkeK ‘a&€ly, jEtJ Sael;\nSince |C| =n — 1, by the induction hypothesis, we have\n(Se)=X(Le)\naeC jet Sa€l;\n\nwhich yields our identity.\n\nCase 2. The sequence I;, has at least two elements. In this case, let 8 be the first element\nof I (and thus of J;,), let I’ be the sequence obtained from J by deleting its first element £,\nlet Ij, be the sequence obtained from J;,, by deleting its first element 6, and let J, = Ip, for\ni =2,...,p. Recall that J = (ko,...,k,) and kK = (ki,...,k,). The sequence I’ has n — 1\nelements, so by the induction hypothesis applied to J’ and the [;,, we get\n\nYa=O (Lae) = (LH a) + (HL):\nael! keK acl, al}, JET Sal;\nIf we add the lefthand side to ag, by definition we get\non\nael\n\nIf we add the righthand side to ag, using associativity and the definition of an indexed sum,\nwe get\n\nw+ ((X~)+(Z(Le)))= (+ (L4)) +S)\n\nacl, jEJ Sael; acl,\n\n-(L)*(E(E))\n-¥(E«).\n\nkeK \\a€ly\n\nas claimed. Oo\n\n\n\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf I = (1, . . . , n), we also write\n∑n\n\ni=1 ai instead of\n∑\n\ni∈I ai. Since + is associative, Propo-\nsition 3.2 shows that the sum\n\n∑n\ni=1 ai is independent of the grouping of its elements, which\n\njustifies the use the notation a1 + · · ·+ an (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\ncan show that the sum\n\n∑\ni∈I ai does not depend on the ordering of the index set I.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: A× A→ A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (ai)i∈I of elements in A, we have∑\n\nα∈I\naα =\n\n∑\nα∈J\n\naα.\n\nProof. We proceed by induction on the number p of elements in I. If p = 1, we have I = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that I = (1, . . . , p) and that J is a permutation\n(i1, . . . , ip) of I. First, assume that 2 ≤ i1 ≤ p−1, let J ′ be the sequence obtained from J by\ndeleting i1, I ′ be the sequence obtained from I by deleting i1, and let P = (1, 2, . . . , i1−1) and\nQ = (i1 + 1, . . . , p−1, p). Observe that the sequence I ′ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J ′ and I ′, and then by Proposition 3.2\napplied to I ′ and its partition (P,Q), we have\n\n∑\nα∈J ′\n\naα =\n∑\nα∈I′\n\naα =\n\n(i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n.\n\nIf we add the lefthand side to ai1 , by definition we get∑\nα∈J\n\naα.\n\nIf we add the righthand side to ai1 , we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n.\n\nUsing associativity, we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n=\n\n(\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n,\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf J = (1,...,n), we also write }°\"' | a; instead of )7,., a;. Since + is associative, Propo-\nsition 3.2 shows that the sum }>;\"_, a; is independent of the grouping of its elements, which\njustifies the use the notation a, +---+ a, (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\n\ncan show that the sum }°,., a; does not depend on the ordering of the index set J.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: Ax A— A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (a;)icr of elements in A, we have\n\nSota = Soa.\n\naéel aces\n\nProof. We proceed by induction on the number p of elements in J. If p = 1, we have J = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that J = (1,...,p) and that J is a permutation\n(i1,...,%)) of I. First, assume that 2 < 7, < p—1, let J’ be the sequence obtained from J by\ndeleting i, I’ be the sequence obtained from J by deleting 7,, and let P = (1,2,...,i;—1) and\nQ = (%44+1,...,p—1,p). Observe that the sequence I’ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J’ and I’, and then by Proposition 3.2\napplied to J’ and its partition (P,Q), we have\n\n44-1 Pp\n) da = u=() a) +() w).\naceJ’ ael’ i=l I=14+1\n\nIf we add the lefthand side to a;,, by definition we get\n\nSo.\n\naed\n\nIf we add the righthand side to a;,, we get\n\n(Eo) (29)\n\nUsing associativity, we get\n\no(E5)+(E9)-+E)-(E9)\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i1 − 1), we get(\n\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n=\n\n(i1−1∑\ni=1\n\nai\n\n)\n+ ai1 +\n\n( p∑\ni=i1+1\n\nai\n\n)\n\n=\n\np∑\ni=1\n\nai,\n\nas claimed.\n\nThe cases where i1 = 1 or i1 = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence).\n\nHaving done all this, we can now make sense of sums of the form\n∑\n\ni∈I ai, for any finite\nindexed set I and any family a = (ai)i∈I of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since I is finite, it is in bijection with the set {1, . . . , n} for some n ∈ N, and any\ntotal ordering � on I corresponds to a permutation I� of {1, . . . , n} (where we identify a\npermutation with its image). For any total ordering � on I, we define\n\n∑\ni∈I,� ai as∑\n\ni∈I,�\nai =\n\n∑\nj∈I�\n\naj.\n\nThen for any other total ordering �′ on I, we have∑\ni∈I,�′\n\nai =\n∑\nj∈I�′\n\naj,\n\nand since I� and I�′ are different permutations of {1, . . . , n}, by Proposition 3.3, we have∑\nj∈I�\n\naj =\n∑\nj∈I�′\n\naj.\n\nTherefore, the sum\n∑\n\ni∈I,� ai does not depend on the total ordering on I. We define the sum∑\ni∈I ai as the common value\n\n∑\ni∈I,� ai for all total orderings � of I.\n\nHere are some examples with A = R:\n\n1. If I = {1, 2, 3}, a = {(1, 2), (2,−3), (3,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n2. If I = {2, 5, 7}, a = {(2, 2), (5,−3), (7,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n3. If I = {r, g, b}, a = {(r, 2), (g,−3), (b, 1)}, then\n∑\n\ni∈I ai = 2− 3 + 1 = 0.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i; — 1), we get\n\n(+ (E9))(E9)- Ea) ome(E9)\n\nP\n\nS ai,\n\ni=1\n\nas claimed.\n\nThe cases where 7; = 1 or 7; = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence). O\n\nHaving done all this, we can now make sense of sums of the form }°,., a;, for any finite\nindexed set J and any family a = (a;);c7 of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since J is finite, it is in bijection with the set {1,...,n} for some n € N, and any\ntotal ordering < on J corresponds to a permutation J, of {1,...,n} (where we identify a\npermutation with its image). For any total ordering =< on J, we define }0,-; a as\n\nSo a= ray,\n\ni€l,x jel\n\nThen for any other total ordering =<’ on J, we have\n\na= Da\n\ni€1,<! j€ler\nand since J, and J are different permutations of {1,...,n}, by Proposition 3.3, we have\n) ay= ) aj.\nJET JEL:\n\nTherefore, the sum )7,-; . a; does not depend on the total ordering on J. We define the sum\nier % as the common value $7,-;~ a for all total orderings = of J.\n\nHere are some examples with A = R:\n1. If J = {1,2,3}, a = {(1, 2), (2, -3), (3, V2)}, then 0),-,a; = 2-34 V2 =-14 v2.\n2. If I = {2,5,7}, a = {(2, 2), (5, -3), (7, V2)}, then ),-, a; = 2-34 V2 =—-14 v2.\n\n3. If l= {r, 9, Bf, a= {(r, 2), (9, —3), (0, 1)}, then ier ay = 2—3+1=0.\n\n\n\n\n66 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n3.4 Linear Independence, Subspaces\n\nOne of the most useful properties of vector spaces is that they possess bases. What this\nmeans is that in every vector space E, there is some set of vectors, {e1, . . . , en}, such that\nevery vector v ∈ E can be written as a linear combination,\n\nv = λ1e1 + · · ·+ λnen,\n\nof the ei, for some scalars, λ1, . . . , λn ∈ R. Furthermore, the n-tuple, (λ1, . . . , λn), as above\nis unique.\n\nThis description is fine when E has a finite basis, {e1, . . . , en}, but this is not always the\ncase! For example, the vector space of real polynomials, R[X], does not have a finite basis\nbut instead it has an infinite basis, namely\n\n1, X, X2, . . . , Xn, . . .\n\nOne might wonder if it is possible for a vector space to have bases of different sizes, or even\nto have a finite basis as well as an infinite basis. We will see later on that this is not possible;\nall bases of a vector space have the same number of elements (cardinality), which is called\nthe dimension of the space. However, we have the following problem: If a vector space has\nan infinite basis, {e1, e2, . . . , }, how do we define linear combinations? Do we allow linear\ncombinations\n\nλ1e1 + λ2e2 + · · ·\nwith infinitely many nonzero coefficients?\n\nIf we allow linear combinations with infinitely many nonzero coefficients, then we have\nto make sense of these sums and this can only be done reasonably if we define such a sum\nas the limit of the sequence of vectors, s1, s2, . . . , sn, . . ., with s1 = λ1e1 and\n\nsn+1 = sn + λn+1en+1.\n\nBut then, how do we define such limits? Well, we have to define some topology on our space,\nby means of a norm, a metric or some other mechanism. This can indeed be done and this\nis what Banach spaces and Hilbert spaces are all about but this seems to require a lot of\nmachinery.\n\nA way to avoid limits is to restrict our attention to linear combinations involving only\nfinitely many vectors. We may have an infinite supply of vectors but we only form linear\ncombinations involving finitely many nonzero coefficients. Technically, this can be done by\nintroducing families of finite support . This gives us the ability to manipulate families of\nscalars indexed by some fixed infinite set and yet to be treat these families as if they were\nfinite.\n\nWith these motivations in mind, given a set A, recall that an I-indexed family (ai)i∈I\nof elements of A (for short, a family) is a function a : I → A, or equivalently a set of pairs\n{(i, ai) | i ∈ I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is finite if I is finite.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 67\n\nRemark: When considering a family (ai)i∈I , there is no reason to assume that I is ordered.\nThe crucial point is that every element of the family is uniquely indexed by an element of\nI. Thus, unless specified otherwise, we do not assume that the elements of an index set are\nordered.\n\nIf A is an abelian group with identity 0, we say that a family (ai)i∈I has finite support if\nai = 0 for all i ∈ I − J , where J is a finite subset of I (the support of the family).\n\nGiven two disjoint sets I and J , the union of two families (ui)i∈I and (vj)j∈J , denoted as\n(ui)i∈I ∪ (vj)j∈J , is the family (wk)k∈(I∪J) defined such that wk = uk if k ∈ I, and wk = vk\nif k ∈ J . Given a family (ui)i∈I and any element v, we denote by (ui)i∈I ∪k (v) the family\n(wi)i∈I∪{k} defined such that, wi = ui if i ∈ I, and wk = v, where k is any index such that\nk /∈ I. Given a family (ui)i∈I , a subfamily of (ui)i∈I is a family (uj)j∈J where J is any subset\nof I.\n\nIn this chapter, unless specified otherwise, is assumed that all families of scalars have\nfinite support .\n\nDefinition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family\n(ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nWhen I = ∅, we stipulate that v = 0. (By Proposition 3.3, sums of the form\n∑\n\ni∈I λiui are\nwell defined.) We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I\nof scalars in K, ∑\n\ni∈I\nλiui = 0 implies that λi = 0 for all i ∈ I.\n\nEquivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars\nin K such that ∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I.\n\nWe agree that when I = ∅, the family ∅ is linearly independent.\n\nObserve that defining linear combinations for families of vectors rather than for sets of\nvectors has the advantage that the vectors being combined need not be distinct. For example,\nfor I = {1, 2, 3} and the families (u, v, u) and (λ1, λ2, λ1), the linear combination∑\n\ni∈I\nλiui = λ1u+ λ2v + λ1u\n\nmakes sense. Using sets of vectors in the definition of a linear combination does not allow\nsuch linear combinations; this is too restrictive.\n\n\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (ui)i∈I is linearly dependent iff either I consists of a\nsingle element, say i, and ui = 0, or |I| ≥ 2 and some uj in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (λi)i∈I of scalars in K such that∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I,\n\nand since |I| ≥ 2, the set I − {j} is nonempty and we get\n\nuj =\n∑\n\ni∈(I−{j})\n−λ−1\n\nj λiui.\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (ui)i∈I is linearly independent iff either I = ∅, or I\nconsists of a single element i and ui 6= 0, or |I| ≥ 2 and no vector uj in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen I is nonempty, if the family (ui)i∈I is linearly independent, note that ui 6= 0 for\nall i ∈ I. Otherwise, if ui = 0 for some i ∈ I, then we get a nontrivial linear dependence∑\n\ni∈I λiui = 0 by picking any nonzero λi and letting λk = 0 for all k ∈ I with k 6= i, since\nλi0 = 0. If |I| ≥ 2, we must also have ui 6= uj for all i, j ∈ I with i 6= j, since otherwise we\nget a nontrivial linear dependence by picking λi = λ and λj = −λ for any nonzero λ, and\nletting λk = 0 for all k ∈ I with k 6= i, j.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, as we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n\n1. Any two distinct scalars λ, µ 6= 0 in K are linearly dependent.\n\n2. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent. See Figure\n3.7.\n\n3. In R4, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are linearly indepen-\ndent.\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (u;);<7 is linearly dependent iff either J consists of a\nsingle element, say 7, and u; = 0, or |I| > 2 and some u; in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (A;)ic7 of scalars in AK such that\n\nSo dit = 0 and A; #0 for some j € J,\n\ntel\n\nand since |I| > 2, the set J — {7} is nonempty and we get\n\nie(I—{3})\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (u;)jc7 is linearly independent iff either J = @, or I\nconsists of a single element i and u; ¥ 0, or |J| > 2 and no vector wu; in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen J is nonempty, if the family (u;)icr is linearly independent, note that u; 4 0 for\nalli € I. Otherwise, if u; = 0 for some 2 € J, then we get a nontrivial linear dependence\nier Mii = O by picking any nonzero 4; and letting A, = 0 for all k € J with k ¥ 7, since\nA;0 = 0. If |Z] > 2, we must also have u; 4 u; for all 7,7 € J with i 4 j, since otherwise we\nget a nontrivial linear dependence by picking A; = A and A; = —A for any nonzero A, and\nletting A, = 0 for all k € I with k £2, 7.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, aS we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n1. Any two distinct scalars A, #0 in K are linearly dependent.\n\n2. In R°, the vectors (1,0,0), (0,1,0), and (0,0,1) are linearly independent. See Figure\n3.7.\n\n3. In R*, the vectors (1,1,1,1), (0,1,1,1), (0,0,1,1), and (0,0,0,1) are linearly indepen-\ndent.\n\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 69\n\nFigure 3.7: A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),\nand the blue vector (0, 0, 1) in R3.\n\n4. In R2, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly dependent, since\n\nw = 2u+ v.\n\nSee Figure 3.8.\n\n(2,3)\n\n2u\n\nv\n\nw\n\nFigure 3.8: A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector\nv = (0, 1), and the vector sum w = 2u+ v.\n\nWhen I is finite, we often assume that it is the set I = {1, 2, . . . , n}. In this case, we\ndenote the family (ui)i∈I as (u1, . . . , un).\n\nThe notion of a subspace of a vector space is defined as follows.\n\nDefinition 3.4. Given a vector space E, a subset F of E is a linear subspace (or subspace)\nof E iff F is nonempty and λu+ µv ∈ F for all u, v ∈ F , and all λ, µ ∈ K.\n\n\n\n70 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is easy to see that a subspace F of E is indeed a vector space, since the restriction\nof +: E × E → E to F × F is indeed a function +: F × F → F , and the restriction of\n· : K × E → E to K × F is indeed a function · : K × F → F .\n\nSince a subspace F is nonempty, if we pick any vector u ∈ F and if we let λ = µ = 0,\nthen λu+ µu = 0u+ 0u = 0, so every subspace contains the vector 0.\n\nThe following facts also hold. The proof is left as an exercise.\n\nProposition 3.4.\n\n(1) The intersection of any family (even infinite) of subspaces of a vector space E is a\nsubspace.\n\n(2) Let F be any subspace of a vector space E. For any nonempty finite index set I,\nif (ui)i∈I is any family of vectors ui ∈ F and (λi)i∈I is any family of scalars, then∑\n\ni∈I λiui ∈ F .\n\nThe subspace {0} will be denoted by (0), or even 0 (with a mild abuse of notation).\n\nExample 3.3.\n\n1. In R2, the set of vectors u = (x, y) such that\n\nx+ y = 0\n\nis the subspace illustrated by Figure 3.9.\n\nFigure 3.9: The subspace x+ y = 0 is the line through the origin with slope −1. It consists\nof all vectors of the form λ(−1, 1).\n\n2. In R3, the set of vectors u = (x, y, z) such that\n\nx+ y + z = 0\n\nis the subspace illustrated by Figure 3.10.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+y+z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n ≥ 0, the set of polynomials f(X) ∈ R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n×n matrices is a subspace of the space of n×n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace 〈S〉 (or Span(S)) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\nof E, leaving as an exercise the verification that every subspace containing S also contains\nSpan(S).\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u =\n∑\n\ni∈I λiui\nand v =\n\n∑\nj∈J µjvj are any two linear combinations in Span(S), for any two scalars λ, µ ∈ K,\n\nλu+ µv = λ\n∑\ni∈I\n\nλiui + µ\n∑\nj∈J\n\nµjvj\n\n=\n∑\ni∈I\n\nλλiui +\n∑\nj∈J\n\nµµjvj\n\n=\n∑\ni∈I−J\n\nλλiui +\n∑\ni∈I∩J\n\n(λλi + µµi)ui +\n∑\nj∈J−I\n\nµµjvj,\n\nwhich is a linear combination with index set I ∪ J , and thus λu + µv ∈ Span(S), which\nproves that Span(S) is a subspace.\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+ y+ z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n > 0, the set of polynomials f(X) € R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n x n matrices is a subspace of the space of n x n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace (S) (or Span(S')) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\n\nof EF, leaving as an exercise the verification that every subspace containing S' also contains\nSpan().\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u = S),.; Aim\nandv=)> jeg Mj¥j are any two linear combinations in Span(.S), for any two scalars A, uw € K,\n\nAu + pv = NSO jue + HD p50;\n\ni€l jed\n= S- ANU; + S- LLjV5\niel jeJ\n= SO Mii + SS OA + mpedur + SS pyr,\nieI—J i€INI jeJ—1\n\nwhich is a linear combination with index set J U J, and thus Au + yu € Span(S), which\nproves that Span(S) is a subspace. O\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n\n\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations\n∑\n\ni∈I λiui for which∑\ni∈I\n\nλi = 1.\n\nThese are called affine combinations . One should realize that every linear combination∑\ni∈I λiui can be viewed as an affine combination. For example, if k is an index not\n\nin I, if we let J = I ∪ {k}, uk = 0, and λk = 1−∑i∈I λi, then\n∑\n\nj∈J λjuj is an affine\ncombination and ∑\n\ni∈I\nλiui =\n\n∑\nj∈J\n\nλjuj.\n\nHowever, we get new spaces. For example, in R3, the set of all affine combinations of\nthe three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0, 0, 0), it is not a linear\nsubspace.\n\n(2) Consider combinations\n∑\n\ni∈I λiui for which\n\nλi ≥ 0, for all i ∈ I.\n\nThese are called positive (or conic) combinations . It turns out that positive combina-\ntions of families of vectors are cones . They show up naturally in convex optimization.\n\n(3) Consider combinations\n∑\n\ni∈I λiui for which we require (1) and (2), that is∑\ni∈I\n\nλi = 1, and λi ≥ 0 for all i ∈ I.\n\nThese are called convex combinations . Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum\n\n∑\ni∈I λiui makes sense, we restrict our attention to families of finite\n\nsupport.\n\nDefinition 3.5. Given any field K, a family of scalars (λi)i∈I has finite support if λi = 0\nfor all i ∈ I − J , for some finite subset J of I.\n\nIf (λi)i∈I is a family of scalars of finite support, for any vector space E over K, for any\n(possibly infinite) family (ui)i∈I of vectors ui ∈ E, we define the linear combination\n\n∑\ni∈I λiui\n\nas the finite linear combination\n∑\n\nj∈J λjuj, where J is any finite subset of I such that λi = 0\nfor all i ∈ I − J . In general, results stated for finite families also hold for families of finite\nsupport.\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations }7,-, A;u; for which\n\nSov = 1.\n\nvel\n\nThese are called affine combinations. One should realize that every linear combination\nier Ais Can be viewed as an affine combination. For example, if k is an index not\nin I, if we let J= JU {k}, up = 0, and Ay =1—)97,-, i, then )7,-, Aju is an affine\n\ncombination and\n\nSs\" AjUj = S- Aj Uy.\n\niel jed\nHowever, we get new spaces. For example, in R°, the set of all affine combinations of\nthe three vectors e; = (1,0,0),e2 = (0,1,0), and e3 = (0,0,1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0,0,0), it is not a linear\nsubspace.\n\n(2) Consider combinations }7,-, A;u; for which\nA; > 0, for allie J.\n\nThese are called positive (or conic) combinations. It turns out that positive combina-\ntions of families of vectors are cones. They show up naturally in convex optimization.\n\n(3) Consider combinations )>,-, A;u; for which we require (1) and (2), that is\n\nwel\nSox Hh, and \\;>0 foralli eT.\n\nwel\n\nThese are called conver combinations. Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum 5°,_, A;u; makes sense, we restrict our attention to families of finite\nsupport.\n\niel\n\nDefinition 3.5. Given any field K, a family of scalars (\\;)ier has finite support if A; = 0\nfor all 2 € I — J, for some finite subset J of J.\n\nIf (A;)icr is a family of scalars of finite support, for any vector space F over K, for any\n(possibly infinite) family (u;);er of vectors u; € E, we define the linear combination 7) ,-, Ait\nas the finite linear combination )> jeg AjUj, Where J is any finite subset of J such that A; = 0\nfor all: € I — J. In general, results stated for finite families also hold for families of finite\nsupport.\n\n\n\n\n3.5. BASES OF A VECTOR SPACE 73\n\n3.5 Bases of a Vector Space\n\nGiven a vector space E, given a family (vi)i∈I , the subset V of E consisting of the null vector\n0 and of all linear combinations of (vi)i∈I is easily seen to be a subspace of E. The family\n(vi)i∈I is an economical way of representing the entire subspace V , but such a family would\nbe even nicer if it was not redundant. Subspaces having such an “efficient” generating family\n(called a basis) play an important role and motivate the following definition.\n\nDefinition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors\nvi ∈ V spans V or generates V iff for every v ∈ V , there is some family (λi)i∈I of scalars in\nK such that\n\nv =\n∑\ni∈I\n\nλivi.\n\nWe also say that the elements of (vi)i∈I are generators of V and that V is spanned by (vi)i∈I ,\nor generated by (vi)i∈I . If a subspace V of E is generated by a finite family (vi)i∈I , we say\nthat V is finitely generated . A family (ui)i∈I that spans V and is linearly independent is\ncalled a basis of V .\n\nExample 3.4.\n\n1. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Figure 3.9, form a basis.\n\n2. The vectors (1, 1, 1, 1), (1, 1,−1,−1), (1,−1, 0, 0), (0, 0, 1,−1) form a basis of R4 known\nas the Haar basis . This basis and its generalization to dimension 2n are crucial in\nwavelet theory.\n\n3. In the subspace of polynomials in R[X] of degree at most n, the polynomials 1, X,X2,\n. . . , Xn form a basis.\n\n4. The Bernstein polynomials\n\n(\nn\nk\n\n)\n(1 − X)n−kXk for k = 0, . . . , n, also form a basis of\n\nthat space. These polynomials play a major role in the theory of spline curves .\n\nThe first key result of linear algebra is that every vector space E has a basis. We begin\nwith a crucial lemma which formalizes the mechanism for building a basis incrementally.\n\nLemma 3.6. Given a linearly independent family (ui)i∈I of elements of a vector space E, if\nv ∈ E is not a linear combination of (ui)i∈I , then the family (ui)i∈I ∪k (v) obtained by adding\nv to the family (ui)i∈I is linearly independent (where k /∈ I).\n\nProof. Assume that µv+\n∑\n\ni∈I λiui = 0, for any family (λi)i∈I of scalars in K. If µ 6= 0, then\nµ has an inverse (because K is a field), and thus we have v = −∑i∈I(µ\n\n−1λi)ui, showing\nthat v is a linear combination of (ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But\nthen, we have\n\n∑\ni∈I λiui = 0, and since the family (ui)i∈I is linearly independent, we have\n\nλi = 0 for all i ∈ I.\n\n\n\n74 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThe next theorem holds in general, but the proof is more sophisticated for vector spaces\nthat do not have a finite set of generators. Thus, in this chapter, we only prove the theorem\nfor finitely generated vector spaces.\n\nTheorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any\nlinearly independent subfamily L = (uj)j∈J of S (where J ⊆ I), there is a basis B of E such\nthat L ⊆ B ⊆ S.\n\nProof. Consider the set of linearly independent families B such that L ⊆ B ⊆ S. Since this\nset is nonempty and finite, it has some maximal element (that is, a subfamily B = (uh)h∈H\nof S with H ⊆ I of maximum cardinality), say B = (uh)h∈H . We claim that B generates\nE. Indeed, if B does not generate E, then there is some up ∈ S that is not a linear\ncombination of vectors in B (since S generates E), with p /∈ H. Then by Lemma 3.6, the\nfamily B′ = (uh)h∈H∪{p} is linearly independent, and since L ⊆ B ⊂ B′ ⊆ S, this contradicts\nthe maximality of B. Thus, B is a basis of E such that L ⊆ B ⊆ S.\n\nRemark: Theorem 3.7 also holds for vector spaces that are not finitely generated. In this\ncase, the problem is to guarantee the existence of a maximal linearly independent family B\nsuch that L ⊆ B ⊆ S. The existence of such a maximal family can be shown using Zorn’s\nlemma, see Appendix B and the references given there.\n\nA situation where the full generality of Theorem 3.7 is needed is the case of the vector\nspace R over the field of coefficients Q. The numbers 1 and\n\n√\n2 are linearly independent\n\nover Q, so according to Theorem 3.7, the linearly independent family L = (1,\n√\n\n2) can be\nextended to a basis B of R. Since R is uncountable and Q is countable, such a basis must\nbe uncountable!\n\nThe notion of a basis can also be defined in terms of the notion of maximal linearly\nindependent family and minimal generating family.\n\nDefinition 3.7. Let (vi)i∈I be a family of vectors in a vector space E. We say that (vi)i∈I a\nmaximal linearly independent family of E if it is linearly independent, and if for any vector\nw ∈ E, the family (vi)i∈I ∪k {w} obtained by adding w to the family (vi)i∈I is linearly\ndependent. We say that (vi)i∈I a minimal generating family of E if it spans E, and if for\nany index p ∈ I, the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does\nnot span E.\n\nThe following proposition giving useful properties characterizing a basis is an immediate\nconsequence of Lemma 3.6.\n\nProposition 3.8. Given a vector space E, for any family B = (vi)i∈I of vectors of E, the\nfollowing properties are equivalent:\n\n(1) B is a basis of E.\n\n\n\n3.5. BASES OF A VECTOR SPACE 75\n\n(2) B is a maximal linearly independent family of E.\n\n(3) B is a minimal generating family of E.\n\nProof. We will first prove the equivalence of (1) and (2). Assume (1). Since B is a basis, it is\na linearly independent family. We claim that B is a maximal linearly independent family. If\nB is not a maximal linearly independent family, then there is some vector w ∈ E such that\nthe family B′ obtained by adding w to B is linearly independent. However, since B is a basis\nof E, the vector w can be expressed as a linear combination of vectors in B, contradicting\nthe fact that B′ is linearly independent.\n\nConversely, assume (2). We claim that B spans E. If B does not span E, then there is\nsome vector w ∈ E which is not a linear combination of vectors in B. By Lemma 3.6, the\nfamily B′ obtained by adding w to B is linearly independent. Since B is a proper subfamily\nof B′, this contradicts the assumption that B is a maximal linearly independent family.\nTherefore, B must span E, and since B is also linearly independent, it is a basis of E.\n\nNow we will prove the equivalence of (1) and (3). Again, assume (1). Since B is a basis,\nit is a generating family of E. We claim that B is a minimal generating family. If B is not\na minimal generating family, then there is a proper subfamily B′ of B that spans E. Then,\nevery w ∈ B−B′ can be expressed as a linear combination of vectors from B′, contradicting\nthe fact that B is linearly independent.\n\nConversely, assume (3). We claim that B is linearly independent. If B is not linearly\nindependent, then some vector w ∈ B can be expressed as a linear combination of vectors\nin B′ = B − {w}. Since B generates E, the family B′ also generates E, but B′ is a\nproper subfamily of B, contradicting the minimality of B. Since B spans E and is linearly\nindependent, it is a basis of E.\n\nThe second key result of linear algebra is that for any two bases (ui)i∈I and (vj)j∈J of a\nvector space E, the index sets I and J have the same cardinality. In particular, if E has a\nfinite basis of n elements, every basis of E has n elements, and the integer n is called the\ndimension of the vector space E.\n\nTo prove the second key result, we can use the following replacement lemma due to\nSteinitz. This result shows the relationship between finite linearly independent families and\nfinite families of generators of a vector space. We begin with a version of the lemma which is\na bit informal, but easier to understand than the precise and more formal formulation given\nin Proposition 3.10. The technical difficulty has to do with the fact that some of the indices\nneed to be renamed.\n\nProposition 3.9. (Replacement lemma, version 1) Given a vector space E, let (u1, . . . , um)\nbe any finite linearly independent family in E, and let (v1, . . . , vn) be any finite family such\nthat every ui is a linear combination of (v1, . . . , vn). Then we must have m ≤ n, and there\nis a replacement of m of the vectors vj by (u1, . . . , um), such that after renaming some of the\nindices of the vjs, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same\nsubspace of E.\n\n\n\n76 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProof. We proceed by induction on m. When m = 0, the family (u1, . . . , um) is empty, and\nthe proposition holds trivially. For the induction step, we have a linearly independent family\n(u1, . . . , um, um+1). Consider the linearly independent family (u1, . . . , um). By the induction\nhypothesis, m ≤ n, and there is a replacement of m of the vectors vj by (u1, . . . , um), such\nthat after renaming some of the indices of the vs, the families (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace of E. The vector um+1 can also be expressed as a lin-\near combination of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate\nthe same subspace, um+1 can be expressed as a linear combination of (u1, . . . , um, vm+1, . . .,\nvn), say\n\num+1 =\nm∑\ni=1\n\nλiui +\nn∑\n\nj=m+1\n\nλjvj.\n\nWe claim that λj 6= 0 for some j with m+ 1 ≤ j ≤ n, which implies that m+ 1 ≤ n.\n\nOtherwise, we would have\n\num+1 =\nm∑\ni=1\n\nλiui,\n\na nontrivial linear dependence of the ui, which is impossible since (u1, . . . , um+1) are linearly\nindependent.\n\nTherefore, m + 1 ≤ n, and after renaming indices if necessary, we may assume that\nλm+1 6= 0, so we get\n\nvm+1 = −\nm∑\ni=1\n\n(λ−1\nm+1λi)ui − λ−1\n\nm+1um+1 −\nn∑\n\nj=m+2\n\n(λ−1\nm+1λj)vj.\n\nObserve that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1, vm+2, . . . , vn) generate\nthe same subspace, since um+1 is a linear combination of (u1, . . . , um, vm+1, . . . , vn) and vm+1\n\nis a linear combination of (u1, . . . , um+1, vm+2, . . . , vn). Since (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1, vm+2, . . . , vn) and\nand (v1, . . . , vn) generate the same subspace, which concludes the induction hypothesis.\n\nHere is an example illustrating the replacement lemma. Consider sequences (u1, u2, u3)\nand (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly independent family and with the uis\nexpressed in terms of the vjs as follows:\n\nu1 = v4 + v5\n\nu2 = v3 + v4 − v5\n\nu3 = v1 + v2 + v3.\n\nFrom the first equation we get\n\nv4 = u1 − v5,\n\n\n\n3.5. BASES OF A VECTOR SPACE 77\n\nand by substituting in the second equation we have\n\nu2 = v3 + v4 − v5 = v3 + u1 − v5 − v5 = u1 + v3 − 2v5.\n\nFrom the above equation we get\n\nv3 = −u1 + u2 + 2v5,\n\nand so\nu3 = v1 + v2 + v3 = v1 + v2 − u1 + u2 + 2v5.\n\nFinally, we get\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nTherefore we have\n\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nv3 = −u1 + u2 + 2v5\n\nv4 = u1 − v5,\n\nwhich shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3, v4, v5). The vectors\n(v1, v3, v4) have been replaced by (u1, u2, u3), and the vectors left over are (v2, v5). We can\nrename them (v4, v5).\n\nFor the sake of completeness, here is a more formal statement of the replacement lemma\n(and its proof).\n\nProposition 3.10. (Replacement lemma, version 2) Given a vector space E, let (ui)i∈I be\nany finite linearly independent family in E, where |I| = m, and let (vj)j∈J be any finite family\nsuch that every ui is a linear combination of (vj)j∈J , where |J | = n. Then there exists a set\nL and an injection ρ : L→ J (a relabeling function) such that L ∩ I = ∅, |L| = n−m, and\nthe families (ui)i∈I ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E. In particular,\nm ≤ n.\n\nProof. We proceed by induction on |I| = m. When m = 0, the family (ui)i∈I is empty, and\nthe proposition holds trivially with L = J (ρ is the identity). Assume |I| = m+ 1. Consider\nthe linearly independent family (ui)i∈(I−{p}), where p is any member of I. By the induction\nhypothesis, there exists a set L and an injection ρ : L → J such that L ∩ (I − {p}) = ∅,\n|L| = n−m, and the families (ui)i∈(I−{p})∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace\nof E. If p ∈ L, we can replace L by (L− {p}) ∪ {p′} where p′ does not belong to I ∪ L, and\nreplace ρ by the injection ρ′ which agrees with ρ on L − {p} and such that ρ′(p′) = ρ(p).\nThus, we can always assume that L ∩ I = ∅. Since up is a linear combination of (vj)j∈J\nand the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E, up is\na linear combination of (ui)i∈(I−{p}) ∪ (vρ(l))l∈L. Let\n\nup =\n∑\n\ni∈(I−{p})\nλiui +\n\n∑\nl∈L\n\nλlvρ(l). (1)\n\n\n\n78 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf λl = 0 for all l ∈ L, we have ∑\ni∈(I−{p})\n\nλiui − up = 0,\n\ncontradicting the fact that (ui)i∈I is linearly independent. Thus, λl 6= 0 for some l ∈ L, say\nl = q. Since λq 6= 0, we have\n\nvρ(q) =\n∑\n\ni∈(I−{p})\n(−λ−1\n\nq λi)ui + λ−1\nq up +\n\n∑\nl∈(L−{q})\n\n(−λ−1\nq λl)vρ(l). (2)\n\nWe claim that the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (ui)i∈I ∪ (vρ(l))l∈(L−{q}) generate the\nsame subset of E. Indeed, the second family is obtained from the first by replacing vρ(q) by up,\nand vice-versa, and up is a linear combination of (ui)i∈(I−{p})∪ (vρ(l))l∈L, by (1), and vρ(q) is a\nlinear combination of (ui)i∈I∪(vρ(l))l∈(L−{q}), by (2). Thus, the families (ui)i∈I∪(vρ(l))l∈(L−{q})\nand (vj)j∈J generate the same subspace of E, and the proposition holds for L−{q} and the\nrestriction of the injection ρ : L→ J to L−{q}, since L∩ I = ∅ and |L| = n−m imply that\n(L− {q}) ∩ I = ∅ and |L− {q}| = n− (m+ 1).\n\nThe idea is that m of the vectors vj can be replaced by the linearly independent uis in\nsuch a way that the same subspace is still generated. The purpose of the function ρ : L→ J\nis to pick n −m elements j1, . . . , jn−m of J and to relabel them l1, . . . , ln−m in such a way\nthat these new indices do not clash with the indices in I; this way, the vectors vj1 , . . . , vjn−m\nwho “survive” (i.e. are not replaced) are relabeled vl1 , . . . , vln−m , and the other m vectors vj\nwith j ∈ J −{j1, . . . , jn−m} are replaced by the ui. The index set of this new family is I ∪L.\n\nActually, one can prove that Proposition 3.10 implies Theorem 3.7 when the vector space\nis finitely generated. Putting Theorem 3.7 and Proposition 3.10 together, we obtain the\nfollowing fundamental theorem.\n\nTheorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E\ncontains a subfamily (uj)j∈J which is a basis of E. Any linearly independent family (ui)i∈I\ncan be extended to a family (uj)j∈J which is a basis of E (with I ⊆ J). Furthermore, for\nevery two bases (ui)i∈I and (vj)j∈J of E, we have |I| = |J | = n for some fixed integer n ≥ 0.\n\nProof. The first part follows immediately by applying Theorem 3.7 with L = ∅ and S =\n(ui)i∈I . For the second part, consider the family S ′ = (ui)i∈I ∪ (vh)h∈H , where (vh)h∈H is any\nfinitely generated family generating E, and with I ∩ H = ∅. Then apply Theorem 3.7 to\nL = (ui)i∈I and to S ′. For the last statement, assume that (ui)i∈I and (vj)j∈J are bases of\nE. Since (ui)i∈I is linearly independent and (vj)j∈J spans E, Proposition 3.10 implies that\n|I| ≤ |J |. A symmetric argument yields |J | ≤ |I|.\n\nRemark: Theorem 3.11 also holds for vector spaces that are not finitely generated. This\ncan be shown as follows. Let (ui)i∈I be a basis of E, let (vj)j∈J be a generating family of E,\n\n\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that I is infinite. For every j ∈ J , let Lj ⊆ I be the finite set\n\nLj = {i ∈ I | vj =\n∑\ni∈I\n\nλiui, λi 6= 0}.\n\nLet L =\n⋃\nj∈J Lj. By definition L ⊆ I, and since (ui)i∈I is a basis of E, we must have I = L,\n\nsince otherwise (ui)i∈L would be another basis of E, and this would contradict the fact that\n(ui)i∈I is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe Lj are finite, I would be finite. But then, since I =\n\n⋃\nj∈J Lj with J infinite and the Lj\n\nfinite, by a standard result of set theory, |I| ≤ |J |. If (vj)j∈J is also a basis, by a symmetric\nargument, we obtain |J | ≤ |I|, and thus, |I| = |J | for any two bases (ui)i∈I and (vj)j∈J of E.\n\nDefinition 3.8. When a vector space E is not finitely generated, we say that E is of infinite\ndimension. The dimension of a finitely generated vector space E is the common dimension\nn of all of its bases and is denoted by dim(E).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K\nand a 6= 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n ≥ 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n−1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (ui)i∈I be a basis of a vector space E. For any vector v ∈ E, since the family (ui)i∈I\ngenerates E, there is a family (λi)i∈I of scalars in K, such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nA very important fact is that the family (λi)i∈I is unique.\n\nProposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E,\nand assume that v =\n\n∑\ni∈I λiui. Then the family (λi)i∈I of scalars such that v =\n\n∑\ni∈I λiui\n\nis unique iff (ui)i∈I is linearly independent.\n\nProof. First, assume that (ui)i∈I is linearly independent. If (µi)i∈I is another family of scalars\nin K such that v =\n\n∑\ni∈I µiui, then we have∑\n\ni∈I\n(λi − µi)ui = 0,\n\nand since (ui)i∈I is linearly independent, we must have λi−µi = 0 for all i ∈ I, that is, λi = µi\nfor all i ∈ I. The converse is shown by contradiction. If (ui)i∈I was linearly dependent, there\nwould be a family (µi)i∈I of scalars not all null such that∑\n\ni∈I\nµiui = 0\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that J is infinite. For every 7 € J, let L; C I be the finite set\n\nL; = {i el | vj = Sodus, Ai A OF-\ntel\nLet L= Uses L,;. By definition L C I, and since (u;);er is a basis of E, we must have J = L,\nsince otherwise (u;);ez would be another basis of FE’, and this would contradict the fact that\n(uwi)icr is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe L; are finite, J would be finite. But then, since J = U,-; £; with J infinite and the L;\nfinite, by a standard result of set theory, || < |J|. If (v;)j;e, is also a basis, by a symmetric\nargument, we obtain |J| < ||, and thus, |/| = |J| for any two bases (u;)ie7 and (v;)je7 of E.\n\nDefinition 3.8. When a vector space F is not finitely generated, we say that F is of infinite\ndimension. The dimension of a finitely generated vector space FE is the common dimension\nn of all of its bases and is denoted by dim(£).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a €\nand a # 0 is a basis. Thus dim(/‘) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n > 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n—1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (u;);er be a basis of a vector space EF. For any vector v € E, since the family (w,;)ie7\ngenerates EF’, there is a family (A;);c, of scalars in A’, such that\n\ni€l\nA very important fact is that the family (\\;)icr is unique.\n\nProposition 3.12. Given a vector space E, let (u;);er be a family of vectors in E. Letv € E,\nand assume that v = Yo,-, Ait. Then the family (Aj)ier of scalars such that v = Doi.) iti\nis unique iff (u;)ier ts linearly independent.\n\nwel\n\nProof. First, assume that (u;)jer is linearly independent. If (u;);er is another family of scalars\nin K such that v = }0,-; fii, then we have\n\nYOu = Hi)us = 0,\niel\n\nand since (w,;)ic7 is linearly independent, we must have \\;—/4; = 0 for alli € J, that is, A; = 14;\nfor alli € I. The converse is shown by contradiction. If (u;);¢7; was linearly dependent, there\nwould be a family (ju;);er of scalars not all null such that\n\nwel\n\n\n\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand µj 6= 0 for some j ∈ I. But then,\n\nv =\n∑\ni∈I\n\nλiui + 0 =\n∑\ni∈I\n\nλiui +\n∑\ni∈I\n\nµiui =\n∑\ni∈I\n\n(λi + µi)ui,\n\nwith λj 6= λj+µj since µj 6= 0, contradicting the assumption that (λi)i∈I is the unique family\nsuch that v =\n\n∑\ni∈I λiui.\n\nDefinition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is\nthe unique family of scalars in K such that\n\nv =\n∑\ni∈I\n\nxiui,\n\neach xi is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I .\n\nGiven a field K and any (nonempty) set I, we can form a vector space K(I) which, in\nsome sense, is the standard vector space of dimension |I|.\nDefinition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the\ncartesian product KI consisting of all families (λi)i∈I with finite support of scalars in K.3\n\nWe define addition and multiplication by a scalar as follows:\n\n(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,\n\nand\nλ · (µi)i∈I = (λµi)i∈I .\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K(I) is a vector space. Furthermore, because families with finite support are consid-\nered, the family (ei)i∈I of vectors ei, defined such that (ei)j = 0 if j 6= i and (ei)i = 1, is\nclearly a basis of the vector space K(I). When I = {1, . . . , n}, we denote K(I) by Kn. The\nfunction ι : I → K(I), such that ι(i) = ei for every i ∈ I, is clearly an injection.\n\n� When I is a finite set, K(I) = KI , but this is false when I is infinite. In fact, dim(K(I)) =\n|I|, but dim(KI) is strictly greater when I is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where KI denotes the set of all functions from I to K.\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand yu; #0 for some j € J. But then,\n\niel iel ie] ie]\nwith \\; A A; +; since wu; A 0, contradicting the assumption that (A;);er is the unique family\nsuch that v = So,e; Aiti- O\n\nDefinition 3.10. If (u;)jc7 is a basis of a vector space FE, for any vector v € EF, if (x;);e7 is\nthe unique family of scalars in K such that\n\nv= y Tis,\n\ntel\n\neach x; is called the component (or coordinate) of index i of v with respect to the basis (u;)ier-\n\nGiven a field K and any (nonempty) set J, we can form a vector space K“) which, in\nsome sense, is the standard vector space of dimension |J].\n\nDefinition 3.11. Given a field K and any (nonempty) set J, let K“™ be the subset of the\ncartesian product K?‘ consisting of all families (\\;);e¢7 with finite support of scalars in K.°\nWe define addition and multiplication by a scalar as follows:\n\n(Ai)ier + (Mi)ier = (Ai + Midier,\n\nand\nA+ (Mi)ier = (AMi)ier-\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K“ is a vector space. Furthermore, because families with finite support are consid-\nered, the family (e;)ie; of vectors e;, defined such that (e;); = 0 if 7 A i and (e;); = 1, is\nclearly a basis of the vector space K“). When J = {1,...,n}, we denote K“) by K”. The\nfunction 1: [+ K“, such that v(i) = e; for every i € I, is clearly an injection.\n\n© When J is a finite set, K“) = K', but this is false when J is infinite. In fact, dim(K”) =\n|Z|, but dim(K“) is strictly greater when J is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where K! denotes the set of all functions from I to K.\n\n\n\n\n3.6. MATRICES 81\n\nDefinition 3.12. If K = R or K = C, an m×n-matrix over K is a family (ai j)1≤i≤m, 1≤j≤n\nof scalars in K, represented by an array\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nIn the special case where m = 1, we have a row vector , represented by\n\n(a1 1 · · · a1n)\n\nand in the special case where n = 1, we have a column vector , represented bya1 1\n...\n\nam 1\n\n .\n\nIn these last two cases, we usually omit the constant index 1 (first index in case of a row,\nsecond index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K)\nor Mm,n. An n × n-matrix is called a square matrix of dimension n. The set of all square\nmatrices of dimension n is denoted by Mn(K), or Mn.\n\nRemark: As defined, a matrix A = (ai j)1≤i≤m, 1≤j≤n is a family , that is, a function from\n{1, 2, . . . ,m} × {1, 2, . . . , n} to K. As such, there is no reason to assume an ordering on\nthe indices. Thus, the matrix A can be represented in many different ways as an array, by\nadopting different orders for the rows or the columns. However, it is customary (and usually\nconvenient) to assume the natural ordering on the sets {1, 2, . . . ,m} and {1, 2, . . . , n}, and\nto represent A as an array according to this ordering of the rows and columns.\n\nWe define some operations on matrices as follows.\n\nDefinition 3.13. Given two m × n matrices A = (ai j) and B = (bi j), we define their sum\nA+B as the matrix C = (ci j) such that ci j = ai j + bi j; that is,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n+\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbm 1 bm 2 . . . bmn\n\n\n\n=\n\n\na1 1 + b1 1 a1 2 + b1 2 . . . a1n + b1n\n\na2 1 + b2 1 a2 2 + b2 2 . . . a2n + b2n\n...\n\n...\n. . .\n\n...\nam 1 + bm 1 am 2 + bm 2 . . . amn + bmn\n\n .\n\n\n\n82 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nFor any matrix A = (ai j), we let −A be the matrix (−ai j). Given a scalar λ ∈ K, we define\nthe matrix λA as the matrix C = (ci j) such that ci j = λai j; that is\n\nλ\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n =\n\n\nλa1 1 λa1 2 . . . λa1n\n\nλa2 1 λa2 2 . . . λa2n\n...\n\n...\n. . .\n\n...\nλam 1 λam 2 . . . λamn\n\n .\n\nGiven an m×n matrices A = (ai k) and an n× p matrices B = (bk j), we define their product\nAB as the m× p matrix C = (ci j) such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nb1 1 b1 2 . . . b1 p\n\nb2 1 b2 2 . . . b2 p\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bn p\n\n =\n\n\nc1 1 c1 2 . . . c1 p\n\nc2 1 c2 2 . . . c2 p\n...\n\n...\n. . .\n\n...\ncm 1 cm 2 . . . cmp\n\n ,\n\nnote that the entry of index i and j of the matrix AB obtained by multiplying the matrices\nA and B can be identified with the product of the row matrix corresponding to the i-th row\nof A with the column matrix corresponding to the j-column of B:\n\n(ai 1 · · · ai n)\n\nb1 j\n...\nbn j\n\n =\nn∑\nk=1\n\nai kbk j.\n\nDefinition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0\neverywhere else is called the identity matrix . It is denoted by\n\nIn =\n\n\n1 0 . . . 0\n0 1 . . . 0\n...\n\n...\n. . .\n\n...\n0 0 . . . 1\n\n\nDefinition 3.15. Given an m × n matrix A = (ai j), its transpose A> = (a>j i), is the\nn×m-matrix such that a>j i = ai j, for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n.\n\nThe transpose of a matrix A is sometimes denoted by At, or even by tA. Note that the\ntranspose A> of a matrix A has the property that the j-th row of A> is the j-th column of\n\n\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5× 6 matrix\n\nA =\n\n\n1 2 3 4 5 6\n7 1 2 3 4 5\n8 7 1 2 3 4\n9 8 7 1 2 3\n10 9 8 7 1 2\n\n ,\n\nthen A> is the 6× 5 matrix\n\nA> =\n\n\n1 7 8 9 10\n2 1 7 8 9\n3 2 1 7 8\n4 3 2 1 7\n5 4 3 2 1\n6 5 4 3 2\n\n .\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm× n matrix A and any n× p matrix B, if we denote the columns of A by A1, . . . , An and\nthe rows of B by B1, . . . , Bn, then we have\n\nAB = A1B1 + · · ·+ AnBn.\n\nFor every square matrix A of dimension n, it is immediately verified that AIn = InA = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A−1. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = I, or a right\ninverse, that is a matrix C such that AC = I, then A is actually invertible; so B = A−1 and\nC = A−1. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set Mm,n(K) of m×n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m × n-matrices Eij = (eh k), are defined such that ei j = 1, and\neh k = 0, if h 6= i or k 6= j; in other words, the (i, j)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the Eij matrices for m = 2 and n = 3:\n\nE11 =\n\n(\n1 0 0\n0 0 0\n\n)\n, E12 =\n\n(\n0 1 0\n0 0 0\n\n)\n, E13 =\n\n(\n0 0 1\n0 0 0\n\n)\nE21 =\n\n(\n0 0 0\n1 0 0\n\n)\n, E22 =\n\n(\n0 0 0\n0 1 0\n\n)\n, E23 =\n\n(\n0 0 0\n0 0 1\n\n)\n.\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5 x 6 matrix\n\n1 23 4 5 6\n\n7 12 3 4 5\nA=/]8 712 3 4],\n\n9 8 712 8\n\n1098 7 1 2\n\nthen A! is the 6 x 5 matrix\n\n1 7 8 9 10\n\n2178 9\n\nTt 3.217 8\n\nA l43217\n\n5 43 2 1\n\n65 4 3 2\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm Xn matrix A and any n Xx p matrix B, if we denote the columns of A by A!,..., A” and\nthe rows of B by B,,..., By, then we have\n\nAB = A'B, +---+A\"B,.\nFor every square matrix A of dimension n, it is immediately verified that Al, = [,A = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = I, exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A+. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = J, or a right\ninverse, that is a matrix C such that AC = J, then A is actually invertible; so B = A~' and\nC= A7!. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set My,,(J¢) of m x n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m x n-matrices E;; = (en,), are defined such that e;; = 1, and\neng = 0, if h A7 or k ¥ J; in other words, the (7, 7)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the £;; matrices for m = 2 and n = 3:\n\n1 0 0 0 1 0 0\nEy — (; ) ’ Ex, — (; 0 ) ’ E\\3 —_ (;\n0 0 0 0 0 0\nEo, — (; ) ’ Dey) — (j 1 ) ’ Eo —_ (\n\noOo oe\nor\nNW\n\noOo GO\n\n\n\n\n84 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is clear that every matrix A = (ai j) ∈ Mm,n(K) can be written in a unique way as\n\nA =\nm∑\ni=1\n\nn∑\nj=1\n\nai jEij.\n\nThus, the family (Eij)1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension\nmn.\n\nRemark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (com-\nmutative) ring rather than a field. In this more general setting, the framework of vector\nspaces is too narrow, but we can consider structures over a commutative ring A satisfying\nall the axioms of Definition 3.1. Such structures are called modules . The theory of modules\nis (much) more complicated than that of vector spaces. For example, modules do not always\nhave a basis, and other properties holding for vector spaces usually fail for modules. When\na module has a basis, it is called a free module. For example, when A is a commutative\nring, the structure An is a module such that the vectors ei, with (ei)i = 1 and (ei)j = 0 for\nj 6= i, form a basis of An. Many properties of vector spaces still hold for An. Thus, An is a\nfree module. As another example, when A is a commutative ring, Mm,n(A) is a free module\nwith basis (Ei,j)1≤i≤m,1≤j≤n. Polynomials over a commutative ring also form a free module\nof infinite dimension.\n\nThe properties listed in Proposition 3.13 are easily verified, although some of the com-\nputations are a bit tedious. A more conceptual proof is given in Proposition 4.1.\n\nProposition 3.13. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nThe properties of Proposition 3.13 together with the fact that AIn = InA = A for all\nsquare n×n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).\nThis is a noncommutative ring with zero divisors, as shown by the following example.\n\n\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A,B be the 2× 2-matrices\n\nA =\n\n(\n1 0\n0 0\n\n)\n, B =\n\n(\n0 0\n1 0\n\n)\n,\n\nthen\n\nAB =\n\n(\n1 0\n0 0\n\n)(\n0 0\n1 0\n\n)\n=\n\n(\n0 0\n0 0\n\n)\n,\n\nand\n\nBA =\n\n(\n0 0\n1 0\n\n)(\n1 0\n0 0\n\n)\n=\n\n(\n0 0\n1 0\n\n)\n.\n\nThus AB 6= BA, and AB = 0, even though both A,B 6= 0.\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F . A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\n\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces E and F , a linear map between E and F is a\nfunction f : E → F satisfying the following two conditions:\n\nf(x+ y) = f(x) + f(y) for all x, y ∈ E;\n\nf(λx) = λf(x) for all λ ∈ K, x ∈ E.\n\nSetting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui).\n\nThe above identity is shown by induction on |I| using the properties of Definition 3.18.\n\nExample 3.6.\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A, B be the 2 x 2-matrices\n1 0 0 0\n4=(0o)- (0):\n1 0\\ /0 O 0 0\nap=(5 (Co) = (0 0):\n0 0\\ /1l 0 0 0\npa=(1 a) (0.0) =( 0):\n\nThus AB 4 BA, and AB = 0, even though both A, B ¥ 0.\n\nthen\n\nand\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F’. A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces EF’ and F’, a linear map between FE and F is a\nfunction f: EF — F satisfying the following two conditions:\n\nf(at+y) = f(x) + f(y) for all x,y € E;\nf(Ar) = Af (x) forallAc Kk, xe LE.\n\nSetting x = y = 0 in the first identity, we get f(0) =0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(u;)icr of vectors in EL, given any family (A;)ier of scalars in K, we have\n\nThe above identity is shown by induction on |/| using the properties of Definition 3.18.\n\nExample 3.6.\n\n\n\n\n86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n1. The map f : R2 → R2 defined such that\n\nx′ = x− y\ny′ = x+ y\n\nis a linear map. The reader should check that it is the composition of a rotation by\nπ/4 with a magnification of ratio\n\n√\n2.\n\n2. For any vector space E, the identity map id : E → E given by\n\nid(u) = u for all u ∈ E\n\nis a linear map. When we want to be more precise, we write idE instead of id.\n\n3. The map D : R[X]→ R[X] defined such that\n\nD(f(X)) = f ′(X),\n\nwhere f ′(X) is the derivative of the polynomial f(X), is a linear map.\n\n4. The map Φ: C([a, b])→ R given by\n\nΦ(f) =\n\n∫ b\n\na\n\nf(t)dt,\n\nwhere C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear\nmap.\n\n5. The function 〈−,−〉 : C([a, b])× C([a, b])→ R given by\n\n〈f, g〉 =\n\n∫ b\n\na\n\nf(t)g(t)dt,\n\nis linear in each of the variable f , g. It also satisfies the properties 〈f, g〉 = 〈g, f〉 and\n〈f, f〉 = 0 iff f = 0. It is an example of an inner product .\n\nDefinition 3.19. Given a linear map f : E → F , we define its image (or range) Im f = f(E),\nas the set\n\nIm f = {y ∈ F | (∃x ∈ E)(y = f(x))},\n\nand its Kernel (or nullspace) Ker f = f−1(0), as the set\n\nKer f = {x ∈ E | f(x) = 0}.\n\n86\n\nCHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n. The map f: R? > R? defined such that\n\n/\n\nty\n\n/\n\nis a linear map. The reader should check that it is the composition of a rotation by\nx/4 with a magnification of ratio V2.\n\n. For any vector space E, the identity map id: E > E given by\n\nid(u) =u foralue E\n\nis a linear map. When we want to be more precise, we write idg instead of id.\n\n. The map D: R[X] — R[X] defined such that\n\nwhere f’(X) is the derivative of the polynomial f(X), is a linear map.\n\n. The map ®: C({a,b]) > R given by\n\na(f) = / f(t)at,\n\nwhere C(|a, b]) is the set of continuous functions defined on the interval |a, b], is a linear\nmap.\n\n. The function (—, —): C([a,b]) x C([a, b]) > R given by\n\n(f,9) = / f(t)g(t)dt,\n\nis linear in each of the variable f, g. It also satisfies the properties (f,g) = (g, f) and\n(f, f) = 0 iff f = 0. It is an example of an inner product.\n\nDefinition 3.19. Given a linear map f: EF — F’, we define its image (or range) Im f = f(F),\nas the set\n\nIm f= {ty € F'| (Se € E\\(y = f(2))},\n\nand its Kernel (or nullspace) Ker f = f~'(0), as the set\n\nKer f = {x € E | f(x) =O}.\n\n\n\n\n3.7. LINEAR MAPS 87\n\nThe derivative map D : R[X] → R[X] from Example 3.6(3) has kernel the constant\npolynomials, so KerD = R. If we consider the second derivative D ◦D : R[X]→ R[X], then\nthe kernel of D ◦D consists of all polynomials of degree ≤ 1. The image of D : R[X]→ R[X]\nis actually R[X] itself, because every polynomial P (X) = a0X\n\nn + · · ·+ an−1X + an of degree\nn is the derivative of the polynomial Q(X) of degree n+ 1 given by\n\nQ(X) = a0\nXn+1\n\nn+ 1\n+ · · ·+ an−1\n\nX2\n\n2\n+ anX.\n\nOn the other hand, if we consider the restriction of D to the vector space R[X]n of polyno-\nmials of degree ≤ n, then the kernel of D is still R, but the image of D is the R[X]n−1, the\nvector space of polynomials of degree ≤ n− 1.\n\nProposition 3.14. Given a linear map f : E → F , the set Im f is a subspace of F and the\nset Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0) (where\n(0) is the trivial subspace {0}).\n\nProof. Given any x, y ∈ Im f , there are some u, v ∈ E such that x = f(u) and y = f(v),\nand for all λ, µ ∈ K, we have\n\nf(λu+ µv) = λf(u) + µf(v) = λx+ µy,\n\nand thus, λx+ µy ∈ Im f , showing that Im f is a subspace of F .\n\nGiven any x, y ∈ Ker f , we have f(x) = 0 and f(y) = 0, and thus,\n\nf(λx+ µy) = λf(x) + µf(y) = 0,\n\nthat is, λx+ µy ∈ Ker f , showing that Ker f is a subspace of E.\n\nFirst, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.\nHowever, if f(x) = f(y), then f(x) − f(y) = 0, and by linearity of f we get f(x − y) = 0.\nBecause Ker f = (0), we must have x − y = 0, that is x = y, so f is injective. Conversely,\nassume that f is injective. If x ∈ Ker f , that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),\nand by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff\nKer f = (0).\n\nSince by Proposition 3.14, the image Im f of a linear map f is a subspace of F , we can\ndefine the rank rk(f) of f as the dimension of Im f .\n\nDefinition 3.20. Given a linear map f : E → F , the rank rk(f) of f is the dimension of\nthe image Im f of f .\n\nA fundamental property of bases in a vector space is that they allow the definition of\nlinear maps as unique homomorphic extensions, as shown in the following proposition.\n\n\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F , given any basis (ui)i∈I of E,\ngiven any other family of vectors (vi)i∈I in F , there is a unique linear map f : E → F such\nthat f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent,\nand f is surjective iff (vi)i∈I generates F .\n\nProof. If such a linear map f : E → F exists, since (ui)i∈I is a basis of E, every vector x ∈ E\ncan written uniquely as a linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nDefine the function f : E → F , by letting\n\nf(x) =\n∑\ni∈I\n\nxivi\n\nfor every x =\n∑\n\ni∈I xiui. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(ui) = vi.\n\nNow assume that f is injective. Let (λi)i∈I be any family of scalars, and assume that∑\ni∈I\n\nλivi = 0.\n\nSince vi = f(ui) for every i ∈ I, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui) =\n∑\ni∈I\n\nλivi = 0.\n\nSince f is injective iff Ker f = (0), we have∑\ni∈I\n\nλiui = 0,\n\nand since (ui)i∈I is a basis, we have λi = 0 for all i ∈ I, which shows that (vi)i∈I is linearly\nindependent. Conversely, assume that (vi)i∈I is linearly independent. Since (ui)i∈I is a basis\nof E, every vector x ∈ E is a linear combination x =\n\n∑\ni∈I λiui of (ui)i∈I . If\n\nf(x) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nthen ∑\ni∈I\n\nλivi =\n∑\ni∈I\n\nλif(ui) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nand λi = 0 for all i ∈ I because (vi)i∈I is linearly independent, which means that x = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise.\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F’, given any basis (u;)icr of E,\ngiven any other family of vectors (v;)icr in F, there is a unique linear map f: E > F such\nthat f(u;) = v; for alli € I. Furthermore, f is injective iff (vi)ier is linearly independent,\nand f is surjective iff (vi)icr generates F.\n\nProof. If such a linear map f: E — F exists, since (u;);c7 is a basis of E, every vector x € E\ncan written uniquely as a linear combination\n\nv= S- VyUj;,\nwel\nand by linearity, we must have\nf(z) = S- aif (ui) = S- LjUi-\niel i€l\nDefine the function f: E > F, by letting\nf(x) = Ss” LiVj\nwel\nfor every © = )0,-, iu. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(u;) = vu.\nNow assume that f is injective. Let (\\;)jcr be any family of scalars, and assume that\ni€l\nSince uv; = f(u;) for every i € I, we have\nfO, AjUi) = S- Af (ui) = S° Ai; = 0.\nie] i€l ier\nSince f is injective iff Ker f = (0), we have\nS- Aju = 0,\ni€l\nand since (wu;)ier is a basis, we have A; = 0 for all i € J, which shows that (v;)jer is linearly\n\nindependent. Conversely, assume that (v;);c7 is linearly independent. Since (u;);<r is a basis\nof E, every vector x € F is a linear combination « = )0,., Aiui of (u)ier. If\n\nf(z) = 0D Aju) = 9,\n\nthen\n\nSs\" Avi = S- Ai f (ui) = f>~ Aju) = 0,\n\niel iel ier\nand A; = 0 for all i € I because (v;)jer is linearly independent, which means that x7 = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise. im\n\n\n\n\n3.7. LINEAR MAPS 89\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nu = (0,0,1)\n3 v = (1,1)1v = (-1,1)\n\n2\n\nv = (1,0)\n3\n\nf(u )1\nf(u )\n\n2\n-\n\n2f(u  )3\n\nE = \n\nf\n\nF =\nR\n\nR\n2\n\n3\n\nf is not injective\n\ndefining f\n\nFigure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),\nv3 = (1, 0), define the unique linear map f : R3 → R2 by f(u1) = v1, f(u2) = v2, and\nf(u3) = v3. This map is surjective but not injective since f(u1 − u2) = f(u1) − f(u2) =\n(1, 1)− (−1, 1) = (2, 0) = 2f(u3) = f(2u3).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R3 and V = R2\n\nBy the second part of Proposition 3.15, an injective linear map f : E → F sends a basis\n(ui)i∈I to a linearly independent family (f(ui))i∈I of F , which is also a basis when f is\nbijective. Also, when E and F have the same finite dimension n, (ui)i∈I is a basis of E, and\nf : E → F is injective, then (f(ui))i∈I is a basis of F (by Proposition 3.8).\n\nWe can now show that the vector space K(I) of Definition 3.11 has a universal property\nthat amounts to saying that K(I) is the vector space freely generated by I. Recall that\nι : I → K(I), such that ι(i) = ei for every i ∈ I, is an injection from I to K(I).\n\nProposition 3.16. Given any set I, for any vector space F , and for any function f : I → F ,\nthere is a unique linear map f : K(I) → F , such that\n\nf = f ◦ ι,\n\nas in the following diagram:\n\nI ι //\n\nf !!CCCCCCCCC K(I)\n\nf\n��\nF\n\nProof. If such a linear map f : K(I) → F exists, since f = f ◦ ι, we must have\n\nf(i) = f(ι(i)) = f(ei),\n\n3.7. LINEAR MAPS 89\n\ndefining f\n\n2f(u3)\n\nfis not injective\n\nFigure 3.11: Given wu; = (1,0,0), w2 = (0,1,0), us = (0,0,1) and vy; = (1,1), ve = (-1,1),\nv3 = (1,0), define the unique linear map f: R? — R? by f(u1) = v1, flue) = vo, and\nf(u3) = v3. This map is surjective but not injective since f(u; — u2) = f(u1) — f(ue) =\n\n(1, 1) _ (-1, 1) = (2, 0) = 2 f (us) = f(2us).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R? and V = R?\n\nBy the second part of Proposition 3.15, an injective linear map f: E — F sends a basis\n(u;)icr to a linearly independent family (f(wu;))icr of F', which is also a basis when f is\nbijective. Also, when F and F have the same finite dimension n, (u;);ey is a basis of E', and\nf: E > F is injective, then (f(ui))ier is a basis of F' (by Proposition 3.8).\n\nWe can now show that the vector space K™) of Definition 3.11 has a universal property\nthat amounts to saying that AK“) is the vector space freely generated by J. Recall that\nu: I + K“, such that «(i) = e; for every i € I, is an injection from I to KY),\n\nProposition 3.16. Given any set I, for any vector space F’, and for any function f: I > F,\nthere is a unique linear map f: K\\“) — F, such that\n\nf=fou,\n\nas in the following diagram:\n\nProof. If such a linear map f: K“ > F exists, since f = f ov, we must have\n\nF(t) = Fu) = Fei),\n\n\n\n\n90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nfor every i ∈ I. However, the family (ei)i∈I is a basis of K(I), and (f(i))i∈I is a family of\nvectors in F , and by Proposition 3.15, there is a unique linear map f : K(I) → F such that\nf(ei) = f(i) for every i ∈ I, which proves the existence and uniqueness of a linear map f\nsuch that f = f ◦ ι.\n\nThe following simple proposition is also useful.\n\nProposition 3.17. Given any two vector spaces E and F , with F nontrivial, given any\nfamily (ui)i∈I of vectors in E, the following properties hold:\n\n(1) The family (ui)i∈I generates E iff for every family of vectors (vi)i∈I in F , there is at\nmost one linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\n(2) The family (ui)i∈I is linearly independent iff for every family of vectors (vi)i∈I in F ,\nthere is some linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\nProof. (1) If there is any linear map f : E → F such that f(ui) = vi for all i ∈ I, since\n(ui)i∈I generates E, every vector x ∈ E can be written as some linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nThis shows that f is unique if it exists. Conversely, assume that (ui)i∈I does not generate E.\nSince F is nontrivial, there is some some vector y ∈ F such that y 6= 0. Since (ui)i∈I does\nnot generate E, there is some vector w ∈ E that is not in the subspace generated by (ui)i∈I .\nBy Theorem 3.11, there is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the\nsame subspace. Since by hypothesis, w ∈ E is not in the subspace generated by (ui)i∈I0 , by\nLemma 3.6 and by Theorem 3.11 again, there is a basis (ej)j∈I0∪J of E, such that ei = ui\nfor all i ∈ I0, and w = ej0 for some j0 ∈ J . Letting (vi)i∈I be the family in F such that\nvi = 0 for all i ∈ I, defining f : E → F to be the constant linear map with value 0, we have\na linear map such that f(ui) = 0 for all i ∈ I. By Proposition 3.15, there is a unique linear\nmap g : E → F such that g(w) = y, and g(ej) = 0 for all j ∈ (I0 ∪ J)− {j0}. By definition\nof the basis (ej)j∈I0∪J of E, we have g(ui) = 0 for all i ∈ I, and since f 6= g, this contradicts\nthe fact that there is at most one such map. See Figure 3.12.\n\n(2) If the family (ui)i∈I is linearly independent, then by Theorem 3.11, (ui)i∈I can be\nextended to a basis of E, and the conclusion follows by Proposition 3.15. Conversely, assume\nthat (ui)i∈I is linearly dependent. Then there is some family (λi)i∈I of scalars (not all zero)\nsuch that ∑\n\ni∈I\nλiui = 0.\n\n\n\n3.7. LINEAR MAPS 91\nf\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nw = (0,0,1)\n\nw = (0,0,1)\n\ndefining f as the zero\n\ndefining g\ny = (1,0)\n\ng(w) = y\n\nFigure 3.12: Let E = R3 and F = R2. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not\ngenerate R3 since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach\nxy-plane to the origin.\n\nBy the assumption, for any nonzero vector y ∈ F , for every i ∈ I, there is some linear map\nfi : E → F , such that fi(ui) = y, and fi(uj) = 0, for j ∈ I − {i}. Then we would get\n\n0 = fi(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλifi(ui) = λiy,\n\nand since y 6= 0, this implies λi = 0 for every i ∈ I. Thus, (ui)i∈I is linearly independent.\n\nGiven vector spaces E, F , and G, and linear maps f : E → F and g : F → G, it is easily\nverified that the composition g ◦ f : E → G of f and g is a linear map.\n\nDefinition 3.21. A linear map f : E → F is an isomorphism iff there is a linear map\ng : F → E, such that\n\ng ◦ f = idE and f ◦ g = idF . (∗)\n\nThe map g in Definition 3.21 is unique. This is because if g and h both satisfy g◦f = idE,\nf ◦ g = idF , h ◦ f = idE, and f ◦ h = idF , then\n\ng = g ◦ idF = g ◦ (f ◦ h) = (g ◦ f) ◦ h = idE ◦ h = h.\n\nThe map g satisfying (∗) above is called the inverse of f and it is also denoted by f−1.\n\n\n\n92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nObserve that Proposition 3.15 shows that if F = Rn, then we get an isomorphism between\nany vector space E of dimension |J | = n and Rn. Proposition 3.15 also implies that if E\nand F are two vector spaces, (ui)i∈I is a basis of E, and f : E → F is a linear map which is\nan isomorphism, then the family (f(ui))i∈I is a basis of F .\n\nOne can verify that if f : E → F is a bijective linear map, then its inverse f−1 : F → E,\nas a function, is also a linear map, and thus f is an isomorphism.\n\nAnother useful corollary of Proposition 3.15 is this:\n\nProposition 3.18. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be\nany linear map. The following properties hold:\n\n(1) If f has a left inverse g, that is, if g is a linear map such that g ◦ f = id, then f is an\nisomorphism and f−1 = g.\n\n(2) If f has a right inverse h, that is, if h is a linear map such that f ◦ h = id, then f is\nan isomorphism and f−1 = h.\n\nProof. (1) The equation g ◦ f = id implies that f is injective; this is a standard result\nabout functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since\ng ◦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.15, since f is injective,\n(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of\nE (if (f(u1), . . . , f(un)) doesn’t span E, then it can be extended to a basis of dimension\nstrictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous\nobservation its inverse is a linear map. We also have\n\ng = g ◦ id = g ◦ (f ◦ f−1) = (g ◦ f) ◦ f−1 = id ◦ f−1 = f−1.\n\n(2) The equation f ◦ h = id implies that f is surjective; this is a standard result about\nfunctions (for any y ∈ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By\nProposition 3.15, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension\nn, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans\nE, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).\nThen f is bijective, and by a previous observation its inverse is a linear map. We also have\n\nh = id ◦ h = (f−1 ◦ f) ◦ h = f−1 ◦ (f ◦ h) = f−1 ◦ id = f−1.\n\nThis completes the proof.\n\nDefinition 3.22. The set of all linear maps between two vector spaces E and F is denoted by\nHom(E,F ) or by L(E;F ) (the notation L(E;F ) is usually reserved to the set of continuous\nlinear maps, where E and F are normed vector spaces). When we wish to be more precise and\nspecify the field K over which the vector spaces E and F are defined we write HomK(E,F ).\n\n\n\n3.8. QUOTIENT SPACES 93\n\nThe set Hom(E,F ) is a vector space under the operations defined in Example 3.1, namely\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ E, and\n(λf)(x) = λf(x)\n\nfor all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which\nuses the commutativity of ∗ in the field K (typically, K = R or K = C). Indeed, we have\n\n(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).\n\nWhen E and F have finite dimensions, the vector space Hom(E,F ) also has finite di-\nmension, as we shall see shortly.\n\nDefinition 3.23. When E = F , a linear map f : E → E is also called an endomorphism.\nThe space Hom(E,E) is also denoted by End(E).\n\nIt is also important to note that composition confers to Hom(E,E) a ring structure.\nIndeed, composition is an operation ◦ : Hom(E,E) × Hom(E,E) → Hom(E,E), which is\nassociative and has an identity idE, and the distributivity properties hold:\n\n(g1 + g2) ◦ f = g1 ◦ f + g2 ◦ f ;\n\ng ◦ (f1 + f2) = g ◦ f1 + g ◦ f2.\n\nThe ring Hom(E,E) is an example of a noncommutative ring.\n\nIt is easily seen that the set of bijective linear maps f : E → E is a group under compo-\nsition.\n\nDefinition 3.24. Bijective linear maps f : E → E are also called automorphisms . The\ngroup of automorphisms of E is called the general linear group (of E), and it is denoted by\nGL(E), or by Aut(E), or when E = Rn, by GL(n,R), or even by GL(n).\n\nAlthough in this book, we will not have many occasions to use quotient spaces, they are\nfundamental in algebra. The next section may be omitted until needed.\n\n3.8 Quotient Spaces\n\nLet E be a vector space, and let M be any subspace of E. The subspace M induces a relation\n≡M on E, defined as follows: For all u, v ∈ E,\n\nu ≡M v iff u− v ∈M .\n\nWe have the following simple proposition.\n\n\n\n94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.19. Given any vector space E and any subspace M of E, the relation ≡M\nis an equivalence relation with the following two congruential properties:\n\n1. If u1 ≡M v1 and u2 ≡M v2, then u1 + u2 ≡M v1 + v2, and\n\n2. if u ≡M v, then λu ≡M λv.\n\nProof. It is obvious that ≡M is an equivalence relation. Note that u1 ≡M v1 and u2 ≡M v2\n\nare equivalent to u1 − v1 = w1 and u2 − v2 = w2, with w1, w2 ∈M , and thus,\n\n(u1 + u2)− (v1 + v2) = w1 + w2,\n\nand w1 + w2 ∈ M , since M is a subspace of E. Thus, we have u1 + u2 ≡M v1 + v2. If\nu− v = w, with w ∈M , then\n\nλu− λv = λw,\n\nand λw ∈M , since M is a subspace of E, and thus λu ≡M λv.\n\nProposition 3.19 shows that we can define addition and multiplication by a scalar on the\nset E/M of equivalence classes of the equivalence relation ≡M .\n\nDefinition 3.25. Given any vector space E and any subspaceM of E, we define the following\noperations of addition and multiplication by a scalar on the set E/M of equivalence classes\nof the equivalence relation ≡M as follows: for any two equivalence classes [u], [v] ∈ E/M , we\nhave\n\n[u] + [v] = [u+ v],\n\nλ[u] = [λu].\n\nBy Proposition 3.19, the above operations do not depend on the specific choice of represen-\ntatives in the equivalence classes [u], [v] ∈ E/M . It is also immediate to verify that E/M is\na vector space. The function π : E → E/F , defined such that π(u) = [u] for every u ∈ E, is\na surjective linear map called the natural projection of E onto E/F . The vector space E/M\nis called the quotient space of E by the subspace M .\n\nGiven any linear map f : E → F , we know that Ker f is a subspace of E, and it is\nimmediately verified that Im f is isomorphic to the quotient space E/Ker f .\n\n3.9 Linear Forms and the Dual Space\n\nWe already observed that the field K itself (K = R or K = C) is a vector space (over itself).\nThe vector space Hom(E,K) of linear maps from E to the field K, the linear forms, plays\na particular role. In this section, we only define linear forms and show that every finite-\ndimensional vector space has a dual basis. A more advanced presentation of dual spaces and\nduality is given in Chapter 11.\n\n\n\n3.9. LINEAR FORMS AND THE DUAL SPACE 95\n\nDefinition 3.26. Given a vector space E, the vector space Hom(E,K) of linear maps from\nE to the field K is called the dual space (or dual) of E. The space Hom(E,K) is also denoted\nby E∗, and the linear maps in E∗ are called the linear forms , or covectors . The dual space\nE∗∗ of the space E∗ is called the bidual of E.\n\nAs a matter of notation, linear forms f : E → K will also be denoted by starred symbol,\nsuch as u∗, x∗, etc.\n\nIf E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear\nform f ∗ ∈ E∗, for every x = x1u1 + · · ·+ xnun ∈ E, by linearity we have\n\nf ∗(x) = f ∗(u1)x1 + · · ·+ f ∗(un)xn\n\n= λ1x1 + · · ·+ λnxn,\n\nwith λi = f ∗(ui) ∈ K for every i, 1 ≤ i ≤ n. Thus, with respect to the basis (u1, . . . , un),\nthe linear form f ∗ is represented by the row vector\n\n(λ1 · · · λn),\n\nwe have\n\nf ∗(x) =\n(\nλ1 · · · λn\n\n)x1\n...\nxn\n\n ,\n\na linear combination of the coordinates of x, and we can view the linear form f ∗ as a linear\nequation. If we decide to use a column vector of coefficients\n\nc =\n\nc1\n...\ncn\n\n\ninstead of a row vector, then the linear form f ∗ is defined by\n\nf ∗(x) = c>x.\n\nThe above notation is often used in machine learning.\n\nExample 3.7. Given any differentiable function f : Rn → R, by definition, for any x ∈ Rn,\nthe total derivative dfx of f at x is the linear form dfx : Rn → R defined so that for all\nu = (u1, . . . , un) ∈ Rn,\n\ndfx(u) =\n\n(\n∂f\n\n∂x1\n\n(x) · · · ∂f\n\n∂xn\n(x)\n\n)u1\n...\nun\n\n =\nn∑\ni=1\n\n∂f\n\n∂xi\n(x)ui.\n\n\n\n96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nExample 3.8. Let C([0, 1]) be the vector space of continuous functions f : [0, 1]→ R. The\nmap I : C([0, 1])→ R given by\n\nI(f) =\n\n∫ 1\n\n0\n\nf(x)dx for any f ∈ C([0, 1])\n\nis a linear form (integration).\n\nExample 3.9. Consider the vector space Mn(R) of real n×n matrices. Let tr : Mn(R)→ R\nbe the function given by\n\ntr(A) = a11 + a22 + · · ·+ ann,\n\ncalled the trace of A. It is a linear form. Let s : Mn(R)→ R be the function given by\n\ns(A) =\nn∑\n\ni,j=1\n\naij,\n\nwhere A = (aij). It is immediately verified that s is a linear form.\n\nGiven a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear\nform u∗i ∈ E∗, and the u∗i have some remarkable properties.\n\nDefinition 3.27. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.15,\nfor every i ∈ I, there is a unique linear form u∗i such that\n\nu∗i (uj) =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor every j ∈ I. The linear form u∗i is called the coordinate form of index i w.r.t. the basis\n(ui)i∈I .\n\nRemark: Given an index set I, authors often define the so called “Kronecker symbol” δi j\nsuch that\n\nδi j =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor all i, j ∈ I. Then, u∗i (uj) = δi j.\n\nThe reason for the terminology coordinate form is as follows: If E has finite dimension\nand if (u1, . . . , un) is a basis of E, for any vector\n\nv = λ1u1 + · · ·+ λnun,\n\nwe have\n\nu∗i (v) = u∗i (λ1u1 + · · ·+ λnun)\n\n= λ1u\n∗\ni (u1) + · · ·+ λiu\n\n∗\ni (ui) + · · ·+ λnu\n\n∗\ni (un)\n\n= λi,\n\n\n\n3.10. SUMMARY 97\n\nsince u∗i (uj) = δi j. Therefore, u∗i is the linear function that returns the ith coordinate of a\nvector expressed over the basis (u1, . . . , un).\n\nThe following theorem shows that in finite-dimension, every basis (u1, . . . , un) of a vector\nspace E yields a basis (u∗1, . . . , u\n\n∗\nn) of the dual space E∗, called a dual basis .\n\nTheorem 3.20. (Existence of dual bases) Let E be a vector space of dimension n. The\nfollowing properties hold: For every basis (u1, . . . , un) of E, the family of coordinate forms\n(u∗1, . . . , u\n\n∗\nn) is a basis of E∗ (called the dual basis of (u1, . . . , un)).\n\nProof. (a) If v∗ ∈ E∗ is any linear form, consider the linear form\n\nf ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nObserve that because u∗i (uj) = δi j,\n\nf ∗(ui) = (v∗(u1)u∗1 + · · ·+ v∗(un)u∗n)(ui)\n\n= v∗(u1)u∗1(ui) + · · ·+ v∗(ui)u\n∗\ni (ui) + · · ·+ v∗(un)u∗n(ui)\n\n= v∗(ui),\n\nand so f ∗ and v∗ agree on the basis (u1, . . . , un), which implies that\n\nv∗ = f ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nTherefore, (u∗1, . . . , u\n∗\nn) spans E∗. We claim that the covectors u∗1, . . . , u\n\n∗\nn are linearly inde-\n\npendent. If not, we have a nontrivial linear dependence\n\nλ1u\n∗\n1 + · · ·+ λnu\n\n∗\nn = 0,\n\nand if we apply the above linear form to each ui, using a familar computation, we get\n\n0 = λiu\n∗\ni (ui) = λi,\n\nproving that u∗1, . . . , u\n∗\nn are indeed linearly independent. Therefore, (u∗1, . . . , u\n\n∗\nn) is a basis of\n\nE∗.\n\nIn particular, Theorem 3.20 shows a finite-dimensional vector space and its dual E∗ have\nthe same dimension.\n\n3.10 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The notion of a vector space.\n\n• Families of vectors.\n\n\n\n98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n• Linear combinations of vectors; linear dependence and linear independence of a family\nof vectors.\n\n• Linear subspaces .\n\n• Spanning (or generating) family; generators , finitely generated subspace; basis of a\nsubspace.\n\n• Every linearly independent family can be extended to a basis (Theorem 3.7).\n\n• A family B of vectors is a basis iff it is a maximal linearly independent family iff it is\na minimal generating family (Proposition 3.8).\n\n• The replacement lemma (Proposition 3.10).\n\n• Any two bases in a finitely generated vector space E have the same number of elements ;\nthis is the dimension of E (Theorem 3.11).\n\n• Hyperplanes .\n\n• Every vector has a unique representation over a basis (in terms of its coordinates).\n\n• Matrices\n\n• Column vectors , row vectors .\n\n• Matrix operations : addition, scalar multiplication, multiplication.\n\n• The vector space Mm,n(K) of m × n matrices over the field K; The ring Mn(K) of\nn× n matrices over the field K.\n\n• The notion of a linear map.\n\n• The image Im f (or range) of a linear map f .\n\n• The kernel Ker f (or nullspace) of a linear map f .\n\n• The rank rk(f) of a linear map f .\n\n• The image and the kernel of a linear map are subspaces. A linear map is injective iff\nits kernel is the trivial space (0) (Proposition 3.14).\n\n• The unique homomorphic extension property of linear maps with respect to bases\n(Proposition 3.15 ).\n\n• Quotient spaces .\n\n• The vector space of linear maps HomK(E,F ).\n\n\n\n3.11. PROBLEMS 99\n\n• Linear forms (covectors) and the dual space E∗.\n\n• Coordinate forms.\n\n• The existence of dual bases (in finite dimension).\n\n3.11 Problems\n\nProblem 3.1. Let H be the set of 3× 3 upper triangular matrices given by\n\nH =\n\n\n1 a b\n\n0 1 c\n0 0 1\n\n | a, b, c ∈ R\n\n .\n\n(1) Prove that H with the binary operation of matrix multiplication is a group; find\nexplicitly the inverse of every matrix in H. Is H abelian (commutative)?\n\n(2) Given two groups G1 and G2, recall that a homomorphism if a function ϕ : G1 → G2\n\nsuch that\nϕ(ab) = ϕ(a)ϕ(b), a, b ∈ G1.\n\nProve that ϕ(e1) = e2 (where ei is the identity element of Gi) and that\n\nϕ(a−1) = (ϕ(a))−1, a ∈ G1.\n\n(3) Let S1 be the unit circle, that is\n\nS1 = {eiθ = cos θ + i sin θ | 0 ≤ θ < 2π},\n\nand let ϕ be the function given by\n\nϕ\n\n1 a b\n0 1 c\n0 0 1\n\n = (a, c, eib).\n\nProve that ϕ is a surjective function onto G = R × R × S1, and that if we define\nmultiplication on this set by\n\n(x1, y1, u1) · (x2, y2, u2) = (x1 + x2, y1 + y2, e\nix1y2u1u2),\n\nthen G is a group and ϕ is a group homomorphism from H onto G.\n\n(4) The kernel of a homomorphism ϕ : G1 → G2 is defined as\n\nKer (ϕ) = {a ∈ G1 | ϕ(a) = e2}.\n\nFind explicitly the kernel of ϕ and show that it is a subgroup of H.\n\n\n\n100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.2. For any m ∈ Z with m > 0, the subset mZ = {mk | k ∈ Z} is an abelian\nsubgroup of Z. Check this.\n\n(1) Give a group isomorphism (an invertible homomorphism) from mZ to Z.\n\n(2) Check that the inclusion map i : mZ→ Z given by i(mk) = mk is a group homomor-\nphism. Prove that if m ≥ 2 then there is no group homomorphism p : Z → mZ such that\np ◦ i = id.\n\nRemark: The above shows that abelian groups fail to have some of the properties of vector\nspaces. We will show later that a linear map satisfying the condition p◦ i = id always exists.\n\nProblem 3.3. Let E = R× R, and define the addition operation\n\n(x1, y1) + (x2, y2) = (x1 + x2, y1,+y2), x1, x2, y1, y2 ∈ R,\n\nand the multiplication operation · : R× E → E by\n\nλ · (x, y) = (λx, y), λ, x, y ∈ R.\n\nShow that E with the above operations + and · is not a vector space. Which of the\naxioms is violated?\n\nProblem 3.4. (1) Prove that the axioms of vector spaces imply that\n\nα · 0 = 0\n\n0 · v = 0\n\nα · (−v) = −(α · v)\n\n(−α) · v = −(α · v),\n\nfor all v ∈ E and all α ∈ K, where E is a vector space over K.\n\n(2) For every λ ∈ R and every x = (x1, . . . , xn) ∈ Rn, define λx by\n\nλx = λ(x1, . . . , xn) = (λx1, . . . , λxn).\n\nRecall that every vector x = (x1, . . . , xn) ∈ Rn can be written uniquely as\n\nx = x1e1 + · · ·+ xnen,\n\nwhere ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i. For any operation · : R×Rn →\nRn, if · satisfies the Axiom (V1) of a vector space, then prove that for any α ∈ R, we have\n\nα · x = α · (x1e1 + · · ·+ xnen) = α · (x1e1) + · · ·+ α · (xnen).\n\nConclude that · is completely determined by its action on the one-dimensional subspaces of\nRn spanned by e1, . . . , en.\n\n\n\n3.11. PROBLEMS 101\n\n(3) Use (2) to define operations · : R × Rn → Rn that satisfy the Axioms (V1–V3), but\nfor which Axiom V4 fails.\n\n(4) For any operation · : R×Rn → Rn, prove that if · satisfies the Axioms (V2–V3), then\nfor every rational number r ∈ Q and every vector x ∈ Rn, we have\n\nr · x = r(1 · x).\n\nIn the above equation, 1 · x is some vector (y1, . . . , yn) ∈ Rn not necessarily equal to x =\n(x1, . . . , xn), and\n\nr(1 · x) = (ry1, . . . , ryn),\n\nas in Part (2).\n\nUse (4) to conclude that any operation · : Q×Rn → Rn that satisfies the Axioms (V1–V3)\nis completely determined by the action of 1 on the one-dimensional subspaces of Rn spanned\nby e1, . . . , en.\n\nProblem 3.5. Let A1 be the following matrix:\n\nA1 =\n\n 2 3 1\n1 2 −1\n−3 −5 1\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 2,−7) over the basis consisting of the column vectors of A1.\n\nProblem 3.6. Let A2 be the following matrix:\n\nA2 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 3 0\n\n .\n\nExpress the fourth column of A2 as a linear combination of the first three columns of A2. Is\nthe vector x = (7, 14,−1, 2) a linear combination of the columns of A2?\n\nProblem 3.7. Let A3 be the following matrix:\n\nA3 =\n\n1 1 1\n1 1 2\n1 2 3\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 9, 14) over the basis consisting of the column vectors of A3.\n\n\n\n102 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.8. Let A4 be the following matrix:\n\nA4 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 4 0\n\n .\n\nProve that the columns of A4 are linearly independent. Find the coordinates of the vector\nx = (7, 14,−1, 2) over the basis consisting of the column vectors of A4.\n\nProblem 3.9. Consider the following Haar matrix\n\nH =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\nProve that the columns of H are linearly independent.\n\nHint . Compute the product H>H.\n\nProblem 3.10. Consider the following Hadamard matrix\n\nH4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n .\n\nProve that the columns of H4 are linearly independent.\n\nHint . Compute the product H>4 H4.\n\nProblem 3.11. In solving this problem, do not use determinants.\n\n(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some vector space E.\nAssume that each vi is a linear combination of the ujs, so that\n\nvi = ai 1u1 + · · ·+ aimum, 1 ≤ i ≤ m,\n\nand that the matrix A = (ai j) is an upper-triangular matrix, which means that if 1 ≤ j <\ni ≤ m, then ai j = 0. Prove that if (u1, . . . , um) are linearly independent and if all the\ndiagonal entries of A are nonzero, then (v1, . . . , vm) are also linearly independent.\n\nHint . Use induction on m.\n\n(2) Let A = (ai j) be an upper-triangular matrix. Prove that if all the diagonal entries of\nA are nonzero, then A is invertible and the inverse A−1 of A is also upper-triangular.\n\nHint . Use induction on m.\n\nProve that if A is invertible, then all the diagonal entries of A are nonzero.\n\n(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related as in (1), then\n(u1, . . . , um) are linearly independent iff (v1, . . . , vm) are linearly independent.\n\n\n\n3.11. PROBLEMS 103\n\nProblem 3.12. In solving this problem, do not use determinants. Consider the n × n\nmatrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n0 1 2 0 . . . 0 0\n0 0 1 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 0 1 2 0\n0 0 . . . 0 0 1 2\n0 0 . . . 0 0 0 1\n\n\n.\n\n(1) Find the solution x = (x1, . . . , xn) of the linear system\n\nAx = b,\n\nfor\n\nb =\n\n\nb1\n\nb2\n...\nbn\n\n .\n\n(2) Prove that the matrix A is invertible and find its inverse A−1. Given that the number\nof atoms in the universe is estimated to be ≤ 1082, compare the size of the coefficients the\ninverse of A to 1082, if n ≥ 300.\n\n(3) Assume b is perturbed by a small amount δb (note that δb is a vector). Find the new\nsolution of the system\n\nA(x+ δx) = b+ δb,\n\nwhere δx is also a vector. In the case where b = (0, . . . , 0, 1), and δb = (0, . . . , 0, ε), show\nthat\n\n|(δx)1| = 2n−1|ε|.\n(where (δx)1 is the first component of δx).\n\n(4) Prove that (A− I)n = 0.\n\nProblem 3.13. An n × n matrix N is nilpotent if there is some integer r ≥ 1 such that\nN r = 0.\n\n(1) Prove that if N is a nilpotent matrix, then the matrix I −N is invertible and\n\n(I −N)−1 = I +N +N2 + · · ·+N r−1.\n\n(2) Compute the inverse of the following matrix A using (1):\n\nA =\n\n\n1 2 3 4 5\n0 1 2 3 4\n0 0 1 2 3\n0 0 0 1 2\n0 0 0 0 1\n\n .\n\n\n\n104 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.14. (1) Let A be an n×n matrix. If A is invertible, prove that for any x ∈ Rn,\nif Ax = 0, then x = 0.\n\n(2) Let A be an m × n matrix and let B be an n ×m matrix. Prove that Im − AB is\ninvertible iff In −BA is invertible.\n\nHint . If for all x ∈ Rn, Mx = 0 implies that x = 0, then M is invertible.\n\nProblem 3.15. Consider the following n× n matrix, for n ≥ 3:\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n(1) If we denote the columns of B by b1, . . . , bn, prove that\n\n(n− 3)b1 − (b2 + · · ·+ bn) = 2(n− 2)e1\n\nb1 − b2 = 2(e1 + e2)\n\nb1 − b3 = 2(e1 + e3)\n\n...\n...\n\nb1 − bn = 2(e1 + en),\n\nwhere e1, . . . , en are the canonical basis vectors of Rn.\n\n(2) Prove that B is invertible and that its inverse A = (aij) is given by\n\na11 =\n(n− 3)\n\n2(n− 2)\n, ai1 = − 1\n\n2(n− 2)\n2 ≤ i ≤ n\n\nand\n\naii = − (n− 3)\n\n2(n− 2)\n, 2 ≤ i ≤ n\n\naji =\n1\n\n2(n− 2)\n, 2 ≤ i ≤ n, j 6= i.\n\n(3) Show that the n diagonal n× n matrices Di defined such that the diagonal entries of\nDi are equal the entries (from top down) of the ith column of B form a basis of the space of\n\n\n\n3.11. PROBLEMS 105\n\nn × n diagonal matrices (matrices with zeros everywhere except possibly on the diagonal).\nFor example, when n = 4, we have\n\nD1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n D2 =\n\n\n−1 0 0 0\n0 −1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nD3 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 −1 0\n0 0 0 1\n\n , D4 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 −1\n\n .\n\nProblem 3.16. Given any m×n matrix A and any n×p matrix B, if we denote the columns\nof A by A1, . . . , An and the rows of B by B1, . . . , Bn, prove that\n\nAB = A1B1 + · · ·+ AnBn.\n\nProblem 3.17. Let f : E → F be a linear map which is also a bijection (it is injective and\nsurjective). Prove that the inverse function f−1 : F → E is linear.\n\nProblem 3.18. Given two vectors spaces E and F , let (ui)i∈I be any basis of E and let\n(vi)i∈I be any family of vectors in F . Prove that the unique linear map f : E → F such that\nf(ui) = vi for all i ∈ I is surjective iff (vi)i∈I spans F .\n\nProblem 3.19. Let f : E → F be a linear map with dim(E) = n and dim(F ) = m. Prove\nthat f has rank 1 iff f is represented by an m× n matrix of the form\n\nA = uv>\n\nwith u a nonzero column vector of dimension m and v a nonzero column vector of dimension\nn.\n\nProblem 3.20. Find a nontrivial linear dependence among the linear forms\n\nϕ1(x, y, z) = 2x− y + 3z, ϕ2(x, y, z) = 3x− 5y + z, ϕ3(x, y, z) = 4x− 7y + z.\n\nProblem 3.21. Prove that the linear forms\n\nϕ1(x, y, z) = x+ 2y + z, ϕ2(x, y, z) = 2x+ 3y + 3z, ϕ3(x, y, z) = 3x+ 7y + z\n\nare linearly independent. Express the linear form ϕ(x, y, z) = x+y+z as a linear combination\nof ϕ1, ϕ2, ϕ3.\n\n\n\n106 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n\n\nChapter 4\n\nMatrices and Linear Maps\n\nIn this chapter, all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n4.1 Representation of Linear Maps by Matrices\n\nProposition 3.15 shows that given two vector spaces E and F and a basis (uj)j∈J of E, every\nlinear map f : E → F is uniquely determined by the family (f(uj))j∈J of the images under\nf of the vectors in the basis (uj)j∈J .\n\nIf we also have a basis (vi)i∈I of F , then every vector f(uj) can be written in a unique\nway as\n\nf(uj) =\n∑\ni∈I\n\nai jvi,\n\nwhere j ∈ J , for a family of scalars (ai j)i∈I . Thus, with respect to the two bases (uj)j∈J\nof E and (vi)i∈I of F , the linear map f is completely determined by a “I × J-matrix”\nM(f) = (ai j)i∈I, j∈J .\n\nRemark: Note that we intentionally assigned the index set J to the basis (uj)j∈J of E, and\nthe index set I to the basis (vi)i∈I of F , so that the rows of the matrix M(f) associated\nwith f : E → F are indexed by I, and the columns of the matrix M(f) are indexed by J .\nObviously, this causes a mildly unpleasant reversal. If we had considered the bases (ui)i∈I of\nE and (vj)j∈J of F , we would obtain a J × I-matrix M(f) = (aj i)j∈J, i∈I . No matter what\nwe do, there will be a reversal! We decided to stick to the bases (uj)j∈J of E and (vi)i∈I of\nF , so that we get an I × J-matrix M(f), knowing that we may occasionally suffer from this\ndecision!\n\nWhen I and J are finite, and say, when |I| = m and |J | = n, the linear map f is\ndetermined by the matrix M(f) whose entries in the j-th column are the components of the\n\n107\n\n\n\n108 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nvector f(uj) over the basis (v1, . . . , vm), that is, the matrix\n\nM(f) =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nwhose entry on Row i and Column j is ai j (1 ≤ i ≤ m, 1 ≤ j ≤ n).\n\nWe will now show that when E and F have finite dimension, linear maps can be very\nconveniently represented by matrices, and that composition of linear maps corresponds to\nmatrix multiplication. We will follow rather closely an elegant presentation method due to\nEmil Artin.\n\nLet E and F be two vector spaces, and assume that E has a finite basis (u1, . . . , un) and\nthat F has a finite basis (v1, . . . , vm). Recall that we have shown that every vector x ∈ E\ncan be written in a unique way as\n\nx = x1u1 + · · ·+ xnun,\n\nand similarly every vector y ∈ F can be written in a unique way as\n\ny = y1v1 + · · ·+ ymvm.\n\nLet f : E → F be a linear map between E and F . Then for every x = x1u1 + · · ·+ xnun in\nE, by linearity, we have\n\nf(x) = x1f(u1) + · · ·+ xnf(un).\n\nLet\nf(uj) = a1 jv1 + · · ·+ amjvm,\n\nor more concisely,\n\nf(uj) =\nm∑\ni=1\n\nai jvi,\n\nfor every j, 1 ≤ j ≤ n. This can be expressed by writing the coefficients a1j, a2j, . . . , amj of\nf(uj) over the basis (v1, . . . , vm), as the jth column of a matrix, as shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThen substituting the right-hand side of each f(uj) into the expression for f(x), we get\n\nf(x) = x1(\nm∑\ni=1\n\nai 1vi) + · · ·+ xn(\nm∑\ni=1\n\nai nvi),\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 109\n\nwhich, by regrouping terms to obtain a linear combination of the vi, yields\n\nf(x) = (\nn∑\nj=1\n\na1 jxj)v1 + · · ·+ (\nn∑\nj=1\n\namjxj)vm.\n\nThus, letting f(x) = y = y1v1 + · · ·+ ymvm, we have\n\nyi =\nn∑\nj=1\n\nai jxj (1)\n\nfor all i, 1 ≤ i ≤ m.\n\nTo make things more concrete, let us treat the case where n = 3 and m = 2. In this case,\n\nf(u1) = a11v1 + a21v2\n\nf(u2) = a12v1 + a22v2\n\nf(u3) = a13v1 + a23v2,\n\nwhich in matrix form is expressed by\n\nf(u1) f(u2) f(u3)\n\nv1\n\nv2\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)\n,\n\nand for any x = x1u1 + x2u2 + x3u3, we have\n\nf(x) = f(x1u1 + x2u2 + x3u3)\n\n= x1f(u1) + x2f(u2) + x3f(u3)\n\n= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)\n\n= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.\n\nConsequently, since\ny = y1v1 + y2v2,\n\nwe have\n\ny1 = a11x1 + a12x2 + a13x3\n\ny2 = a21x1 + a22x2 + a23x3.\n\nThis agrees with the matrix equation(\ny1\n\ny2\n\n)\n=\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)x1\n\nx2\n\nx3\n\n .\n\nWe now formalize the representation of linear maps by matrices.\n\n\n\n110 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nDefinition 4.1. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E,\nand (v1, . . . , vm) be a basis for F . Each vector x ∈ E expressed in the basis (u1, . . . , un) as\nx = x1u1 + · · ·+ xnun is represented by the column matrix\n\nM(x) =\n\nx1\n...\nxn\n\n\nand similarly for each vector y ∈ F expressed in the basis (v1, . . . , vm).\n\nEvery linear map f : E → F is represented by the matrix M(f) = (ai j), where ai j is the\ni-th component of the vector f(uj) over the basis (v1, . . . , vm), i.e., where\n\nf(uj) =\nm∑\ni=1\n\nai jvi, for every j, 1 ≤ j ≤ n.\n\nThe coefficients a1j, a2j, . . . , amj of f(uj) over the basis (v1, . . . , vm) form the jth column of\nthe matrix M(f) shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThe matrix M(f) associated with the linear map f : E → F is called the matrix of f with\nrespect to the bases (u1, . . . , un) and (v1, . . . , vm). When E = F and the basis (v1, . . . , vm)\nis identical to the basis (u1, . . . , un) of E, the matrix M(f) associated with f : E → E (as\nabove) is called the matrix of f with respect to the basis (u1, . . . , un).\n\nRemark: As in the remark after Definition 3.12, there is no reason to assume that the\nvectors in the bases (u1, . . . , un) and (v1, . . . , vm) are ordered in any particular way. However,\nit is often convenient to assume the natural ordering. When this is so, authors sometimes\nrefer to the matrix M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un)\nand (v1, . . . , vm).\n\nLet us illustrate the representation of a linear map by a matrix in a concrete situation.\nLet E be the vector space R[X]4 of polynomials of degree at most 4, let F be the vector\nspace R[X]3 of polynomials of degree at most 3, and let the linear map be the derivative\nmap d: that is,\n\nd(P +Q) = dP + dQ\n\nd(λP ) = λdP,\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 111\n\nwith λ ∈ R. We choose (1, x, x2, x3, x4) as a basis of E and (1, x, x2, x3) as a basis of F .\nThen the 4 × 5 matrix D associated with d is obtained by expressing the derivative dxi of\neach basis vector xi for i = 0, 1, 2, 3, 4 over the basis (1, x, x2, x3). We find\n\nD =\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n .\n\nIf P denotes the polynomial\n\nP = 3x4 − 5x3 + x2 − 7x+ 5,\n\nwe have\ndP = 12x3 − 15x2 + 2x− 7.\n\nThe polynomial P is represented by the vector (5,−7, 1,−5, 3), the polynomial dP is repre-\nsented by the vector (−7, 2,−15, 12), and we have\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n5\n−7\n1\n−5\n3\n\n =\n\n\n−7\n2\n−15\n12\n\n ,\n\nas expected! The kernel (nullspace) of d consists of the polynomials of degree 0, that is, the\nconstant polynomials. Therefore dim(Ker d) = 1, and from\n\ndim(E) = dim(Ker d) + dim(Im d)\n\n(see Theorem 6.13), we get dim(Im d) = 4 (since dim(E) = 5).\n\nFor fun, let us figure out the linear map from the vector space R[X]3 to the vector space\nR[X]4 given by integration (finding the primitive, or anti-derivative) of xi, for i = 0, 1, 2, 3).\nThe 5× 4 matrix S representing\n\n∫\nwith respect to the same bases as before is\n\nS =\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n .\n\nWe verify that DS = I4,\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n\n\n112 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThis is to be expected by the fundamental theorem of calculus since the derivative of an\nintegral returns the function. As we will shortly see, the above matrix product corresponds\nto this functional composition. The equation DS = I4 shows that S is injective and has D\nas a left inverse. However, SD 6= I5, and instead\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n =\n\n\n0 0 0 0 0\n0 1 0 0 0\n0 0 1 0 0\n0 0 0 1 0\n0 0 0 0 1\n\n ,\n\nbecause constant polynomials (polynomials of degree 0) belong to the kernel of D.\n\n4.2 Composition of Linear Maps and Matrix\n\nMultiplication\n\nLet us now consider how the composition of linear maps is expressed in terms of bases.\n\nLet E, F , and G, be three vectors spaces with respective bases (u1, . . . , up) for E,\n(v1, . . . , vn) for F , and (w1, . . . , wm) for G. Let g : E → F and f : F → G be linear maps.\nAs explained earlier, g : E → F is determined by the images of the basis vectors uj, and\nf : F → G is determined by the images of the basis vectors vk. We would like to understand\nhow f ◦ g : E → G is determined by the images of the basis vectors uj.\n\nRemark: Note that we are considering linear maps g : E → F and f : F → G, instead\nof f : E → F and g : F → G, which yields the composition f ◦ g : E → G instead of\ng ◦ f : E → G. Our perhaps unusual choice is motivated by the fact that if f is represented\nby a matrix M(f) = (ai k) and g is represented by a matrix M(g) = (bk j), then f ◦g : E → G\nis represented by the product AB of the matrices A and B. If we had adopted the other\nchoice where f : E → F and g : F → G, then g ◦ f : E → G would be represented by the\nproduct BA. Personally, we find it easier to remember the formula for the entry in Row i and\nColumn j of the product of two matrices when this product is written by AB, rather than\nBA. Obviously, this is a matter of taste! We will have to live with our perhaps unorthodox\nchoice.\n\nThus, let\n\nf(vk) =\nm∑\ni=1\n\nai kwi,\n\nfor every k, 1 ≤ k ≤ n, and let\n\ng(uj) =\nn∑\nk=1\n\nbk jvk,\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 113\n\nfor every j, 1 ≤ j ≤ p; in matrix form, we have\n\nf(v1) f(v2) . . . f(vn)\n\nw1\n\nw2\n...\nwm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n\nand\n\ng(u1) g(u2) . . . g(up)\n\nv1\n\nv2\n...\nvn\n\n\nb11 b12 . . . b1p\n\nb21 b22 . . . b2p\n...\n\n...\n. . .\n\n...\nbn1 bn2 . . . bnp\n\n .\n\nBy previous considerations, for every\n\nx = x1u1 + · · ·+ xpup,\n\nletting g(x) = y = y1v1 + · · ·+ ynvn, we have\n\nyk =\n\np∑\nj=1\n\nbk jxj (2)\n\nfor all k, 1 ≤ k ≤ n, and for every\n\ny = y1v1 + · · ·+ ynvn,\n\nletting f(y) = z = z1w1 + · · ·+ zmwm, we have\n\nzi =\nn∑\nk=1\n\nai kyk (3)\n\nfor all i, 1 ≤ i ≤ m. Then if y = g(x) and z = f(y), we have z = f(g(x)), and in view of (2)\nand (3), we have\n\nzi =\nn∑\nk=1\n\nai k(\n\np∑\nj=1\n\nbk jxj)\n\n=\nn∑\nk=1\n\np∑\nj=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\nn∑\nk=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\n(\nn∑\nk=1\n\nai kbk j)xj.\n\n\n\n114 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThus, defining ci j such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p, we have\n\nzi =\n\np∑\nj=1\n\nci jxj (4)\n\nIdentity (4) shows that the composition of linear maps corresponds to the product of\nmatrices.\n\nThen given a linear map f : E → F represented by the matrix M(f) = (ai j) w.r.t. the\nbases (u1, . . . , un) and (v1, . . . , vm), by Equation (1), namely\n\nyi =\nn∑\nj=1\n\nai jxj 1 ≤ i ≤ m,\n\nand the definition of matrix multiplication, the equation y = f(x) corresponds to the matrix\nequation M(y) = M(f)M(x), that is,y1\n\n...\nym\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nam 1 . . . amn\n\n\nx1\n\n...\nxn\n\n .\n\nRecall that\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nx1\n\nx2\n...\nxn\n\n = x1\n\n\na1 1\n\na2 1\n...\n\nam 1\n\n+ x2\n\n\na1 2\n\na2 2\n...\n\nam 2\n\n+ · · ·+ xn\n\n\na1n\n\na2n\n...\n\namn\n\n .\n\nSometimes, it is necessary to incorporate the bases (u1, . . . , un) and (v1, . . . , vm) in the\nnotation for the matrix M(f) expressing f with respect to these bases. This turns out to be\na messy enterprise!\n\nWe propose the following course of action:\n\nDefinition 4.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases of E and F , and\ndenote by MU ,V(f) the matrix of f with respect to the bases U and V . Furthermore, write\nxU for the coordinates M(x) = (x1, . . . , xn) of x ∈ E w.r.t. the basis U and write yV for the\ncoordinates M(y) = (y1, . . . , ym) of y ∈ F w.r.t. the basis V . Then\n\ny = f(x)\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 115\n\nis expressed in matrix form by\nyV = MU ,V(f)xU .\n\nWhen U = V , we abbreviate MU ,V(f) as MU(f).\n\nThe above notation seems reasonable, but it has the slight disadvantage that in the\nexpression MU ,V(f)xU , the input argument xU which is fed to the matrix MU ,V(f) does not\nappear next to the subscript U in MU ,V(f). We could have used the notation MV,U(f), and\nsome people do that. But then, we find a bit confusing that V comes before U when f maps\nfrom the space E with the basis U to the space F with the basis V . So, we prefer to use the\nnotation MU ,V(f).\n\nBe aware that other authors such as Meyer [124] use the notation [f ]U ,V , and others such\nas Dummit and Foote [55] use the notation MV\n\nU (f), instead of MU ,V(f). This gets worse!\nYou may find the notation MU\n\nV (f) (as in Lang [108]), or U [f ]V , or other strange notations.\n\nDefinition 4.2 shows that the function which associates to a linear map f : E → F the\nmatrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm) has the property that matrix mul-\ntiplication corresponds to composition of linear maps. This allows us to transfer properties\nof linear maps to matrices. Here is an illustration of this technique:\n\nProposition 4.1. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nProof. (1) Every m× n matrix A = (ai j) defines the function fA : Kn → Km given by\n\nfA(x) = Ax,\n\nfor all x ∈ Kn. It is immediately verified that fA is linear and that the matrix M(fA)\nrepresenting fA over the canonical bases in Kn and Km is equal to A. Then Formula (4)\nproves that\n\nM(fA ◦ fB) = M(fA)M(fB) = AB,\n\nso we get\nM((fA ◦ fB) ◦ fC) = M(fA ◦ fB)M(fC) = (AB)C\n\n\n\n116 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nand\n\nM(fA ◦ (fB ◦ fC)) = M(fA)M(fB ◦ fC) = A(BC),\n\nand since composition of functions is associative, we have (fA ◦ fB) ◦ fC = fA ◦ (fB ◦ fC),\nwhich implies that\n\n(AB)C = A(BC).\n\n(2) It is immediately verified that if f1, f2 ∈ HomK(E,F ), A,B ∈ Mm,n(K), (u1, . . . , un) is\nany basis of E, and (v1, . . . , vm) is any basis of F , then\n\nM(f1 + f2) = M(f1) +M(f2)\n\nfA+B = fA + fB.\n\nThen we have\n\n(A+B)C = M(fA+B)M(fC)\n\n= M(fA+B ◦ fC)\n\n= M((fA + fB) ◦ fC))\n\n= M((fA ◦ fC) + (fB ◦ fC))\n\n= M(fA ◦ fC) +M(fB ◦ fC)\n\n= M(fA)M(fC) +M(fB)M(fC)\n\n= AC +BC.\n\nThe equation A(C + D) = AC + AD is proven in a similar fashion, and the last two\nequations are easily verified. We could also have verified all the identities by making matrix\ncomputations.\n\nNote that Proposition 4.1 implies that the vector space Mn(K) of square matrices is a\n(noncommutative) ring with unit In. (It even shows that Mn(K) is an associative algebra.)\n\nThe following proposition states the main properties of the mapping f 7→M(f) between\nHom(E,F ) and Mm,n. In short, it is an isomorphism of vector spaces.\n\nProposition 4.2. Given three vector spaces E, F , G, with respective bases (u1, . . . , up),\n(v1, . . . , vn), and (w1, . . . , wm), the mapping M : Hom(E,F )→ Mn,p that associates the ma-\ntrix M(g) to a linear map g : E → F satisfies the following properties for all x ∈ E, all\ng, h : E → F , and all f : F → G:\n\nM(g(x)) = M(g)M(x)\n\nM(g + h) = M(g) +M(h)\n\nM(λg) = λM(g)\n\nM(f ◦ g) = M(f)M(g),\n\n\n\n4.3. CHANGE OF BASIS MATRIX 117\n\nwhere M(x) is the column vector associated with the vector x and M(g(x)) is the column\nvector associated with g(x), as explained in Definition 4.1.\n\nThus, M : Hom(E,F ) → Mn,p is an isomorphism of vector spaces, and when p = n\nand the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up), M : Hom(E,E) → Mn is an\nisomorphism of rings.\n\nProof. That M(g(x)) = M(g)M(x) was shown by Definition 4.2 or equivalently by Formula\n(1). The identities M(g+ h) = M(g) +M(h) and M(λg) = λM(g) are straightforward, and\nM(f ◦ g) = M(f)M(g) follows from Identity (4) and the definition of matrix multiplication.\nThe mapping M : Hom(E,F ) → Mn,p is clearly injective, and since every matrix defines a\nlinear map (see Proposition 4.1), it is also surjective, and thus bijective. In view of the above\nidentities, it is an isomorphism (and similarly for M : Hom(E,E)→ Mn, where Proposition\n4.1 is used to show that Mn is a ring).\n\nIn view of Proposition 4.2, it seems preferable to represent vectors from a vector space\nof finite dimension as column vectors rather than row vectors. Thus, from now on, we will\ndenote vectors of Rn (or more generally, of Kn) as column vectors.\n\n4.3 Change of Basis Matrix\n\nIt is important to observe that the isomorphism M : Hom(E,F )→ Mn,p given by Proposition\n4.2 depends on the choice of the bases (u1, . . . , up) and (v1, . . . , vn), and similarly for the\nisomorphism M : Hom(E,E) → Mn, which depends on the choice of the basis (u1, . . . , un).\nThus, it would be useful to know how a change of basis affects the representation of a linear\nmap f : E → F as a matrix. The following simple proposition is needed.\n\nProposition 4.3. Let E be a vector space, and let (u1, . . . , un) be a basis of E. For every\nfamily (v1, . . . , vn), let P = (ai j) be the matrix defined such that vj =\n\n∑n\ni=1 ai jui. The matrix\n\nP is invertible iff (v1, . . . , vn) is a basis of E.\n\nProof. Note that we have P = M(f), the matrix associated with the unique linear map\nf : E → E such that f(ui) = vi. By Proposition 3.15, f is bijective iff (v1, . . . , vn) is a basis\nof E. Furthermore, it is obvious that the identity matrix In is the matrix associated with the\nidentity id : E → E w.r.t. any basis. If f is an isomorphism, then f ◦f−1 = f−1◦f = id, and\nby Proposition 4.2, we get M(f)M(f−1) = M(f−1)M(f) = In, showing that P is invertible\nand that M(f−1) = P−1.\n\nProposition 4.3 suggests the following definition.\n\nDefinition 4.3. Given a vector space E of dimension n, for any two bases (u1, . . . , un) and\n(v1, . . . , vn) of E, let P = (ai j) be the invertible matrix defined such that\n\nvj =\nn∑\ni=1\n\nai jui,\n\n\n\n118 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nwhich is also the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), in that order . Indeed, we express each id(vj) = vj over the basis (u1, . . . , un).\nThe coefficients a1j, a2j, . . . , anj of vj over the basis (u1, . . . , un) form the jth column of the\nmatrix P shown below:\n\nv1 v2 . . . vn\n\nu1\n\nu2\n...\nun\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nan1 an2 . . . ann\n\n .\n\nThe matrix P is called the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn).\n\nClearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un) is P−1. Since P =\n(ai j) is the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), given any vector x ∈ E, if x = x1u1 + · · ·+xnun over the basis (u1, . . . , un) and\nx = x′1v1 + · · ·+ x′nvn over the basis (v1, . . . , vn), from Proposition 4.2, we havex1\n\n...\nxn\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nan 1 . . . ann\n\n\nx\n\n′\n1\n...\nx′n\n\n ,\n\nshowing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed in terms of the\nnew coordinates (x′i) of x (over (v1, . . . , vn)).\n\nNow we face the painful task of assigning a “good” notation incorporating the bases\nU = (u1, . . . , un) and V = (v1, . . . , vn) into the notation for the change of basis matrix from\nU to V . Because the change of basis matrix from U to V is the matrix of the identity map\nidE with respect to the bases V and U in that order , we could denote it by MV,U(id) (Meyer\n[124] uses the notation [I]V,U). We prefer to use an abbreviation for MV,U(id).\n\nDefinition 4.4. The change of basis matrix from U to V is denoted\n\nPV,U .\n\nNote that\nPU ,V = P−1\n\nV,U .\n\nThen, if we write xU = (x1, . . . , xn) for the old coordinates of x with respect to the basis U\nand xV = (x′1, . . . , x\n\n′\nn) for the new coordinates of x with respect to the basis V , we have\n\nxU = PV,U xV , xV = P−1\nV,U xU .\n\nThe above may look backward, but remember that the matrix MU ,V(f) takes input\nexpressed over the basis U to output expressed over the basis V . Consequently, PV,U takes\ninput expressed over the basis V to output expressed over the basis U , and xU = PV,U xV\nmatches this point of view!\n\n\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n� Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto V as PU ,V = P−1\n\nV,U . Under this point of view, the old basis U is expressed in terms of\nthe new basis V . We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms of the old basis\n(u1, . . ., un), we observe that the coordinates (xi) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant .\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R2, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1) and v2 = (−1, 1).\nThe change of basis matrix P from the basis U = (u1, u2) to the basis V = (v1, v2) is\n\nP =\n\n(\n1 −1\n1 1\n\n)\nand its inverse is\n\nP−1 =\n\n(\n1/2 1/2\n−1/2 1/2\n\n)\n.\n\nThe old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms of the new\ncoordinates (x′1, x\n\n′\n2) with respect to (v1, v2) by(\n\nx1\n\nx2\n\n)\n=\n\n(\n1 −1\n1 1\n\n)(\nx′1\nx′2\n\n)\n,\n\nand the new coordinates (x′1, x\n′\n2) with respect to (v1, v2) are expressed in terms of the old\n\ncoordinates (x1, x2) with respect to (u1, u2) by(\nx′1\nx′2\n\n)\n=\n\n(\n1/2 1/2\n−1/2 1/2\n\n)(\nx1\n\nx2\n\n)\n.\n\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\nand consider the bases U = (1, x, x2, x3) and V = (B3\n\n0(x), B3\n1(x), B3\n\n2(x), B3\n3(x)), where\n\nB3\n0(x), B3\n\n1(x), B3\n2(x), B3\n\n3(x) are the Bernstein polynomials of degree 3, given by\n\nB3\n0(x) = (1− x)3 B3\n\n1(x) = 3(1− x)2x B3\n2(x) = 3(1− x)x2 B3\n\n3(x) = x3.\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix PV,U is\ngiven by\n\nPV,U =\n\n\n1 0 0 0\n−3 3 0 0\n3 −6 3 0\n−1 3 −3 1\n\n .\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n© Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto Vas Puy = Py 1. Under this point of view, the old basis U/ is expressed in terms of\nthe new basis V. We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = P)y expresses the new basis (v1,...,Un) in terms of the old basis\n(U1,.--, Un), we observe that the coordinates (;) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant.\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R’, with wu, = (1,0), wa = (0,1), vy = (1,1) and ve = (-1,1).\nThe change of basis matrix P from the basis U = (uy, ug) to the basis V = (vj, v2) is\n\np=(, 7)\n\npte (1 ta):\n\nThe old coordinates (21,22) with respect to (ui,u2) are expressed in terms of the new\ncoordinates (x, 24) with respect to (v1, v2) by\n\n(2) = 7) @)\n\nand the new coordinates (2,25) with respect to (v1, v2) are expressed in terms of the old\ncoordinates (21,22) with respect to (ui, U2) by\n\nvy) _ ( 1/2 1/2) (x\nX5 —1/2 1/2) \\a)\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\n\nand consider the bases UY = (1,2,x?,73) and V = (B3(x), B3(x), B3(x), B3(x)), where\nBe (x), Bi(x), B3(x), B3(x) are the Bernstein polynomials of degree 3, given by\n\nB3(x) = (1-2)? B3(x) = 3(1—2)?x B3 (x) = 3(1 — x)2? B3 (x) = 2°.\n\nand its inverse is\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix Pyy is\ngiven by\n\n1 0 0 0\n3 3 0 0\nPou = 3-6 3 0\n-1 3 -3 1\n\n\n\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of PV,U is\n\nP−1\nV,U =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n .\n\nTherefore, the coordinates of the polynomial 2x3 − x+ 1 over the basis V are\n1\n\n2/3\n1/3\n2\n\n =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n\n\n\n1\n−1\n0\n2\n\n ,\n\nand so\n\n2x3 − x+ 1 = B3\n0(x) +\n\n2\n\n3\nB3\n\n1(x) +\n1\n\n3\nB3\n\n2(x) + 2B3\n3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn)\n\nbe two bases of E, and let V = (v1, . . . , vm) and V ′ = (v′1, . . . , v\n′\nm) be two bases of F . Let\n\nP = PU ′,U be the change of basis matrix from U to U ′, and let Q = PV ′,V be the change of\nbasis matrix from V to V ′. For any linear map f : E → F , let M(f) = MU ,V(f) be the matrix\nassociated to f w.r.t. the bases U and V, and let M ′(f) = MU ′,V ′(f) be the matrix associated\nto f w.r.t. the bases U ′ and V ′. We have\n\nM ′(f) = Q−1M(f)P,\n\nor more explicitly\n\nMU ′,V ′(f) = P−1\nV ′,VMU ,V(f)PU ′,U = PV,V ′MU ,V(f)PU ′,U .\n\nProof. Since f : E → F can be written as f = idF ◦ f ◦ idE, since P is the matrix of idE\nw.r.t. the bases (u′1, . . . , u\n\n′\nn) and (u1, . . . , un), and Q−1 is the matrix of idF w.r.t. the bases\n\n(v1, . . . , vm) and (v′1, . . . , v\n′\nm), by Proposition 4.2, we have M ′(f) = Q−1M(f)P .\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn) be\n\ntwo bases of E. Let P = PU ′,U be the change of basis matrix from U to U ′. For any linear\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of Pyy is\n\n10 0 0\npi_|{i 1/3 0 0\nU~11 9/3 1/3 0\n\n11 11\n\nTherefore, the coordinates of the polynomial 27° — x + 1 over the basis V are\n\n1 10 0 0\\/1\n\n2/3) [1 1/3 0 oO] f-1\n\n1/3 1 2/3 1/3 of | 0 J?\n2 11 1 1) \\2\n\nand so\n\n2 1\n2x? —x2+1= Be(x) + 3 Pile) + 3 B2(@) + 2B3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\n/\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1,..., tn) andU' = (u},...,ui,)\n\nan\n\nbe two bases of E, and let V = (u,...,Um) and V! = (v},...,v/,) be two bases of F'. Let\nP= Pyy be the change of basis matriz from U to U', and let Q = Pyy be the change of\nbasis matriz from V to V'. For any linear map f: E > F, let M(f) = Mu y(f) be the matriz\nassociated to f w.r.t. the basesU and V, and let M'(f) = My y(f) be the matrix associated\n\nto f w.r.t. the bases UW’ and VY’. We have\n\nM'(f) =Q°M(f)P,\n\nor more explicitly\nMuy (f) = Poy Muy (Pf) Pau = Poy Muy (f)Puru-\n\nProof. Since f: E — F can be written as f = idg o f oidg, since P is the matrix of idg\n\nw.r.t. the bases (uj,...,u/,) and (w1,...,Un), and Q7! is the matrix of idp w.r.t. the bases\n\nan\n\n(v1,---,Um) and (v},...,v/,), by Proposition 4.2, we have M’(f) = Q-'M(f)P. O\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (uj,...,Un) andU’ = (u},...,ui,) be\ntwo bases of E. Let P = Pyy be the change of basis matrix from U to U'. For any linear\n\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f : E → E, let M(f) = MU(f) be the matrix associated to f w.r.t. the basis U , and let\nM ′(f) = MU ′(f) be the matrix associated to f w.r.t. the basis U ′. We have\n\nM ′(f) = P−1M(f)P,\n\nor more explicitly,\n\nMU ′(f) = P−1\nU ′,UMU(f)PU ′,U = PU ,U ′MU(f)PU ′,U .\n\nExample 4.3. Let E = R2, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1) are the canonical\nbasis vectors, let V = (v1, v2) = (e1, e1 − e2), and let\n\nA =\n\n(\n2 1\n0 1\n\n)\n.\n\nThe change of basis matrix P = PV,U from U to V is\n\nP =\n\n(\n1 1\n0 −1\n\n)\n,\n\nand we check that\nP−1 = P.\n\nTherefore, in the basis V , the matrix representing the linear map f defined by A is\n\nA′ = P−1AP = PAP =\n\n(\n1 1\n0 −1\n\n)(\n2 1\n0 1\n\n)(\n1 1\n0 −1\n\n)\n=\n\n(\n2 0\n0 1\n\n)\n= D,\n\na diagonal matrix. In the basis V , it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v1 direction and it is the identity in the v2 direction. Observe that v1 and\nv2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v1 and v2 are corresponding eigenvectors . We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two n×n matrices A and B are said to be similar iff there is some invertible\nmatrix P such that\n\nB = P−1AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n × n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f: E + E, let M(f) = Mu (f) be the matrix associated to f w.r.t. the basis U, and let\nM'(f) = Mw(f) be the matrix associated to f w.r.t. the basis U'. We have\n\nM'(f) =PUM(f)P.\nor more explicitly,\nMu(f) = Pry Mul f) Pau = Puw Mu(f) Pew.\n\nExample 4.3. Let E = R?, U = (e1, e2) where e; = (1,0) and e2 = (0,1) are the canonical\nbasis vectors, let V = (v1, v2) = (€1, €1 — €2), and let\n\n=)\n\nThe change of basis matrix P = Pyy from U to Y is\n\n11\np=(p 4):\n\nP'=P.\n\nand we check that\n\nTherefore, in the basis Y, the matrix representing the linear map f defined by A is\n\nb plyap.. —f1 1\\(2 I) fa 1\\_ (2 0) ~\nserarcrare( EDGE )-2\n\na diagonal matrix. In the basis VY, it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v, direction and it is the identity in the v2 direction. Observe that v, and\nV2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v; and vy are corresponding eigenvectors. We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two nxn matrices A and B are said to be similar iff there is some invertible\n\nmatrix P such that\nB=P'AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n x n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n\n\n\n122 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nmatrix A is similar to its transpose A>. The proof requires advanced concepts (the Jordan\nform or similarity invariants).\n\nIf U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change of basis matrix\n\nP = PV,U =\n\n\na11 a12 · · · a1n\n\na21 a22 · · · a2n\n...\n\n...\n. . .\n\n...\nan1 an2 · · · ann\n\n\nfrom (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists of the coordinates\nof vj over the basis (u1, . . . , un), which means that\n\nvj =\nn∑\ni=1\n\naijui.\n\nIt is natural to extend the matrix notation and to express the vector\n\nv1\n...\nvn\n\n in En as the\n\nproduct of a matrix times the vector\n\nu1\n...\nun\n\n in En, namely as\n\n\nv1\n\nv2\n...\nvn\n\n =\n\n\na11 a21 · · · an1\n\na12 a22 · · · an2\n...\n\n...\n. . .\n\n...\na1n a2n · · · ann\n\n\n\nu1\n\nu2\n...\nun\n\n ,\n\nbut notice that the matrix involved is not P , but its transpose P>.\n\nThis observation has the following consequence: if U = (u1, . . . , un) and V = (v1, . . . , vn)\nare two bases of E and if v1\n\n...\nvn\n\n = A\n\nu1\n...\nun\n\n ,\n\nthat is,\n\nvi =\nn∑\nj=1\n\naijuj,\n\nfor any vector w ∈ E, if\n\nw =\nn∑\ni=1\n\nxiui =\nn∑\nk=1\n\nykvk,\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 123\n\nthen x1\n...\nxn\n\n = A>\n\ny1\n...\nyn\n\n ,\n\nand so y1\n...\nyn\n\n = (A>)−1\n\nx1\n...\nxn\n\n .\n\nIt is easy to see that (A>)−1 = (A−1)>. Also, if U = (u1, . . . , un), V = (v1, . . . , vn), and\nW = (w1, . . . , wn) are three bases of E, and if the change of basis matrix from U to V is\nP = PV,U and the change of basis matrix from V to W is Q = PW,V , thenv1\n\n...\nvn\n\n = P>\n\nu1\n...\nun\n\n ,\n\nw1\n...\nwn\n\n = Q>\n\nv1\n...\nvn\n\n ,\n\nso w1\n...\nwn\n\n = Q>P>\n\nu1\n...\nun\n\n = (PQ)>\n\nu1\n...\nun\n\n ,\n\nwhich means that the change of basis matrix PW,U from U to W is PQ. This proves that\n\nPW,U = PV,UPW,V .\n\nEven though matrices are indispensable since they are the major tool in applications of\nlinear algebra, one should not lose track of the fact that\n\nlinear maps are more fundamental because they are intrinsic\nobjects that do not depend on the choice of bases.\n\nConsequently, we advise the reader to try to think in terms of\nlinear maps rather than reduce everything to matrices.\n\nIn our experience, this is particularly effective when it comes to proving results about\nlinear maps and matrices, where proofs involving linear maps are often more “conceptual.”\nThese proofs are usually more general because they do not depend on the fact that the\ndimension is finite. Also, instead of thinking of a matrix decomposition as a purely algebraic\noperation, it is often illuminating to view it as a geometric decomposition. This is the case of\nthe SVD, which in geometric terms says that every linear map can be factored as a rotation,\nfollowed by a rescaling along orthogonal axes and then another rotation.\n\nAfter all,\n\n\n\n124 CHAPTER 4. MATRICES AND LINEAR MAPS\n\na matrix is a representation of a linear map,\n\nand most decompositions of a matrix reflect the fact that with a suitable choice of a basis\n(or bases), the linear map is a represented by a matrix having a special shape. The problem\nis then to find such bases.\n\nStill, for the beginner, matrices have a certain irresistible appeal, and we confess that\nit takes a certain amount of practice to reach the point where it becomes more natural to\ndeal with linear maps. We still recommend it! For example, try to translate a result stated\nin terms of matrices into a result stated in terms of linear maps. Whenever we tried this\nexercise, we learned something.\n\nAlso, always try to keep in mind that\n\nlinear maps are geometric in nature; they act on space.\n\n4.5 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The representation of linear maps by matrices .\n\n• The matrix representation mapping M : Hom(E,F ) → Mn,p and the representation\nisomorphism (Proposition 4.2).\n\n• Change of basis matrix and Proposition 4.4.\n\n4.6 Problems\n\nProblem 4.1. Prove that the column vectors of the matrix A1 given by\n\nA1 =\n\n1 2 3\n2 3 7\n1 3 1\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B1 over the basis consisting\nof the column vectors of A1 given by\n\nB1 =\n\n3 5 1\n1 2 1\n4 3 −6\n\n\n\n\n\n4.6. PROBLEMS 125\n\nare the columns of the matrix P1 given by\n\nP1 =\n\n−27 −61 −41\n9 18 9\n4 10 8\n\n .\n\nGive a nontrivial linear dependence of the columns of P1. Check that B1 = A1P1. Is the\nmatrix B1 invertible?\n\nProblem 4.2. Prove that the column vectors of the matrix A2 given by\n\nA2 =\n\n\n1 1 1 1\n1 2 1 3\n1 1 2 2\n1 1 1 3\n\n\nare linearly independent.\n\nProve that the column vectors of the matrix B2 given by\n\nB2 =\n\n\n1 −2 2 −2\n0 −3 2 −3\n3 −5 5 −4\n3 −4 4 −4\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B2 over the basis consisting\nof the column vectors of A2 are the columns of the matrix P2 given by\n\nP2 =\n\n\n2 0 1 −1\n−3 1 −2 1\n1 −2 2 −1\n1 −1 1 −1\n\n .\n\nCheck that A2P2 = B2. Prove that\n\nP−1\n2 =\n\n\n−1 −1 −1 1\n2 1 1 −2\n2 1 2 −3\n−1 −1 0 −1\n\n .\n\nWhat are the coordinates over the basis consisting of the column vectors of B2 of the vector\nwhose coordinates over the basis consisting of the column vectors of A1 are (2,−3, 0, 0)?\n\n\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nB2\n0(t) = (1− t)2 B2\n\n1(t) = 2(1− t)t B2\n2(t) = t2\n\nB3\n0(t) = (1− t)3 B3\n\n1(t) = 3(1− t)2t B3\n2(t) = 3(1− t)t2 B3\n\n3(t) = t3,\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B2\n0(t), B2\n\n1(t), B2\n2(t) are expressed as linear com-\n\nbinations of the basis (1, t, t2) of the vector space of polynomials of degree at most 2 as\nfollows: B2\n\n0(t)\nB2\n\n1(t)\nB2\n\n2(t)\n\n =\n\n1 −2 1\n0 2 −2\n0 0 1\n\n1\nt\nt2\n\n .\n\nProve that\nB2\n\n0(t) +B2\n1(t) +B2\n\n2(t) = 1.\n\n(2) Show that the Bernstein polynomials B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t) are expressed as linear\ncombinations of the basis (1, t, t2, t3) of the vector space of polynomials of degree at most 3\nas follows: \n\nB3\n0(t)\n\nB3\n1(t)\n\nB3\n2(t)\n\nB3\n3(t)\n\n =\n\n\n1 −3 3 −1\n0 3 −6 3\n0 0 3 −3\n0 0 0 1\n\n\n\n\n1\nt\nt2\n\nt3\n\n .\n\nProve that\nB3\n\n0(t) +B3\n1(t) +B3\n\n2(t) +B3\n3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient\n(\nm\nk\n\n)\nis given by(\n\nm\n\nk\n\n)\n=\n\nm!\n\nk!(m− k)!\n,\n\nwith 0 ≤ k ≤ m.\n\nFor any m ≥ 1, we have the m+ 1 Bernstein polynomials of degree m given by\n\nBm\nk (t) =\n\n(\nm\n\nk\n\n)\n(1− t)m−ktk, 0 ≤ k ≤ m.\n\n(1) Prove that\n\nBm\nk (t) =\n\nm∑\nj=k\n\n(−1)j−k\n(\nm\n\nj\n\n)(\nj\n\nk\n\n)\ntj. (∗)\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nBU) =(-0) BA) =BU—N* BA =30—HP BR) =e\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B3(t), B?(t), B3(t) are expressed as linear com-\nbinations of the basis (1,¢,¢?) of the vector space of polynomials of degree at most 2 as\nfollows:\n\nBe(t) 1-2 1\nBet)}=|0 2 -2 t\nB3(t) 0 0 1 ?\n\nProve that\nB3(t) + Be(t) + BS(t) = 1.\n\n(2) Show that the Bernstein polynomials B3(t), B}(t), B3(t), B3(t) are expressed as linear\ncombinations of the basis (1, t, ¢?, #2) of the vector space of polynomials of degree at most 3\nas follows:\n\nBe(t) 1-3 3 -1 1\nBeth} {0 3 -6 3 t\nBt)} {0 0 3 -3) |\nB3(t) 00 0 1 t?\n\nProve that\nBe (t) + Be(t) + B3(t) + B3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient (\"\") is given by\nm\\ m!\nkk} kM(m—k)V\n\nFor any m > 1, we have the m-+1 Bernstein polynomials of degree m given by\n\nwithO<k<m.\n\nBrit) = (‘\") (1—t)\"*t*®, O<k<m.\n\n(1) Prove that\n\n\n\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bm\n0 (t), . . . , Bm\n\nm(t) are linearly independent.\n\n(2) Prove that\n\nBm\n0 (t) + · · ·+Bm\n\nm(t) = 1.\n\n(3) What can you say about the symmetries of the (m+ 1)× (m+ 1) matrix expressing\nBm\n\n0 , . . . , B\nm\nm in terms of the basis 1, t, . . . , tm?\n\nProve your claim (beware that in equation (∗) the coefficient of tj in Bm\nk is the entry on\n\nthe (k+1)th row of the (j+1)th column, since 0 ≤ k, j ≤ m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the ti in terms of the Bernstein polynomials\nBm\n\n0 (t), . . . , Bm\nm(t), with 0 ≤ i ≤ m.\n\nFirst, prove that\n\nti =\nm−i∑\nj=0\n\ntiBm−i\nj (t), 0 ≤ i ≤ m.\n\nThen prove that (\nm\n\ni\n\n)(\nm− i\nj\n\n)\n=\n\n(\nm\n\ni+ j\n\n)(\ni+ j\n\ni\n\n)\n.\n\nUse the above facts to prove that\n\nti =\nm−i∑\nj=0\n\n(\ni+j\ni\n\n)(\nm\ni\n\n) Bm\ni+j(t).\n\nConclude that the Bernstein polynomials Bm\n0 (t), . . . , Bm\n\nm(t) form a basis of the vector\nspace of polynomials of degree ≤ m.\n\nCompute the matrix expressing 1, t, t2 in terms of B2\n0(t), B2\n\n1(t), B2\n2(t), and the matrix\n\nexpressing 1, t, t2, t3 in terms of B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t).\n\nYou should find 1 1 1\n0 1/2 1\n0 0 1\n\n\nand \n\n1 1 1 1\n0 1/3 2/3 1\n0 0 1/3 1\n0 0 0 1\n\n .\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bj’(t),..., Bi (t) are linearly independent.\n(2) Prove that\nBo (t) +--+ Bt) =1.\n(3) What can you say about the symmetries of the (m+ 1) x (m+ 1) matrix expressing\nBo',..., BM in terms of the basis 1,t,...,t?\n\nProve your claim (beware that in equation («) the coefficient of t? in Bi” is the entry on\nthe (k+1)th row of the (j+1)th column, since 0 < k, 7 < m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the t’ in terms of the Bernstein polynomials\nBy (t),..., B(t), with 0<i<m.\n\nFirst, prove that\n\n= S UB (t), O<i<m.\nj=0\n\nOlen monies)\n\nUse the above facts to prove that\n\nThen prove that\n\ni’ Bm (t).\n(\"\") ig (t)\n\nConclude that the Bernstein polynomials Bj’(t),...,B/\"(t) form a basis of the vector\nspace of polynomials of degree < m.\n\nCompute the matrix expressing 1,t,¢? in terms of B3(t), B?(t), B3(t), and the matrix\nexpressing 1,t,t*,¢° in terms of B3(t), BR(t), B3(t), B3(t).\n\nYou should find\n\nand\n\noO CF\n\n_\noOo ™.\n\nw\nre bo\n—~=\nwm w\nRee\n\n\n\n\n128 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) A polynomial curve C(t) of degree m in the plane is the set of points\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\ngiven by two polynomials of degree ≤ m,\n\nx(t) = α0t\nm1 + α1t\n\nm1−1 + · · ·+ αm1\n\ny(t) = β0t\nm2 + β1t\n\nm2−1 + · · ·+ βm2 ,\n\nwith 1 ≤ m1,m2 ≤ m and α0, β0 6= 0.\n\nProve that there exist m+ 1 points b0, . . . , bm ∈ R2 so that\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\n= Bm\n\n0 (t)b0 +Bm\n1 (t)b1 + · · ·+Bm\n\nm(t)bm\n\nfor all t ∈ R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1 generally on the\ncurve?\n\nWe say that the curve C is a Bézier curve and (b0, . . . , bm) is the list of control points of\nthe curve (control points need not be distinct).\n\nRemark: Because Bm\n0 (t) + · · · + Bm\n\nm(t) = 1 and Bm\ni (t) ≥ 0 when t ∈ [0, 1], the curve\n\nsegment C[0, 1] corresponding to t ∈ [0, 1] belongs to the convex hull of the control points.\nThis is an important property of Bézier curves which is used in geometric modeling to\nfind the intersection of curve segments. Bézier curves play an important role in computer\ngraphics and geometric modeling, but also in robotics because they can be used to model\nthe trajectories of moving objects.\n\nProblem 4.5. Consider the n× n matrix\n\nA =\n\n\n\n0 0 0 · · · 0 −an\n1 0 0 · · · 0 −an−1\n\n0 1 0 · · · 0 −an−2\n...\n\n. . . . . . . . .\n...\n\n...\n\n0 0 0\n. . . 0 −a2\n\n0 0 0 · · · 1 −a1\n\n\n,\n\nwith an 6= 0.\n\n(1) Find a matrix P such that\nA> = P−1AP.\n\nWhat happens when an = 0?\n\nHint . First, try n = 3, 4, 5. Such a matrix must have zeros above the “antidiagonal,” and\nidentical entries pij for all i, j ≥ 0 such that i+ j = n+ k, where k = 1, . . . , n.\n\n(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be chosen so that\nthe entries in P−1 are also integers.\n\n\n\n4.6. PROBLEMS 129\n\nProblem 4.6. For any matrix A ∈ Mn(C), let RA and LA be the maps from Mn(C) to itself\ndefined so that\n\nLA(B) = AB, RA(B) = BA, for all B ∈ Mn(C).\n\n(1) Check that LA and RA are linear, and that LA and RB commute for all A,B.\n\nLet adA : Mn(C)→ Mn(C) be the linear map given by\n\nadA(B) = LA(B)−RA(B) = AB −BA = [A,B], for all B ∈ Mn(C).\n\nNote that [A,B] is the Lie bracket.\n\n(2) Prove that if A is invertible, then LA and RA are invertible; in fact, (LA)−1 = LA−1\n\nand (RA)−1 = RA−1 . Prove that if A = PBP−1 for some invertible matrix P , then\n\nLA = LP ◦ LB ◦ L−1\nP , RA = R−1\n\nP ◦RB ◦RP .\n\n(3) Recall that the n2 matrices Eij defined such that all entries in Eij are zero except\nthe (i, j)th entry, which is equal to 1, form a basis of the vector space Mn(C). Consider the\npartial ordering of the Eij defined such that for i = 1, . . . , n, if n ≥ j > k ≥ 1, then then Eij\nprecedes Eik, and for j = 1, . . . , n, if 1 ≤ i < h ≤ n, then Eij precedes Ehj.\n\nDraw the Hasse diagram of the partial order defined above when n = 3.\n\nThere are total orderings extending this partial ordering. How would you find them\nalgorithmically? Check that the following is such a total order:\n\n(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).\n\n(4) Let the total order of the basis (Eij) extending the partial ordering defined in (2) be\ngiven by\n\n(i, j) < (h, k) iff\n\n{\ni = h and j > k\nor i < h.\n\nLet R be the n× n permutation matrix given by\n\nR =\n\n\n0 0 . . . 0 1\n0 0 . . . 1 0\n...\n\n...\n. . .\n\n...\n...\n\n0 1 . . . 0 0\n1 0 . . . 0 0\n\n .\n\nObserve that R−1 = R. Prove that for any n ≥ 1, the matrix of LA is given by A⊗In, and the\nmatrix of RA is given by In⊗RA>R (over the basis (Eij) ordered as specified above), where\n⊗ is the Kronecker product (also called tensor product) of matrices defined in Definition 5.4.\n\nHint . Figure out what are RB(Eij) = EijB and LB(Eij) = BEij.\n\n\n\n130 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) Prove that if A is upper triangular, then the matrices representing LA and RA are\nalso upper triangular.\n\nNote that if instead of the ordering\n\nE1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,\n\nthat I proposed you use the standard lexicographic ordering\n\nE11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,\n\nthen the matrix representing LA is still A⊗ In, but the matrix representing RA is In ⊗ A>.\nIn this case, if A is upper-triangular, then the matrix of RA is lower triangular . This is the\nmotivation for using the first basis (avoid upper becoming lower).\n\n\n\nChapter 5\n\nHaar Bases, Haar Wavelets,\nHadamard Matrices\n\nIn this chapter, we discuss two types of matrices that have applications in computer science\nand engineering:\n\n(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool in signal pro-\ncessing and computer graphics.\n\n2) Hadamard matrices which have applications in error correcting codes, signal processing,\nand low rank approximation.\n\n5.1 Introduction to Signal Compression Using Haar\n\nWavelets\n\nWe begin by considering Haar wavelets in R4. Wavelets play an important role in audio\nand video signal processing, especially for compressing long signals into much smaller ones\nthat still retain enough information so that when they are played, we can’t see or hear any\ndifference.\n\nConsider the four vectors w1, w2, w3, w4 given by\n\nw1 =\n\n\n1\n1\n1\n1\n\n w2 =\n\n\n1\n1\n−1\n−1\n\n w3 =\n\n\n1\n−1\n0\n0\n\n w4 =\n\n\n0\n0\n1\n−1\n\n .\n\nNote that these vectors are pairwise orthogonal, so they are indeed linearly independent\n(we will see this in a later chapter). Let W = {w1, w2, w3, w4} be the Haar basis , and let\nU = {e1, e2, e3, e4} be the canonical basis of R4. The change of basis matrix W = PW,U from\n\n131\n\n\n\n132 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nU to W is given by\n\nW =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nand we easily find that the inverse of W is given by\n\nW−1 =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n .\n\nSo the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4) over the Haar basis\nW , with \n\nc1\n\nc2\n\nc3\n\nc4\n\n =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n\n\n\n6\n4\n5\n1\n\n =\n\n\n4\n1\n1\n2\n\n .\n\nGiven a signal v = (v1, v2, v3, v4), we first transform v into its coefficients c = (c1, c2, c3, c4)\nover the Haar basis by computing c = W−1v. Observe that\n\nc1 =\nv1 + v2 + v3 + v4\n\n4\n\nis the overall average value of the signal v. The coefficient c1 corresponds to the background\nof the image (or of the sound). Then, c2 gives the coarse details of v, whereas, c3 gives the\ndetails in the first part of v, and c4 gives the details in the second half of v.\n\nReconstruction of the signal consists in computing v = Wc. The trick for good compres-\nsion is to throw away some of the coefficients of c (set them to zero), obtaining a compressed\nsignal ĉ, and still retain enough crucial information so that the reconstructed signal v̂ = Wĉ\nlooks almost as good as the original signal v. Thus, the steps are:\n\ninput v −→ coefficients c = W−1v −→ compressed ĉ −→ compressed v̂ = Wĉ.\n\nThis kind of compression scheme makes modern video conferencing possible.\n\nIt turns out that there is a faster way to find c = W−1v, without actually using W−1.\nThis has to do with the multiscale nature of Haar wavelets.\n\nGiven the original signal v = (6, 4, 5, 1) shown in Figure 5.1, we compute averages and\nhalf differences obtaining Figure 5.2. We get the coefficients c3 = 1 and c4 = 2. Then\nagain we compute averages and half differences obtaining Figure 5.3. We get the coefficients\nc1 = 4 and c2 = 1. Note that the original signal v can be reconstructed from the two signals\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 133\n\n6 4 5 1\n\nFigure 5.1: The original signal v.\n\n5 5 3 3\n\n1\n\n−1\n\n2\n\n−2\n\nFigure 5.2: First averages and first half differences.\n\nin Figure 5.2, and the signal on the left of Figure 5.2 can be reconstructed from the two\nsignals in Figure 5.3. In particular, the data from Figure 5.2 gives us\n\n5 + 1 =\nv1 + v2\n\n2\n+\nv1 − v2\n\n2\n= v1\n\n5− 1 =\nv1 + v2\n\n2\n− v1 − v2\n\n2\n= v2\n\n3 + 2 =\nv3 + v4\n\n2\n+\nv3 − v4\n\n2\n= v3\n\n3− 2 =\nv3 + v4\n\n2\n− v3 − v4\n\n2\n= v4.\n\n5.2 Haar Bases and Haar Matrices, Scaling Properties\n\nof Haar Wavelets\n\nThe method discussed in Section 5.2 can be generalized to signals of any length 2n. The\nprevious case corresponds to n = 2. Let us consider the case n = 3. The Haar basis\n\n\n\n134 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n4 4 4 4\n1 1\n\n−1 −1\n\nFigure 5.3: Second averages and second half differences.\n\n(w1, w2, w3, w4, w5, w6, w7, w8) is given by the matrix\n\nW =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nThe columns of this matrix are orthogonal, and it is easy to see that\n\nW−1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W>.\n\nA pattern is beginning to emerge. It looks like the second Haar basis vector w2 is the\n“mother” of all the other basis vectors, except the first, whose purpose is to perform aver-\naging. Indeed, in general, given\n\nw2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n2n\n\n,\n\nthe other Haar basis vectors are obtained by a “scaling and shifting process.” Starting from\nw2, the scaling process generates the vectors\n\nw3, w5, w9, . . . , w2j+1, . . . , w2n−1+1,\n\nsuch that w2j+1+1 is obtained from w2j+1 by forming two consecutive blocks of 1 and −1\nof half the size of the blocks in w2j+1, and setting all other entries to zero. Observe that\nw2j+1 has 2j blocks of 2n−j elements. The shifting process consists in shifting the blocks of\n1 and −1 in w2j+1 to the right by inserting a block of (k − 1)2n−j zeros from the left, with\n0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Note that our convention is to use j as the scaling index and\nk as the shifting index. Thus, we obtain the following formula for w2j+k:\n\nw2j+k(i) =\n\n\n0 1 ≤ i ≤ (k − 1)2n−j\n\n1 (k − 1)2n−j + 1 ≤ i ≤ (k − 1)2n−j + 2n−j−1\n\n−1 (k − 1)2n−j + 2n−j−1 + 1 ≤ i ≤ k2n−j\n\n0 k2n−j + 1 ≤ i ≤ 2n,\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 135\n\nwith 0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Of course\n\nw1 = (1, . . . , 1)︸ ︷︷ ︸\n2n\n\n.\n\nThe above formulae look a little better if we change our indexing slightly by letting k vary\nfrom 0 to 2j − 1, and using the index j instead of 2j.\n\nDefinition 5.1. The vectors of the Haar basis of dimension 2n are denoted by\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\nwhere\n\nhjk(i) =\n\n\n0 1 ≤ i ≤ k2n−j\n\n1 k2n−j + 1 ≤ i ≤ k2n−j + 2n−j−1\n\n−1 k2n−j + 2n−j−1 + 1 ≤ i ≤ (k + 1)2n−j\n\n0 (k + 1)2n−j + 1 ≤ i ≤ 2n,\n\nwith 0 ≤ j ≤ n− 1 and 0 ≤ k ≤ 2j − 1. The 2n × 2n matrix whose columns are the vectors\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\n(in that order), is called the Haar matrix of dimension 2n, and is denoted by Wn.\n\nIt turns out that there is a way to understand these formulae better if we interpret a\nvector u = (u1, . . . , um) as a piecewise linear function over the interval [0, 1).\n\nDefinition 5.2. Given a vector u = (u1, . . . , um), the piecewise linear function plf(u) is\ndefined such that\n\nplf(u)(x) = ui,\ni− 1\n\nm\n≤ x <\n\ni\n\nm\n, 1 ≤ i ≤ m.\n\nIn words, the function plf(u) has the value u1 on the interval [0, 1/m), the value u2 on\n[1/m, 2/m), etc., and the value um on the interval [(m− 1)/m, 1).\n\nFor example, the piecewise linear function associated with the vector\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3)\n\nis shown in Figure 5.4.\nThen each basis vector hjk corresponds to the function\n\nψjk = plf(hjk).\n\nIn particular, for all n, the Haar basis vectors\n\nh0\n0 = w2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n\n2n\n\n\n\n136 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n−2\n\n−1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nFigure 5.4: The piecewise linear function plf(u).\n\nyield the same piecewise linear function ψ given by\n\nψ(x) =\n\n\n1 if 0 ≤ x < 1/2\n\n−1 if 1/2 ≤ x < 1\n\n0 otherwise,\n\nwhose graph is shown in Figure 5.5. It is easy to see that ψjk is given by the simple expression\n\n1\n\n1\n\n−1\n\n0\n\nFigure 5.5: The Haar wavelet ψ.\n\nψjk(x) = ψ(2jx− k), 0 ≤ j ≤ n− 1, 0 ≤ k ≤ 2j − 1.\n\nThe above formula makes it clear that ψjk is obtained from ψ by scaling and shifting.\n\nDefinition 5.3. The function φ0\n0 = plf(w1) is the piecewise linear function with the constant\n\nvalue 1 on [0, 1), and the functions ψjk = plf(hjk) together with φ0\n0 are known as the Haar\n\nwavelets .\n\nRather than using W−1 to convert a vector u to a vector c of coefficients over the Haar\nbasis, and the matrix W to reconstruct the vector u from its Haar coefficients c, we can use\nfaster algorithms that use averaging and differencing.\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 137\n\nIf c is a vector of Haar coefficients of dimension 2n, we compute the sequence of vectors\nu0, u1, . . ., un as follows:\n\nu0 = c\n\nuj+1 = uj\n\nuj+1(2i− 1) = uj(i) + uj(2j + i)\n\nuj+1(2i) = uj(i)− uj(2j + i),\n\nfor j = 0, . . . , n− 1 and i = 1, . . . , 2j. The reconstructed vector (signal) is u = un.\n\nIf u is a vector of dimension 2n, we compute the sequence of vectors cn, cn−1, . . . , c0 as\nfollows:\n\ncn = u\n\ncj = cj+1\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/2,\n\nfor j = n− 1, . . . , 0 and i = 1, . . . , 2j. The vector over the Haar basis is c = c0.\n\nWe leave it as an exercise to implement the above programs in Matlab using two variables\nu and c, and by building iteratively 2j. Here is an example of the conversion of a vector to\nits Haar coefficients for n = 3.\n\nGiven the sequence u = (31, 29, 23, 17,−6,−8,−2,−4), we get the sequence\n\nc3 = (31, 29, 23, 17,−6,−8, 2,−4)\n\nc2 =\n\n(\n31 + 29\n\n2\n,\n23 + 17\n\n2\n,\n−6− 8\n\n2\n,\n−2− 4\n\n2\n,\n31− 29\n\n2\n,\n23− 17\n\n2\n,\n−6− (−8)\n\n2\n,\n−2− (−4)\n\n2\n\n)\n= (30, 20,−7,−3, 1, 3, 1, 1)\n\nc1 =\n\n(\n30 + 20\n\n2\n,\n−7− 3\n\n2\n,\n30− 20\n\n2\n,\n−7− (−3)\n\n2\n, 1, 3, 1, 1\n\n)\n= (25,−5, 5,−2, 1, 3, 1, 1)\n\nc0 =\n\n(\n25− 5\n\n2\n,\n25− (−5)\n\n2\n, 5,−2, 1, 3, 1, 1\n\n)\n= (10, 15, 5,−2, 1, 3, 1, 1)\n\nso c = (10, 15, 5,−2, 1, 3, 1, 1). Conversely, given c = (10, 15, 5,−2, 1, 3, 1, 1), we get the\nsequence\n\nu0 = (10, 15, 5,−2, 1, 3, 1, 1)\n\nu1 = (10 + 15, 10− 15, 5,−2, 1, 3, 1, 1) = (25,−5, 5,−2, 1, 3, 1, 1)\n\nu2 = (25 + 5, 25− 5,−5 + (−2),−5− (−2), 1, 3, 1, 1) = (30, 20,−7,−3, 1, 3, 1, 1)\n\nu3 = (30 + 1, 30− 1, 20 + 3, 20− 3,−7 + 1,−7− 1,−3 + 1,−3− 1)\n\n= (31, 29, 23, 17,−6,−8,−2,−4),\n\nwhich gives back u = (31, 29, 23, 17,−6,−8,−2,−4).\n\n\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix Wn of dimension 2n\n\nthat makes it clearer why the columns of Wn are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split Wn into two\n2n × 2n−1 matrices, then the second matrix containing the last 2n−1 columns of Wn has a\nvery simple structure: it consists of the vector\n\n(1,−1, 0, . . . , 0)︸ ︷︷ ︸\n2n\n\nand 2n−1 − 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 0 0\n−1 0 0 0\n0 1 0 0\n0 −1 0 0\n0 0 1 0\n0 0 −1 0\n0 0 0 1\n0 0 0 −1\n\n\n.\n\nObserve that this matrix can be obtained from the identity matrix I2n−1 , in our example\n\nI4 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nby forming the 2n × 2n−1 matrix obtained by replacing each 1 by the column vector(\n1\n−1\n\n)\nand each zero by the column vector (\n\n0\n0\n\n)\n.\n\nNow the first half of Wn, that is the matrix consisting of the first 2n−1 columns of Wn, can\nbe obtained from Wn−1 by forming the 2n× 2n−1 matrix obtained by replacing each 1 by the\ncolumn vector (\n\n1\n1\n\n)\n,\n\neach −1 by the column vector (\n−1\n−1\n\n)\n,\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix W,, of dimension 2”\nthat makes it clearer why the columns of W,, are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split W,, into two\n2” x 2\"-! matrices, then the second matrix containing the last 2\"~' columns of W,, has a\nvery simple structure: it consists of the vector\n\n(1, -1,0,...,0)\n\nXX w)\n\\-\n\nQn\n\nand 2”~! — 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 OO\n-l1 0 O 0\n0 1 0 0\n0 -l 0 0O\n0 0 1 0\n0 O -l 0O\n0 O O 1\n0 O QO -Il\n\nObserve that this matrix can be obtained from the identity matrix [gn-1, in our example\n\n0 0\n\n0\nI, 0 ’\n1\n\noOo OrF\noor ©\nor c&\n\nby forming the 2” x 2”~! matrix obtained by replacing each 1 by the column vector\n\n(4)\n()\n\nNow the first half of W,,, that is the matrix consisting of the first 2”~! columns of W,,, can\nbe obtained from W,,_; by forming the 2” x 2”! matrix obtained by replacing each 1 by the\ncolumn vector\n\nand each zero by the column vector\n\neach —1 by the column vector\n\n\n\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector (\n0\n0\n\n)\n.\n\nFor n = 3, the first half of W3 is the matrix\n\n1 1 1 0\n1 1 1 0\n1 1 −1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 1\n1 −1 0 −1\n1 −1 0 −1\n\n\nwhich is indeed obtained from\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given a m×n matrix A = (aij) and a p×q matrix B = (bij), the Kronecker\nproduct (or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined. Then it is immediately verified that Wn is given by\nthe following neat recursive equations:\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector\n\nFor n = 3, the first half of W3 is the matrix\n\nje ee ee ee ee oe oe\n\nwhich is indeed obtained from\n\nW2 =\n\nfo oe ee ee\n\n-1 0 -l\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given am xn matrix A = (a;;) anda pxq matrix B = (b;;), the Kronecker\nproduct (or tensor product) A® B of A and B is the mp x nq matrix\n\nay,B ai2B te QinB\nA 2 B= an B an 7 : Amn B\nAmB Am2PB vee AmnP\n\nIt can be shown that © is associative and that\n\n(A@ B)\\(C®D) =AC@BD\n(A@B)'=A' @B',\n\nwhenever AC and BD are well defined. Then it is immediately verified that W,, is given by\nthe following neat recursive equations:\n\nHe-(000(0) 2(2))\n\n\n\n\n140 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nwith W0 = (1). If we let\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n,\n\nthen it is not hard to use the Kronecker product formulation of Wn to obtain a rigorous\nproof of the equation\n\nW>\nn Wn = Bn, for all n ≥ 1.\n\nThe above equation offers a clean justification of the fact that the columns of Wn are pairwise\northogonal.\n\nObserve that the right block (of size 2n × 2n−1) shows clearly how the detail coefficients\nin the second half of the vector c are added and subtracted to the entries in the first half of\nthe partially reconstructed vector after n− 1 steps.\n\n5.4 Multiresolution Signal Analysis with Haar Bases\n\nAn important and attractive feature of the Haar basis is that it provides a multiresolution\nanalysis of a signal. Indeed, given a signal u, if c = (c1, . . . , c2n) is the vector of its Haar coef-\nficients, the coefficients with low index give coarse information about u, and the coefficients\nwith high index represent fine information. For example, if u is an audio signal corresponding\nto a Mozart concerto played by an orchestra, c1 corresponds to the “background noise,” c2\n\nto the bass, c3 to the first cello, c4 to the second cello, c5, c6, c7, c7 to the violas, then the\nviolins, etc. This multiresolution feature of wavelets can be exploited to compress a signal,\nthat is, to use fewer coefficients to represent it. Here is an example.\n\nConsider the signal\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3),\n\nwhose Haar transform is\nc = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).\n\nThe piecewise-linear curves corresponding to u and c are shown in Figure 5.6. Since some of\nthe coefficients in c are small (smaller than or equal to 0.2) we can compress c by replacing\nthem by 0. We get\n\nc2 = (2, 0, 0, 3, 0, 0, 2, 0),\n\nand the reconstructed signal is\n\nu2 = (2, 2, 2, 2, 7, 3,−1,−1).\n\nThe piecewise-linear curves corresponding to u2 and c2 are shown in Figure 5.7.\n\n\n\n5.4. MULTIRESOLUTION SIGNAL ANALYSIS WITH HAAR BASES 141\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n2\n\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.6: A signal and its Haar transform.\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.7: A compressed signal and its compressed Haar transform.\n\nAn interesting (and amusing) application of the Haar wavelets is to the compression of\naudio signals. It turns out that if your type load handel in Matlab an audio file will be\nloaded in a vector denoted by y, and if you type sound(y), the computer will play this piece\nof music. You can convert y to its vector of Haar coefficients c. The length of y is 73113,\nso first tuncate the tail of y to get a vector of length 65536 = 216. A plot of the signals\ncorresponding to y and c is shown in Figure 5.8. Then run a program that sets all coefficients\nof c whose absolute value is less that 0.05 to zero. This sets 37272 coefficients to 0. The\nresulting vector c2 is converted to a signal y2. A plot of the signals corresponding to y2 and\nc2 is shown in Figure 5.9. When you type sound(y2), you find that the music doesn’t differ\nmuch from the original, although it sounds less crisp. You should play with other numbers\ngreater than or less than 0.05. You should hear what happens when you type sound(c). It\nplays the music corresponding to the Haar transform c of y, and it is quite funny.\n\n\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n0 1 2 3 4 5 6 7\nx 104\n\n−1\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2n × 2n matrix Wn of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W>\n\nn is not the inverse of Wn,\nbut rather the matrix\n\nW−1\nn = DnW\n\n>\nn\n\nwith Dn = diag\n(\n\n2−n, 2−n︸︷︷︸\n20\n\n, 2−(n−1), 2−(n−1)︸ ︷︷ ︸\n21\n\n, 2−(n−2), . . . , 2−(n−2)︸ ︷︷ ︸\n22\n\n, . . . , 2−1, . . . , 2−1︸ ︷︷ ︸\n2n−1\n\n)\n.\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n-0.6-\n\n-0.8\n0\n\nx10\" x10!\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n1 7 7 7 7 7 7 0.6\n\n-0.6+\n\nA L L L L L L 08\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2” x 2” matrix W,, of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W,! is not the inverse of Wp,\nbut rather the matrix\n\nW,' = D,,W,!\n\nwith D,, = diag (2-\", gam Q-(M-V) Q-(M=1) g-(n-2)g(n-2)— gd ).\nae 2 .\n2° 21 22 gn—1\n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n\nHn = WnD\n1\n2\nn\n\nwhose columns are the normalized Haar basis vectors, with\n\nD\n1\n2\nn = diag\n\n(\n2−\n\nn\n2 , 2−\n\nn\n2︸︷︷︸\n\n20\n\n, 2−\nn−1\n\n2 , 2−\nn−1\n\n2︸ ︷︷ ︸\n21\n\n, 2−\nn−2\n\n2 , . . . , 2−\nn−2\n\n2︸ ︷︷ ︸\n22\n\n, . . . , 2−\n1\n2 , . . . , 2−\n\n1\n2︸ ︷︷ ︸\n\n2n−1\n\n)\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c = H>n u\nthe normalized Haar coefficients of u.\n\nBecause Hn is orthogonal, H−1\nn = H>n .\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H>n u and Hnc as follows: When computing the sequence of ujs, use\n\nuj+1(2i− 1) = (uj(i) + uj(2j + i))/\n√\n\n2\n\nuj+1(2i) = (uj(i)− uj(2j + i))/\n√\n\n2,\n\nand when computing the sequence of cjs, use\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/\n√\n\n2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/\n√\n\n2.\n\nNote that things are now more symmetric, at the expense of a division by\n√\n\n2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/\n√\n\n2n\n\nand u by\n√\n\n2n. This is because the norm of the basis functions ψjk is not equal to 1 (under\n\nthe inner product 〈f, g〉 =\n∫ 1\n\n0\nf(t)g(t)dt). The normalized basis functions are the functions√\n\n2jψjk.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix Wn, the method using Hn being identical (except that H−1\n\nn = H>n , but this does\nnot hold for W−1\n\nn ). Given a 2m × 2n matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W−1\n\nn , obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W−1\n\nm . Because columns and rows\nare exchanged in the first step,\n\nB = A(W−1\nn )>,\n\nand in the second step C = W−1\nm B, thus, we have\n\nC = W−1\nm A(W−1\n\nn )> = DmW\n>\nmAWnDn.\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n1\nHH, = W,D;i\nwhose columns are the normalized Haar basis vectors, with\n\n4 . -R 9 m jg mel Qj m=1 2-2 _n=2 = _1\nDy = diag(272,2°2,2°> 2,27 2 272 ,...,2707 ,...,272,...,2°2\n— A A\n“ a “- -~_—\"’\n20 Q1 92 gn-1\n\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c= Hu\nthe normalized Haar coefficients of wu.\n\nBecause H,, is orthogonal, Hy! = H,.\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H,'u and H,,c as follows: When computing the sequence of u/s, use\n\nw+1(2i — 1) = (w(t) + wi (2’ +: 1))/V2\nw*\"(2i) = (wi (i) — w (2? + i))/v2,\n\nand when computing the sequence of c’s, use\n\n(i) = (At (21 — 1) + At (21))/V2\n(2) +i) = (F*1(2i — 1) — F121) /Vv2.\n\nNote that things are now more symmetric, at the expense of a division by 2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/2”\nand u by V2”. This is because el norm t the basis functions wi is not equal to 1 (under\n\nthe inner product (f,g) = fr fl . The normalized basis functions are the functions\nV2.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix W,,, the method using H,, being identical (except that H7' = H,’, but this does\nnot hold for W,-'). Given a 2” x 2” matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W,-', obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W,,'. Because columns and rows\nare exchanged in the first step,\n\nB= A(W,,')\",\nand in the second step C = W,;'B, thus, we have\n\nC=W,'A(W,')' = D,W,) AW, Dn-\n\n\n\n\n144 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIn the other direction, given a 2m × 2n matrix C of Haar coefficients, we reconstruct the\nmatrix A (the image) by first applying Wm to the columns of C, obtaining B, and then W>\n\nn\n\nto the rows of B. Therefore\n\nA = WmCW\n>\nn .\n\nOf course, we don’t actually have to invert Wm and Wn and perform matrix multiplications.\nWe just have to use our algorithms using averaging and differencing. Here is an example.\n\nIf the data matrix (the image) is the 8× 8 matrix\n\nA =\n\n\n\n64 2 3 61 60 6 7 57\n9 55 54 12 13 51 50 16\n17 47 46 20 21 43 42 24\n40 26 27 37 36 30 31 33\n32 34 35 29 28 38 39 25\n41 23 22 44 45 19 18 48\n49 15 14 52 53 11 10 56\n8 58 59 5 4 62 63 1\n\n\n,\n\nthen applying our algorithms, we find that\n\nC =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0.5 0.5 27 −25 23 −21\n0 0 −0.5 −0.5 −11 9 −7 5\n0 0 0.5 0.5 −5 7 −9 11\n0 0 −0.5 −0.5 21 −23 25 −27\n\n\n.\n\nAs we can see, C has more zero entries than A; it is a compressed version of A. We can\nfurther compress C by setting to 0 all entries of absolute value at most 0.5. Then we get\n\nC2 =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0 0 27 −25 23 −21\n0 0 0 0 −11 9 −7 5\n0 0 0 0 −5 7 −9 11\n0 0 0 0 21 −23 25 −27\n\n\n.\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 145\n\nWe find that the reconstructed image is\n\nA2 =\n\n\n\n63.5 1.5 3.5 61.5 59.5 5.5 7.5 57.5\n9.5 55.5 53.5 11.5 13.5 51.5 49.5 15.5\n17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5\n39.5 25.5 27.5 37.5 35.5 29.5 31.5 33.5\n31.5 33.5 35.5 29.5 27.5 37.5 39.5 25.5\n41.5 23.5 21.5 43.5 45.5 19.5 17.5 47.5\n49.5 15.5 13.5 51.5 53.5 11.5 9.5 55.5\n7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5\n\n\n,\n\nwhich is pretty close to the original image matrix A.\n\nIt turns out that Matlab has a wonderful command, image(X) (also imagesc(X), which\noften does a better job), which displays the matrix X has an image in which each entry\nis shown as a little square whose gray level is proportional to the numerical value of that\nentry (lighter if the value is higher, darker if the value is closer to zero; negative values are\ntreated as zero). The images corresponding to A and C are shown in Figure 5.10. The\n\nFigure 5.10: An image and its Haar transform.\n\ncompressed images corresponding to A2 and C2 are shown in Figure 5.11. The compressed\nversions appear to be indistinguishable from the originals!\n\nIf we use the normalized matrices Hm and Hn, then the equations relating the image\nmatrix A and its normalized Haar transform C are\n\nC = H>mAHn\n\nA = HmCH\n>\nn .\n\nThe Haar transform can also be used to send large images progressively over the internet.\nIndeed, we can start sending the Haar coefficients of the matrix C starting from the coarsest\n\n\n\n146 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.11: Compressed image and its Haar transform.\n\ncoefficients (the first column from top down, then the second column, etc.), and at the\nreceiving end we can start reconstructing the image as soon as we have received enough\ndata.\n\nObserve that instead of performing all rounds of averaging and differencing on each row\nand each column, we can perform partial encoding (and decoding). For example, we can\nperform a single round of averaging and differencing for each row and each column. The\nresult is an image consisting of four subimages, where the top left quarter is a coarser version\nof the original, and the rest (consisting of three pieces) contain the finest detail coefficients.\nWe can also perform two rounds of averaging and differencing, or three rounds, etc. The\nsecond round of averaging and differencing is applied to the top left quarter of the image.\nGenerally, the kth round is applied to the 2m+1−k × 2n+1−k submatrix consisting of the first\n2m+1−k rows and the first 2n+1−k columns (1 ≤ k ≤ n) of the matrix obtained at the end of\nthe previous round. This process is illustrated on the image shown in Figure 5.12. The result\nof performing one round, two rounds, three rounds, and nine rounds of averaging is shown in\nFigure 5.13. Since our images have size 512× 512, nine rounds of averaging yields the Haar\ntransform, displayed as the image on the bottom right. The original image has completely\ndisappeared! We leave it as a fun exercise to modify the algorithms involving averaging and\ndifferencing to perform k rounds of averaging/differencing. The reconstruction algorithm is\na little tricky.\n\nA nice and easily accessible account of wavelets and their uses in image processing and\ncomputer graphics can be found in Stollnitz, Derose and Salesin [166]. A very detailed\naccount is given in Strang and and Nguyen [170], but this book assumes a fair amount of\nbackground in signal processing.\n\nWe can find easily a basis of 2n × 2n = 22n vectors wij (2n × 2n matrices) for the linear\nmap that reconstructs an image from its Haar coefficients, in the sense that for any 2n × 2n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 147\n\nFigure 5.12: Original drawing by Durer.\n\nmatrix C of Haar coefficients, the image matrix A is given by\n\nA =\n2n∑\ni=1\n\n2n∑\nj=1\n\ncijwij.\n\nIndeed, the matrix wij is given by the so-called outer product\n\nwij = wi(wj)\n>.\n\nSimilarly, there is a basis of 2n × 2n = 22n vectors hij (2n × 2n matrices) for the 2D Haar\ntransform, in the sense that for any 2n × 2n matrix A, its matrix C of Haar coefficients is\ngiven by\n\nC =\n2n∑\ni=1\n\n2n∑\nj=1\n\naijhij.\n\nIf the columns of W−1 are w′1, . . . , w\n′\n2n , then\n\nhij = w′i(w\n′\nj)\n>.\n\nWe leave it as exercise to compute the bases (wij) and (hij) for n = 2, and to display the\ncorresponding images using the command imagesc.\n\n\n\n148 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.13: Haar tranforms after one, two, three, and nine rounds of averaging.\n\n\n\n5.6. HADAMARD MATRICES 149\n\n5.6 Hadamard Matrices\n\nThere is another famous family of matrices somewhat similar to Haar matrices, but these\nmatrices have entries +1 and −1 (no zero entries).\n\nDefinition 5.6. A real n × n matrix H is a Hadamard matrix if hij = ±1 for all i, j such\nthat 1 ≤ i, j ≤ n and if\n\nH>H = nIn.\n\nThus the columns of a Hadamard matrix are pairwise orthogonal. Because H is a square\nmatrix, the equation H>H = nIn shows that H is invertible, so we also have HH> = nIn.\nThe following matrices are example of Hadamard matrices:\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n, H4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n ,\n\nand\n\nH8 =\n\n\n\n1 1 1 1 1 1 1 1\n1 −1 1 −1 1 −1 1 −1\n1 1 −1 −1 1 1 −1 −1\n1 −1 −1 1 1 −1 −1 1\n1 1 1 1 −1 −1 −1 −1\n1 −1 1 −1 −1 1 −1 1\n1 1 −1 −1 −1 −1 1 1\n1 −1 −1 1 −1 1 1 −1\n\n\n.\n\nA natural question is to determine the positive integers n for which a Hadamard matrix\nof dimension n exists, but surprisingly this is an open problem. The Hadamard conjecture is\nthat for every positive integer of the form n = 4k, there is a Hadamard matrix of dimension\nn.\n\nWhat is known is a necessary condition and various sufficient conditions.\n\nTheorem 5.1. If H is an n×n Hadamard matrix, then either n = 1, 2, or n = 4k for some\npositive integer k.\n\nSylvester introduced a family of Hadamard matrices and proved that there are Hadamard\nmatrices of dimension n = 2m for all m ≥ 1 using the following construction.\n\nProposition 5.2. (Sylvester, 1867) If H is a Hadamard matrix of dimension n, then the\nblock matrix of dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\n\n\n150 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIf we start with\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n,\n\nwe obtain an infinite family of symmetric Hadamard matrices usually called Sylvester–\nHadamard matrices and denoted by H2m . The Sylvester–Hadamard matrices H2, H4 and\nH8 are shown on the previous page.\n\nIn 1893, Hadamard gave examples of Hadamard matrices for n = 12 and n = 20. At the\npresent, Hadamard matrices are known for all n = 4k ≤ 1000, except for n = 668, 716, and\n892.\n\nHadamard matrices have various applications to error correcting codes, signal processing,\nand numerical linear algebra; see Seberry, Wysocki and Wysocki [152] and Tropp [175]. For\nexample, there is a code based on H32 that can correct 7 errors in any 32-bit encoded block,\nand can detect an eighth. This code was used on a Mariner spacecraft in 1969 to transmit\npictures back to the earth.\n\nFor every m ≥ 0, the piecewise affine functions plf((H2m)i) associated with the 2m rows\nof the Sylvester–Hadamard matrix H2m are functions on [0, 1] known as the Walsh functions .\nIt is customary to index these 2m functions by the integers 0, 1, . . . , 2m−1 in such a way that\nthe Walsh function Wal(k, t) is equal to the function plf((H2m)i) associated with the Row i\nof H2m that contains k changes of signs between consecutive groups of +1 and consecutive\ngroups of −1. For example, the fifth row of H8, namely(\n\n1 −1 −1 1 1 −1 −1 1\n)\n,\n\nhas five consecutive blocks of +1s and −1s, four sign changes between these blocks, and thus\nis associated with Wal(4, t). In particular, Walsh functions corresponding to the rows of H8\n\n(from top down) are:\n\nWal(0, t), Wal(7, t), Wal(3, t), Wal(4, t), Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).\n\nBecause of the connection between Sylvester–Hadamard matrices and Walsh functions,\nSylvester–Hadamard matrices are called Walsh–Hadamard matrices by some authors. For\nevery m, the 2m Walsh functions are pairwise orthogonal. The countable set of Walsh\nfunctions Wal(k, t) for all m ≥ 0 and all k such that 0 ≤ k ≤ 2m − 1 can be ordered in\nsuch a way that it is an orthogonal Hilbert basis of the Hilbert space L2([0, 1)]; see Seberry,\nWysocki and Wysocki [152].\n\nThe Sylvester–Hadamard matrix H2m plays a role in various algorithms for dimension\nreduction and low-rank matrix approximation. There is a type of structured dimension-\nreduction map known as the subsampled randomized Hadamard transform, for short SRHT;\nsee Tropp [175] and Halko, Martinsson and Tropp [86]. For ` � n = 2m, an SRHT matrix\nis an `× n matrix of the form\n\nΦ =\n\n√\nn\n\n`\nRHD,\n\nwhere\n\n\n\n5.7. SUMMARY 151\n\n1. D is a random n× n diagonal matrix whose entries are independent random signs.\n\n2. H = n−1/2Hn, a normalized Sylvester–Hadamard matrix of dimension n.\n\n3. R is a random ` × n matrix that restricts an n-dimensional vector to ` coordinates,\nchosen uniformly at random.\n\nIt is explained in Tropp [175] that for any input x such that ‖x‖2 = 1, the probability\n\nthat |(HDx)i| ≥\n√\nn−1 log(n) for any i is quite small. Thus HD has the effect of “flattening”\n\nthe input x. The main result about the SRHT is that it preserves the geometry of an entire\nsubspace of vectors; see Tropp [175] (Theorem 1.3).\n\n5.7 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Haar basis vectors and a glimpse at Haar wavelets .\n\n• Kronecker product (or tensor product) of matrices.\n\n• Hadamard and Sylvester–Hadamard matrices.\n\n• Walsh functions.\n\n5.8 Problems\n\nProblem 5.1. (Haar extravaganza) Consider the matrix\n\nW3,3 =\n\n\n\n1 0 0 0 1 0 0 0\n1 0 0 0 −1 0 0 0\n0 1 0 0 0 1 0 0\n0 1 0 0 0 −1 0 0\n0 0 1 0 0 0 1 0\n0 0 1 0 0 0 −1 0\n0 0 0 1 0 0 0 1\n0 0 0 1 0 0 0 −1\n\n\n.\n\n(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,3c of applying\nW3,3 to c is\n\nW3,3c = (c1 + c5, c1 − c5, c2 + c6, c2 − c6, c3 + c7, c3 − c7, c4 + c8, c4 − c8),\n\nthe last step in reconstructing a vector from its Haar coefficients.\n\n\n\n152 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n(2) Prove that the inverse of W3,3 is (1/2)W>\n3,3. Prove that the columns and the rows of\n\nW3,3 are orthogonal.\n\n(3) Let W3,2 and W3,1 be the following matrices:\n\nW3,2 =\n\n\n\n1 0 1 0 0 0 0 0\n1 0 −1 0 0 0 0 0\n0 1 0 1 0 0 0 0\n0 1 0 −1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n, W3,1 =\n\n\n\n1 1 0 0 0 0 0 0\n1 −1 0 0 0 0 0 0\n0 0 1 0 0 0 0 0\n0 0 0 1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n.\n\nShow that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c of applying W3,2\n\nto c is\nW3,2c = (c1 + c3, c1 − c3, c2 + c4, c2 − c4, c5, c6, c7, c8),\n\nthe second step in reconstructing a vector from its Haar coefficients, and the result W3,1c of\napplying W3,1 to c is\n\nW3,1c = (c1 + c2, c1 − c2, c3, c4, c5, c6, c7, c8),\n\nthe first step in reconstructing a vector from its Haar coefficients.\n\nConclude that\nW3,3W3,2W3,1 = W3,\n\nthe Haar matrix\n\nW3 =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nHint . First check that\n\nW3,2W3,1 =\n\n(\nW2 04,4\n\n04,4 I4\n\n)\n,\n\nwhere\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\n\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W−1\n\n3 are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W−1\n\n3 orthogonal? Find the inverse of W3,2 and\nthe inverse of W3,1.\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n ≥ 2, the 2n × 2n matrix Wn,n is obtained form the two rows\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n, 1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n,−1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\nby shifting them 2n−1 − 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (c1, c2, . . . , c2n), show that Wn,nc is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W−1\n\nn,n = (1/2)W>\nn,n,\n\nand that the columns and the rows of Wn,n are orthogonal.\n\n(2) Given a m× n matrix A = (aij) and a p× q matrix B = (bij), the Kronecker product\n(or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown (and you may use these facts without proof) that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined.\n\nCheck that\n\nWn,n =\n\n(\nI2n−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\nand that\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n.\n\nUse the above to reprove that\n\nWn,nW\n>\nn,n = 2I2n .\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W35 and W3, are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W; ' are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W;' orthogonal? Find the inverse of W3,9 and\nthe inverse of Ws).\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n > 2, the 2” x 2” matrix W,,,, is obtained form the two rows\n\nwe we\n\ngn-1 gn-1\n1,0,.. ,0, —1, 0, ,0\n\na ww\n\ngn-1 gn—-1\n\nby shifting them 2”~! — 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (C1, C2,...,C2n), show that W,,,c is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W,,, = (1/2)W,),,\nand that the columns and the rows of W,,,, are orthogonal.\n\n(2) Given am x n matrix A = (a;;) and a p x q matrix B = (b;;), the Kronecker product\n(or tensor product) A ® B of A and B is the mp x nq matrix\n\nayiB aygB -:+ ayy,B\nAg pa | OW CR G08\nAmB Am2B vee AmnP\n\nIt can be shown (and you may use these facts without proof) that @ is associative and that\n\n(A@ B)(C®D) =AC @ BD\n(A@B)'=A' @B',\n\nwhenever AC’ and BD are well defined.\n\nCheck that\n1 1\nHan=(t09(2) tove(4)),\n1 1\nHis (tise(?) tes0(3)),\n\nUse the above to reprove that\n\nand that\n\nWrnW. n= 2lon.\n\n\n\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n.\n\nProve that\nW>\nn Wn = Bn, for all n ≥ 1.\n\n(3) The matrix Wn,i is obtained from the matrix Wi,i (1 ≤ i ≤ n− 1) as follows:\n\nWn,i =\n\n(\nWi,i 02i,2n−2i\n\n02n−2i,2i I2n−2i\n\n)\n.\n\nIt consists of four blocks, where 02i,2n−2i and 02n−2i,2i are matrices of zeros and I2n−2i is the\nidentity matrix of dimension 2n − 2i.\n\nExplain what Wn,i does to c and prove that\n\nWn,nWn,n−1 · · ·Wn,1 = Wn,\n\nwhere Wn is the Haar matrix of dimension 2n.\n\nHint . Use induction on k, with the induction hypothesis\n\nWn,kWn,k−1 · · ·Wn,1 =\n\n(\nWk 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProve that the columns and rows of Wn,k are orthogonal, and use this to prove that the\ncolumns of Wn and the rows of W−1\n\nn are orthogonal. Are the rows of Wn orthogonal? Are\nthe columns of W−1\n\nn orthogonal? Prove that\n\nW−1\nn,k =\n\n(\n1\n2\nW>\nk,k 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\nof dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k, t) for k = 0, 1, . . . , 7.\n\nProblem 5.5. Describe a recursive algorithm to compute the productH2m x of the Sylvester–\nHadamard matrix H2m by a vector x ∈ R2m that uses m recursive calls.\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n1 0 2 0\nm=2(5 t)= (0 3)\nand for n > 1,\nB, 0\nBri = 2 ( 0 .)\nProve that\n\nW/W, =Bn, for alln>1.\n\n(3) The matrix W,,; is obtained from the matrix W;; (1 <7 <n -—1) as follows:\n\nWri = ( Wii Man)\n\nQon_9i, 24 Ton _9i\n\nIt consists of four blocks, where 09: 9n_9i and Ogn_9i 9: are matrices of zeros and Jn_»: is the\nidentity matrix of dimension 2” — 2°.\n\nExplain what W,,; does to c and prove that\nWrinWrn—1 ute Writ = Wr,\n\nwhere W,, is the Haar matrix of dimension 2”.\n\nHint. Use induction on k, with the induction hypothesis\n\nWrrWnr pis Wra = ( We 2)\n\nOon_9k 9k Ton _9k\n\nProve that the columns and rows of W,,, are orthogonal, and use this to prove that the\ncolumns of W,, and the rows of W,! are orthogonal. Are the rows of W,, orthogonal? Are\nthe columns of W,-! orthogonal? Prove that\n\n1 T\nw-! _ 5W ik Ook on_9k\nnk .\n\nOon_9k 9k Ton _9k\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\n\nof dimension 2n,\nH H\nH —-H]’\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k,t) for k = 0,1,..., 7.\n\nis a Hadamard matrix.\n\nProblem 5.5. Describe a recursive algorithm to compute the product Hom x of the Sylvester—\nHadamard matrix Hym by a vector x € R?” that uses m recursive calls.\n\n\n\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E\n\n∐\nF of two vector spaces using the cartesian\n\nproduct E × F , we don’t quite get the right notion because elements of E × F are ordered\npairs, but we want E\n\n∐\nF = F\n\n∐\nE. Thus, we want to think of the elements of E\n\n∐\nF as\n\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(Ei)i∈{1,2}, and more generally of a family (Ei)i∈I . For simplicity, we begin by considering\nthe case where I = {1, 2}.\nDefinition 6.1. Given a family (Ei)i∈{1,2} of two vector spaces, we define the (external)\ndirect sum E1\n\n∐\nE2 (or coproduct) of the family (Ei)i∈{1,2} as the set\n\nE1\n\n∐\nE2 = {{〈1, u〉, 〈2, v〉} | u ∈ E1, v ∈ E2},\n\nwith addition\n\n{〈1, u1〉, 〈2, v1〉}+ {〈1, u2〉, 〈2, v2〉} = {〈1, u1 + u2〉, 〈2, v1 + v2〉},\nand scalar multiplication\n\nλ{〈1, u〉, 〈2, v〉} = {〈1, λu〉, 〈2, λv〉}.\nWe define the injections in1 : E1 → E1\n\n∐\nE2 and in2 : E2 → E1\n\n∐\nE2 as the linear maps\n\ndefined such that,\nin1(u) = {〈1, u〉, 〈2, 0〉},\n\nand\nin2(v) = {〈1, 0〉, 〈2, v〉}.\n\n155\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field AK. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E' || F of two vector spaces using the cartesian\nproduct E x F’, we don’t quite get the right notion because elements of EF x F' are ordered\npairs, but we want E || F = F [| £. Thus, we want to think of the elements of E'[] F as\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(E;)iefi2}, and more generally of a family (£;)ier. For simplicity, we begin by considering\nthe case where I = {1,2}.\n\nDefinition 6.1. Given a family (F;)ic{1,2} of two vector spaces, we define the (external)\ndirect sum E\\ [| E, (or coproduct) of the family (E;):e(1,2} as the set\n\nE, [| & = {{(1,u), (2,0)} we Ey, v € By},\nwith addition\n{(1, u1), (2, v1) } + {(1, U2), (2, v2) } = {(1, U1 + U2), (2, Ut + V2) },\n\nand scalar multiplication\n\nA{(1,u), (2,v)} = {(1, Nu), (2, rv) }-\n\nWe define the injections inj: Ey > E, [|] EF, and ing: Ey > E, [|] FE as the linear maps\n\ndefined such that,\niny(u) = {(1, u), (2, 0),\n\nand\n\ning(v) = {(1, 0), (2, v)}.\n\n155\n\n\n\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\n\nE2\n\n∐\nE1 = {{〈2, v〉, 〈1, u〉} | v ∈ E2, u ∈ E1} = E1\n\n∐\nE2.\n\nThus, every member {〈1, u〉, 〈2, v〉} of E1\n\n∐\nE2 can be viewed as an unordered pair consisting\n\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, E1\n\n∐\nE2 is just the product\n\n∏\ni∈{1,2}Ei of the family (Ei)i∈{1,2}.\n\n� This is not to be confused with the cartesian product E1×E2. The vector space E1×E2\n\nis the set of all ordered pairs 〈u, v〉, where u ∈ E1, and v ∈ E2, with addition and\nmultiplication by a scalar defined such that\n\n〈u1, v1〉+ 〈u2, v2〉 = 〈u1 + u2, v1 + v2〉,\nλ〈u, v〉 = 〈λu, λv〉.\n\nThere is a bijection between\n∏\n\ni∈{1,2}Ei and E1 × E2, but as we just saw, elements of∏\ni∈{1,2}Ei are certain sets. The product E1 × · · · × En of any number of vector spaces\n\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E1 and E2, the set E1\n\n∐\nE2 is a vector\n\nspace. For every pair of linear maps, f : E1 → G and g : E2 → G, there is a unique linear\nmap, f + g : E1\n\n∐\nE2 → G, such that (f + g) ◦ in1 = f and (f + g) ◦ in2 = g, as in the\n\nfollowing diagram:\nE1\n\nin1\n\n��\n\nf\n\n''PPPPPPPPPPPPPPPP\n\nE1\n\n∐\nE2\n\nf+g // G\n\nE2\n\nin2\n\nOO\n\ng\n\n77nnnnnnnnnnnnnnnn\n\nProof. Define\n(f + g)({〈1, u〉, 〈2, v〉}) = f(u) + g(v),\n\nfor every u ∈ E1 and v ∈ E2. It is immediately verified that f + g is the unique linear map\nwith the required properties.\n\nWe already noted that E1\n\n∐\nE2 is in bijection with E1 ×E2. If we define the projections\n\nπ1 : E1\n\n∐\nE2 → E1 and π2 : E1\n\n∐\nE2 → E2, such that\n\nπ1({〈1, u〉, 〈2, v〉}) = u,\n\nand\nπ2({〈1, u〉, 〈2, v〉}) = v,\n\nwe have the following proposition.\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\nFy] ] FE. = {{(2,v), (l.u)} |v € Be, we BY} = E, [] F.\n\nThus, every member {(1, wu), (2,v)} of E [] £2 can be viewed as an unordered pair consisting\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, FE, [[ E> is just the product [Hien.23 EF; of the family (2; )ieg2}-\n\n© This is not to be confused with the cartesian product EF, x E,. The vector space EF; x Es\nis the set of all ordered pairs (u,v), where u € Fy, and v € £5, with addition and\nmultiplication by a scalar defined such that\n\n(U1, U1) + (Ug, V2) = (uy + Ug, U1 + V2),\nA(u,v) = (Au, Av).\n\nThere is a bijection between [];. 12} E, and E, x E,, but as we just saw, elements of\n\nIL- 4.2} Ej, are certain sets. The product FE, x --- x E, of any number of vector spaces\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E, and E>, the set E, [| E> is a vector\nspace. For every pair of linear maps, f: FE, >~ G and g: Ep > G, there is a unique linear\nmap, f +g: E, [|] Ek, - G, such that (f +g) coin, = f and (f + 9) cing = g, as in the\nfollowing diagram:\n\nEy\namy\nEi Il Ey f+g G\nin| a\nEy\n\nProof. Define\n(f + 9) ACL, u), (2,0)}) = Flu) + g(r),\n\nfor every u € FE; and v € Eg. It is immediately verified that f + g is the unique linear map\nwith the required properties. im\n\nWe already noted that FE, [| £2 is in bijection with FE, x E>. If we define the projections\nTy: Fy [| £2 —> EF, and T9: EF, [| £2 > Es, such that\n\nm({(1,u), (2, v)}) =U,\nand\nmo({(1, u), (2, v)}) =v,\n\nwe have the following proposition.\n\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E1 and E2, for every pair of linear maps,\nf : D → E1 and g : D → E2, there is a unique linear map, f × g : D → E1\n\n∐\nE2, such that\n\nπ1 ◦ (f × g) = f and π2 ◦ (f × g) = g, as in the following diagram:\n\nE1\n\nD\nf×g //\n\nf\n\n77nnnnnnnnnnnnnnnn\n\ng\n((PPPPPPPPPPPPPPPP E1\n\n∐\nE2\n\nπ1\n\nOO\n\nπ2\n\n��\nE2\n\nProof. Define\n(f × g)(w) = {〈1, f(w)〉, 〈2, g(w)〉},\n\nfor every w ∈ D. It is immediately verified that f × g is the unique linear map with the\nrequired properties.\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U, V are subspaces of a vector space E, letting i1 : U → E and i2 : V → E be the\ninclusion maps, if U\n\n∐\nV is isomomorphic to E under the map i1 + i2 given by Proposition\n\n6.1, we say that E is a direct sum of U and V , and we write E = U\n∐\nV (with a slight abuse\n\nof notation, since E and U\n∐\nV are only isomorphic). It is also convenient to define the sum\n\nU1 + · · ·+ Up and the internal direct sum U1 ⊕ · · · ⊕ Up of any number of subspaces of E.\n\nDefinition 6.2. Given p ≥ 2 vector spaces E1, . . . , Ep, the product F = E1 × · · · × Ep can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)\n\nλ(u1, . . . , up) = (λu1, . . . , λup),\n\nfor all ui, vi ∈ Ei and all λ ∈ R. The zero vector of E1 × · · · × Ep is the p-tuple\n\n( 0, . . . , 0︸ ︷︷ ︸\np\n\n),\n\nwhere the ith zero is the zero vector of Ei.\n\nWith the above addition and multiplication, the vector space F = E1× · · ·×Ep is called\nthe direct product of the vector spaces E1, . . . , Ep.\n\nAs a special case, when E1 = · · · = Ep = R, we find again the vector space F = Rp. The\nprojection maps pri : E1 × · · · × Ep → Ei given by\n\npri(u1, . . . , up) = ui\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E, and E>, for every pair of linear maps,\nf: D> E, and g: D> En, there is a unique linear map, f x g: D > E, [| Es, such that\n™0(f xg) =f andm0(f x g) = 4, as in the following diagram:\n\nProof. Define\n(f x g)(w) = {C1 F(w)), (2; g(w)) fs\n\nfor every w € D. It is immediately verified that f x g is the unique linear map with the\nrequired properties. C]\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U,V are subspaces of a vector space E, letting i;: U > E and 12: V > E be the\ninclusion maps, if U]]V is isomomorphic to F under the map 7; + 22 given by Proposition\n6.1, we say that E is a direct sum of U and V, and we write E = U|[V (with a slight abuse\nof notation, since F and U [| V are only isomorphic). It is also convenient to define the sum\nU, +---+U, and the internal direct sum U; © --- @ U, of any number of subspaces of F.\n\nDefinition 6.2. Given p > 2 vector spaces E},...,E,, the product F' = EF, x --- x E, can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(U1,.--,Up) + (U1,---, Up) = (Ur + U1, ---, Up + Up)\n\nA(u1,-+-,Up) = (Au, --, AUp),\n\nfor all u;,v; € E; and all A € R. The zero vector of EF, x --- x E, is the p-tuple\n\nwhere the ith zero is the zero vector of E;.\n\nWith the above addition and multiplication, the vector space fF’ = LE) x --- x E, is called\nthe direct product of the vector spaces F),..., Ep.\n\nAs a special case, when EF, = --- = E, = R, we find again the vector space F = R?. The\nprojection maps pr;: Ey x --- x E, + E; given by\n\npri(uy,...,Up) = Uj\n\n\n\n\n158 CHAPTER 6. DIRECT SUMS\n\nare clearly linear. Similarly, the maps ini : Ei → E1 × · · · × Ep given by\n\nini(ui) = (0, . . . , 0, ui, 0, . . . , 0)\n\nare injective and linear. If dim(Ei) = ni and if (ei1, . . . , e\ni\nni\n\n) is a basis of Ei for i = 1, . . . , p,\nthen it is easy to see that the n1 + · · ·+ np vectors\n\n(e1\n1, 0, . . . , 0), . . . , (e1\n\nn1\n, 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ei1, 0, . . . , 0), . . . , (0, . . . , 0, eini , 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ep1), . . . , (0, . . . , 0, epnp)\n\nform a basis of E1 × · · · × Ep, and so\n\ndim(E1 × · · · × Ep) = dim(E1) + · · ·+ dim(Ep).\n\nLet us now consider a vector space E and p subspaces U1, . . . , Up of E. We have a map\n\na : U1 × · · · × Up → E\n\ngiven by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p. It is clear that this map is linear, and so its image is a subspace\nof E denoted by\n\nU1 + · · ·+ Up\n\nand called the sum of the subspaces U1, . . . , Up. By definition,\n\nU1 + · · ·+ Up = {u1 + · · ·+ up | ui ∈ Ui, 1 ≤ i ≤ p},\n\nand it is immediately verified that U1 + · · · + Up is the smallest subspace of E containing\nU1, . . . , Up. This also implies that U1 + · · ·+ Up does not depend on the order of the factors\nUi; in particular,\n\nU1 + U2 = U2 + U1.\n\nDefinition 6.3. For any vector space E and any p ≥ 2 subspaces U1, . . . , Up of E, if the\nmap a defined above is injective, then the sum U1 + · · ·+ Up is called a direct sum and it is\ndenoted by\n\nU1 ⊕ · · · ⊕ Up.\nThe space E is the direct sum of the subspaces Ui if\n\nE = U1 ⊕ · · · ⊕ Up.\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 159\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1.\n\nIf the map a is injective, then by Proposition 3.14 we have Ker a = {( 0, . . . , 0︸ ︷︷ ︸\np\n\n)} where\n\neach 0 is the zero vector of E, which means that if ui ∈ Ui for i = 1, . . . , p and if\n\nu1 + · · ·+ up = 0,\n\nthen (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.\n\nProposition 6.3. If the map a : U1×· · ·×Up → E is injective, then every u ∈ U1 + · · ·+Up\nhas a unique expression as a sum\n\nu = u1 + · · ·+ up,\n\nwith ui ∈ Ui, for i = 1, . . . , p.\n\nProof. If\nu = v1 + · · ·+ vp = w1 + · · ·+ wp,\n\nwith vi, wi ∈ Ui, for i = 1, . . . , p, then we have\n\nw1 − v1 + · · ·+ wp − vp = 0,\n\nand since vi, wi ∈ Ui and each Ui is a subspace, wi−vi ∈ Ui. The injectivity of a implies that\nwi−vi = 0, that is, wi = vi for i = 1, . . . , p, which shows the uniqueness of the decomposition\nof u.\n\nProposition 6.4. If the map a : U1 × · · · ×Up → E is injective, then any p nonzero vectors\nu1, . . . , up with ui ∈ Ui are linearly independent.\n\nProof. To see this, assume that\n\nλ1u1 + · · ·+ λpup = 0\n\nfor some λi ∈ R. Since ui ∈ Ui and Ui is a subspace, λiui ∈ Ui, and the injectivity of a\nimplies that λiui = 0, for i = 1, . . . , p. Since ui 6= 0, we must have λi = 0 for i = 1, . . . , p;\nthat is, u1, . . . , up with ui ∈ Ui and ui 6= 0 are linearly independent.\n\nObserve that if a is injective, then we must have Ui ∩Uj = (0) whenever i 6= j. However,\nthis condition is generally not sufficient if p ≥ 3. For example, if E = R2 and U1 the line\nspanned by e1 = (1, 0), U2 is the line spanned by d = (1, 1), and U3 is the line spanned by\ne2 = (0, 1), then U1∩U2 = U1∩U3 = U2∩U3 = {(0, 0)}, but U1+U2 = U1+U3 = U2+U3 = R2,\nso U1 + U2 + U3 is not a direct sum. For example, d is expressed in two different ways as\n\nd = (1, 1) = (1, 0) + (0, 1) = e1 + e2.\n\n\n\n160 CHAPTER 6. DIRECT SUMS\n\nSee Figure 6.1.\n\ne1\nU1\n\ne\n\nU3\n\n2 (1,1)\n\nU2\n\nFigure 6.1: The linear subspaces U1, U2, and U3 illustrated as lines in R2.\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1. Observe that when the map a is injective,\nthen it is a linear isomorphism between U1 × · · · × Up and U1 ⊕ · · · ⊕ Up. The difference is\nthat U1 × · · · × Up is defined even if the spaces Ui are not assumed to be subspaces of some\ncommon space.\n\nIf E is a direct sum E = U1⊕· · ·⊕Up, since any p nonzero vectors u1, . . . , up with ui ∈ Ui\nare linearly independent, if we pick a basis (uk)k∈Ij in Uj for j = 1, . . . , p, then (ui)i∈I with\nI = I1 ∪ · · · ∪ Ip is a basis of E. Intuitively, E is split into p independent subspaces.\n\nConversely, given a basis (ui)i∈I of E, if we partition the index set I as I = I1 ∪ · · · ∪ Ip,\nthen each subfamily (uk)k∈Ij spans some subspace Uj of E, and it is immediately verified\nthat we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\nDefinition 6.4. Let f : E → E be a linear map. For any subspace U of E, if f(U) ⊆ U we\nsay that U is invariant under f .\n\nAssume that E is finite-dimensional, a direct sum E = U1 ⊕ · · · ⊕ Up, and that each Uj\nis invariant under f . If we pick a basis (ui)i∈I as above with I = I1 ∪ · · · ∪ Ip and with\neach (uk)k∈Ij a basis of Uj, since each Uj is invariant under f , the image f(uk) of every basis\nvector uk with k ∈ Ij belongs to Uj, so the matrix A representing f over the basis (ui)i∈I is\na block diagonal matrix of the form\n\nA =\n\n\nA1\n\nA2\n\n. . .\n\nAp\n\n ,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 161\n\nwith each block Aj a dj × dj-matrix with dj = dim(Uj) and all other entries equal to 0. If\ndj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.\n\nThere are natural injections from each Ui to E denoted by ini : Ui → E.\n\nNow, if p = 2, it is easy to determine the kernel of the map a : U1 × U2 → E. We have\n\na(u1, u2) = u1 + u2 = 0 iff u1 = −u2, u1 ∈ U1, u2 ∈ U2,\n\nwhich implies that\n\nKer a = {(u,−u) | u ∈ U1 ∩ U2}.\nNow, U1 ∩ U2 is a subspace of E and the linear map u 7→ (u,−u) is clearly an isomorphism\nbetween U1 ∩U2 and Ker a, so Ker a is isomorphic to U1 ∩U2. As a consequence, we get the\nfollowing result:\n\nProposition 6.5. Given any vector space E and any two subspaces U1 and U2, the sum\nU1 + U2 is a direct sum iff U1 ∩ U2 = (0).\n\nAn interesting illustration of the notion of direct sum is the decomposition of a square\nmatrix into its symmetric part and its skew-symmetric part. Recall that an n × n matrix\nA ∈ Mn is symmetric if A> = A, skew -symmetric if A> = −A. It is clear that s\n\nS(n) = {A ∈ Mn | A> = A} and Skew(n) = {A ∈ Mn | A> = −A}\n\nare subspaces of Mn, and that S(n)∩Skew(n) = (0). Observe that for any matrix A ∈ Mn,\nthe matrix H(A) = (A + A>)/2 is symmetric and the matrix S(A) = (A − A>)/2 is skew-\nsymmetric. Since\n\nA = H(A) + S(A) =\nA+ A>\n\n2\n+\nA− A>\n\n2\n,\n\nwe see that Mn = S(n) + Skew(n), and since S(n)∩Skew(n) = (0), we have the direct sum\n\nMn = S(n)⊕ Skew(n).\n\nRemark: The vector space Skew(n) of skew-symmetric matrices is also denoted by so(n).\nIt is the Lie algebra of the group SO(n).\n\nProposition 6.5 can be generalized to any p ≥ 2 subspaces at the expense of notation.\nThe proof of the following proposition is left as an exercise.\n\nProposition 6.6. Given any vector space E and any p ≥ 2 subspaces U1, . . . , Up, the fol-\nlowing properties are equivalent:\n\n(1) The sum U1 + · · ·+ Up is a direct sum.\n\n\n\n162 CHAPTER 6. DIRECT SUMS\n\n(2) We have\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0), i = 1, . . . , p.\n\n(3) We have\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nBecause of the isomorphism\n\nU1 × · · · × Up ≈ U1 ⊕ · · · ⊕ Up,\n\nwe have\n\nProposition 6.7. If E is any vector space, for any (finite-dimensional) subspaces U1, . . .,\nUp of E, we have\n\ndim(U1 ⊕ · · · ⊕ Up) = dim(U1) + · · ·+ dim(Up).\n\nIf E is a direct sum\nE = U1 ⊕ · · · ⊕ Up,\n\nsince every u ∈ E can be written in a unique way as\n\nu = u1 + · · ·+ up\n\nwith ui ∈ Ui for i = 1 . . . , p, we can define the maps πi : E → Ui, called projections , by\n\nπi(u) = πi(u1 + · · ·+ up) = ui.\n\nIt is easy to check that these maps are linear and satisfy the following properties:\n\nπj ◦ πi =\n\n{\nπi if i = j\n\n0 if i 6= j,\n\nπ1 + · · ·+ πp = idE.\n\nFor example, in the case of the direct sum\n\nMn = S(n)⊕ Skew(n),\n\nthe projection onto S(n) is given by\n\nπ1(A) = H(A) =\nA+ A>\n\n2\n,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nπ2(A) = S(A) =\nA− A>\n\n2\n.\n\nClearly, H(A)+S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that f ◦ f = f is said to be idempotent . Thus, the projections πi are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p ≥ 2 linear maps fi : E → E, if\n\nfj ◦ fi =\n\n{\nfi if i = j\n\n0 if i 6= j,\n\nf1 + · · ·+ fp = idE,\n\nthen if we let Ui = fi(E), we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f : E → E is an idempotent linear map, i.e.,\nf ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set I. First,\nlet us recall the notion of the product of a family (Ei)i∈I . Given a family of sets (Ei)i∈I , its\nproduct\n\n∏\ni∈I Ei, is the set of all functions f : I → ⋃\n\ni∈I Ei, such that, f(i) ∈ Ei, for every\ni ∈ I. It is one of the many versions of the axiom of choice, that, if Ei 6= ∅ for every i ∈ I,\nthen\n\n∏\ni∈I Ei 6= ∅. A member f ∈ ∏i∈I Ei, is often denoted as (fi)i∈I . For every i ∈ I, we\n\nhave the projection πi :\n∏\n\ni∈I Ei → Ei, defined such that, πi((fi)i∈I) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let I be any nonempty set, and let (Ei)i∈I be a family of vector spaces.\nThe (external) direct sum\n\n∐\ni∈I Ei (or coproduct) of the family (Ei)i∈I is defined as follows:∐\n\ni∈I Ei consists of all f ∈ ∏i∈I Ei, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(fi)i∈I + (gi)i∈I = (fi + gi)i∈I ,\n\nλ(fi)i∈I = (λfi)i∈I .\n\nWe also have injection maps ini : Ei →\n∐\n\ni∈I Ei, defined such that, ini(x) = (fi)i∈I , where\nfi = x, and fj = 0, for all j ∈ (I − {i}).\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nm™(A) = S(A) =\n\nClearly, H(A) +$(A) = A, H(H(A)) = H(A), $(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that fo f = f is said to be idempotent. Thus, the projections 7; are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p > 2 linear maps f;: E > E, if\n\nfi ifisj\nfo fi= “pe ye\n0 ift AF),\nfitet fp = ide,\nthen if we let U; = f,(E), we have a direct sum\n\nE=U,®-:®@U,.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f: E > E is an idempotent linear map, 1.e.,\nfof=f, then we have a direct sum\n\nE=Kerf @lImf,\n\nso that f is the projection onto its image Im f.\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set J. First,\nlet us recall the notion of the product of a family (£;);e7. Given a family of sets (F;)jer, its\nproduct [],-; i, is the set of all functions f: J > U,_, Ei, such that, f(i) € Ej, for every\ni € I. It is one of the many versions of the axiom of choice, that, if E; 4 @ for every 7 € J,\nthen [],<; £; 4 0. A member f € [J,-; Fi, is often denoted as (f;)ier. For every i € I, we\nhave the projection 7: [[,<; Ei + Ej, defined such that, m((fi)ier) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let J be any nonempty set, and let (F;)ic7 be a family of vector spaces.\nThe (external) direct sum [],-; E; (or coproduct) of the family (£;)ier is defined as follows:\n\nL<, £i consists of all f € [],<; i, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(flier + (Qi)ier = (fi + M)ier,\nA fidier = (Afidier-\n\nWe also have injection maps in;: E; > |],-; Fi, defined such that, in;(x) = (fi)ier, where\n\nfi =x, and f; =0, for all 7 € (J — {i}).\n\ntel\n\n\n\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (Ei)i∈I be a family of vector spaces, and\nlet G be any vector space. The direct sum\n\n∐\ni∈I Ei is a vector space, and for every family\n\n(hi)i∈I of linear maps hi : Ei → G, there is a unique linear map(∑\ni∈I\n\nhi\n\n)\n:\n∐\ni∈I\n\nEi → G,\n\nsuch that, (\n∑\n\ni∈I hi) ◦ ini = hi, for every i ∈ I.\n\nRemarks:\n\n(1) One might wonder why the direct sum\n∐\n\ni∈I Ei consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (Ei)i∈I as\n\n∏\ni∈I Ei? The product space\n\n∏\ni∈I Ei with addition and scalar\n\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map ĥ :\n\n∏\ni∈I Ei → G such that ĥ ◦ ini = hi for all ∈ I must be given by\n\nh̃((ui)∈I) =\n∑\ni∈I\n\nhi(ui),\n\nand if I is infinite, the sum on the right-hand side is infinite, and thus undefined! If I\nis finite then\n\n∏\ni∈I Ei and\n\n∐\ni∈I Ei are isomorphic.\n\n(2) When Ei = E, for all i ∈ I, we denote\n∐\n\ni∈I Ei by E(I). In particular, when Ei = K,\nfor all i ∈ I, we find the vector space K(I) of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f : E → F be a linear map. If\nf : E → F is injective, then there is a surjective linear map r : F → E called a retraction,\nsuch that r ◦ f = idE. See Figure 6.2. If f : E → F is surjective, then there is an injective\nlinear map s : F → E called a section, such that f ◦ s = idF . See Figure 6.3.\n\nProof. Let (ui)i∈I be a basis of E. Since f : E → F is an injective linear map, by Proposition\n3.15, (f(ui))i∈I is linearly independent in F . By Theorem 3.7, there is a basis (vj)j∈J of F ,\nwhere I ⊆ J , and where vi = f(ui), for all i ∈ I. By Proposition 3.15, a linear map r : F → E\ncan be defined such that r(vi) = ui, for all i ∈ I, and r(vj) = w for all j ∈ (J − I), where w\nis any given vector in E, say w = 0. Since r(f(ui)) = ui for all i ∈ I, by Proposition 3.15,\nwe have r ◦ f = idE.\n\nNow, assume that f : E → F is surjective. Let (vj)j∈J be a basis of F . Since f : E → F\nis surjective, for every vj ∈ F , there is some uj ∈ E such that f(uj) = vj. Since (vj)j∈J is a\nbasis of F , by Proposition 3.15, there is a unique linear map s : F → E such that s(vj) = uj.\nAlso, since f(s(vj)) = vj, by Proposition 3.15 (again), we must have f ◦ s = idF .\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (E;)icr be a family of vector spaces, and\nlet G be any vector space. The direct sum [],-,; Ei; 1s a vector space, and for every family\n(hi)icr of linear maps h;: E; + G, there is a unique linear map\n\n(Soni): []2-¢\n\n1EL tel\n\nsuch that, (Yoje, hi) 9 ini = hy, for every i € I.\n\nRemarks:\n\n(1) One might wonder why the direct sum [],_,; £; consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (E;)ier as [[,<; H;? The product space [],., £; with addition and scalar\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map h: [Ler E; > G such that ho in; = h; for all € J must be given by\n\nh((ui)er) = >, hi(ws);\niel\nand if J is infinite, the sum on the right-hand side is infinite, and thus undefined! If /\n\nis finite then [[,-, £; and [],_,; £; are isomorphic.\n\n(2) When E; = £, for all i € I, we denote [],-, E; by E. In particular, when E; = K,\nfor all i € I, we find the vector space K“ of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f: E + F be a linear map. If\nf: E - F is injective, then there is a surjective linear map r: F — E called a retraction,\nsuch that ro f =idg. See Figure 6.2. If f: E > F 1s surjective, then there is an injective\nlinear map s: F — E called a section, such that fos =idp. See Figure 6.3.\n\nProof. Let (u;)ier be a basis of E. Since f: E — F is an injective linear map, by Proposition\n3.15, (f(us))ier is linearly independent in F. By Theorem 3.7, there is a basis (v;)jey of F,\nwhere J C J, and where v; = f(u;), for alli € J. By Proposition 3.15, a linear map r: F > EF\ncan be defined such that r(v;) = u;, for all i € I, and r(v;) = w for all j € (J — J), where w\nis any given vector in FE, say w = 0. Since r(f(u;)) = u; for all i € I, by Proposition 3.15,\nwe have ro f = idg.\n\nNow, assume that f: E — F is surjective. Let (v;)je7 be a basis of F. Since f: E> F\nis surjective, for every v; € F, there is some u; € E such that f(u;) = v;. Since (v;)jey is a\nbasis of F’, by Proposition 3.15, there is a unique linear map s: F' > E such that s(v;) = uj;.\nAlso, since f(s(v;)) = v;, by Proposition 3.15 (again), we must have f os = idp. O\n\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 165\n\nu = (1,0)\n1\n\nu = (1,1)\n2\n\nf(x,y) = (x,y,0)\n\nv = f(u ) = (1,0,0)1 1 v  = f(u ) = (1,1,0)2 2\n\nv = (0,0,1)\n3\n\nr(x,y,z) = (x,y)\nE = R\n\n2\n\nF = R3\n\nFigure 6.2: Let f : E → F be the injective linear map from R2 to R3 given by f(x, y) =\n(x, y, 0). Then a surjective retraction is given by r : R3 → R2 is given by r(x, y, z) = (x, y).\nObserve that r(v1) = u1, r(v2) = u2, and r(v3) = 0 .\n\nThe converse of Proposition 6.11 is obvious.\n\nWe are now ready to prove a very crucial result relating the rank and the dimension of\nthe kernel of a linear map.\n\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation\n\nWe begin with the following fundamental proposition.\n\nProposition 6.12. Let E, F and G, be three vector spaces, f : E → F an injective linear\nmap, g : F → G a surjective linear map, and assume that Im f = Ker g. Then, the following\nproperties hold. (a) For any section s : G → F of g, we have F = Ker g ⊕ Im s, and the\nlinear map f + s : E ⊕G→ F is an isomorphism.1\n\n(b) For any retraction r : F → E of f , we have F = Im f ⊕Ker r.2\n\nE\nf //\n\nF\nr\noo\n\ng //\nG\n\ns\noo\n\nProof. (a) Since s : G→ F is a section of g, we have g ◦ s = idG, and for every u ∈ F ,\n\ng(u− s(g(u))) = g(u)− g(s(g(u))) = g(u)− g(u) = 0.\n\n1The existence of a section s : G→ F of g follows from Proposition 6.11.\n2The existence of a retraction r : F → E of f follows from Proposition 6.11.\n\n\n\n166 CHAPTER 6. DIRECT SUMS\n\nThus, u − s(g(u)) ∈ Ker g, and we have F = Ker g + Im s. On the other hand, if u ∈\nKer g ∩ Im s, then u = s(v) for some v ∈ G because u ∈ Im s, g(u) = 0 because u ∈ Ker g,\nand so,\n\ng(u) = g(s(v)) = v = 0,\n\nbecause g ◦ s = idG, which shows that u = s(v) = 0. Thus, F = Ker g ⊕ Im s, and since by\nassumption, Im f = Ker g, we have F = Im f ⊕ Im s. But then, since f and s are injective,\nf + s : E ⊕G→ F is an isomorphism. The proof of (b) is very similar.\n\nNote that we can choose a retraction r : F → E so that Ker r = Im s, since\nF = Ker g ⊕ Im s = Im f ⊕ Im s and f is injective so we can set r ≡ 0 on Im s.\n\nGiven a sequence of linear maps E\nf−→ F\n\ng−→ G, when Im f = Ker g, we say that the\n\nsequence E\nf−→ F\n\ng−→ G is exact at F . If in addition to being exact at F , f is injective\nand g is surjective, we say that we have a short exact sequence, and this is denoted as\n\n0 −→ E\nf−→ F\n\ng−→ G −→ 0.\n\nThe property of a short exact sequence given by Proposition 6.12 is often described by saying\n\nthat 0 −→ E\nf−→ F\n\ng−→ G −→ 0 is a (short) split exact sequence.\n\nAs a corollary of Proposition 6.12, we have the following result which shows that given\na linear map f : E → F , its domain E is the direct sum of its kernel Ker f with some\nisomorphic copy of its image Im f .\n\nTheorem 6.13. (Rank-nullity theorem) Let E and F be vector spaces, and let f : E → F\nbe a linear map. Then, E is isomorphic to Ker f ⊕ Im f , and thus,\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).\n\nSee Figure 6.3.\n\nProof. Consider\n\nKer f\ni−→ E\n\nf ′−→ Im f,\n\nwhere Ker f\ni−→ E is the inclusion map, and E\n\nf ′−→ Im f is the surjection associated\n\nwith E\nf−→ F . Then, we apply Proposition 6.12 to any section Im f\n\ns−→ E of f ′ to\nget an isomorphism between E and Ker f ⊕ Im f , and Proposition 6.7, to get dim(E) =\ndim(Ker f) + dim(Im f).\n\nDefinition 6.6. The dimension dim(Ker f) of the kernel of a linear map f is called the\nnullity of f .\n\nWe now derive some important results using Theorem 6.13.\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 167\n\nu = (0,1,1)\n\n2\n\nu = (1,0,1)\n1\n\nKer f\n\nf  = f(u  ) = (1,0)11\n\nf  =  f(u  ) = (0, 1)2 2 f(u) = (1,1)\n\nf(x,y,z) = (x,y)\n\ns(x,y) = (x,y,x+y)\n\nu = (1,1,1)\n\ns (f(u)) = (1,1,2)\n\nh = (0,0,-1)\n\nFigure 6.3: Let f : E → F be the linear map from R3 to R2 given by f(x, y, z) = (x, y).\nThen s : R2 → R3 is given by s(x, y) = (x, y, x + y) and maps the pink R2 isomorphically\nonto the slanted pink plane of R3 whose equation is −x − y + z = 0. Theorem 6.13 shows\nthat R3 is the direct sum of the plane −x− y + z = 0 and the kernel of f which the orange\nz-axis.\n\nProposition 6.14. Given a vector space E, if U and V are any two subspaces of E, then\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nan equation known as Grassmann’s relation.\n\nProof. Recall that U + V is the image of the linear map\n\na : U × V → E\n\ngiven by\n\na(u, v) = u+ v,\n\nand that we proved earlier that the kernel Ker a of a is isomorphic to U ∩ V . By Theorem\n6.13,\n\ndim(U × V ) = dim(Ker a) + dim(Im a),\n\nbut dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩ V ), and Im a = U + V , so the\nGrassmann relation holds.\n\nThe Grassmann relation can be very useful to figure out whether two subspace have a\nnontrivial intersection in spaces of dimension > 3. For example, it is easy to see that in R5,\nthere are subspaces U and V with dim(U) = 3 and dim(V ) = 2 such that U ∩ V = (0); for\nexample, let U be generated by the vectors (1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be\n\n\n\n168 CHAPTER 6. DIRECT SUMS\n\ngenerated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim that if dim(U) = 3\nand dim(V ) = 3, then dim(U ∩ V ) ≥ 1. Indeed, by the Grassmann relation, we have\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nnamely\n\n3 + 3 = 6 = dim(U + V ) + dim(U ∩ V ),\n\nand since U + V is a subspace of R5, dim(U + V ) ≤ 5, which implies\n\n6 ≤ 5 + dim(U ∩ V ),\n\nthat is 1 ≤ dim(U ∩ V ).\n\nAs another consequence of Proposition 6.14, if U and V are two hyperplanes in a vector\nspace of dimension n, so that dim(U) = n− 1 and dim(V ) = n− 1, the reader should show\nthat\n\ndim(U ∩ V ) ≥ n− 2,\n\nand so, if U 6= V , then\n\ndim(U ∩ V ) = n− 2.\n\nHere is a characterization of direct sums that follows directly from Theorem 6.13.\n\nProposition 6.15. If U1, . . . , Up are any subspaces of a finite dimensional vector space E,\nthen\n\ndim(U1 + · · ·+ Up) ≤ dim(U1) + · · ·+ dim(Up),\n\nand\n\ndim(U1 + · · ·+ Up) = dim(U1) + · · ·+ dim(Up)\n\niff the Uis form a direct sum U1 ⊕ · · · ⊕ Up.\n\nProof. If we apply Theorem 6.13 to the linear map\n\na : U1 × · · · × Up → U1 + · · ·+ Up\n\ngiven by a(u1, . . . , up) = u1 + · · ·+ up, we get\n\ndim(U1 + · · ·+ Up) = dim(U1 × · · · × Up)− dim(Ker a)\n\n= dim(U1) + · · ·+ dim(Up)− dim(Ker a),\n\nso the inequality follows. Since a is injective iff Ker a = (0), the Uis form a direct sum iff\nthe second equation holds.\n\nAnother important corollary of Theorem 6.13 is the following result:\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 169\n\nProposition 6.16. Let E and F be two vector spaces with the same finite dimension\ndim(E) = dim(F ) = n. For every linear map f : E → F , the following properties are\nequivalent:\n\n(a) f is bijective.\n\n(b) f is surjective.\n\n(c) f is injective.\n\n(d) Ker f = (0).\n\nProof. Obviously, (a) implies (b).\n\nIf f is surjective, then Im f = F , and so dim(Im f) = n. By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which means that\nKer f = (0), and so f is injective (see Proposition 3.14). This proves that (b) implies (c).\n\nIf f is injective, then by Proposition 3.14, Ker f = (0), so (c) implies (d).\n\nFinally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is injective (by Proposi-\ntion 3.14). By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(Ker f) = 0, we get\n\ndim(Im f) = dim(E) = dim(F ),\n\nwhich proves that f is also surjective, and thus bijective. This proves that (d) implies (a)\nand concludes the proof.\n\nOne should be warned that Proposition 6.16 fails in infinite dimension.\n\nThe following Proposition will also be useful.\n\nProposition 6.17. Let E be a vector space. If E = U ⊕ V and E = U ⊕W , then there is\nan isomorphism f : V → W between V and W .\n\nProof. Let R be the relation between V and W , defined such that\n\n〈v, w〉 ∈ R iff w − v ∈ U.\n\nWe claim that R is a functional relation that defines a linear isomorphism f : V → W\nbetween V and W , where f(v) = w iff 〈v, w〉 ∈ R (R is the graph of f). If w − v ∈ U and\nw′ − v ∈ U , then w′ − w ∈ U , and since U ⊕W is a direct sum, U ∩W = (0), and thus\n\n\n\n170 CHAPTER 6. DIRECT SUMS\n\nw′ − w = 0, that is w′ = w. Thus, R is functional. Similarly, if w − v ∈ U and w − v′ ∈ U ,\nthen v′ − v ∈ U , and since U ⊕ V is a direct sum, U ∩ V = (0), and v′ = v. Thus, f is\ninjective. Since E = U ⊕ V , for every w ∈ W , there exists a unique pair 〈u, v〉 ∈ U × V ,\nsuch that w = u+ v. Then, w− v ∈ U , and f is surjective. We also need to verify that f is\nlinear. If\n\nw − v = u\n\nand\nw′ − v′ = u′,\n\nwhere u, u′ ∈ U , then, we have\n\n(w + w′)− (v + v′) = (u+ u′),\n\nwhere u+ u′ ∈ U . Similarly, if\nw − v = u\n\nwhere u ∈ U , then we have\nλw − λv = λu,\n\nwhere λu ∈ U . Thus, f is linear.\n\nGiven a vector space E and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U ⊕ V depends only on U . We call dim(V ) the\ncodimension of U , and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f : E → F is the dimension dim(Im f) of the image\nsubspace Im f of F .\n\nWe have the following simple proposition.\n\nProposition 6.18. Given a linear map f : E → F , the following properties hold:\n\n(i) rk(f) = codim(Ker f).\n\n(ii) rk(f) + dim(Ker f) = dim(E).\n\n(iii) rk(f) ≤ min(dim(E), dim(F )).\n\nProof. Since by Proposition 6.13, dim(E) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = codim(Ker f). Since rk(f) = dim(Im f), (ii) follows\nfrom dim(E) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F , we have\nrk(f) ≤ dim(F ), and since rk(f) + dim(Ker f) = dim(E), we have rk(f) ≤ dim(E).\n\nThe rank of a matrix is defined as follows.\n\n170 CHAPTER 6. DIRECT SUMS\n\nw' —w = 0, that is w’ = w. Thus, R is functional. Similarly, if w—-v ¢€U andw—v' €U,\nthen v' — v € U, and since U @ V is a direct sum, UNV = (0), and v’ = v. Thus, f is\ninjective. Since EF = U @V, for every w € W, there exists a unique pair (u,v) € U x V,\nsuch that w = u+v. Then, w—v € U, and f is surjective. We also need to verify that f is\nlinear. If\n\nw-v=U\n\nand\n/ / /\nw—-v =u,\n\nwhere u, u’ € U, then, we have\n(wtw')—(v+v') =(utv),\n\nwhere u+u' € U. Similarly, if\n\nwhere u € U, then we have\nAw — Av = Au,\n\nwhere Au € U. Thus, f is linear. O\n\nGiven a vector space F and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U @V depends only on U. We call dim(V) the\ncodimension of U, and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f: E — F is the dimension dim(Im f) of the image\nsubspace Im f of F’.\n\nWe have the following simple proposition.\nProposition 6.18. Given a linear map f: E — F, the following properties hold:\n(i) rk(f) = codim(Ker f).\n(ii) rk(f) + dim(Ker f) = dim(E).\n(iit) rk(f) < min(dim(£), dim(F)).\n\nProof. Since by Proposition 6.13, dim(#) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = = codim(Ker f). Since rk(f) = dim/(Im f), (ii) follows\nfrom dim(F) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F’, we have\nrk(f) < dim(F), and since rk(f) + dim(Ker f) = dim(£), we have rk(f) < dim(E). O\n\nThe rank of a matrix is defined as follows.\n\n\n\n\n6.3. SUMMARY 171\n\nDefinition 6.7. Given a m × n-matrix A = (ai j) over the field K, the rank rk(A) of the\nmatrix A is the maximum number of linearly independent columns of A (viewed as vectors\nin Km).\n\nIn view of Proposition 3.8, the rank of a matrix A is the dimension of the subspace of\nKm generated by the columns of A. Let E and F be two vector spaces, and let (u1, . . . , un)\nbe a basis of E, and (v1, . . . , vm) a basis of F . Let f : E → F be a linear map, and let M(f)\nbe its matrix w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the\ndimension of Im f , which is generated by (f(u1), . . . , f(un)), the rank of f is the maximum\nnumber of linearly independent vectors in (f(u1), . . . , f(un)), which is equal to the number\nof linearly independent columns of M(f), since F and Km are isomorphic. Thus, we have\nrk(f) = rk(M(f)), for every matrix representing f .\n\nWe will see later, using duality, that the rank of a matrix A is also equal to the maximal\nnumber of linearly independent rows of A.\n\nIf U is a hyperplane, then E = U ⊕ V for some subspace V of dimension 1. However, a\nsubspace V of dimension 1 is generated by any nonzero vector v ∈ V , and thus we denote\nV by Kv, and we write E = U ⊕ Kv. Clearly, v /∈ U . Conversely, let x ∈ E be a vector\nsuch that x /∈ U (and thus, x 6= 0). We claim that E = U ⊕ Kx. Indeed, since U is a\nhyperplane, we have E = U ⊕Kv for some v /∈ U (with v 6= 0). Then, x ∈ E can be written\nin a unique way as x = u + λv, where u ∈ U , and since x /∈ U , we must have λ 6= 0, and\nthus, v = −λ−1u + λ−1x. Since E = U ⊕Kv, this shows that E = U + Kx. Since x /∈ U ,\nwe have U ∩Kx = 0, and thus E = U ⊕Kx. This argument shows that a hyperplane is a\nmaximal proper subspace H of E.\n\nIn Chapter 11, we shall see that hyperplanes are precisely the Kernels of nonnull linear\nmaps f : E → K, called linear forms.\n\n6.3 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Direct products, sums, direct sums .\n\n• Projections .\n\n• The fundamental equation\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)\n\n(Proposition 6.13).\n\n• Grassmann’s relation\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ).\n\n\n\n172 CHAPTER 6. DIRECT SUMS\n\n• Characterizations of a bijective linear map f : E → F .\n\n• Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E. Prove that if V ∪W is\na subspace of E, then either V ⊆ W or W ⊆ V .\n\nProblem 6.2. Prove that for every vector space E, if f : E → E is an idempotent linear\nmap, i.e., f ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nProblem 6.3. Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E and recall\nthat the linear map\n\na : U1 × · · · × Up → E\n\nis given by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p.\n\n(1) If we let Zi ⊆ U1 × · · · × Up be given by\n\nZi =\n\n{(\nu1, . . . , ui−1,−\n\np∑\nj=1,j 6=i\n\nuj, ui+1, . . . , up\n\n) ∣∣∣∣∣\np∑\n\nj=1,j 6=i\nuj ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)}\n,\n\nfor i = 1, . . . , p, then prove that\n\nKer a = Z1 = · · · = Zp.\n\nIn general, for any given i, the condition Ui ∩\n(∑p\n\nj=1,j 6=i Uj\n\n)\n= (0) does not necessarily\n\nimply that Zi = (0). Thus, let\n\nZ =\n\n{(\nu1, . . . , ui−1, ui, ui+1, . . . , up\n\n) ∣∣∣∣ ui = −\np∑\n\nj=1,j 6=i\nuj, ui ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n, 1 ≤ i ≤ p\n\n}\n.\n\nSince Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p,\n\n172 CHAPTER 6. DIRECT SUMS\n\ne Characterizations of a bijective linear map f: E > F.\n\ne Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E’. Prove that if V UW is\na subspace of £, then either V CW or W CV.\n\nProblem 6.2. Prove that for every vector space E, if f: E — E is an idempotent linear\nmap, i.e., fo f = f, then we have a direct sum\n\nE=Ker f @Imf,\nso that f is the projection onto its image Im f.\n\nProblem 6.3. Let U;,...,U, be any p > 2 subspaces of some vector space F and recall\nthat the linear map\na: U,xX+:+xU,> EF\n\nis given by\nA(U1,-.-,Up) = Uy +--+ + Up,\n\nwith u; € U; fori =1,...,p.\n(1) If we let Z; C U; x --- x U, be given by\n\nP\nZi, = { (tus tian ) Uj trys +5 Up)\n\nj=l ji\n\nS wevin( Su}.\n\njHljxi j=l jAt\n\nfori =1,...,p, then prove that\n\nKera = 24, =-:- = Z,.\n\nIn general, for any given i, the condition U;M ( eer v;) = (0) does not necessarily\nimply that Z; = (0). Thus, let\n\nUz=— 3 ww EU 3 uj), 1sisoh,\n\nj=l ix j=l j#i\n\nZ= { (tases tira thstssss sty)\n\nSince Kera = Z; =--: = Z,, we have Z = Kera. Prove that if\n\nun ( 3 5) =) l<ic<p,\n\nj=l Hi\n\n\n\n\n6.4. PROBLEMS 173\n\nthen Z = Ker a = (0).\n\n(2) Prove that U1 + · · ·+ Up is a direct sum iff\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p.\n\nProblem 6.4. Assume that E is finite-dimensional, and let fi : E → E be any p ≥ 2 linear\nmaps such that\n\nf1 + · · ·+ fp = idE.\n\nProve that the following properties are equivalent:\n\n(1) f 2\ni = fi, 1 ≤ i ≤ p.\n\n(2) fj ◦ fi = 0, for all i 6= j, 1 ≤ i, j ≤ p.\n\nHint . Use Problem 6.2.\n\nLet U1, . . . , Up be any p ≥ 2 subspaces of some vector space E. Prove that U1 + · · ·+ Up\nis a direct sum iff\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nProblem 6.5. Given any vector space E, a linear map f : E → E is an involution if\nf ◦ f = id.\n\n(1) Prove that an involution f is invertible. What is its inverse?\n\n(2) Let E1 and E−1 be the subspaces of E defined as follows:\n\nE1 = {u ∈ E | f(u) = u}\nE−1 = {u ∈ E | f(u) = −u}.\n\nProve that we have a direct sum\nE = E1 ⊕ E−1.\n\nHint . For every u ∈ E, write\n\nu =\nu+ f(u)\n\n2\n+\nu− f(u)\n\n2\n.\n\n(3) If E is finite-dimensional and f is an involution, prove that there is some basis of E\nwith respect to which the matrix of f is of the form\n\nIk,n−k =\n\n(\nIk 0\n0 −In−k\n\n)\n,\n\nwhere Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1). Can you give a\ngeometric interpretation of the action of f (especially when k = n− 1)?\n\n\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all (j, k) such that\nj − k ≥ 0. An upper Hessenberg matrix is unreduced if hi+1i 6= 0 for i = 1, . . . , n− 1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n× k matrix.\n\n(1) Prove that the k × k matrix A>A and the matrix A have the same nullspace. Use\nthis to prove that rank(A>A) = rank(A). Similarly, prove that the n × n matrix AA> and\nthe matrix A> have the same nullspace, and conclude that rank(AA>) = rank(A>).\n\nWe will prove later that rank(A>) = rank(A).\n\n(2) Let a1, . . . , ak be k linearly independent vectors in Rn (1 ≤ k ≤ n), and let A be the\nn× k matrix whose ith column is ai. Prove that A>A has rank k, and that it is invertible.\nLet P = A(A>A)−1A> (an n× n matrix). Prove that\n\nP 2 = P\n\nP> = P.\n\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak, or equivalently\nthe set of all vectors in Rn of the form Ax, with x ∈ Rk. Prove that the nullspace U of P is\nthe set of vectors u ∈ Rn such that A>u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of Rn onto the subspace V spanned by a1, . . . , ak, and\nthat\n\nRn = U ⊕ V.\n\nProblem 6.8. A rotation Rθ in the plane R2 is given by the matrix\n\nRθ =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n.\n\n(1) Use Matlab to show the action of a rotation Rθ on a simple figure such as a triangle\nor a rectangle, for various values of θ, including θ = π/6, π/4, π/3, π/2.\n\n(2) Prove that Rθ is invertible and that its inverse is R−θ.\n\n(3) For any two rotations Rα and Rβ, prove that\n\nRβ ◦Rα = Rα ◦Rβ = Rα+β.\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\nSO(2).\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n x n matrix H is upper Hessenberg if hj, = 0 for all (j,k) such that\nj —k > 0. An upper Hessenberg matrix is unreduced if hii4; 4 0 for? =1,...,n—1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n x k matrix.\n\n(1) Prove that the k x k matrix A'A and the matrix A have the same nullspace. Use\nthis to prove that rank(A' A) = rank(A). Similarly, prove that the n x n matrix AA’ and\nthe matrix A’ have the same nullspace, and conclude that rank(AA') = rank(A').\n\nWe will prove later that rank(A') = rank(A).\n\n(2) Let a1,...,a, be & linearly independent vectors in R\" (1 < k <n), and let A be the\nn x k matrix whose ith column is a;. Prove that A'A has rank k, and that it is invertible.\nLet P = A(A'A)~'A! (an n x n matrix). Prove that\n\nP? =P\nP'=P.\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a,...,a,, or equivalently\nthe set of all vectors in R” of the form Az, with x € R*. Prove that the nullspace U of P is\nthe set of vectors u € R” such that A'u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of R” onto the subspace V spanned by ay,,...,a,, and\nthat\n\nR\"=U®6V.\n\nProblem 6.8. A rotation Ro in the plane R? is given by the matrix\ncos@ —sin@\nRo = er cos 6 )\n\n(1) Use Matlab to show the action of a rotation Ry on a simple figure such as a triangle\nor a rectangle, for various values of 0, including 0 = 1/6, 7/4, 7/3, 7/2.\n\n(2) Prove that Rg is invertible and that its inverse is R_g.\n\n(3) For any two rotations Ry and Rg, prove that\nRg ° Ra = Ra ie) Rg = Ra+p-\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\n\nSO(2).\n\n\n\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Rθ,(a1,a2) in R2 given by(\ny1\n\ny2\n\n)\n=\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)(\nx1\n\nx2\n\n)\n+\n\n(\na1\n\na2\n\n)\n.\n\n(1) Prove that if θ 6= k2π, with k ∈ Z, then Rθ,(a1,a2) has a unique fixed point (c1, c2),\nthat is, there is a unique point (c1, c2) such that(\n\nc1\n\nc2\n\n)\n= Rθ,(a1,a2)\n\n(\nc1\n\nc2\n\n)\n,\n\nand this fixed point is given by(\nc1\n\nc2\n\n)\n=\n\n1\n\n2 sin(θ/2)\n\n(\ncos(π/2− θ/2) − sin(π/2− θ/2)\nsin(π/2− θ/2) cos(π/2− θ/2)\n\n)(\na1\n\na2\n\n)\n.\n\n(2) In this question we still assume that θ 6= k2π, with k ∈ Z. By translating the\ncoordinate system with origin (0, 0) to the new coordinate system with origin (c1, c2), which\nmeans that if (x1, x2) are the coordinates with respect to the standard origin (0, 0) and if\n(x′1, x\n\n′\n2) are the coordinates with respect to the new origin (c1, c2), we have\n\nx1 = x′1 + c1\n\nx2 = x′2 + c2\n\nand similarly for (y1, y2) and (y′1, y\n′\n2), then show that(\ny1\n\ny2\n\n)\n= Rθ,(a1,a2)\n\n(\nx1\n\nx2\n\n)\nbecomes (\n\ny′1\ny′2\n\n)\n= Rθ\n\n(\nx′1\nx′2\n\n)\n.\n\nConclude that with respect to the new origin (c1, c2), the affine map Rθ,(a1,a2) becomes\nthe rotation Rθ. We say that Rθ,(a1,a2) is a rotation of center (c1, c2).\n\n(3) Use Matlab to show the action of the affine map Rθ,(a1,a2) on a simple figure such as a\ntriangle or a rectangle, for θ = π/3 and various values of (a1, a2). Display the center (c1, c2)\nof the rotation.\n\nWhat kind of transformations correspond to θ = k2π, with k ∈ Z?\n\n(4) Prove that the inverse of Rθ,(a1,a2) is of the form R−θ,(b1,b2), and find (b1, b2) in terms\nof θ and (a1, a2).\n\n(5) Given two affine maps Rα,(a1,a2) and Rβ,(b1,b2), prove that\n\nRβ,(b1,b2) ◦Rα,(a1,a2) = Rα+β,(t1,t2)\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Ro (a,,a2) in R? given by\n\nV1 cos@ —sind L1 a1\n=(_ + .\nYo sind cos v9 a9\n(1) Prove that if 0 A k27, with k € Z, then Rova,,a,) has a unique fixed point (cy, ce),\nthat is, there is a unique point (c),c2) such that\n\nCy \\ Cy\n(‘:) = Ro (01,02) (“) )\nand this fixed point is given by\n\nc\\ 1 cos(7/2 — 0/2) —sin(a/2—6/2)\\ (a,\nC2)  2sin(@/2) \\sin(/2— 6/2) — cos(m/2 — 6/2) a2)”\n(2) In this question we still assume that 0 # k27, with k € Z. By translating the\ncoordinate system with origin (0,0) to the new coordinate system with origin (c1,c2), which\n\nmeans that if (7,22) are the coordinates with respect to the standard origin (0,0) and if\n(x, 75) are the coordinates with respect to the new origin (c;,c2), we have\n\n!\nT= XU,+\n\n!\nUq = Lo + CQ\n\nand similarly for (y1, y2) and (y{, 45), then show that\n\nY\\ Ly\n(32) = Boon (22)\nYi\\ ry\n(,) = Re (\").\n\nConclude that with respect to the new origin (ci, c2), the affine map Ro (a,,a,) becomes\nthe rotation Rg. We say that Rg (a,,a.) is a rotation of center (c1, C2).\n\nbecomes\n\n(3) Use Matlab to show the action of the affine map Rg (a,,a,) on a simple figure such as a\ntriangle or a rectangle, for 6 = 7/3 and various values of (a1, a2). Display the center (c1, ce)\nof the rotation.\n\nWhat kind of transformations correspond to 6 = k27, with k € Z?\n\n(4) Prove that the inverse of Rg (a,,a.) is of the form R_g (5,,.), and find (b), b2) in terms\nof @ and (a1, a2).\n\n(5) Given two affine maps Ro (a;,a.) and Rgo,,b), prove that\n\nR3.(b1,b2) ° Ra,(ar,a2) = Rots, (t1,t2)\n\n\n\n\n176 CHAPTER 6. DIRECT SUMS\n\nfor some (t1, t2), and find (t1, t2) in terms of β, (a1, a2) and (b1, b2).\n\nEven in the case where (a1, a2) = (0, 0), prove that in general\n\nRβ,(b1,b2) ◦Rα 6= Rα ◦Rβ,(b1,b2).\n\nUse (4)-(5) to show that the affine maps of the plane defined in this problem form a\nnonabelian group denoted SE(2).\n\nProve that Rβ,(b1,b2) ◦Rα,(a1,a2) is not a translation (possibly the identity) iff α+β 6= k2π,\nfor all k ∈ Z. Find its center of rotation when (a1, a2) = (0, 0).\n\nIf α+β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2) is a pure translation. Find the translation vector\nof Rβ,(b1,b2) ◦Rα,(a1,a2).\n\nProblem 6.10. (Affine subspaces) A subset A of Rn is called an affine subspace if either\nA = ∅, or there is some vector a ∈ Rn and some subspace U of Rn such that\n\nA = a+ U = {a+ u | u ∈ U}.\nWe define the dimension dim(A) of A as the dimension dim(U) of U .\n\n(1) If A = a+ U , why is a ∈ A?\n\nWhat are affine subspaces of dimension 0? What are affine subspaces of dimension 1\n(begin with R2)? What are affine subspaces of dimension 2 (begin with R3)?\n\nProve that any nonempty affine subspace is closed under affine combinations.\n\n(2) Prove that if A = a + U is any nonempty affine subspace, then A = b + U for any\nb ∈ A.\n\n(3) Let A be any nonempty subset of Rn closed under affine combinations. For any\na ∈ A, prove that\n\nUa = {x− a ∈ Rn | x ∈ A}\nis a (linear) subspace of Rn such that\n\nA = a+ Ua.\n\nProve that Ua does not depend on the choice of a ∈ A; that is, Ua = Ub for all a, b ∈ A. In\nfact, prove that\n\nUa = U = {y − x ∈ Rn | x, y ∈ A}, for all a ∈ A,\nand so\n\nA = a+ U, for any a ∈ A.\n\nRemark: The subspace U is called the direction of A.\n\n(4) Two nonempty affine subspaces A and B are said to be parallel iff they have the same\ndirection. Prove that that if A 6= B and A and B are parallel, then A ∩ B = ∅.\n\nRemark: The above shows that affine subspaces behave quite differently from linear sub-\nspaces.\n\n\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v1, . . . , vn) ∈ Rn, let\nv̂ ∈ Rn+1 be the vector v̂ = (v1, . . . , vn, 1). Equivalently, v̂ = (v̂1, . . . , v̂n+1) ∈ Rn+1 is the\nvector defined by\n\nv̂i =\n\n{\nvi if 1 ≤ i ≤ n,\n\n1 if i = n+ 1.\n\n(1) For any m+ 1 vectors (u0, u1, . . . , um) with ui ∈ Rn and m ≤ n, prove that if the m\nvectors (u1 − u0, . . . , um − u0) are linearly independent, then the m+ 1 vectors (û0, . . . , ûm)\nare linearly independent.\n\n(2) Prove that if the m + 1 vectors (û0, . . . , ûm) are linearly independent, then for any\nchoice of i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are\nlinearly independent.\n\nAny m+ 1 vectors (u0, u1, . . . , um) such that the m+ 1 vectors (û0, . . . , ûm) are linearly\nindependent are said to be affinely independent .\n\nFrom (1) and (2), the vector (u0, u1, . . . , um) are affinely independent iff for any any choice\nof i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are linearly\nindependent. If m = n, we say that n+ 1 affinely independent vectors (u0, u1, . . . , un) form\nan affine frame of Rn.\n\n(3) if (u0, u1, . . . , un) is an affine frame of Rn, then prove that for every vector v ∈ Rn,\nthere is a unique (n+ 1)-tuple (λ0, λ1, . . . , λn) ∈ Rn+1, with λ0 +λ1 + · · ·+λn = 1, such that\n\nv = λ0u0 + λ1u1 + · · ·+ λnun.\n\nThe scalars (λ0, λ1, . . . , λn) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (u0, u1, . . . , un).\n\nIf we write ei = ui − u0, for i = 1, . . . , n, then prove that we have\n\nv = u0 + λ1e1 + · · ·+ λnen,\n\nand since (e1, . . . , en) is a basis of Rn (by (1) & (2)), the n-tuple (λ1, . . . , λn) consists of the\nstandard coordinates of v − u0 over the basis (e1, . . . , en).\n\nConversely, for any vector u0 ∈ Rn and for any basis (e1, . . . , en) of Rn, let ui = u0 + ei\nfor i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an affine frame of Rn, and for any v ∈ Rn, if\n\nv = u0 + x1e1 + · · ·+ xnen,\n\nwith (x1, . . . , xn) ∈ Rn (unique), then\n\nv = (1− (x1 + · · ·+ xx))u0 + x1u1 + · · ·+ xnun,\n\nso that (1− (x1 + · · ·+xx)), x1, · · · , xn), are the barycentric coordinates of v w.r.t. the affine\nframe (u0, u1, . . . , un).\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v),...,Un) € R”, let\nv0 € R\"! be the vector 0 = (v,...,Un, 1). Equivalently, 0 = (01,...,0n41) € R°! is the\nvector defined by\n\n. {! ifl<i<n,\n\nUi= Lp\n1 ift=n-+1.\n(1) For any m+ 1 vectors (uo, U1,...,Um) with u; € R” and m <n, prove that if the m\nvectors (U1; — Uo,---,;Um — Uo) are linearly independent, then the m+ 1 vectors (to,...,Um)\n\nare linearly independent.\n\n(2) Prove that if the m+ 1 vectors (Uo,...,Um) are linearly independent, then for any\nchoice of 7, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with 7 —i # 0 are\nlinearly independent.\n\nAny m+ 1 vectors (uo, U1,---,Um) such that the m+ 1 vectors (Uo,..., Um) are linearly\nindependent are said to be affinely independent.\n\nFrom (1) and (2), the vector (uo, w1,..., Um) are affinely independent iff for any any choice\nof i, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with j —i ¥ 0 are linearly\nindependent. If m = n, we say that n +1 affinely independent vectors (uo, u1,...,Un) form\nan affine frame of R”.\n\n(3) if (uo, U1,...,Un) is an affine frame of R”, then prove that for every vector v € R”,\nthere is a unique (n+ 1)-tuple (Ag, \\1,---; An) € R°*!, with Ag+ Ai +--+: +An = 1, such that\n\nv= Agtto + Aut + +++ + Antn-\n\nThe scalars (Ao, A1,---,; An) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (uo, U1,.--, Un).\nIf we write e; = uj; — uo, for = 1,...,n, then prove that we have\n\nVv = Uo + Azer +++ + Anen,\n\nand since (€1,...,€n) is a basis of R” (by (1) & (2)), the n-tuple (\\1,..., An) consists of the\n\nstandard coordinates of v — uo over the basis (e1,...,€n).\nConversely, for any vector uo € R” and for any basis (€1,...,€n) of R”, let wu; = uo + e;\nfori =1,...,n. Prove that (uo, u1,..., Un) is an affine frame of R”, and for any v € R”, if\n\nU = Up + LE, Ft + Len,\nwith (a,...,%,) € R” (unique), then\nv= (1— (ay +--+ + 2z))uo + oyu H+ F 2p Un,\n\nso that (1— (a1 +--:+2z)),%1,-+++ ,2n), are the barycentric coordinates of v w.r.t. the affine\nframe (uo, U41,---, Un).\n\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (u0, . . .,\nun) and pairs (u0, (e1, . . . , en)), with (e1, . . . , en) a basis. Given an affine frame (u0, . . . , un),\nwe obtain the basis (e1, . . . , en) with ei = ui−u0, for i = 1, . . . , n; given the pair (u0, (e1, . . .,\nen)) where (e1, . . . , en) is a basis, we obtain the affine frame (u0, . . . , un), with ui = u0 + ei,\nfor i = 1, . . . , n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (u0, . . . , un) and standard coordinates w.r.t. the basis (e1, . . . , en).\nThe barycentric cordinates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the\nstandard coordinates (λ1, . . . , λn) of v − u0; the standard coordinates (x1, . . . , xn) of v − u0\n\nyield the barycentric coordinates (1− (x1 + · · ·+ xn), x1, . . . , xn) of v.\n\n(4) Recall that an affine map is a map f : E → F between vector spaces that preserves\naffine combinations ; that is,\n\nf\n\n(\nm∑\ni=1\n\nλiui\n\n)\n=\n\nm∑\ni=1\n\nλif(ui),\n\nfor all u1 . . . , um ∈ E and all λi ∈ K such that\n∑m\n\ni=1 λi = 1.\n\nLet (u0, . . . , un) be any affine frame in Rn and let (v0, . . . , vn) be any vectors in Rm. Prove\nthat there is a unique affine map f : Rn → Rm such that\n\nf(ui) = vi, i = 0, . . . , n.\n\n(5) Let (a0, . . . , an) be any affine frame in Rn and let (b0, . . . , bn) be any n+ 1 points in\nRn. Prove that there is a unique (n+ 1)× (n+ 1) matrix\n\nA =\n\n(\nB w\n0 1\n\n)\ncorresponding to the unique affine map f such that\n\nf(ai) = bi, i = 0, . . . , n,\n\nin the sense that\nAâi = b̂i, i = 0, . . . , n,\n\nand that A is given by\n\nA =\n(\nb̂0 b̂1 · · · b̂n\n\n) (\nâ0 â1 · · · ân\n\n)−1\n.\n\nMake sure to prove that the bottom row of A is (0, . . . , 0, 1).\n\nIn the special case where (a0, . . . , an) is the canonical affine frame with ai = ei+1 for\ni = 0, . . . , n− 1 and an = (0, . . . , 0) (where ei is the ith canonical basis vector), show that\n\n(\nâ0 â1 · · · ân\n\n)\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n1 1 · · · 1 1\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (up, ...,\nUn) and pairs (uo, (€1,.--,@n)), with (e1,...,e,) a basis. Given an affine frame (uo,..., Un),\nwe obtain the basis (€1,...,€,) with e; = u; — uo, for i = 1,...,; given the pair (uo, (e1,...,\n€,,)) where (€1,...,€n) is a basis, we obtain the affine frame (uo,..., Un), with wu; = uo + &;,\nfor 2 =1,...,n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (uo,...,Un) and standard coordinates w.r.t. the basis (e1,...,€n).\nThe barycentric cordinates (Xo, A1,---,;An) of v (with Ap + Ay +--+: + An = 1) yield the\nstandard coordinates (Ai,...,An) of vu — uo; the standard coordinates (1%1,...,2n) of v — uo\nyield the barycentric coordinates (1 — (a1 +-+++%n),%1,.--,2n) of v.\n\n(4) Recall that an affine map is a map f: EF — F' between vector spaces that preserves\naffine combinations; that is,\n\nf (> se) = » Af (ui);\n\nfor all uy ...,Um € E and all A; € K such that $0\", A; = 1.\n\nLet (uo,..., Un) be any affine frame in R” and let (vo,..., Un) be any vectors in R™. Prove\nthat there is a unique affine map f: R” > R” such that\n\nf(ui) =v;, t=0,...,n.\n\n(5) Let (ao,...,@n) be any affine frame in R” and let (bo,...,b,) be any n + 1 points in\nR”. Prove that there is a unique (n + 1) x (n + 1) matrix\n\nBow\na=(0 4)\ncorresponding to the unique affine map f such that\n\nf(a;) = b;, 1=0,...,N,\n\nin the sense that .\nAa; = b;, 1=0,...,N,\n\nand that A is given by\nA- (i Boe in) (@ @ +++ Gi).\n\nMake sure to prove that the bottom row of A is (0,...,0,1).\n\nIn the special case where (ao,...,@n) is the canonical affine frame with a; = e;4, for\ni=0,...,n—1 and a, = (0,...,0) (where e; is the 7th canonical basis vector), show that\n1 0.:--. 0 0\n\n0 1\n\na\n\nQ)\n\nOo\n\nQ)\n\n=\n\nQ\n\n=\n\n—\"\n\nI|\n\nOeee\nrer © oS\nFe Oa ©\n\ne\ne\n\n\n\n\n6.4. PROBLEMS 179\n\nand\n\n(\nâ0 â1 · · · ân\n\n)−1\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n−1 −1 · · · −1 1\n\n .\n\nFor example, when n = 2, if we write bi = (xi, yi), then we have\n\nA =\n\nx1 x2 x3\n\ny1 y2 y3\n\n1 1 1\n\n 1 0 0\n0 1 0\n−1 −1 1\n\n =\n\nx1 − x3 x2 − x3 x3\n\ny1 − y3 y2 − y3 y3\n\n0 0 1\n\n .\n\n(6) Recall that a nonempty affine subspace A of Rn is any nonempty subset of Rn closed\nunder affine combinations. For any affine map f : Rn → Rm, for any affine subspace A of\nRn, and any affine subspace B of Rm, prove that f(A) is an affine subspace of Rm, and that\nf−1(B) is an affine subspace of Rn.\n\n\n\n180 CHAPTER 6. DIRECT SUMS\n\n\n\nChapter 7\n\nDeterminants\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n7.1 Permutations, Signature of a Permutation\n\nThis chapter contains a review of determinants and their use in linear algebra. We begin\nwith permutations and the signature of a permutation. Next, we define multilinear maps\nand alternating multilinear maps. Determinants are introduced as alternating multilinear\nmaps taking the value 1 on the unit matrix (following Emil Artin). It is then shown how\nto compute a determinant using the Laplace expansion formula, and the connection with\nthe usual definition is made. It is shown how determinants can be used to invert matrices\nand to solve (at least in theory!) systems of linear equations (the Cramer formulae). The\ndeterminant of a linear map is defined. We conclude by defining the characteristic polynomial\nof a matrix (and of a linear map) and by proving the celebrated Cayley-Hamilton theorem\nwhich states that every matrix is a “zero” of its characteristic polynomial (we give two proofs;\none computational, the other one more conceptual).\n\nDeterminants can be defined in several ways. For example, determinants can be defined\nin a fancy way in terms of the exterior algebra (or alternating algebra) of a vector space.\nWe will follow a more algorithmic approach due to Emil Artin. No matter which approach\nis followed, we need a few preliminaries about permutations on a finite set. We need to\nshow that every permutation on n elements is a product of transpositions, and that the\nparity of the number of transpositions involved is an invariant of the permutation. Let\n[n] = {1, 2 . . . , n}, where n ∈ N, and n > 0.\n\nDefinition 7.1. A permutation on n elements is a bijection π : [n]→ [n]. When n = 1, the\nonly function from [1] to [1] is the constant map: 1 7→ 1. Thus, we will assume that n ≥ 2.\nA transposition is a permutation τ : [n]→ [n] such that, for some i < j (with 1 ≤ i < j ≤ n),\nτ(i) = j, τ(j) = i, and τ(k) = k, for all k ∈ [n] − {i, j}. In other words, a transposition\nexchanges two distinct elements i, j ∈ [n]. A cyclic permutation of order k (or k-cycle) is a\n\n181\n\n\n\n182 CHAPTER 7. DETERMINANTS\n\npermutation σ : [n]→ [n] such that, for some sequence (i1, i2, . . . , ik) of distinct elements of\n[n] with 2 ≤ k ≤ n,\n\nσ(i1) = i2, σ(i2) = i3, . . . , σ(ik−1) = ik, σ(ik) = i1,\n\nand σ(j) = j, for j ∈ [n]−{i1, . . . , ik}. The set {i1, . . . , ik} is called the domain of the cyclic\npermutation, and the cyclic permutation is usually denoted by (i1 i2 . . . ik).\n\nIf τ is a transposition, clearly, τ ◦ τ = id. Also, a cyclic permutation of order 2 is a\ntransposition, and for a cyclic permutation σ of order k, we have σk = id. Clearly, the\ncomposition of two permutations is a permutation and every permutation has an inverse\nwhich is also a permutation. Therefore, the set of permutations on [n] is a group often\ndenoted Sn. It is easy to show by induction that the group Sn has n! elements. We will\nalso use the terminology product of permutations (or transpositions), as a synonym for\ncomposition of permutations.\n\nA permutation σ on n elements, say σ(i) = ki for i = 1, . . . , n, can be represented in\nfunctional notation by the 2× n array(\n\n1 · · · i · · · n\nk1 · · · ki · · · kn\n\n)\nknown as Cauchy two-line notation. For example, we have the permutation σ denoted by(\n\n1 2 3 4 5 6\n2 4 3 6 5 1\n\n)\n.\n\nA more concise notation often used in computer science and in combinatorics is to rep-\nresent a permutation by its image, namely by the sequence\n\nσ(1) σ(2) · · · σ(n)\n\nwritten as a row vector without commas separating the entries. The above is known as\nthe one-line notation. For example, in the one-line notation, our previous permutation σ is\nrepresented by\n\n2 4 3 6 5 1.\n\nThe reason for not enclosing the above sequence within parentheses is avoid confusion with\nthe notation for cycles, for which is it customary to include parentheses.\n\nThe following proposition shows the importance of cyclic permutations and transposi-\ntions.\n\nProposition 7.1. For every n ≥ 2, for every permutation π : [n]→ [n], there is a partition\nof [n] into r subsets called the orbits of π, with 1 ≤ r ≤ n, where each set J in this partition\nis either a singleton {i}, or it is of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\n\n\n\n7.1. PERMUTATIONS, SIGNATURE OF A PERMUTATION 183\n\nwhere ri is the smallest integer, such that, πri(i) = i and 2 ≤ ri ≤ n. If π is not the identity,\nthen it can be written in a unique way (up to the order) as a composition π = σ1 ◦ . . . ◦ σs\nof cyclic permutations with disjoint domains, where s is the number of orbits with at least\ntwo elements. Every permutation π : [n]→ [n] can be written as a nonempty composition of\ntranspositions.\n\nProof. Consider the relation Rπ defined on [n] as follows: iRπj iff there is some k ≥ 1 such\nthat j = πk(i). We claim that Rπ is an equivalence relation. Transitivity is obvious. We\nclaim that for every i ∈ [n], there is some least r (1 ≤ r ≤ n) such that πr(i) = i.\n\nIndeed, consider the following sequence of n+ 1 elements:\n\n〈i, π(i), π2(i), . . . , πn(i)〉.\n\nSince [n] only has n distinct elements, there are some h, k with 0 ≤ h < k ≤ n such that\n\nπh(i) = πk(i),\n\nand since π is a bijection, this implies πk−h(i) = i, where 0 ≤ k − h ≤ n. Thus, we proved\nthat there is some integer m ≥ 1 such that πm(i) = i, so there is such a smallest integer r.\n\nConsequently, Rπ is reflexive. It is symmetric, since if j = πk(i), letting r be the least\nr ≥ 1 such that πr(i) = i, then\n\ni = πkr(i) = πk(r−1)(πk(i)) = πk(r−1)(j).\n\nNow, for every i ∈ [n], the equivalence class (orbit) of i is a subset of [n], either the singleton\n{i} or a set of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\nwhere ri is the smallest integer such that πri(i) = i and 2 ≤ ri ≤ n, and in the second case,\nthe restriction of π to J induces a cyclic permutation σi, and π = σ1 ◦ . . . ◦σs, where s is the\nnumber of equivalence classes having at least two elements.\n\nFor the second part of the proposition, we proceed by induction on n. If n = 2, there are\nexactly two permutations on [2], the transposition τ exchanging 1 and 2, and the identity.\nHowever, id2 = τ 2. Now, let n ≥ 3. If π(n) = n, since by the induction hypothesis, the\nrestriction of π to [n − 1] can be written as a product of transpositions, π itself can be\nwritten as a product of transpositions. If π(n) = k 6= n, letting τ be the transposition such\nthat τ(n) = k and τ(k) = n, it is clear that τ ◦ π leaves n invariant, and by the induction\nhypothesis, we have τ ◦ π = τm ◦ . . . ◦ τ1 for some transpositions, and thus\n\nπ = τ ◦ τm ◦ . . . ◦ τ1,\n\na product of transpositions (since τ ◦ τ = idn).\n\n\n\n184 CHAPTER 7. DETERMINANTS\n\nRemark: When π = idn is the identity permutation, we can agree that the composition of\n0 transpositions is the identity. The second part of Proposition 7.1 shows that the transpo-\nsitions generate the group of permutations Sn.\n\nIn writing a permutation π as a composition π = σ1 ◦ . . . ◦ σs of cyclic permutations, it\nis clear that the order of the σi does not matter, since their domains are disjoint. Given\na permutation written as a product of transpositions, we now show that the parity of the\nnumber of transpositions is an invariant.\n\nDefinition 7.2. For every n ≥ 2, since every permutation π : [n] → [n] defines a partition\nof r subsets over which π acts either as the identity or as a cyclic permutation, let ε(π),\ncalled the signature of π, be defined by ε(π) = (−1)n−r, where r is the number of sets in the\npartition.\n\nIf τ is a transposition exchanging i and j, it is clear that the partition associated with\nτ consists of n − 1 equivalence classes, the set {i, j}, and the n − 2 singleton sets {k}, for\nk ∈ [n]− {i, j}, and thus, ε(τ) = (−1)n−(n−1) = (−1)1 = −1.\n\nProposition 7.2. For every n ≥ 2, for every permutation π : [n] → [n], for every transpo-\nsition τ , we have\n\nε(τ ◦ π) = −ε(π).\n\nConsequently, for every product of transpositions such that π = τm ◦ . . . ◦ τ1, we have\n\nε(π) = (−1)m,\n\nwhich shows that the parity of the number of transpositions is an invariant.\n\nProof. Assume that τ(i) = j and τ(j) = i, where i < j. There are two cases, depending\nwhether i and j are in the same equivalence class Jl of Rπ, or if they are in distinct equivalence\nclasses. If i and j are in the same class Jl, then if\n\nJl = {i1, . . . , ip, . . . iq, . . . ik},\n\nwhere ip = i and iq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = iq\n\nand\nτ(π(iq−1)) = τ(iq) = τ(j) = i = ip,\n\nit is clear that Jl splits into two subsets, one of which is {ip, . . . , iq−1}, and thus, the number\nof classes associated with τ ◦ π is r + 1, and ε(τ ◦ π) = (−1)n−r−1 = −(−1)n−r = −ε(π). If i\nand j are in distinct equivalence classes Jl and Jm, say\n\n{i1, . . . , ip, . . . ih}\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{j1, . . . , jq, . . . jk},\n\nwhere ip = i and jq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = jq\n\nand\nτ(π(π−1(jq))) = τ(jq) = τ(j) = i = ip,\n\nwe see that the classes Jl and Jm merge into a single class, and thus, the number of classes\nassociated with τ ◦ π is r − 1, and ε(τ ◦ π) = (−1)n−r+1 = −(−1)n−r = −ε(π).\n\nNow, let π = τm ◦ . . . ◦ τ1 be any product of transpositions. By the first part of the\nproposition, we have\n\nε(π) = (−1)m−1ε(τ1) = (−1)m−1(−1) = (−1)m,\n\nsince ε(τ1) = −1 for a transposition.\n\nRemark: When π = idn is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (−1)0 = ε(id) = +1. From the\nproposition, it is immediate that ε(π′ ◦ π) = ε(π′)ε(π). In particular, since π−1 ◦ π = idn, we\nget ε(π−1) = ε(π).\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over K (free modules, when bases are\nneeded).\n\nLet E1, . . . , En, and F , be vector spaces over a field K, where n ≥ 1.\n\nDefinition 7.3. A function f : E1 × . . . × En → F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every i,\n1 ≤ i ≤ n, for all x1 ∈ E1, . . ., xi−1 ∈ Ei−1, xi+1 ∈ Ei+1, . . ., xn ∈ En, for all x, y ∈ Ei, for all\nλ ∈ K,\n\nf(x1, . . . , xi−1, x+ y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)\n\n+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),\n\nf(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{i1, see Jar : Jkt,\n\nwhere 7, = 7% and j, = j, since\n\nand\n\nT(m(m~*(jq))) =T(Jq) =TU) = t= ty,\nwe see that the classes J; and J;, merge into a single class, and thus, the number of classes\nassociated with 7 om is r—1, and e(7 om) = (—1)\"\"*1 = —(-1)\"\" = —c(z).\n\nNow, let 7 = Tm 0...07, be any product of transpositions. By the first part of the\nproposition, we have\n\nsince €(7,) = —1 for a transposition. O\n\nRemark: When z = id, is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (—1)° = e(id) = +1. From the\nproposition, it is immediate that e(a’ om) = e(n’)e(z). In particular, since 77!\nget e(7!) = e(z).\n\nom =id,, we\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over A (free modules, when bases are\n\nneeded).\nLet E,,...,E,, and F, be vector spaces over a field K, where n > 1.\n\nDefinition 7.3. A function f: FE, x... x E, > F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every 1,\n1<i<n, for all x, € Fy,..., m1 © Fj-4, tin. © Figs, --, Un © En, for all x,y € K, for all\nAE K,\n\nf(@1,---, Ui, + Y, Vig, ---5 En) = f(41,-- +, Vi-1, V, Lig,---, Ln)\n+ f(a, wee Vi-1,Y, Vit1,--- En),\n\nf(“1, te ,Tj—1, AL, Vin, te Ln) = Af (£1, vee Di-1, VU, Vi41,--- En).\n\n\n\n\n186 CHAPTER 7. DETERMINANTS\n\nWhen F = K, we call f an n-linear form (or multilinear form). If n ≥ 2 and E1 =\nE2 = . . . = En, an n-linear map f : E × . . .×E → F is called symmetric, if f(x1, . . . , xn) =\nf(xπ(1), . . . , xπ(n)) for every permutation π on {1, . . . , n}. An n-linear map f : E×. . .×E → F\nis called alternating , if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤ i ≤ n − 1 (in\nother words, when two adjacent arguments are equal). It does no harm to agree that when\nn = 1, a linear map is considered to be both symmetric and alternating, and we will do so.\n\nWhen n = 2, a 2-linear map f : E1 × E2 → F is called a bilinear map. We have already\nseen several examples of bilinear maps. Multiplication · : K × K → K is a bilinear map,\ntreating K as a vector space over itself.\n\nThe operation 〈−,−〉 : E∗×E → K applying a linear form to a vector is a bilinear map.\n\nSymmetric bilinear maps (and multilinear maps) play an important role in geometry\n(inner products, quadratic forms) and in differential calculus (partial derivatives).\n\nA bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈ E.\n\nAlternating multilinear maps satisfy the following simple but crucial properties.\n\nProposition 7.3. Let f : E× . . .×E → F be an n-linear alternating map, with n ≥ 2. The\nfollowing properties hold:\n\n(1)\nf(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .)\n\n(2)\nf(. . . , xi, . . . , xj, . . .) = 0,\n\nwhere xi = xj, and 1 ≤ i < j ≤ n.\n\n(3)\nf(. . . , xi, . . . , xj, . . .) = −f(. . . , xj, . . . , xi, . . .),\n\nwhere 1 ≤ i < j ≤ n.\n\n(4)\nf(. . . , xi, . . .) = f(. . . , xi + λxj, . . .),\n\nfor any λ ∈ K, and where i 6= j.\n\nProof. (1) By multilinearity applied twice, we have\n\nf(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi, xi, . . .) + f(. . . , xi, xi+1, . . .)\n\n+ f(. . . , xi+1, xi, . . .) + f(. . . , xi+1, xi+1, . . .),\n\nand since f is alternating, this yields\n\n0 = f(. . . , xi, xi+1, . . .) + f(. . . , xi+1, xi, . . .),\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 187\n\nthat is, f(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .).\n\n(2) If xi = xj and i and j are not adjacent, we can interchange xi and xi+1, and then xi\nand xi+2, etc, until xi and xj become adjacent. By (1),\n\nf(. . . , xi, . . . , xj, . . .) = εf(. . . , xi, xj, . . .),\n\nwhere ε = +1 or −1, but f(. . . , xi, xj, . . .) = 0, since xi = xj, and (2) holds.\n\n(3) follows from (2) as in (1). (4) is an immediate consequence of (2).\n\nProposition 7.3 will now be used to show a fundamental property of alternating multilin-\near maps. First we need to extend the matrix notation a little bit. Let E be a vector space\nover K. Given an n× n matrix A = (ai j) over K, we can define a map L(A) : En → En as\nfollows:\n\nL(A)1(u) = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nL(A)n(u) = an 1u1 + · · ·+ annun,\n\nfor all u1, . . . , un ∈ E and with u = (u1, . . . , un). It is immediately verified that L(A) is\nlinear. Then given two n×n matrices A = (ai j) and B = (bi j), by repeating the calculations\nestablishing the product of matrices (just before Definition 3.12), we can show that\n\nL(AB) = L(A) ◦ L(B).\n\nIt is then convenient to use the matrix notation to describe the effect of the linear map L(A),\nas \n\nL(A)1(u)\nL(A)2(u)\n\n...\nL(A)n(u)\n\n =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nu1\n\nu2\n...\nun\n\n .\n\nLemma 7.4. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un) and\n(v1, . . . , vn) be two families of n vectors, such that,\n\nv1 = a1 1u1 + · · ·+ an 1un,\n\n. . .\n\nvn = a1nu1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A>\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nf(u1, . . . , un),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nProof. Expanding f(v1, . . . , vn) by multilinearity, we get a sum of terms of the form\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)),\n\nfor all possible functions π : {1, . . . , n} → {1, . . . , n}. However, because f is alternating, only\nthe terms for which π is a permutation are nonzero. By Proposition 7.1, every permutation\nπ is a product of transpositions, and by Proposition 7.2, the parity ε(π) of the number of\ntranspositions only depends on π. Then applying Proposition 7.3 (3) to each transposition\nin π, we get\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)) = ε(π)aπ(1) 1 · · · aπ(n)nf(u1, . . . , un).\n\nThus, we get the expression of the lemma.\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, v2) = f(a11u1 + a21u2, a12u1 + a22u2)\n\n= f(a11u1 + a21u2, a12u1) + f(a11u1 + a21u2, a22u2)\n\n= f(a11u1, a12u1) + f(a21u2, a12u1) + f(a11ua, a22u2) + f(a21u2, a22u2)\n\n= a11a12f(u1, u1) + a21a12f(u2, u1) + a11a22f(u1, u2) + a21a22f(u2, u2)\n\n= a21a12f(u2, u1)a11a22f(u1, u2)\n\n= (a11a22 − a12a22) f(u1, u2).\n\nHopefully the reader will recognize the quantity a11a22− a12a22. It is the determinant of the\n2× 2 matrix\n\nA =\n\n(\na11 a12\n\na21 a22\n\n)\n.\n\nThis is no accident. The quantity\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have\n\nUy U1\nU2 _ At U2\nUn Un\nThen,\nf(ur, te ,Un) = ( S- €(T)an(1)1 oe Arin)n) f(t, te , Un),\nTEGn\nwhere the sum ranges over all permutations 7 on {1,...,n}.\nProof. Expanding f(v1,...,Un) by multilinearity, we get a sum of terms of the form\nAn(1)1*** On(n) nd (Un(1)s+++5Um(n))s\nfor all possible functions 7: {1,...,n}— {1,...,n}. However, because f is alternating, only\n\nthe terms for which 7 is a permutation are nonzero. By Proposition 7.1, every permutation\nm is a product of transpositions, and by Proposition 7.2, the parity e(7) of the number of\ntranspositions only depends on 7. Then applying Proposition 7.3 (3) to each transposition\nin 77, we get\n\nAn(1)1°** An(n) nd (Un(1)>+ «+5 Um(n)) = E(T)An(1) 1° ** Ar(n) nd (Ua, -.. 5 Un):\nThus, we get the expression of the lemma. im\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, 02) = f\n=f\nf\n\nay U + Gg1U2, A121 + A22QU2)\nQ41Uy + Ag U2, A12U1) + f (Qiu + Goi U2, do2U2)\n\nQy1U1, A121) + f(aeit2, di2t1) + f(a11ta, d22U2) + f(ao1U2, d22U2)\n\n—_~~ ~~\n\n= dz G12 f (U2, U1) A114 f (ur, U2)\n\n= (a4 122 _— 12422) f (ui, Ug).\n\nHopefully the reader will recognize the quantity a11d@92 — @12d22. It is the determinant of the\n2 xX 2 matrix\nA- ai1 G12\na21 422\nThis is no accident. The quantity\n\ndet(A) = S° €(T)Ax(1)1°** Ax(n)n\n\nTEGH\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 189\n\nis in fact the value of the determinant of A (which, as we shall see shortly, is also equal to the\ndeterminant of A>). However, working directly with the above definition is quite awkward,\nand we will proceed via a slightly indirect route\n\nRemark: The reader might have been puzzled by the fact that it is the transpose matrix\nA> rather than A itself that appears in Lemma 7.4. The reason is that if we want the generic\nterm in the determinant to be\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the permutation applies to the first index, then we have to express the vjs in terms\nof the uis in terms of A> as we did. Furthermore, since\n\nvj = a1 ju1 + · · ·+ ai jui + · · ·+ an jun,\n\nwe see that vj corresponds to the jth column of the matrix A, and so the determinant is\nviewed as a function of the columns of A.\n\nThe literature is split on this point. Some authors prefer to define a determinant as we\ndid. Others use A itself, which amounts to viewing det as a function of the rows, in which\ncase we get the expression ∑\n\nσ∈Sn\nε(σ)a1σ(1) · · · anσ(n).\n\nCorollary 7.7 show that these two expressions are equal, so it doesn’t matter which is chosen.\nThis is a matter of taste.\n\n7.3 Definition of a Determinant\n\nRecall that the set of all square n × n-matrices with coefficients in a field K is denoted by\nMn(K).\n\nDefinition 7.4. A determinant is defined as any map\n\nD : Mn(K)→ K,\n\nwhich, when viewed as a map on (Kn)n, i.e., a map of the n columns of a matrix, is n-linear\nalternating and such that D(In) = 1 for the identity matrix In. Equivalently, we can consider\na vector space E of dimension n, some fixed basis (e1, . . . , en), and define\n\nD : En → K\n\nas an n-linear alternating map such that D(e1, . . . , en) = 1.\n\n\n\n190 CHAPTER 7. DETERMINANTS\n\nFirst we will show that such maps D exist, using an inductive definition that also gives\na recursive method for computing determinants. Actually, we will define a family (Dn)n≥1\n\nof (finite) sets of maps D : Mn(K)→ K. Second we will show that determinants are in fact\nuniquely defined, that is, we will show that each Dn consists of a single map. This will show\nthe equivalence of the direct definition det(A) of Lemma 7.4 with the inductive definition\nD(A). Finally, we will prove some basic properties of determinants, using the uniqueness\ntheorem.\n\nGiven a matrix A ∈ Mn(K), we denote its n columns by A1, . . . , An. In order to describe\nthe recursive process to define a determinant we need the notion of a minor.\n\nDefinition 7.5. Given any n×n matrix with n ≥ 2, for any two indices i, j with 1 ≤ i, j ≤ n,\nlet Aij be the (n − 1) × (n − 1) matrix obtained by deleting Row i and Column j from A\nand called a minor :\n\nAij =\n\n\n\n×\n×\n\n× × × × × × ×\n×\n×\n×\n×\n\n\n.\n\nFor example, if\n\nA =\n\n\n2 −1 0 0 0\n−1 2 −1 0 0\n0 −1 2 −1 0\n0 0 −1 2 −1\n0 0 0 −1 2\n\n\nthen\n\nA2 3 =\n\n\n2 −1 0 0\n0 −1 −1 0\n0 0 2 −1\n0 0 −1 2\n\n .\n\nDefinition 7.6. For every n ≥ 1, we define a finite set Dn of maps D : Mn(K) → K\ninductively as follows:\n\nWhen n = 1, D1 consists of the single map D such that, D(A) = a, where A = (a), with\na ∈ K.\n\nAssume that Dn−1 has been defined, where n ≥ 2. Then Dn consists of all the maps D\nsuch that, for some i, 1 ≤ i ≤ n,\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+nai nD(Ai n),\n\nwhere for every j, 1 ≤ j ≤ n, D(Ai j) is the result of applying any D in Dn−1 to the minor\nAi j.\n\n\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n� We confess that the use of the same letter D for the member of Dn being defined, and\nfor members of Dn−1, may be slightly confusing. We considered using subscripts to\n\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each Dn contains just one map.\n\nEach (−1)i+jD(Ai j) is called the cofactor of ai j, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row . Given a matrix A ∈ Mn(K),\neach D(A) is called a determinant of A.\n\nWe can think of each member of Dn as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nDn consists of a single map). Assuming this fact, given a n× n-matrix A = (ai j),\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\ndet(A) =\n\n∣∣∣∣∣∣∣∣∣\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n∣∣∣∣∣∣∣∣∣ .\n\nLet us first consider some examples.\n\nExample 7.1.\n\n1. When n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen by expanding according to any row, we have\n\nD(A) = ad− bc.\n\n2. When n = 3, if\n\nA =\n\na1 1 a1 2 a1 3\n\na2 1 a2 2 a2 3\n\na3 1 a3 2 a3 3\n\n ,\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n© We confess that the use of the same letter D for the member of D,, being defined, and\nfor members of D,_1, may be slightly confusing. We considered using subscripts to\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each D, contains just one map.\n\nEach (—1)'*? D(A;;) is called the cofactor of a;;, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row. Given a matrix A € M,(KkK),\neach D(A) is called a determinant of A.\n\nWe can think of each member of D,, as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nD,, consists of a single map). Assuming this fact, given a n x n-matrix A = (a;,;),\n\nQi, Qay2q ..-- Atn\n\na2, G22 ... Aan\nA= . . ;\n\nQn1 GQn2 ---» Ann\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\na1, %aaq .-- Ain\n\na2, G22 ... QAan\ndet(A) =] . j\n\nQn1 Qn2 +--+. Ann\n\nLet us first consider some examples.\nExample 7.1.\n1. When n = 2, if\nA= (: ‘) .\n\nthen by expanding according to any row, we have\nD(A) = ad — be.\n\n2. When n = 3, if\nQ11 G12 13\nA= |do1 G2 93\n\n431 432 433\n\n\n\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\nD(A) = a1 1\n\n∣∣∣∣a2 2 a2 3\n\na3 2 a3 3\n\n∣∣∣∣− a1 2\n\n∣∣∣∣a2 1 a2 3\n\na3 1 a3 3\n\n∣∣∣∣+ a1 3\n\n∣∣∣∣a2 1 a2 2\n\na3 1 a3 2\n\n∣∣∣∣ ,\nthat is,\n\nD(A) = a1 1(a2 2a3 3 − a3 2a2 3)− a1 2(a2 1a3 3 − a3 1a2 3) + a1 3(a2 1a3 2 − a3 1a2 2),\n\nwhich gives the explicit formula\n\nD(A) = a1 1a2 2a3 3 + a2 1a3 2a1 3 + a3 1a1 2a2 3 − a1 1a3 2a2 3 − a2 1a1 2a3 3 − a3 1a2 2a1 3.\n\nWe now show that each D ∈ Dn is a determinant (map).\n\nLemma 7.5. For every n ≥ 1, for every D ∈ Dn as defined in Definition 7.6, D is an\nalternating multilinear map such that D(In) = 1.\n\nProof. By induction on n, it is obvious that D(In) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+jai jD(Ai j) + · · ·+ (−1)i+nai nD(Ai n),\n\nif j 6= k, then by induction, D(Ai j) is linear in Column k, and ai j does not belong to Column\nk, so (−1)i+jai jD(Ai j) is linear in Column k. If j = k, then D(Ai j) does not depend on\nColumn k = j, since Ai j is obtained from A by deleting Row i and Column j = k, and ai j\nbelongs to Column j = k. Thus, (−1)i+jai jD(Ai j) is linear in Column k. Consequently, in\nall cases, (−1)i+jai jD(Ai j) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say Ak = Ak+1. Assume that j 6= k and j 6= k + 1. Then the matrix Ai j has two\nidentical adjacent columns, and by the induction hypothesis, D(Ai j) = 0. The remaining\nterms of D(A) are\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1).\n\nHowever, the two matrices Ai k and Ai k+1 are equal, since we are assuming that Columns k\nand k + 1 of A are identical and Ai k is obtained from A by deleting Row i and Column k\nwhile Ai k+1 is obtained from A by deleting Row i and Column k+ 1. Similarly, ai k = ai k+1,\nsince Columns k and k + 1 of A are equal. But then,\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1) = (−1)i+kai kD(Ai k)− (−1)i+kai kD(Ai k) = 0.\n\nThis shows that D is alternating and completes the proof.\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\na21 422\na31 432\n\na21 423\n\n2 + a13\n431 433\n\nv]\n\nthat is,\nD(A) = 11 (22033 _ 32023) _ 1 2(a21433 _ 31423) + 1 3(G21432 _ 31422),\nwhich gives the explicit formula\n\nD(A) = 011422433 + 421432013 + 431412423 — 411032423 — 421012433 — 4314220)3.\n\nWe now show that each D € D,, is a determinant (map).\n\nLemma 7.5. For every n > 1, for every D € Dy, as defined in Definition 7.6, D is an\nalternating multilinear map such that D(I,) = 1.\n\nProof. By induction on n, it is obvious that D(/,) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (-1)'*\"a;1D(Ajit) +--+ + (-1)' aj D(Aij) + + (-1)GinD(Ain),\n\nif 7 # k, then by induction, D(A;,) is linear in Column k, and a;; does not belong to Column\nk, so (—1)'a,;;D(A;,;) is linear in Column k. If 7 = k, then D(A;;) does not depend on\nColumn k = j, since A;; is obtained from A by deleting Row 7 and Column j = k, and a; ;\nbelongs to Column j = k. Thus, (—1)'’a;;D(A;;) is linear in Column k. Consequently, in\nall cases, (—1)'*7a;;D(A;,;) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say A* = A**t!, Assume that 7 # k and j 4 k+1. Then the matrix A;; has two\nidentical adjacent columns, and by the induction hypothesis, D(A;;) = 0. The remaining\nterms of D(A) are\n\n(—1)'*ajxD(Ain) + (1) ain D(Ainyt)-\n\nHowever, the two matrices A;, and A;,4; are equal, since we are assuming that Columns k\nand k +1 of A are identical and A;; is obtained from A by deleting Row i and Column k\nwhile A;;41 is obtained from A by deleting Row i and Column k +1. Similarly, aj, = ajx4i,\nsince Columns k and k + 1 of A are equal. But then,\n\n(—1)*8a;4D(Aig) + (—1) ai ng (Aina) = (-1)' aie D(Ain) — (-1) \"ain D(Aix) = 0.\nThis shows that D is alternating and completes the proof. im\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 193\n\nTheorem 7.6. For every n ≥ 1, for every D ∈ Dn, for every matrix A ∈ Mn(K), we have\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. As a consequence, Dn consists\nof a single map for every n ≥ 1, and this map is given by the above explicit formula.\n\nProof. Consider the standard basis (e1, . . . , en) of Kn, where (ei)i = 1 and (ei)j = 0, for\nj 6= i. Then each column Aj of A corresponds to a vector vj whose coordinates over the\nbasis (e1, . . . , en) are the components of Aj, that is, we can write\n\nv1 = a1 1e1 + · · ·+ an 1en,\n\n. . .\n\nvn = a1ne1 + · · ·+ annen.\n\nSince by Lemma 7.5, each D is a multilinear alternating map, by applying Lemma 7.4, we\nget\n\nD(A) = D(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nD(e1, . . . , en),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. But D(e1, . . . , en) = D(In),\nand by Lemma 7.5, we have D(In) = 1. Thus,\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nFrom now on we will favor the notation det(A) over D(A) for the determinant of a square\nmatrix.\n\nRemark: There is a geometric interpretation of determinants which we find quite illumi-\nnating. Given n linearly independent vectors (u1, . . . , un) in Rn, the set\n\nPn = {λ1u1 + · · ·+ λnun | 0 ≤ λi ≤ 1, 1 ≤ i ≤ n}\n\nis called a parallelotope. If n = 2, then P2 is a parallelogram and if n = 3, then P3 is a\nparallelepiped , a skew box having u1, u2, u3 as three of its corner sides. See Figures 7.1 and\n7.2.\n\nThen it turns out that det(u1, . . . , un) is the signed volume of the parallelotope Pn (where\nvolume means n-dimensional volume). The sign of this volume accounts for the orientation\nof Pn in Rn.\n\nWe can now prove some properties of determinants.\n\n\n\n194 CHAPTER 7. DETERMINANTS\n\nu = (1,0)1\n\nu = (1,1)\n2\n\nFigure 7.1: The parallelogram in Rw spanned by the vectors u1 = (1, 0) and u2 = (1, 1).\n\nCorollary 7.7. For every matrix A ∈ Mn(K), we have det(A) = det(A>).\n\nProof. By Theorem 7.6, we have\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. Since a permutation is invertible,\nevery product\n\naπ(1) 1 · · · aπ(n)n\n\ncan be rewritten as\na1π−1(1) · · · anπ−1(n),\n\nand since ε(π−1) = ε(π) and the sum is taken over all permutations on {1, . . . , n}, we have∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n),\n\nwhere π and σ range over all permutations. But it is immediately verified that\n\ndet(A>) =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n).\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n194 CHAPTER 7. DETERMINANTS\n\n0.8 4\n0.64\n0.44\n\n0.24\n\n1\nu= (1,0)\n\nFigure 7.1: The parallelogram in R” spanned by the vectors u; = (1,0) and uz = (1,1).\n\nCorollary 7.7. For every matriz A € M,,(K), we have det(A) = det(A‘).\n\nProof. By Theorem 7.6, we have\n\ndet (A) = S- €(17)ax(1) 1° °° An(n)ns\n\nTEGn\n\nwhere the sum ranges over all permutations 7 on {1,...,}. Since a permutation is invertible,\nevery product\n\nAn(1)1°°* Ax(n)n\ncan be rewritten as\n\nA1q-1(1) °° Anal (n)s\n\nand since e(7~!) = e(7) and the sum is taken over all permutations on {1,...,n}, we have\nS- €(1)Ax(1) 1° °° Ag(n)n = S- €(o)ay o(1) °°\" Gno(n)>\nTEGn aEGn\n\nwhere 7 and o range over all permutations. But it is immediately verified that\n\ndet(A') = S- €(7)Q1 (1) *** An o(n): d\n\naEGn\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 195\n\nu = (1,1,0)\n1\n\nu = (0,1,0)\n2\n\nu = (1,1,1)\n3\n\nFigure 7.2: The parallelepiped in R3 spanned by the vectors u1 = (1, 1, 0), u2 = (0, 1, 0), and\nu3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\nx1 x2 . . . xn\nx2\n\n1 x2\n2 . . . x2\n\nn\n...\n\n...\n. . .\n\n...\nxn−1\n\n1 xn−1\n2 . . . xn−1\n\nn\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nWe claim that\n\nV (x1, . . . , xn) =\n∏\n\n1≤i<j≤n\n(xj − xi),\n\nwith V (x1, . . . , xn) = 1, when n = 1. We prove it by induction on n ≥ 1. The case n = 1 is\nobvious. Assume n ≥ 2. We proceed as follows: multiply Row n − 1 by x1 and subtract it\nfrom Row n (the last row), then multiply Row n− 2 by x1 and subtract it from Row n− 1,\netc, multiply Row i− 1 by x1 and subtract it from row i, until we reach Row 1. We obtain\n\n7.3. DEFINITION OF A DETERMINANT\n\n14\n084\nv4\n044\n\n0.25\n\n195\n\nFigure 7.2: The parallelepiped in R® spanned by the vectors u, = (1, 1,0), we = (0,1,0), and\n\nU3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\n1 1 1\n\nLy 2 In\n\n2 2 2\n\nV(a1,...,%n) =] %1  % Ly,\ncp! at grt\n\nWe claim that\n\nV(a1,.--,2%n) = II (x; — 7),\n\n1<i<j<n\n\nwith V(a1,...,%) = 1, when n = 1. We prove it by induction on n > 1. The case n = 1 is\nobvious. Assume n > 2. We proceed as follows: multiply Row n — 1 by x, and subtract it\nfrom Row n (the last row), then multiply Row n — 2 by x; and subtract it from Row n— 1,\netc, multiply Row 7 — 1 by x, and subtract it from row 7, until we reach Row 1. We obtain\n\n\n\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\n0 x2 − x1 . . . xn − x1\n\n0 x2(x2 − x1) . . . xn(xn − x1)\n...\n\n...\n. . .\n\n...\n0 xn−2\n\n2 (x2 − x1) . . . xn−2\nn (xn − x1)\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (xi − x1) from the column of index i − 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV (x1, . . . , xn) = (x2 − x1)(x3 − x1) · · · (xn − x1)V (x2, . . . , xn),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un)\nand (v1, . . . , vn) be two families of n vectors, such that\n\nv1 = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nvn = an 1u1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) = det(A)f(u1, . . . , un).\n\nProof. The only difference with Lemma 7.4 is that here we are using A> instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result.\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\n1 1 a 1\n\n0 L— Ly a In — Ly\nV(a1,---,2n) = 0 %2(%2—- 21)... En (4% — 11)\n\n0 af 7(ag-91) ... 2\"? (a, — 21)\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (7; — x1) from the column of index i — 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV(a1,.--,%n) = (\"2 — ©1)(43 — 1) +++ (pn — 21)V (H0,...,Ln),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f: Ex...x E— F be an n-linear alternating map. Let (u1,..., Un)\nand (U1,...,Un) be two families of n vectors, such that\n\nVy = 441Uy +--+ + A nUn,\n\nUn = An{Uy + +++ + AnnUn-\n\nEquivalently, letting\n\na11 G12 «+. An\nA= a21 (122 -.. Qan .\nQn1 Gn2 Ann\nassume that we have\nU1 U1\nV2 _A U2\nUn, Un\n\nThen,\nf(v1,-.-,Un) = det(A)f (ur, ...,Un)-\n\nProof. The only difference with Lemma 7.4 is that here we are using A' instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result. im\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 197\n\nAs a consequence, we get the very useful property that the determinant of a product of\nmatrices is the product of the determinants of these matrices.\n\nProposition 7.9. For any two n×n-matrices A and B, we have det(AB) = det(A) det(B).\n\nProof. We use Proposition 7.8 as follows: let (e1, . . . , en) be the standard basis of Kn, and\nlet \n\nw1\n\nw2\n...\nwn\n\n = AB\n\n\ne1\n\ne2\n...\nen\n\n .\n\nThen we get\n\ndet(w1, . . . , wn) = det(AB) det(e1, . . . , en) = det(AB),\n\nsince det(e1, . . . , en) = 1. Now letting\nv1\n\nv2\n...\nvn\n\n = B\n\n\ne1\n\ne2\n...\nen\n\n ,\n\nwe get\n\ndet(v1, . . . , vn) = det(B),\n\nand since \nw1\n\nw2\n...\nwn\n\n = A\n\n\nv1\n\nv2\n...\nvn\n\n ,\n\nwe get\n\ndet(w1, . . . , wn) = det(A) det(v1, . . . , vn) = det(A) det(B).\n\nIt should be noted that all the results of this section, up to now, also hold when K is a\ncommutative ring and not necessarily a field. We can now characterize when an n×n-matrix\nA is invertible in terms of its determinant det(A).\n\n7.4 Inverse Matrices and Determinants\n\nIn the next two sections, K is a commutative ring and when needed a field.\n\n\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A ∈ Mn(K), let Ã = (bi j)\nbe the matrix defined such that\n\nbi j = (−1)i+j det(Aj i),\n\nthe cofactor of aj i. The matrix Ã is called the adjugate of A, and each matrix Aj i is called\na minor of the matrix A.\n\nFor example, if\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n ,\n\nwe have\n\nb11 = det(A11) =\n\n∣∣∣∣ −2 −2\n3 −3\n\n∣∣∣∣ = 12 b12 = − det(A21) = −\n∣∣∣∣ 1 1\n\n3 −3\n\n∣∣∣∣ = 6\n\nb13 = det(A31) =\n\n∣∣∣∣ 1 1\n−2 −2\n\n∣∣∣∣ = 0 b21 = − det(A12) = −\n∣∣∣∣ 2 −2\n\n3 −3\n\n∣∣∣∣ = 0\n\nb22 = det(A22) =\n\n∣∣∣∣ 1 1\n3 −3\n\n∣∣∣∣ = −6 b23 = − det(A32) = −\n∣∣∣∣ 1 1\n\n2 −2\n\n∣∣∣∣ = 4\n\nb31 = det(A13) =\n\n∣∣∣∣ 2 −2\n3 3\n\n∣∣∣∣ = 12 b32 = − det(A23) = −\n∣∣∣∣ 1 1\n\n3 3\n\n∣∣∣∣ = 0\n\nb33 = det(A33) =\n\n∣∣∣∣ 1 1\n2 −2\n\n∣∣∣∣ = −4,\n\nwe find that\n\nÃ =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n .\n\n� Note the reversal of the indices in\n\nbi j = (−1)i+j det(Aj i).\n\nThus, Ã is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A ∈ Mn(K), we have\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, A−1 = (det(A))−1Ã.\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A € M,(K), let A = (0; ;)\nbe the matrix defined such that\n\nbij = (-1)'™ det(A;,),\n\nthe cofactor of a;;. The matrix A is called the adjugate of A, and each matrix A,; is called\na minor of the matrix A.\n\nFor example, if\n\n1 1 1\nA= |{2 -—2 -2],\n3 3 -3\nwe have\n2 ~9 1 1\nby, = det(Ay1) = 3 3 | = 12 byg = — det (Agi) = — | 3 3 | = 6\n1 1 2 —2\nbi3 = det(A31) = 2 _2 | =0 by = —det(Aiz) = — | 3-3 | =O\n1 1 1 1\nboo = det (A22) = 3 3 =—6 bos = — det (Az) =~ | 2 —2 | = 4\n2 —2 1 1\n531 = det (Aj3) = 3 3 = 12 b32 = — det (Ags) =~ | 3 3 | =0\n1 1\n633 = det (A33) = a) = —4,\nwe find that\n7 12 6 O\nA=|0 -6 4\n12 0 —4\n\n© Note the reversal of the indices in\nbi = (-1)'\" det (Aj;;).\nThus, A is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A € M,,(K), we have\nAA = AA = det(A)Iy.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, AT! = (det(A))~1A.\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If Ã = (bi j) and AÃ = (ci j), we know that the entry ci j in row i and column j of AÃ\nis\n\nci j = ai 1b1 j + · · ·+ ai kbk j + · · ·+ ai nbn j,\n\nwhich is equal to\n\nai 1(−1)j+1 det(Aj 1) + · · ·+ ai n(−1)j+n det(Aj n).\n\nIf j = i, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\n\nci i = det(A) = ai 1(−1)i+1 det(Ai 1) + · · ·+ ai n(−1)i+n det(Ai n).\n\nIf j 6= i, we can form the matrix A′ by replacing the j-th row of A by the i-th row of A.\nNow the matrix Aj k obtained by deleting row j and column k from A is equal to the matrix\nA′j k obtained by deleting row j and column k from A′, since A and A′ only differ by the j-th\nrow. Thus,\n\ndet(Aj k) = det(A′j k),\n\nand we have\n\nci j = ai 1(−1)j+1 det(A′j 1) + · · ·+ ai n(−1)j+n det(A′j n).\n\nHowever, this is the expansion of det(A′) according to the j-th row, since the j-th row of A′\n\nis equal to the i-th row of A. Furthermore, since A′ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A′) = 0. Thus,\nwe have shown that ci i = det(A), and ci j = 0, when j 6= i, and so\n\nAÃ = det(A)In.\n\nIt is also obvious from the definition of Ã, that\n\nÃ> = Ã>.\n\nThen applying the first part of the argument to A>, we have\n\nA>Ã> = det(A>)In,\n\nand since det(A>) = det(A), Ã> = Ã>, and (ÃA)> = A>Ã>, we get\n\ndet(A)In = A>Ã> = A>Ã> = (ÃA)>,\n\nthat is,\n\n(ÃA)> = det(A)In,\n\nwhich yields\n\nÃA = det(A)In,\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If A= (b;;) and AA= (c;;), we know that the entry c¢;; in row 7 and column j of AA\nis\nCig = Aii1d1g + +++ + Gindeg + +++ + Ginbns,\n\nwhich is equal to\naj1(—1)7*\" det(A; 1) feee Hb in(—1)7*\" det(A;,,).\n\nIf 7 = 7, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\nCi = det (A) = aj1(—1)\"*! det (A;1) fteeet din(—1)'*” det (A; »).\n\nIf 7 4 7%, we can form the matrix A’ by replacing the j-th row of A by the i-th row of A.\nNow the matrix A;;, obtained by deleting row 7 and column k from A is equal to the matrix\nA’,,, obtained by deleting row j and column k from A’, since A and A’ only differ by the j-th\nrow. Thus,\n\ndet(Aj,) = det(Aj,),\n\nand we have\nCijy = ay1(—1)'** det( Aj) +--+ + ain(-1)7*™ det(4j,,,).\n\nHowever, this is the expansion of det(A’) according to the j-th row, since the j-th row of A’\nis equal to the i-th row of A. Furthermore, since A’ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A’) = 0. Thus,\nwe have shown that c;; = det(A), and c;; = 0, when j 47, and so\nAA = det(A) In.\nIt is also obvious from the definition of A, that\nwaa\nThen applying the first part of the argument to A', we have\nA'AT = det(A')In,\nand since det(A‘) = det(A), AY = AT, and (AA)\" = A™AT, we get\ndet(A)I, = AA’ = ATA! = (AA)’,\n\nthat is, 7\n(AA)! = det(A)In,\n\nwhich yields 7\nAA = det(A)In,\n\n\n\n\n200 CHAPTER 7. DETERMINANTS\n\nsince I>n = In. This proves that\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, if det(A) is invertible, we have A−1 = (det(A))−1Ã. Conversely, if A is\ninvertible, from AA−1 = In, by Proposition 7.9, we have det(A) det(A−1) = 1, and det(A) is\ninvertible.\n\nFor example, we saw earlier that\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n and Ã =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n ,\n\nand we have 1 1 1\n2 −2 −2\n3 3 −3\n\n12 6 0\n0 −6 4\n12 0 −4\n\n = 24\n\n1 0 0\n0 1 0\n0 0 1\n\n\nwith det(A) = 24.\n\nWhen K is a field, an element a ∈ K is invertible iff a 6= 0. In this case, the second part\nof the proposition can be stated as A is invertible iff det(A) 6= 0. Note in passing that this\nmethod of computing the inverse of a matrix is usually not practical.\n\n7.5 Systems of Linear Equations and Determinants\n\nWe now consider some applications of determinants to linear independence and to solving\nsystems of linear equations. Although these results hold for matrices over certain rings, their\nproofs require more sophisticated methods. Therefore, we assume again that K is a field\n(usually, K = R or K = C).\n\nLet A be an n×n-matrix, x a column vectors of variables, and b another column vector,\nand let A1, . . . , An denote the columns of A. Observe that the system of equations Ax = b,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nx1\n\nx2\n...\nxn\n\n =\n\n\nb1\n\nb2\n...\nbn\n\n\nis equivalent to\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = b,\n\nsince the equation corresponding to the i-th row is in both cases\n\nai 1x1 + · · ·+ ai jxj + · · ·+ ai nxn = bi.\n\nFirst we characterize linear independence of the column vectors of a matrix A in terms\nof its determinant.\n\n\n\n7.5. SYSTEMS OF LINEAR EQUATIONS AND DETERMINANTS 201\n\nProposition 7.11. Given an n × n-matrix A over a field K, the columns A1, . . . , An of\nA are linearly dependent iff det(A) = det(A1, . . . , An) = 0. Equivalently, A has rank n iff\ndet(A) 6= 0.\n\nProof. First assume that the columns A1, . . . , An of A are linearly dependent. Then there\nare x1, . . . , xn ∈ K, such that\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = 0,\n\nwhere xj 6= 0 for some j. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , 0, . . . , An) = 0,\n\nwhere 0 occurs in the j-th position. By multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = xj det(A1, . . . , An) = 0.\n\nSince xj 6= 0 and K is a field, we must have det(A1, . . . , An) = 0.\n\nConversely, we show that if the columns A1, . . . , An of A are linearly independent, then\ndet(A1, . . . , An) 6= 0. If the columns A1, . . . , An of A are linearly independent, then they\nform a basis of Kn, and we can express the standard basis (e1, . . . , en) of Kn in terms of\nA1, . . . , An. Thus, we have\n\ne1\n\ne2\n...\nen\n\n =\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bnn\n\n\n\nA1\n\nA2\n\n...\nAn\n\n ,\n\nfor some matrix B = (bi j), and by Proposition 7.8, we get\n\ndet(e1, . . . , en) = det(B) det(A1, . . . , An),\n\nand since det(e1, . . . , en) = 1, this implies that det(A1, . . . , An) 6= 0 (and det(B) 6= 0). For\nthe second assertion, recall that the rank of a matrix is equal to the maximum number of\nlinearly independent columns, and the conclusion is clear.\n\nWe now characterize when a system of linear equations of the form Ax = b has a unique\nsolution.\n\nProposition 7.12. Given an n× n-matrix A over a field K, the following properties hold:\n\n(1) For every column vector b, there is a unique column vector x such that Ax = b iff the\nonly solution to Ax = 0 is the trivial vector x = 0, iff det(A) 6= 0.\n\n\n\n202 CHAPTER 7. DETERMINANTS\n\n(2) If det(A) 6= 0, the unique solution of Ax = b is given by the expressions\n\nxj =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n,\n\nknown as Cramer’s rules.\n\n(3) The system of linear equations Ax = 0 has a nonzero solution iff det(A) = 0.\n\nProof. (1) Assume that Ax = b has a single solution x0, and assume that Ay = 0 with y 6= 0.\nThen,\n\nA(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,\n\nand x0 + y 6= x0 is another solution of Ax = b, contradicting the hypothesis that Ax = b has\na single solution x0. Thus, Ax = 0 only has the trivial solution. Now assume that Ax = 0\nonly has the trivial solution. This means that the columns A1, . . . , An of A are linearly\nindependent, and by Proposition 7.11, we have det(A) 6= 0. Finally, if det(A) 6= 0, by\nProposition 7.10, this means that A is invertible, and then for every b, Ax = b is equivalent\nto x = A−1b, which shows that Ax = b has a single solution.\n\n(2) Assume that Ax = b. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , b, . . . , An),\n\nwhere b occurs in the j-th position, by multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\nxj det(A1, . . . , An) = det(A1, . . . , Aj−1, b, Aj+1, . . . , An),\n\nfor every j, 1 ≤ j ≤ n. Since we assumed that det(A) = det(A1, . . . , An) 6= 0, we get the\ndesired expression.\n\n(3) Note that Ax = 0 has a nonzero solution iff A1, . . . , An are linearly dependent (as\nobserved in the proof of Proposition 7.11), which, by Proposition 7.11, is equivalent to\ndet(A) = 0.\n\nAs pleasing as Cramer’s rules are, it is usually impractical to solve systems of linear\nequations using the above expressions. However, these formula imply an interesting fact,\nwhich is that the solution of the system Ax = b are continuous in A and b. If we assume that\nthe entries in A are continuous functions aij(t) and the entries in b are are also continuous\nfunctions bj(t) of a real parameter t, since determinants are polynomial functions of their\nentries, the expressions\n\nxj(t) =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n\nare ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero.\nSimilarly, if the functions aij(t) and bj(t) are differentiable, so are the xj(t).\n\n\n\n7.6. DETERMINANT OF A LINEAR MAP 203\n\n7.6 Determinant of a Linear Map\n\nGiven a vector space E of finite dimension n, given a basis (u1, . . . , un) of E, for every linear\nmap f : E → E, if M(f) is the matrix of f w.r.t. the basis (u1, . . . , un), we can define\ndet(f) = det(M(f)). If (v1, . . . , vn) is any other basis of E, and if P is the change of basis\nmatrix, by Corollary 4.5, the matrix of f with respect to the basis (v1, . . . , vn) is P−1M(f)P .\nBy Proposition 7.9, we have\n\ndet(P−1M(f)P ) = det(P−1) det(M(f)) det(P ) = det(P−1) det(P ) det(M(f)) = det(M(f)).\n\nThus, det(f) is indeed independent of the basis of E.\n\nDefinition 7.8. Given a vector space E of finite dimension, for any linear map f : E → E,\nwe define the determinant det(f) of f as the determinant det(M(f)) of the matrix of f in\nany basis (since, from the discussion just before this definition, this determinant does not\ndepend on the basis).\n\nThen we have the following proposition.\n\nProposition 7.13. Given any vector space E of finite dimension n, a linear map f : E → E\nis invertible iff det(f) 6= 0.\n\nProof. The linear map f : E → E is invertible iff its matrix M(f) in any basis is invertible\n(by Proposition 4.2), iff det(M(f)) 6= 0, by Proposition 7.10.\n\nGiven a vector space of finite dimension n, it is easily seen that the set of bijective linear\nmaps f : E → E such that det(f) = 1 is a group under composition. This group is a\nsubgroup of the general linear group GL(E). It is called the special linear group (of E), and\nit is denoted by SL(E), or when E = Kn, by SL(n,K), or even by SL(n).\n\n7.7 The Cayley–Hamilton Theorem\n\nWe next discuss an interesting and important application of Proposition 7.10, the Cayley–\nHamilton theorem. The results of this section apply to matrices over any commutative ring\nK. First we need the concept of the characteristic polynomial of a matrix.\n\nDefinition 7.9. If K is any commutative ring, for every n × n matrix A ∈ Mn(K), the\ncharacteristic polynomial PA(X) of A is the determinant\n\nPA(X) = det(XI − A).\n\n\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial PA(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen\n\nPA(X) =\n\n∣∣∣∣X − a −b\n−c X − d\n\n∣∣∣∣ = X2 − (a+ d)X + ad− bc.\n\nWe can substitute the matrix A for the variable X in the polynomial PA(X), obtaining a\nmatrix PA. If we write\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn,\n\nthen\nPA = An + c1A\n\nn−1 + · · ·+ cnI.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley–Hamilton) If K is any commutative ring, for every n× n matrix\nA ∈ Mn(K), if we let\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nbe the characteristic polynomial of A, then\n\nPA = An + c1A\nn−1 + · · ·+ cnI = 0.\n\nProof. We can view the matrix B = XI −A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B̃ which is the transpose of the matrix of\n\ncofactors of elements of B. Each entry in B̃ is an (n− 1)× (n− 1) determinant, and thus a\n\npolynomial of degree a most n− 1, so we can write B̃ as\n\nB̃ = Xn−1B0 +Xn−2B1 + · · ·+Bn−1,\n\nfor some n× n matrices B0, . . . , Bn−1 with coefficients in K. For example, when n = 2, we\nhave\n\nB =\n\n(\nX − a −b\n−c X − d\n\n)\n, B̃ =\n\n(\nX − d b\nc X − a\n\n)\n= X\n\n(\n1 0\n0 1\n\n)\n+\n\n(\n−d b\nc −a\n\n)\n.\n\nBy Proposition 7.10, we have\n\nBB̃ = det(B)I = PA(X)I.\n\nOn the other hand, we have\n\nBB̃ = (XI − A)(Xn−1B0 +Xn−2B1 + · · ·+Xn−j−1Bj + · · ·+Bn−1),\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial P4(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\na b\na=(\" i)\n\nX—a —b\n—c X-d\n\nthen\nPa(X) =\n\n=X? (wa) + ad be\n\nWe can substitute the matrix A for the variable X in the polynomial P4(X), obtaining a\nmatrix P,. If we write\nP4(X) =X\" +E, X\" 1 +++ +en,\n\nthen\nPy =A\" +c A\" '+---+e, 1.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley-Hamilton) If K is any commutative ring, for every n x n matriz\nAEM, (kK), if we let\nPy(X) =X\" +X\" 1 +---+e\n\nbe the characteristic polynomial of A, then\nPy =A\" +c, A\" 14+---+e,1 =0.\n\nProof. We can view the matrix B = XJ — A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B which is the transpose of the matrix of\ncofactors of elements of B. Each entry in B is an (n — 1) x (n — 1) determinant, and thus a\npolynomial of degree a most n — 1, so we can write Bas\n\nB= X\"'Bo +X\"? By, +--+ Baa,\n\nfor some n x n matrices Bo,..., By, with coefficients in K. For example, when n = 2, we\n\nhave\nX—-a@a —b ~ X—d b 1 0 —d 0b\nB= (“7 yea) B= ( C xa) =* (i N+ 1).\n\nBy Proposition 7.10, we have\nBB = det(B)I = Pa(X)I.\nOn the other hand, we have\n\nBB=(XI-A)(X\"'Bo +X\" 7B, + +--+ X77 'B; +---+ Br),\n\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 205\n\nand by multiplying out the right-hand side, we get\n\nBB̃ = XnD0 +Xn−1D1 + · · ·+Xn−jDj + · · ·+Dn,\n\nwith\n\nD0 = B0\n\nD1 = B1 − AB0\n\n...\n\nDj = Bj − ABj−1\n\n...\n\nDn−1 = Bn−1 − ABn−2\n\nDn = −ABn−1.\n\nSince\nPA(X)I = (Xn + c1X\n\nn−1 + · · ·+ cn)I,\n\nthe equality\nXnD0 +Xn−1D1 + · · ·+Dn = (Xn + c1X\n\nn−1 + · · ·+ cn)I\n\nis an equality between two matrices, so it requires that all corresponding entries are equal,\nand since these are polynomials, the coefficients of these polynomials must be identical,\nwhich is equivalent to the set of equations\n\nI = B0\n\nc1I = B1 − AB0\n\n...\n\ncjI = Bj − ABj−1\n\n...\n\ncn−1I = Bn−1 − ABn−2\n\ncnI = −ABn−1,\n\nfor all j, with 1 ≤ j ≤ n− 1. If, as in the table below,\n\nAn = AnB0\n\nc1A\nn−1 = An−1(B1 − AB0)\n\n...\n\ncjA\nn−j = An−j(Bj − ABj−1)\n\n...\n\ncn−1A = A(Bn−1 − ABn−2)\n\ncnI = −ABn−1,\n\n\n\n206 CHAPTER 7. DETERMINANTS\n\nwe multiply the first equation by An, the last by I, and generally the (j + 1)th by An−j,\nwhen we add up all these new equations, we see that the right-hand side adds up to 0, and\nwe get our desired equation\n\nAn + c1A\nn−1 + · · ·+ cnI = 0,\n\nas claimed.\n\nAs a concrete example, when n = 2, the matrix\n\nA =\n\n(\na b\nc d\n\n)\nsatisfies the equation\n\nA2 − (a+ d)A+ (ad− bc)I = 0.\n\nMost readers will probably find the proof of Theorem 7.14 rather clever but very myste-\nrious and unmotivated. The conceptual difficulty is that we really need to understand how\npolynomials in one variable “act” on vectors in terms of the matrix A. This can be done and\nyields a more “natural” proof. Actually, the reasoning is simpler and more general if we free\nourselves from matrices and instead consider a finite-dimensional vector space E and some\ngiven linear map f : E → E. Given any polynomial p(X) = a0X\n\nn + a1X\nn−1 + · · ·+ an with\n\ncoefficients in the field K, we define the linear map p(f) : E → E by\n\np(f) = a0f\nn + a1f\n\nn−1 + · · ·+ anid,\n\nwhere fk = f ◦ · · · ◦ f , the k-fold composition of f with itself. Note that\n\np(f)(u) = a0f\nn(u) + a1f\n\nn−1(u) + · · ·+ anu,\n\nfor every vector u ∈ E. Then we define a new kind of scalar multiplication · : K[X]×E → E\nby polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,\n\np(X) · u = p(f)(u).\n\nIt is easy to verify that this is a “good action,” which means that\n\np · (u+ v) = p · u+ p · v\n(p+ q) · u = p · u+ q · u\n\n(pq) · u = p · (q · u)\n\n1 · u = u,\n\nfor all p, q ∈ K[X] and all u, v ∈ E. With this new scalar multiplication, E is a K[X]-module.\n\nIf p = λ is just a scalar in K (a polynomial of degree 0), then\n\nλ · u = (λid)(u) = λu,\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 207\n\nwhich means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial\nX), then\n\nX · u = f(u).\n\nNow if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈ K[X] has the property\nthat\n\np(X) · ei = 0, i = 1, . . . , n,\n\nthen this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the linear map p(f)\nvanishes on E. We can also check, as we did in Section 7.2, that if A and B are two n× n\nmatrices and if (u1, . . . , un) are any n vectors, then\n\nA ·\n\nB ·\nu1\n\n...\nun\n\n\n = (AB) ·\n\nu1\n...\nun\n\n .\n\nThis suggests the plan of attack for our second proof of the Cayley–Hamilton theorem.\nFor simplicity, we prove the theorem for vector spaces over a field. The proof goes through\nfor a free module over a commutative ring.\n\nTheorem 7.15. (Cayley–Hamilton) For every finite-dimensional vector space over a field\nK, for every linear map f : E → E, for every basis (e1, . . . , en), if A is the matrix over f\nover the basis (e1, . . . , en) and if\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nis the characteristic polynomial of A, then\n\nPA(f) = fn + c1f\nn−1 + · · ·+ cnid = 0.\n\nProof. Since the columns of A consist of the vector f(ej) expressed over the basis (e1, . . . , en),\nwe have\n\nf(ej) =\nn∑\ni=1\n\nai jei, 1 ≤ j ≤ n.\n\nUsing our action of K[X] on E, the above equations can be expressed as\n\nX · ej =\nn∑\ni=1\n\nai j · ei, 1 ≤ j ≤ n,\n\nwhich yields\n\nj−1∑\ni=1\n\n−ai j · ei + (X − aj j) · ej +\nn∑\n\ni=j+1\n\n−ai j · ei = 0, 1 ≤ j ≤ n.\n\n\n\n208 CHAPTER 7. DETERMINANTS\n\nObserve that the transpose of the characteristic polynomial shows up, so the above system\ncan be written as\n\nX − a1 1 −a2 1 · · · −an 1\n\n−a1 2 X − a2 2 · · · −an 2\n...\n\n...\n...\n\n...\n−a1n −a2n · · · X − ann\n\n ·\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n .\n\nIf we let B = XI −A>, then as in the previous proof, if B̃ is the transpose of the matrix of\ncofactors of B, we have\n\nB̃B = det(B)I = det(XI − A>)I = det(XI − A)I = PAI.\n\nBut since\n\nB ·\n\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n ,\n\nand since B̃ is matrix whose entries are polynomials in K[X], it makes sense to multiply on\n\nthe left by B̃ and we get\n\nB̃ ·B ·\n\n\ne1\n\ne2\n...\nen\n\n = (B̃B) ·\n\n\ne1\n\ne2\n...\nen\n\n = PAI ·\n\n\ne1\n\ne2\n...\nen\n\n = B̃ ·\n\n\n0\n0\n...\n0\n\n =\n\n\n0\n0\n...\n0\n\n ;\n\nthat is,\nPA · ej = 0, j = 1, . . . , n,\n\nwhich proves that PA(f) = 0, as claimed.\n\nIfK is a field, then the characteristic polynomial of a linear map f : E → E is independent\nof the basis (e1, . . . , en) chosen in E. To prove this, observe that the matrix of f over another\nbasis will be of the form P−1AP , for some inverible matrix P , and then\n\ndet(XI − P−1AP ) = det(XP−1IP − P−1AP )\n\n= det(P−1(XI − A)P )\n\n= det(P−1) det(XI − A) det(P )\n\n= det(XI − A).\n\nTherefore, the characteristic polynomial of a linear map is intrinsic to f , and it is denoted\nby Pf .\n\nThe zeros (roots) of the characteristic polynomial of a linear map f are called the eigen-\nvalues of f . They play an important role in theory and applications. We will come back to\nthis topic later on.\n\n\n\n7.8. PERMANENTS 209\n\n7.8 Permanents\n\nRecall that the explicit formula for the determinant of an n× n matrix is\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n.\n\nIf we drop the sign ε(π) of every permutation from the above formula, we obtain a quantity\nknown as the permanent :\n\nper(A) =\n∑\nπ∈Sn\n\naπ(1) 1 · · · aπ(n)n.\n\nPermanents and determinants were investigated as early as 1812 by Cauchy. It is clear from\nthe above definition that the permanent is a multilinear symmetric form. We also have\n\nper(A) = per(A>),\n\nand the following unsigned version of the Laplace expansion formula:\n\nper(A) = ai 1per(Ai 1) + · · ·+ ai jper(Ai j) + · · ·+ ai nper(Ai n),\n\nfor i = 1, . . . , n. However, unlike determinants which have a clear geometric interpretation as\nsigned volumes, permanents do not have any natural geometric interpretation. Furthermore,\ndeterminants can be evaluated efficiently, for example using the conversion to row reduced\nechelon form, but computing the permanent is hard.\n\nPermanents turn out to have various combinatorial interpretations. One of these is in\nterms of perfect matchings of bipartite graphs which we now discuss.\n\nSee Definition 20.5 for the definition of an undirected graph. A bipartite (undirected)\ngraph G = (V,E) is a graph whose set of nodes V can be partitioned into two nonempty\ndisjoint subsets V1 and V2, such that every edge e ∈ E has one endpoint in V1 and one\nendpoint in V2.\n\nAn example of a bipartite graph with 14 nodes is shown in Figure 7.3; its nodes are\npartitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and {y1, y2, y3, y4, y5, y6, y7}.\n\nA matching in a graph G = (V,E) (bipartite or not) is a set M of pairwise non-adjacent\nedges, which means that no two edges in M share a common vertex. A perfect matching is\na matching such that every node in V is incident to some edge in the matching M (every\nnode in V is an endpoint of some edge in M). Figure 7.4 shows a perfect matching (in red)\nin the bipartite graph G.\n\nObviously, a perfect matching in a bipartite graph can exist only if its set of nodes has\na partition in two blocks of equal size, say {x1, . . . , xm} and {y1, . . . , ym}. Then there is\na bijection between perfect matchings and bijections π : {x1, . . . , xm} → {y1, . . . , ym} such\nthat π(xi) = yj iff there is an edge between xi and yj.\n\nNow every bipartite graph G with a partition of its nodes into two sets of equal size as\nabove is represented by an m × m matrix A = (aij) such that aij = 1 iff there is an edge\n\n\n\n210 CHAPTER 7. DETERMINANTS\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.3: A bipartite graph G.\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.4: A perfect matching in the bipartite graph G.\n\nbetween xi and yj, and aij = 0 otherwise. Using the interpretation of perfect matchings as\nbijections π : {x1, . . . , xm} → {y1, . . . , ym}, we see that the permanent per(A) of the (0, 1)-\nmatrix A representing the bipartite graph G counts the number of perfect matchings in G.\n\nIn a famous paper published in 1979, Leslie Valiant proves that computing the permanent\nis a #P-complete problem. Such problems are suspected to be intractable. It is known that\nif a polynomial-time algorithm existed to solve a #P-complete problem, then we would have\nP = NP , which is believed to be very unlikely.\n\nAnother combinatorial interpretation of the permanent can be given in terms of systems\nof distinct representatives. Given a finite set S, let (A1, . . . , An) be any sequence of nonempty\nsubsets of S (not necessarily distinct). A system of distinct representatives (for short SDR)\nof the sets A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈ Ai for i =\n1, . . . , n. The number of SDR’s of a sequence of sets plays an important role in combinatorics.\nNow, if S = {1, 2, . . . , n} and if we associate to any sequence (A1, . . . , An) of nonempty\nsubsets of S the matrix A = (aij) defined such that aij = 1 if j ∈ Ai and aij = 0 otherwise,\nthen the permanent per(A) counts the number of SDR’s of the sets A1, . . . , An.\n\nThis interpretation of permanents in terms of SDR’s can be used to prove bounds for the\npermanents of various classes of matrices. Interested readers are referred to van Lint and\n\n\n\n7.9. SUMMARY 211\n\nWilson [178] (Chapters 11 and 12). In particular, a proof of a theorem known as Van der\nWaerden conjecture is given in Chapter 12. This theorem states that for any n × n matrix\nA with nonnegative entries in which all row-sums and column-sums are 1 (doubly stochastic\nmatrices), we have\n\nper(A) ≥ n!\n\nnn\n,\n\nwith equality for the matrix in which all entries are equal to 1/n.\n\n7.9 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Permutations , transpositions , basics transpositions .\n\n• Every permutation can be written as a composition of permutations.\n\n• The parity of the number of transpositions involved in any decomposition of a permu-\ntation σ is an invariant; it is the signature ε(σ) of the permutation σ.\n\n• Multilinear maps (also called n-linear maps); bilinear maps .\n\n• Symmetric and alternating multilinear maps.\n\n• A basic property of alternating multilinear maps (Lemma 7.4) and the introduction of\nthe formula expressing a determinant.\n\n• Definition of a determinant as a multlinear alternating map D : Mn(K)→ K such that\nD(I) = 1.\n\n• We define the set of algorithms Dn, to compute the determinant of an n× n matrix.\n\n• Laplace expansion according to the ith row ; cofactors .\n\n• We prove that the algorithms in Dn compute determinants (Lemma 7.5).\n\n• We prove that all algorithms in Dn compute the same determinant (Theorem 7.6).\n\n• We give an interpretation of determinants as signed volumes .\n\n• We prove that det(A) = det(A>).\n\n• We prove that det(AB) = det(A) det(B).\n\n• The adjugate Ã of a matrix A.\n\n• Formula for the inverse in terms of the adjugate.\n\n\n\n212 CHAPTER 7. DETERMINANTS\n\n• A matrix A is invertible iff det(A) 6= 0.\n\n• Solving linear equations using Cramer’s rules .\n\n• Determinant of a linear map.\n\n• The characteristic polynomial of a matrix.\n\n• The Cayley–Hamilton theorem.\n\n• The action of the polynomial ring induced by a linear map on a vector space.\n\n• Permanents .\n\n• Permanents count the number of perfect matchings in bipartite graphs.\n\n• Computing the permanent is a #P-perfect problem (L. Valiant).\n\n• Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3–6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R2 of coordinates (c1−a1, c2−a2) and (b1−a1, b2−a2),\nprove that they are linearly dependent iff∣∣∣∣∣∣\n\na1 b1 c1\n\na2 b2 c2\n\n1 1 1\n\n∣∣∣∣∣∣ = 0.\n\n(2) Given three vectors in R3 of coordinates (d1−a1, d2−a2, d3−a3), (c1−a1, c2−a2, c3−a3),\nand (b1 − a1, b2 − a2, b3 − a3), prove that they are linearly dependent iff∣∣∣∣∣∣∣∣\n\na1 b1 c1 d1\n\na2 b2 c2 d2\n\na3 b3 c3 d3\n\n1 1 1 1\n\n∣∣∣∣∣∣∣∣ = 0.\n\n212 CHAPTER 7. DETERMINANTS\n\nA matrix A is invertible iff det(A) ¥ 0.\n\ne Solving linear equations using Cramer’s rules.\n\ne Determinant of a linear map.\n\ne The characteristic polynomial of a matrix.\n\ne The Cayley—Hamilton theorem.\n\ne The action of the polynomial ring induced by a linear map on a vector space.\ne Permanents.\n\ne Permanents count the number of perfect matchings in bipartite graphs.\n\ne Computing the permanent is a #P-perfect problem (L. Valiant).\n\ne Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3-6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R? of coordinates (cj —a,, C2—a@2) and (bj —a, by—a2),\nprove that they are linearly dependent iff\n\nay by Cy\nag bo C2) = 0.\n1 11\n\n(2) Given three vectors in R® of coordinates (d;—a1, d2—a2, d3—a3), (c1—a1, C2—2, C343),\nand (b1 — a1, bg — dg, b3 — a3), prove that they are linearly dependent iff\n\nay by Cc dy\naz by C2 dy\naz bg cz dg\n\n1 1 1éiéid1\n\n= 0.\n\n\n\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+ n)× (m+ n) block matrix (over any field K) given by\n\nA =\n\n(\nA1 A2\n\n0 A4\n\n)\n,\n\nwhere A1 is an m×m matrix, A2 is an m×n matrix, and A4 is an n×n matrix. Prove that\ndet(A) = det(A1) det(A4).\n\nUse the above result to prove that if A is an upper triangular n×n matrix, then det(A) =\na11a22 · · · ann.\n\nProblem 7.4. Prove that if n ≥ 3, then\n\ndet\n\n\n1 + x1y1 1 + x1y2 · · · 1 + x1yn\n1 + x2y1 1 + x2y2 · · · 1 + x2yn\n\n...\n...\n\n...\n...\n\n1 + xny1 1 + xny2 · · · 1 + xnyn\n\n = 0.\n\nProblem 7.5. Prove that ∣∣∣∣∣∣∣∣\n1 4 9 16\n4 9 16 25\n9 16 25 36\n16 25 36 49\n\n∣∣∣∣∣∣∣∣ = 0.\n\nProblem 7.6. Consider the n× n symmetric matrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n2 5 2 0 . . . 0 0\n0 2 5 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 2 5 2 0\n0 0 . . . 0 2 5 2\n0 0 . . . 0 0 2 5\n\n\n.\n\n(1) Find an upper-triangular matrix R such that A = R>R.\n\n(2) Prove that det(A) = 1.\n\n(3) Consider the sequence\n\np0(λ) = 1\n\np1(λ) = 1− λ\npk(λ) = (5− λ)pk−1(λ)− 4pk−2(λ) 2 ≤ k ≤ n.\n\nProve that\ndet(A− λI) = pn(λ).\n\nRemark: It can be shown that pn(λ) has n distinct (real) roots and that the roots of pk(λ)\nseparate the roots of pk+1(λ).\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+n) x (m+n) block matrix (over any field AK’) given by\n\nA, Ape\na=(o 4).\nwhere A, is an m Xm matrix, Ag is an m X n matrix, and A, is an n x n matrix. Prove that\ndet(A) = det(A;) det(Ay).\nUse the above result to prove that if A is an upper triangular n x n matrix, then det(A) =\n\n411422 °°* Ann-\n\nProblem 7.4. Prove that if n > 3, then\n\nL+ayyr Llt+ayye +++ 1+21y,\n1 + T2Y1 1 + T2Y2 °°\" 1 + T2Un\ndet . . . . = 0.\nL+apy1 L4+%nyo +++ 14+ 2nyn\nProblem 7.5. Prove that\n1 4 9 16\n4 9 16 25) _ 0\n9 16 25 36, ©\n16 25 36 49\nProblem 7.6. Consider the n x n symmetric matrix\n12 0 0 . 0 0\n25 2 O 0 0\n02 5 2 . 0 0\nA=]: : tet\n0 0 2 5 2 0\n0 0 0 2 5 2\n0 0 0 0 2 5\n\n(1) Find an upper-triangular matrix R such that A = R'R.\n(2) Prove that det(A) = 1.\n(3) Consider the sequence\npo(A) = 1\npi(A) =1-A\npr(A) = (5 — A)pe_-1(A) — 4pp-2(A) 2<k <n.\n\nProve that\ndet(A — AT) = pp (A).\n\nRemark: It can be shown that p,(A) has n distinct (real) roots and that the roots of pz (A)\nseparate the roots of pz41(A).\n\n\n\n\n214 CHAPTER 7. DETERMINANTS\n\nProblem 7.7. Let B be the n× n matrix (n ≥ 3) given by\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n.\n\nProve that\ndet(B) = (−1)n(n− 2)2n−1.\n\nProblem 7.8. Given a field K (say K = R or K = C), given any two polynomials\np(X), q(X) ∈ K[X], we says that q(X) divides p(X) (and that p(X) is a multiple of q(X))\niff there is some polynomial s(X) ∈ K[X] such that\n\np(X) = q(X)s(X).\n\nIn this case we say that q(X) is a factor of p(X), and if q(X) has degree at least one, we\nsay that q(X) is a nontrivial factor of p(X).\n\nLet f(X) and g(X) be two polynomials in K[X] with\n\nf(X) = a0X\nm + a1X\n\nm−1 + · · ·+ am\n\nof degree m ≥ 1 and\ng(X) = b0X\n\nn + b1X\nn−1 + · · ·+ bn\n\nof degree n ≥ 1 (with a0, b0 6= 0).\n\nYou will need the following result which you need not prove:\n\nTwo polynomials f(X) and g(X) with deg(f) = m ≥ 1 and deg(g) = n ≥ 1 have some\ncommon nontrivial factor iff there exist two nonzero polynomials p(X) and q(X) such that\n\nfp = gq,\n\nwith deg(p) ≤ n− 1 and deg(q) ≤ m− 1.\n\n(1) Let Pm denote the vector space of all polynomials in K[X] of degree at most m− 1,\nand let T : Pn × Pm → Pm+n be the map given by\n\nT (p, q) = fp+ gq, p ∈ Pn, q ∈ Pm,\n\nwhere f and g are some fixed polynomials of degree m ≥ 1 and n ≥ 1.\n\nProve that the map T is linear.\n\n\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f, g) = 0, where R(f, g) is\nthe determinant given by\n\nR(f, g) =\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\na0 a1 · · · · · · am 0 · · · · · · · · · · · · 0\n0 a0 a1 · · · · · · am 0 · · · · · · · · · 0\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · · · · · · · · · · 0 a0 a1 · · · · · · am\nb0 b1 · · · · · · · · · · · · · · · bn 0 · · · 0\n0 b0 b1 · · · · · · · · · · · · · · · bn 0 · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · 0 b0 b1 · · · · · · · · · · · · · · · bn\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\n.\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n+m)× (n+m) matrix, with the first row\n(involving the ais) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the bjs) occurring m times, each time shifted over to the right\nby one column.\n\nHint . Express the matrix of T over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.\n\n(b) m = 1 and n ≥ 2 arbitrary.\n\n(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.\n\n(5) Compute the resultant of f(X) = X3 + pX + q and g(X) = 3X2 + p, and\n\nf(X) = a0X\n2 + a1X + a2\n\ng(X) = b0X\n2 + b1X + b2.\n\nIn the second case, you should get\n\n4R(f, g) = (2a0b2 − a1b1 + 2a2b0)2 − (4a0a2 − a2\n1)(4b0b2 − b2\n\n1).\n\nProblem 7.9. Let A,B,C,D be n× n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\ndet\n\n(\nA B\nC D\n\n)\n= det(AD − CB).\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f,g) =0, where R(f,g) is\nthe determinant given by\n\nao ay eee wee Am 0 cee eee eee eee 0\n\n0 ag Gy cts tt Gm Oo ret tee eee O\nRg) =f\n\n0 eee eee eee wee 0 ao ay eee eee Am\n\na | 0\n\nOS | 0\n\nOe 0 dg Bp tet tte tet tee ee Dy\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n +m) x (n +m) matrix, with the first row\n(involving the a;s) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the b;s) occurring m times, each time shifted over to the right\nby one column.\n\nHint. Express the matrix of T’ over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a\n\n)\n(b) m=1 and n > 2 arbitrary.\n)\n\n(c\n\n3\n\n= n=l, and write f(X) =aX +b and g(X) =cX +d.\n\nf(X) =aX? + bX +c and g(X) = 2aX +b.\n\n(5) Compute the resultant of f(X) = X°+pX +q and g(X) = 3X? +p, and\n\nf(X) = ag.X? +a,X + ag\ng(X) => by X? + bX + bo.\n\nIn the second case, you should get\nAR(f, g) = (2Qagbe _ a,b, + 2aybp)? _ (daga2 —_ a*) (4bgbo _ bi).\n\nProblem 7.9. Let A,B,C, D be n x n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\nA B\ndet (< b) = det(AD — CB).\n\n\n\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then | det(H)| = nn/2.\n\n(3) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then\n\ndet\n\n(\nH H\nH −H\n\n)\n= (2n)n.\n\nProblem 7.10. Compute the product of the following determinants∣∣∣∣∣∣∣∣\na −b −c −d\nb a −d c\nc d a −b\nd −c b a\n\n∣∣∣∣∣∣∣∣\n∣∣∣∣∣∣∣∣\nx −y −z −t\ny x −t z\nz t x −y\nt −z y x\n\n∣∣∣∣∣∣∣∣\nto prove the following identity (due to Euler):\n\n(a2 + b2 + c2 + d2)(x2 + y2 + z2 + t2) = (ax+ by + cz + dt)2 + (ay − bx+ ct− dz)2\n\n+ (az − bt− cx+ dy)2 + (at+ bz − cy + dx)2.\n\nProblem 7.11. Let A be an n × n matrix with integer entries. Prove that A−1 exists and\nhas integer entries if and only if det(A) = ±1.\n\nProblem 7.12. Let A be an n× n real or complex matrix.\n\n(1) Prove that if A> = −A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n\n(2) Prove that ∣∣∣∣∣∣∣∣\n0 a b c\n−a 0 d e\n−b −d 0 f\n−c −e −f 0\n\n∣∣∣∣∣∣∣∣ = (af − be+ dc)2.\n\nProblem 7.13. A Cauchy matrix is a matrix of the form\n\n1\n\nλ1 − σ1\n\n1\n\nλ1 − σ2\n\n· · · 1\n\nλ1 − σn\n1\n\nλ2 − σ1\n\n1\n\nλ2 − σ2\n\n· · · 1\n\nλ2 − σn\n...\n\n...\n...\n\n...\n1\n\nλn − σ1\n\n1\n\nλn − σ2\n\n· · · 1\n\nλn − σn\n\n\nwhere λi 6= σj, for all i, j, with 1 ≤ i, j ≤ n. Prove that the determinant Cn of a Cauchy\nmatrix as above is given by\n\nCn =\n\n∏n\ni=2\n\n∏i−1\nj=1(λi − λj)(σj − σi)∏n\ni=1\n\n∏n\nj=1(λi − σj)\n\n.\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n x n Hadamard matrix (n > 2), then | det(H)| =n\".\n(3) Prove that if H is ann x n Hadamard matrix (n > 2), then\n\ndet (ji \") = (2n)\".\n\nProblem 7.10. Compute the product of the following determinants\n\na —b -—c —d\\|x -y -z -t\nb a -d clly a -t 2z\nc dad a —bl|z t aw -y\nd -c b allt -z y «\n\nto prove the following identity (due to Euler):\n\n(P4+RP4C4P)(a? +y? +240) = (ax + by + cz +dt) + (ay — bx + ct — dz)?\n+ (az — bt — cx + dy)? + (at + bz — cy + dx)’.\n\nProblem 7.11. Let A be an n x n matrix with integer entries. Prove that A! exists and\nhas integer entries if and only if det(A) = +1.\n\nProblem 7.12. Let A be an n x n real or complex matrix.\n(1) Prove that if A' = —A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n(2) Prove that\n\n0 a ob e\n—a 0 de} _ 9\npb -d 0 f = (af — be + dc)’.\n—c -e -f 0\nProblem 7.13. A Cauchy matrix is a matrix of the form\n1 1 1\nAy —- 0, ATO At — On\n1 1 ALT 972 Fy\nAy — O71 Ay — J2 A2 — On\n1 1 | 1\nMn — 01 An — 02 An — On\n\nwhere \\; 4 o;, for all 7,7, with 1 < 7,7 <n. Prove that the determinant C,, of a Cauchy\nmatrix as above is given by\n\nC= [Tj-2 Wai — r;)(a; — 9)\n\nTie Tj — 9j)\n\n\n\n\n7.11. PROBLEMS 217\n\nProblem 7.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct scalars in R and let\n(β1, . . . , βm+1) be any sequence of scalars in R, not necessarily distinct.\n\n(1) Prove that there is a unique polynomial P of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\nHint . Remember Vandermonde!\n\n(2) Let Li(X) be the polynomial of degree m given by\n\nLi(X) =\n(X − α1) · · · (X − αi−1)(X − αi+1) · · · (X − αm+1)\n\n(αi − α1) · · · (αi − αi−1)(αi − αi+1) · · · (αi − αm+1)\n, 1 ≤ i ≤ m+ 1.\n\nThe polynomials Li(X) are known as Lagrange polynomial interpolants . Prove that\n\nLi(αj) = δi j 1 ≤ i, j ≤ m+ 1.\n\nProve that\nP (X) = β1L1(X) + · · ·+ βm+1Lm+1(X)\n\nis the unique polynomial of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\n(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that they form a basis\nof all polynomials of degree at most m.\n\nHow is 1 (the constant polynomial 1) expressed over the basis (L1(X), . . . , Lm+1(X))?\n\nGive the expression of every polynomial P (X) of degree at most m over the basis\n(L1(X), . . . , Lm+1(X)).\n\n(4) Prove that the dual basis (L∗1, . . . , L\n∗\nm+1) of the basis (L1(X), . . . , Lm+1(X)) consists\n\nof the linear forms L∗i given by\nL∗i (P ) = P (αi),\n\nfor every polynomial P of degree at most m; this is simply evaluation at αi.\n\n\n\n218 CHAPTER 7. DETERMINANTS\n\n\n\nChapter 8\n\nGaussian Elimination,\nLU-Factorization, Cholesky\nFactorization, Reduced Row Echelon\nForm\n\nIn this chapter we assume that all vector spaces are over the field R. All results that do not\nrely on the ordering on R or on taking square roots hold for arbitrary fields.\n\n8.1 Motivating Example: Curve Interpolation\n\nCurve interpolation is a problem that arises frequently in computer graphics and in robotics\n(path planning). There are many ways of tackling this problem and in this section we will\ndescribe a solution using cubic splines . Such splines consist of cubic Bézier curves. They\nare often used because they are cheap to implement and give more flexibility than quadratic\nBézier curves.\n\nA cubic Bézier curve C(t) (in R2 or R3) is specified by a list of four control points\n(b0, b2, b2, b3) and is given parametrically by the equation\n\nC(t) = (1− t)3 b0 + 3(1− t)2t b1 + 3(1− t)t2 b2 + t3 b3.\n\nClearly, C(0) = b0, C(1) = b3, and for t ∈ [0, 1], the point C(t) belongs to the convex hull of\nthe control points b0, b1, b2, b3. The polynomials\n\n(1− t)3, 3(1− t)2t, 3(1− t)t2, t3\n\nare the Bernstein polynomials of degree 3.\n\nTypically, we are only interested in the curve segment corresponding to the values of t in\nthe interval [0, 1]. Still, the placement of the control points drastically affects the shape of the\ncurve segment, which can even have a self-intersection; See Figures 8.1, 8.2, 8.3 illustrating\nvarious configurations.\n\n219\n\n\n\n220 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.1: A “standard” Bézier curve.\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.2: A Bézier curve with an inflection point.\n\n\n\n8.1. MOTIVATING EXAMPLE: CURVE INTERPOLATION 221\n\nb0\n\nb1b2\n\nb3\n\nFigure 8.3: A self-intersecting Bézier curve.\n\nInterpolation problems require finding curves passing through some given data points and\npossibly satisfying some extra constraints.\n\nA Bézier spline curve F is a curve which is made up of curve segments which are Bézier\ncurves, say C1, . . . , Cm (m ≥ 2). We will assume that F defined on [0,m], so that for\ni = 1, . . . ,m,\n\nF (t) = Ci(t− i+ 1), i− 1 ≤ t ≤ i.\n\nTypically, some smoothness is required between any two junction points, that is, between\nany two points Ci(1) and Ci+1(0), for i = 1, . . . ,m − 1. We require that Ci(1) = Ci+1(0)\n(C0-continuity), and typically that the derivatives of Ci at 1 and of Ci+1 at 0 agree up to\nsecond order derivatives. This is called C2-continuity , and it ensures that the tangents agree\nas well as the curvatures.\n\nThere are a number of interpolation problems, and we consider one of the most common\nproblems which can be stated as follows:\n\nProblem: Given N + 1 data points x0, . . . , xN , find a C2 cubic spline curve F such that\nF (i) = xi for all i, 0 ≤ i ≤ N (N ≥ 2).\n\nA way to solve this problem is to find N + 3 auxiliary points d−1, . . . , dN+1, called de\nBoor control points , from which N Bézier curves can be found. Actually,\n\nd−1 = x0 and dN+1 = xN\n\n\n\n222 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nso we only need to find N + 1 points d0, . . . , dN .\n\nIt turns out that the C2-continuity constraints on the N Bézier curves yield only N − 1\nequations, so d0 and dN can be chosen arbitrarily. In practice, d0 and dN are chosen according\nto various end conditions, such as prescribed velocities at x0 and xN . For the time being, we\nwill assume that d0 and dN are given.\n\nFigure 8.4 illustrates an interpolation problem involving N + 1 = 7 + 1 = 8 data points.\nThe control points d0 and d7 were chosen arbitrarily.\n\nx0 = d−1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\nFigure 8.4: A C2 cubic interpolation spline curve passing through the points x0, x1, x2, x3,\nx4, x5, x6, x7.\n\nIt can be shown that d1, . . . , dN−1 are given by the linear system\n7\n2\n\n1\n1 4 1 0\n\n. . . . . . . . .\n\n0 1 4 1\n1 7\n\n2\n\n\n\n\nd1\n\nd2\n...\n\ndN−2\n\ndN−1\n\n =\n\n\n6x1 − 3\n\n2\nd0\n\n6x2\n...\n\n6xN−2\n\n6xN−1 − 3\n2\ndN\n\n .\n\nWe will show later that the above matrix is invertible because it is strictly diagonally\ndominant.\n\n\n\n8.2. GAUSSIAN ELIMINATION 223\n\nOnce the above system is solved, the Bézier cubics C1, . . ., CN are determined as follows\n(we assume N ≥ 2): For 2 ≤ i ≤ N − 1, the control points (bi0, b\n\ni\n1, b\n\ni\n2, b\n\ni\n3) of Ci are given by\n\nbi0 = xi−1\n\nbi1 =\n2\n\n3\ndi−1 +\n\n1\n\n3\ndi\n\nbi2 =\n1\n\n3\ndi−1 +\n\n2\n\n3\ndi\n\nbi3 = xi.\n\nThe control points (b1\n0, b\n\n1\n1, b\n\n1\n2, b\n\n1\n3) of C1 are given by\n\nb1\n0 = x0\n\nb1\n1 = d0\n\nb1\n2 =\n\n1\n\n2\nd0 +\n\n1\n\n2\nd1\n\nb1\n3 = x1,\n\nand the control points (bN0 , b\nN\n1 , b\n\nN\n2 , b\n\nN\n3 ) of CN are given by\n\nbN0 = xN−1\n\nbN1 =\n1\n\n2\ndN−1 +\n\n1\n\n2\ndN\n\nbN2 = dN\n\nbN3 = xN .\n\nFigure 8.5 illustrates this process spline interpolation for N = 7.\n\nWe will now describe various methods for solving linear systems. Since the matrix of the\nabove system is tridiagonal, there are specialized methods which are more efficient than the\ngeneral methods. We will discuss a few of these methods.\n\n8.2 Gaussian Elimination\n\nLet A be an n × n matrix, let b ∈ Rn be an n-dimensional vector and assume that A is\ninvertible. Our goal is to solve the system Ax = b. Since A is assumed to be invertible,\nwe know that this system has a unique solution x = A−1b. Experience shows that two\ncounter-intuitive facts are revealed:\n\n(1) One should avoid computing the inverse A−1 of A explicitly. This is inefficient since\nit would amount to solving the n linear systems Au(j) = ej for j = 1, . . . , n, where\nej = (0, . . . , 1, . . . , 0) is the jth canonical basis vector of Rn (with a 1 is the jth slot).\nBy doing so, we would replace the resolution of a single system by the resolution of n\nsystems, and we would still have to multiply A−1 by b.\n\n\n\n224 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nx0 = d1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\n1\n1b =\n\n1\n2b\n\nb\n2\n1\n\nb\n2\n2\n\nb\n\nb1\n3\n\nb2\n3\n\nb1\n4\n\nb2\n4\n\nb1\n5\n\nb2\n5\n\nb1\n6\n\nb2\n6\n\n1\n7\n\nb\n7\n2=\n\nFigure 8.5: A C2 cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color\ncoded Bézier cubics.\n\n(2) One does not solve (large) linear systems by computing determinants (using Cramer’s\nformulae) since this method requires a number of additions (resp. multiplications)\nproportional to (n+ 1)! (resp. (n+ 2)!).\n\nThe key idea on which most direct methods (as opposed to iterative methods, that look\nfor an approximation of the solution) are based is that if A is an upper-triangular matrix,\nwhich means that aij = 0 for 1 ≤ j < i ≤ n (resp. lower-triangular, which means that\naij = 0 for 1 ≤ i < j ≤ n), then computing the solution x is trivial. Indeed, say A is an\nupper-triangular matrix\n\nA =\n\n\n\na1 1 a1 2 · · · a1n−2 a1n−1 a1n\n\n0 a2 2 · · · a2n−2 a2n−1 a2n\n\n0 0\n. . .\n\n...\n...\n\n...\n. . .\n\n...\n...\n\n0 0 · · · 0 an−1n−1 an−1n\n\n0 0 · · · 0 0 ann\n\n\n.\n\nThen det(A) = a1 1a2 2 · · · ann 6= 0, which implies that ai i 6= 0 for i = 1, . . . , n, and we can\nsolve the system Ax = b from bottom-up by back-substitution. That is, first we compute\n\n\n\n8.2. GAUSSIAN ELIMINATION 225\n\nxn from the last equation, next plug this value of xn into the next to the last equation and\ncompute xn−1 from it, etc. This yields\n\nxn = a−1\nnnbn\n\nxn−1 = a−1\nn−1n−1(bn−1 − an−1nxn)\n\n...\n\nx1 = a−1\n1 1 (b1 − a1 2x2 − · · · − a1nxn).\n\nNote that the use of determinants can be avoided to prove that if A is invertible then\nai i 6= 0 for i = 1, . . . , n. Indeed, it can be shown directly (by induction) that an upper (or\nlower) triangular matrix is invertible iff all its diagonal entries are nonzero.\n\nIf A is lower-triangular, we solve the system from top-down by forward-substitution.\n\nThus, what we need is a method for transforming a matrix to an equivalent one in upper-\ntriangular form. This can be done by elimination. Let us illustrate this method on the\nfollowing example:\n\n2x + y + z = 5\n4x − 6y = −2\n−2x + 7y + 2z = 9.\n\nWe can eliminate the variable x from the second and the third equation as follows: Subtract\ntwice the first equation from the second and add the first equation to the third. We get the\nnew system\n\n2x + y + z = 5\n− 8y − 2z = −12\n\n8y + 3z = 14.\n\nThis time we can eliminate the variable y from the third equation by adding the second\nequation to the third:\n\n2x + y + z = 5\n− 8y − 2z = −12\n\nz = 2.\n\nThis last system is upper-triangular. Using back-substitution, we find the solution: z = 2,\ny = 1, x = 1.\n\nObserve that we have performed only row operations. The general method is to iteratively\neliminate variables using simple row operations (namely, adding or subtracting a multiple of\na row to another row of the matrix) while simultaneously applying these operations to the\nvector b, to obtain a system, MAx = Mb, where MA is upper-triangular. Such a method is\ncalled Gaussian elimination. However, one extra twist is needed for the method to work in\nall cases: It may be necessary to permute rows, as illustrated by the following example:\n\nx + y + z = 1\nx + y + 3z = 1\n2x + 5y + 8z = 1.\n\n\n\n226 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nIn order to eliminate x from the second and third row, we subtract the first row from the\nsecond and we subtract twice the first row from the third:\n\nx + y + z = 1\n2z = 0\n\n3y + 6z = −1.\n\nNow the trouble is that y does not occur in the second row; so, we can’t eliminate y from\nthe third row by adding or subtracting a multiple of the second row to it. The remedy is\nsimple: Permute the second and the third row! We get the system:\n\nx + y + z = 1\n3y + 6z = −1\n\n2z = 0,\n\nwhich is already in triangular form. Another example where some permutations are needed\nis:\n\nz = 1\n−2x + 7y + 2z = 1\n4x − 6y = −1.\n\nFirst we permute the first and the second row, obtaining\n\n−2x + 7y + 2z = 1\nz = 1\n\n4x − 6y = −1,\n\nand then we add twice the first row to the third, obtaining:\n\n−2x + 7y + 2z = 1\nz = 1\n\n8y + 4z = 1.\n\nAgain we permute the second and the third row, getting\n\n−2x + 7y + 2z = 1\n8y + 4z = 1\n\nz = 1,\n\nan upper-triangular system. Of course, in this example, z is already solved and we could\nhave eliminated it first, but for the general method, we need to proceed in a systematic\nfashion.\n\nWe now describe the method of Gaussian elimination applied to a linear system Ax = b,\nwhere A is assumed to be invertible. We use the variable k to keep track of the stages of\nelimination. Initially, k = 1.\n\n\n\n8.2. GAUSSIAN ELIMINATION 227\n\n(1) The first step is to pick some nonzero entry ai 1 in the first column of A. Such an\nentry must exist, since A is invertible (otherwise, the first column of A would be the\nzero vector, and the columns of A would not be linearly independent. Equivalently, we\nwould have det(A) = 0). The actual choice of such an element has some impact on the\nnumerical stability of the method, but this will be examined later. For the time being,\nwe assume that some arbitrary choice is made. This chosen element is called the pivot\nof the elimination step and is denoted π1 (so, in this first step, π1 = ai 1).\n\n(2) Next we permute the row (i) corresponding to the pivot with the first row. Such a\nstep is called pivoting . So after this permutation, the first element of the first row is\nnonzero.\n\n(3) We now eliminate the variable x1 from all rows except the first by adding suitable\nmultiples of the first row to these rows. More precisely we add −ai 1/π1 times the first\nrow to the ith row for i = 2, . . . , n. At the end of this step, all entries in the first\ncolumn are zero except the first.\n\n(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively repeat Steps\n(1), (2), (3) on the (n− k + 1)× (n− k + 1) subsystem obtained by deleting the first\nk − 1 rows and k − 1 columns from the current system.\n\nIf we let A1 = A and Ak = (a\n(k)\ni j ) be the matrix obtained after k − 1 elimination steps\n\n(2 ≤ k ≤ n), then the kth elimination step is applied to the matrix Ak of the form\n\nAk =\n\n\n\na\n(k)\n1 1 a\n\n(k)\n1 2 · · · · · · · · · a\n\n(k)\n1n\n\n0 a\n(k)\n2 2 · · · · · · · · · a\n\n(k)\n2n\n\n...\n. . . . . .\n\n...\n...\n\n0 0 0 a\n(k)\nk k · · · a\n\n(k)\nk n\n\n...\n...\n\n...\n...\n\n...\n\n0 0 0 a\n(k)\nnk · · · a\n\n(k)\nnn\n\n\n.\n\nActually, note that\na\n\n(k)\ni j = a\n\n(i)\ni j\n\nfor all i, j with 1 ≤ i ≤ k − 2 and i ≤ j ≤ n, since the first k − 1 rows remain unchanged\nafter the (k − 1)th step.\n\nWe will prove later that det(Ak) = ± det(A). Consequently, Ak is invertible. The fact\nthat Ak is invertible iff A is invertible can also be shown without determinants from the fact\nthat there is some invertible matrix Mk such that Ak = MkA, as we will see shortly.\n\nSince Ak is invertible, some entry a\n(k)\ni k with k ≤ i ≤ n is nonzero. Otherwise, the last\n\nn − k + 1 entries in the first k columns of Ak would be zero, and the first k columns of\nAk would yield k vectors in Rk−1. But then the first k columns of Ak would be linearly\n\n\n\n228 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\ndependent and Ak would not be invertible, a contradiction. This situation is illustrated by\nthe following matrix for n = 5 and k = 3:\n\na\n(3)\n1 1 a\n\n(3)\n1 2 a\n\n(3)\n1 3 a\n\n(3)\n1 3 a\n\n(3)\n1 5\n\n0 a\n(3)\n2 2 a\n\n(3)\n2 3 a\n\n(3)\n2 4 a\n\n(3)\n2 5\n\n0 0 0 a\n(3)\n3 4 a\n\n(3)\n3 5\n\n0 0 0 a\n(3)\n4 4 a\n\n(3)\n4n\n\n0 0 0 a\n(3)\n5 4 a\n\n(3)\n5 5\n\n .\n\nThe first three columns of the above matrix are linearly dependent.\n\nSo one of the entries a\n(k)\ni k with k ≤ i ≤ n can be chosen as pivot, and we permute the kth\n\nrow with the ith row, obtaining the matrix α(k) = (α\n(k)\nj l ). The new pivot is πk = α\n\n(k)\nk k , and\n\nwe zero the entries i = k + 1, . . . , n in column k by adding −α(k)\ni k /πk times row k to row i.\n\nAt the end of this step, we have Ak+1. Observe that the first k − 1 rows of Ak are identical\nto the first k − 1 rows of Ak+1.\n\nThe process of Gaussian elimination is illustrated in schematic form below:\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 × × ×\n0 × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 0 ×\n\n .\n\n8.3 Elementary Matrices and Row Operations\n\nIt is easy to figure out what kind of matrices perform the elementary row operations used\nduring Gaussian elimination. The key point is that if A = PB, where A,B are m × n\nmatrices and P is a square matrix of dimension m, if (as usual) we denote the rows of A and\nB by A1, . . . , Am and B1, . . . , Bm, then the formula\n\naij =\nm∑\nk=1\n\npikbkj\n\ngiving the (i, j)th entry in A shows that the ith row of A is a linear combination of the rows\nof B:\n\nAi = pi1B1 + · · ·+ pimBm.\n\nTherefore, multiplication of a matrix on the left by a square matrix performs row opera-\ntions . Similarly, multiplication of a matrix on the right by a square matrix performs column\noperations\n\nThe permutation of the kth row with the ith row is achieved by multiplying A on the left\nby the transposition matrix P (i, k), which is the matrix obtained from the identity matrix\n\n\n\n8.3. ELEMENTARY MATRICES AND ROW OPERATIONS 229\n\nby permuting rows i and k, i.e.,\n\nP (i, k) =\n\n\n\n1\n1\n\n0 1\n1\n\n. . .\n\n1\n1 0\n\n1\n1\n\n\n.\n\nFor example, if m = 3,\n\nP (1, 3) =\n\n0 0 1\n0 1 0\n1 0 0\n\n ,\n\nthen\n\nP (1, 3)B =\n\n0 0 1\n0 1 0\n1 0 0\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n =\n\nb31 b32 · · · · · · · · · b3n\n\nb21 b22 · · · · · · · · · b2n\n\nb11 b12 · · · · · · · · · b1n\n\n .\n\nObserve that det(P (i, k)) = −1. Furthermore, P (i, k) is symmetric (P (i, k)> = P (i, k)), and\n\nP (i, k)−1 = P (i, k).\n\nDuring the permutation Step (2), if row k and row i need to be permuted, the matrix A\nis multiplied on the left by the matrix Pk such that Pk = P (i, k), else we set Pk = I.\n\nAdding β times row j to row i (with i 6= j) is achieved by multiplying A on the left by\nthe elementary matrix ,\n\nEi,j;β = I + βei j,\n\nwhere\n\n(ei j)k l =\n\n{\n1 if k = i and l = j\n0 if k 6= i or l 6= j,\n\ni.e.,\n\nEi,j;β =\n\n\n\n1\n1\n\n1\n1\n\n. . .\n\n1\nβ 1\n\n1\n1\n\n\nor Ei,j;β =\n\n\n\n1\n1\n\n1 β\n1\n\n. . .\n\n1\n1\n\n1\n1\n\n\n,\n\n\n\n230 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\non the left, i > j, and on the right, i < j. The index i is the index of the row that is changed\nby the multiplication. For example, if m = 3 and we want to add twice row 1 to row 3, since\nβ = 2, j = 1 and i = 3, we form\n\nE3,1;2 = I + 2e31 =\n\n1 0 0\n0 1 0\n0 0 1\n\n+\n\n0 0 0\n0 0 0\n2 0 0\n\n =\n\n1 0 0\n0 1 0\n2 0 1\n\n ,\n\nand calculate\n\nE3,1;2B =\n\n1 0 0\n0 1 0\n2 0 1\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n\n=\n\n b11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\n2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n\n\n .\n\nObserve that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I − βei j and that det(Ei,j;β) = 1.\nTherefore, during Step 3 (the elimination step), the matrix A is multiplied on the left by a\nproduct Ek of matrices of the form Ei,k;βi,k , with i > k.\n\nConsequently, we see that\nAk+1 = EkPkAk,\n\nand then\nAk = Ek−1Pk−1 · · ·E1P1A.\n\nThis justifies the claim made earlier that Ak = MkA for some invertible matrix Mk; we can\npick\n\nMk = Ek−1Pk−1 · · ·E1P1,\n\na product of invertible matrices.\n\nThe fact that det(P (i, k)) = −1 and that det(Ei,j;β) = 1 implies immediately the fact\nclaimed above: We always have\n\ndet(Ak) = ± det(A).\n\nFurthermore, since\nAk = Ek−1Pk−1 · · ·E1P1A\n\nand since Gaussian elimination stops for k = n, the matrix\n\nAn = En−1Pn−1 · · ·E2P2E1P1A\n\nis upper-triangular. Also note that if we letM = En−1Pn−1 · · ·E2P2E1P1, then det(M) = ±1,\nand\n\ndet(A) = ± det(An).\n\nThe matrices P (i, k) and Ei,j;β are called elementary matrices . We can summarize the\nabove discussion in the following theorem:\n\n\n\n8.4. LU -FACTORIZATION 231\n\nTheorem 8.1. (Gaussian elimination) Let A be an n× n matrix (invertible or not). Then\nthere is some invertible matrix M so that U = MA is upper-triangular. The pivots are all\nnonzero iff A is invertible.\n\nProof. We already proved the theorem when A is invertible, as well as the last assertion.\nNow A is singular iff some pivot is zero, say at Stage k of the elimination. If so, we must\nhave a\n\n(k)\ni k = 0 for i = k, . . . , n; but in this case, Ak+1 = Ak and we may pick Pk = Ek = I.\n\nRemark: Obviously, the matrix M can be computed as\n\nM = En−1Pn−1 · · ·E2P2E1P1,\n\nbut this expression is of no use. Indeed, what we need is M−1; when no permutations are\nneeded, it turns out that M−1 can be obtained immediately from the matrices Ek’s, in fact,\nfrom their inverses, and no multiplications are necessary.\n\nRemark: Instead of looking for an invertible matrix M so that MA is upper-triangular, we\ncan look for an invertible matrix M so that MA is a diagonal matrix. Only a simple change\nto Gaussian elimination is needed. At every Stage k, after the pivot has been found and\npivoting been performed, if necessary, in addition to adding suitable multiples of the kth\nrow to the rows below row k in order to zero the entries in column k for i = k + 1, . . . , n,\nalso add suitable multiples of the kth row to the rows above row k in order to zero the\nentries in column k for i = 1, . . . , k − 1. Such steps are also achieved by multiplying on\nthe left by elementary matrices Ei,k;βi,k , except that i < k, so that these matrices are not\nlower-triangular matrices. Nevertheless, at the end of the process, we find that An = MA,\nis a diagonal matrix.\n\nThis method is called the Gauss-Jordan factorization. Because it is more expensive than\nGaussian elimination, this method is not used much in practice. However, Gauss-Jordan\nfactorization can be used to compute the inverse of a matrix A. Indeed, we find the jth\ncolumn of A−1 by solving the system Ax(j) = ej (where ej is the jth canonical basis vector\nof Rn). By applying Gauss-Jordan, we are led to a system of the form Djx\n\n(j) = Mjej, where\nDj is a diagonal matrix, and we can immediately compute x(j).\n\nIt remains to discuss the choice of the pivot, and also conditions that guarantee that no\npermutations are needed during the Gaussian elimination process. We begin by stating a\nnecessary and sufficient condition for an invertible matrix to have an LU -factorization (i.e.,\nGaussian elimination does not require pivoting).\n\n8.4 LU-Factorization\n\nDefinition 8.1. We say that an invertible matrix A has an LU-factorization if it can be\nwritten as A = LU , where U is upper-triangular invertible and L is lower-triangular, with\nLi i = 1 for i = 1, . . . , n.\n\n\n\n232 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nA lower-triangular matrix with diagonal entries equal to 1 is called a unit lower-triangular\nmatrix. Given an n×n matrix A = (ai j), for any k with 1 ≤ k ≤ n, let A(1 : k, 1 : k) denote\nthe submatrix of A whose entries are ai j, where 1 ≤ i, j ≤ k.1 For example, if A is the 5× 5\nmatrix\n\nA =\n\n\na11 a12 a13 a14 a15\n\na21 a22 a23 a24 a25\n\na31 a32 a33 a34 a35\n\na41 a42 a43 a44 a45\n\na51 a52 a53 a54 a55\n\n ,\n\nthen\n\nA(1 : 3, 1 : 3) =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n .\n\nProposition 8.2. Let A be an invertible n × n-matrix. Then A has an LU-factorization\nA = LU iff every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n. Furthermore, when A\nhas an LU-factorization, we have\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nwhere πk is the pivot obtained after k− 1 elimination steps. Therefore, the kth pivot is given\nby\n\nπk =\n\na11 = det(A(1 : 1, 1 : 1)) if k = 1\ndet(A(1 : k, 1 : k))\n\ndet(A(1 : k − 1, 1 : k − 1))\nif k = 2, . . . , n.\n\nProof. First assume that A = LU is an LU -factorization of A. We can write\n\nA =\n\n(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nL1 0\nL3 L4\n\n)(\nU1 U2\n\n0 U4\n\n)\n=\n\n(\nL1U1 L1U2\n\nL3U1 L3U2 + L4U4\n\n)\n,\n\nwhere L1, L4 are unit lower-triangular and U1, U4 are upper-triangular. (Note, A(1 : k, 1 : k),\nL1, and U1 are k×k matrices; A2 and U2 are k× (n−k) matrices; A3 and L3 are (n−k)×k\nmatrices; A4, L4, and U4 are (n− k)× (n− k) matrices.) Thus,\n\nA(1 : k, 1 : k) = L1U1,\n\nand since U is invertible, U1 is also invertible (the determinant of U is the product of the\ndiagonal entries in U , which is the product of the diagonal entries in U1 and U4). As L1 is\ninvertible (since its diagonal entries are equal to 1), we see that A(1 : k, 1 : k) is invertible\nfor k = 1, . . . , n.\n\nConversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We just need to\nshow that Gaussian elimination does not need pivoting. We prove by induction on k that\nthe kth step does not need pivoting.\n\n1We are using Matlab’s notation.\n\n\n\n8.4. LU -FACTORIZATION 233\n\nThis holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 6= 0. Assume that no pivoting\nwas necessary for the first k − 1 steps (2 ≤ k ≤ n− 1). In this case, we have\n\nEk−1 · · ·E2E1A = Ak,\n\nwhere L = Ek−1 · · ·E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 : k) is upper-\ntriangular, so that LA = Ak can be written as(\n\nL1 0\nL3 L4\n\n)(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nU1 B2\n\n0 B4\n\n)\n,\n\nwhere L1 is unit lower-triangular and U1 is upper-triangular. (Once again A(1 : k, 1 : k), L1,\nand U1 are k × k matrices; A2 and B2 are k × (n− k) matrices; A3 and L3 are (n− k)× k\nmatrices; A4, L4, and B4 are (n− k)× (n− k) matrices.) But then,\n\nL1A(1 : k, 1 : k)) = U1,\n\nwhere L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 : k, 1 : k) is\ninvertible, U1 is also invertible, which implies that (U1)kk 6= 0, since U1 is upper-triangular.\nTherefore, no pivoting is needed in Step k, establishing the induction step. Since det(L1) = 1,\nwe also have\n\ndet(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k)) = det(A(1 : k, 1 : k)),\n\nand since U1 is upper-triangular and has the pivots π1, . . . , πk on its diagonal, we get\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nas claimed.\n\nRemark: The use of determinants in the first part of the proof of Proposition 8.2 can be\navoided if we use the fact that a triangular matrix is invertible iff all its diagonal entries are\nnonzero.\n\nCorollary 8.3. (LU-Factorization) Let A be an invertible n × n-matrix. If every matrix\nA(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian elimination requires no pivoting\nand yields an LU-factorization A = LU .\n\nProof. We proved in Proposition 8.2 that in this case Gaussian elimination requires no\npivoting. Then since every elementary matrix Ei,k;β is lower-triangular (since we always\narrange that the pivot πk occurs above the rows that it operates on), since E−1\n\ni,k;β = Ei,k;−β\nand the Eks are products of Ei,k;βi,ks, from\n\nEn−1 · · ·E2E1A = U,\n\nwhere U is an upper-triangular matrix, we get\n\nA = LU,\n\nwhere L = E−1\n1 E−1\n\n2 · · ·E−1\nn−1 is a lower-triangular matrix. Furthermore, as the diagonal\n\nentries of each Ei,k;β are 1, the diagonal entries of each Ek are also 1.\n\n\n\n234 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nExample 8.1. The reader should verify that\n2 1 1 0\n4 3 3 1\n8 7 9 5\n6 7 9 8\n\n =\n\n\n1 0 0 0\n2 1 0 0\n4 3 1 0\n3 4 1 1\n\n\n\n\n2 1 1 0\n0 1 1 1\n0 0 2 2\n0 0 0 2\n\n\nis an LU -factorization.\n\nOne of the main reasons why the existence of an LU -factorization for a matrix A is\ninteresting is that if we need to solve several linear systems Ax = b corresponding to the\nsame matrix A, we can do this cheaply by solving the two triangular systems\n\nLw = b, and Ux = w.\n\nThere is a certain asymmetry in the LU -decomposition A = LU of an invertible matrix A.\nIndeed, the diagonal entries of L are all 1, but this is generally false for U . This asymmetry\ncan be eliminated as follows: if\n\nD = diag(u11, u22, . . . , unn)\n\nis the diagonal matrix consisting of the diagonal entries in U (the pivots), then we if let\nU ′ = D−1U , we can write\n\nA = LDU ′,\n\nwhere L is lower- triangular, U ′ is upper-triangular, all diagonal entries of both L and U ′\n\nare 1, and D is a diagonal matrix of pivots. Such a decomposition leads to the following\ndefinition.\n\nDefinition 8.2. We say that an invertible n×n matrix A has an LDU -factorization if it can\nbe written as A = LDU ′, where L is lower- triangular, U ′ is upper-triangular, all diagonal\nentries of both L and U ′ are 1, and D is a diagonal matrix.\n\nWe will see shortly than if A is real symmetric, then U ′ = L>.\n\nAs we will see a bit later, real symmetric positive definite matrices satisfy the condition of\nProposition 8.2. Therefore, linear systems involving real symmetric positive definite matrices\ncan be solved by Gaussian elimination without pivoting. Actually, it is possible to do better:\nthis is the Cholesky factorization.\n\nIf a square invertible matrix A has an LU -factorization, then it is possible to find L and U\nwhile performing Gaussian elimination. Recall that at Step k, we pick a pivot πk = a\n\n(k)\nik 6= 0\n\nin the portion consisting of the entries of index j ≥ k of the k-th column of the matrix Ak\nobtained so far, we swap rows i and k if necessary (the pivoting step), and then we zero the\nentries of index j = k + 1, . . . , n in column k. Schematically, we have the following steps:\n\n× × × × ×\n0 × × × ×\n0 × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n\n pivot\n=⇒\n\n\n× × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n0 × × × ×\n0 × × × ×\n\n elim\n=⇒\n\n\n× × × × ×\n0 × × × ×\n0 0 × × ×\n0 0 × × ×\n0 0 × × ×\n\n .\n\n\n\n8.4. LU -FACTORIZATION 235\n\nMore precisely, after permuting row k and row i (the pivoting step), if the entries in column\nk below row k are αk+1k, . . . , αnk, then we add −αjk/πk times row k to row j; this process\nis illustrated below: \n\na\n(k)\nkk\n\na\n(k)\nk+1k\n...\n\na\n(k)\nik\n...\n\na\n(k)\nnk\n\n\npivot\n=⇒\n\n\n\na\n(k)\nik\n\na\n(k)\nk+1k\n...\n\na\n(k)\nkk\n...\n\na\n(k)\nnk\n\n\n=\n\n\n\nπk\nαk+1k\n\n...\nαik\n...\nαnk\n\n\nelim\n=⇒\n\n\n\nπk\n0\n...\n0\n...\n0\n\n\n.\n\nThen if we write `jk = αjk/πk for j = k + 1, . . . , n, the kth column of L is\n\n0\n...\n0\n1\n\n`k+1k\n...\n`nk\n\n\n.\n\nObserve that the signs of the multipliers −αjk/πk have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n .\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the `ij:\n\nL−1 =\n\n\n1 0 0 · · · 0\n−`21 1 0 · · · 0\n−`31 −`32 1 · · · 0\n\n...\n...\n\n...\n. . . 0\n\n−`n1 −`n2 −`n3 · · · 1\n\n .\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = En−1 · · ·E1A,\n\n8.4. LU-FACTORIZATION 235\n\nMore precisely, after permuting row & and row 7 (the pivoting step), if the entries in column\nk below row k are Qp+iz,---,Qnk, then we add —a,;,/m, times row k to row j; this process\nis illustrated below:\n\n‘s i) (me) fe\nk+1k Qesik Ok+1k 0\npivot _ elim\na () al) Qik 0\n(k) (k) ray 0)\nOnk Onk mk\n\nThen if we write Cj, = ajx/m, for j =k+1,...,n, the kth column of L is\n\nObserve that the signs of the multipliers —a,,/a, have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\n1 0 0\nly, 1 0\nL=|n 2 1\n\nFo OOO\n\nCnt ln2 lng\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the ¢,;:\n\n1 0 0 0\n\nbo} 1 0 0\n\nLot =| —f31 —l32 1 0\n: : 0\n\n—lny —ln2 ng 1\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = E,_,--- FA,\n\n\n\n\n236 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nthen\n\nEk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · −`k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · −`nk 0 · · · 1\n\n\nand E−1\n\nk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · `k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · `nk 0 · · · 1\n\n\n,\n\nso the kth column of Ek is the kth column of L−1.\n\nHere is an example illustrating the method.\n\nExample 8.2. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: The first pivot is π1 = 1 in row 1, and we substract\nrow 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n L1 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 0 0 1\n\n .\n\nThe next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and add 0 times row\n2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L2 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n .\n\n\n\n8.5. PA = LU FACTORIZATION 237\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n = A.\n\nWe now show how to extend the above method to deal with pivoting efficiently. This is\nthe PA = LU factorization.\n\n8.5 PA = LU Factorization\n\nThe following easy proposition shows that, in principle, A can be premultiplied by some\npermutation matrix P , so that PA can be converted to upper-triangular form without using\nany pivoting. Permutations are discussed in some detail in Section 30.3, but for now we\njust need this definition. For the precise connection between the notion of permutation (as\ndiscussed in Section 30.3) and permutation matrices, see Problem 8.16.\n\nDefinition 8.3. A permutation matrix is a square matrix that has a single 1 in every row\nand every column and zeros everywhere else.\n\nIt is shown in Section 30.3 that every permutation matrix is a product of transposition\nmatrices (the P (i, k)s), and that P is invertible with inverse P>.\n\nProposition 8.4. Let A be an invertible n × n-matrix. There is some permutation matrix\nP so that (PA)(1 : k, 1 : k) is invertible for k = 1, . . . , n.\n\nProof. The case n = 1 is trivial, and so is the case n = 2 (we swap the rows if necessary). If\nn ≥ 3, we proceed by induction. Since A is invertible, its columns are linearly independent;\nin particular, its first n− 1 columns are also linearly independent. Delete the last column of\nA. Since the remaining n− 1 columns are linearly independent, there are also n− 1 linearly\nindependent rows in the corresponding n × (n − 1) matrix. Thus, there is a permutation\nof these n rows so that the (n − 1) × (n − 1) matrix consisting of the first n − 1 rows is\ninvertible. But then there is a corresponding permutation matrix P1, so that the first n− 1\nrows and columns of P1A form an invertible matrix A′. Applying the induction hypothesis\nto the (n− 1)× (n− 1) matrix A′, we see that there some permutation matrix P2 (leaving\nthe nth row fixed), so that (P2P1A)(1 : k, 1 : k) is invertible, for k = 1, . . . , n − 1. Since A\nis invertible in the first place and P1 and P2 are invertible, P1P2A is also invertible, and we\nare done.\n\nRemark: One can also prove Proposition 8.4 using a clever reordering of the Gaussian\nelimination steps suggested by Trefethen and Bau [174] (Lecture 21). Indeed, we know that\n\n\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices Pi and products of elementary matrices\nEi, so that\n\nAn = En−1Pn−1 · · ·E2P2E1P1A,\n\nwhere U = An is upper-triangular. For example, when n = 4, we have E3P3E2P2E1P1A = U .\nWe can define new matrices E ′1, E\n\n′\n2, E\n\n′\n3 which are still products of elementary matrices so\n\nthat we have\nE ′3E\n\n′\n2E\n′\n1P3P2P1A = U.\n\nIndeed, if we let E ′3 = E3, E ′2 = P3E2P\n−1\n3 , and E ′1 = P3P2E1P\n\n−1\n2 P−1\n\n3 , we easily verify that\neach E ′k is a product of elementary matrices and that\n\nE ′3E\n′\n2E\n′\n1P3P2P1 = E3(P3E2P\n\n−1\n3 )(P3P2E1P\n\n−1\n2 P−1\n\n3 )P3P2P1 = E3P3E2P2E1P1.\n\nIt can also be proven that E ′1, E\n′\n2, E\n\n′\n3 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nE ′k = Pn−1 · · ·Pk+1EkP\n\n−1\nk+1 · · ·P−1\n\nn−1,\n\nand we have\nE ′n−1 · · ·E ′1Pn−1 · · ·P1A = U,\n\nwhere each E ′j is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU -factorization yields the matrices L,U ,\nand P , such that PA = LU . To describe this new method, since the diagonal entries of L\nare 1s, it is convenient to write\n\nL = I + Λ.\n\nThen in assembling the matrix Λ while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of Λ (really Λk−1) that we make on the rows of A\n(really Ak) during a pivoting step involving row k and row i. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand Λ. Here is an example illustrating this method.\n\nExample 8.3. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: We initialize Λ0 = 0 and P0 = I4. The first pivot is\nπ1 = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 0 −2 0\n0 −2 −1 1\n0 −2 −1 −1\n\n Λ1 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices P; and products of elementary matrices\nE;, so that\nAn = n—1P n—1 ute Fo P, EK, P,A,\n\nwhere U = A, is upper-triangular. For example, when n = 4, we have £3 P3E)P,E,P,A = U.\nWe can define new matrices E}, £5, £3 which are still products of elementary matrices so\nthat we have\n\nEEE P3P,P,A = U.\nIndeed, if we let E, = E3, BE, = P3E,P;', and E} = P3P,E,P;'P;*, we easily verify that\neach Ej, is a product of elementary matrices and that\n\nEEE) P3P2P, = E3(P3E2P;')(P3P)E, Py! Ps')P3P)P, = F3P3E2P)E,P,.\n\nIt can also be proven that FE}, £5, E4 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nEy = Pyrite ++ Pei EePay Poh,\n\nand we have\n\nEly EL Py: RA=U,\nwhere each E* is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU-factorization yields the matrices L, U,\nand P, such that PA = LU. To describe this new method, since the diagonal entries of L\nare ls, it is convenient to write\n\nL=I+A.\n\nThen in assembling the matrix A while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of A (really A;,_1) that we make on the rows of A\n(really A;,) during a pivoting step involving row k and row 7. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand A. Here is an example illustrating this method.\n\nExample 8.3. Given\n\n11 1 £0\n\n1 1 —-1 0\nA= A, = 1-1 0 14?\n\n1-1 0 —-1\n\nwe have the following sequence of steps: We initialize Ag = 0 and Po = Jy. The first pivot is\nm7, = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\n11 1 +0 0000 100 0\n0 0 -2 0 100 0 0100\nAg 0 -2 -1 1 Ay 1000 B= 159 1 0\n0 —2 -1 -l 100 0 0001\n\n\n\n\n8.5. PA = LU FACTORIZATION 239\n\nThe next pivot is π2 = −2 in row 3, so we permute row 2 and 3; we also apply this permutation\nto Λ and P :\n\nA′3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n Λ′2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nNext we subtract row 2 from row 4 (and add 0 times row 2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ3 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = Λ3 + I =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n P = P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n\nand\n\nPA =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n .\n\nUsing the idea in the remark before the above example, we can prove the theorem below\nwhich shows the correctness of the algorithm for computing P,L and U using a simple\nadaptation of Gaussian elimination.\n\n\n\n240 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nWe are not aware of a detailed proof of Theorem 8.5 in the standard texts. Although\nGolub and Van Loan [80] state a version of this theorem as their Theorem 3.1.4, they say\nthat “The proof is a messy subscripting argument.” Meyer [124] also provides a sketch of\nproof (see the end of Section 3.10). In view of this situation, we offer a complete proof.\nIt does involve a lot of subscripts and superscripts, but in our opinion, it contains some\ntechniques that go far beyond symbol manipulation.\n\nTheorem 8.5. For every invertible n× n-matrix A, the following hold:\n\n(1) There is some permutation matrix P , some upper-triangular matrix U , and some unit\nlower-triangular matrix L, so that PA = LU (recall, Li i = 1 for i = 1, . . . , n). Fur-\nthermore, if P = I, then L and U are unique and they are produced as a result of\nGaussian elimination without pivoting.\n\n(2) If En−1 . . . E1A = U is the result of Gaussian elimination without pivoting, write as\n\nusual Ak = Ek−1 . . . E1A (with Ak = (a\n(k)\nij )), and let `ik = a\n\n(k)\nik /a\n\n(k)\nkk , with 1 ≤ k ≤ n− 1\n\nand k + 1 ≤ i ≤ n. Then\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n ,\n\nwhere the kth column of L is the kth column of E−1\nk , for k = 1, . . . , n− 1.\n\n(3) If En−1Pn−1 · · ·E1P1A = U is the result of Gaussian elimination with some pivoting,\nwrite Ak = Ek−1Pk−1 · · ·E1P1A, and define Ek\n\nj , with 1 ≤ j ≤ n− 1 and j ≤ k ≤ n− 1,\nsuch that, for j = 1, . . . , n− 2,\n\nEj\nj = Ej\n\nEk\nj = PkE\n\nk−1\nj Pk, for k = j + 1, . . . , n− 1,\n\nand\nEn−1\nn−1 = En−1.\n\nThen,\n\nEk\nj = PkPk−1 · · ·Pj+1EjPj+1 · · ·Pk−1Pk\n\nU = En−1\nn−1 · · ·En−1\n\n1 Pn−1 · · ·P1A,\n\nand if we set\n\nP = Pn−1 · · ·P1\n\nL = (En−1\n1 )−1 · · · (En−1\n\nn−1)−1,\n\n\n\n8.5. PA = LU FACTORIZATION 241\n\nthen\n\nPA = LU. (†1)\n\nFurthermore,\n\n(Ek\nj )−1 = I + Ekj , 1 ≤ j ≤ n− 1, j ≤ k ≤ n− 1,\n\nwhere Ekj is a lower triangular matrix of the form\n\nEkj =\n\n\n\n0 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 0 0 · · · 0\n\n0 · · · `\n(k)\nj+1j 0 · · · 0\n\n...\n...\n\n...\n...\n\n. . .\n...\n\n0 · · · `\n(k)\nnj 0 · · · 0\n\n\n,\n\nwe have\n\nEk\nj = I − Ekj ,\n\nand\n\nEkj = PkEk−1\nj , 1 ≤ j ≤ n− 2, j + 1 ≤ k ≤ n− 1,\n\nwhere Pk = I or else Pk = P (k, i) for some i such that k + 1 ≤ i ≤ n; if Pk 6= I, this\nmeans that (Ek\n\nj )−1 is obtained from (Ek−1\nj )−1 by permuting the entries on rows i and\n\nk in column j. Because the matrices (Ek\nj )−1 are all","extracted_metadata":{"xmp:ModifyDate":["2020-03-10T20:05:09Z"],"xmpMM:DocumentID":["uuid:ce41e11f-d564-4756-b319-b610f878631b"],"pdf:docinfo:creator_tool":["TeX"],"access_permission:can_print_degraded":["true"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"access_permission:fill_in_form":["true"],"xmp:CreateDate":["2019-08-02T23:26:13Z"],"Content-Length":["20812100"],"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"pdf:docinfo:created":["2019-08-02T23:26:13Z"],"pdf:docinfo:producer":["pdfTeX-1.40.17"],"access_permission:extract_content":["true"],"pdf:docinfo:modified":["2020-03-10T20:05:09Z"],"resourceName":["Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf"],"pdf:docinfo:trapped":["False"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"access_permission:can_modify":["true"],"xmp:CreatorTool":["TeX"],"pdf:hasXFA":["false"],"PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"],"dcterms:modified":["2020-03-10T20:05:09Z"],"Content-Type":["application/pdf"],"xmp:MetadataDate":["2020-03-10T20:05:09Z"],"access_permission:extract_for_accessibility":["true"],"access_permission:modify_annotations":["true"],"pdf:hasCollection":["false"],"access_permission:can_print":["true"],"pdf:producer":["pdfTeX-1.40.17"],"access_permission:assemble_document":["true"],"dcterms:created":["2019-08-02T23:26:13Z"],"pdf:PDFVersion":["1.5"],"pdf:encrypted":["false"],"dc:format":["application/pdf; version=1.5"],"pdf:hasMarkedContent":["false"],"pdf:docinfo:custom:PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"],"pdf:charsPerPage":["264","1","1263","2199","2131","2172","2059","1963","1915","2250","2056","1853","2008","2098","1054","10","22","23","1330","1775","1213","1058","1293","984","1576","1795","1042","1384","833","728","1496","1390","971","1656","1435","1217","1623","1155","1581","1434","1514","1191","20","0","659","1398","944","800","1316","1428","1296","2192","843","572","533","1323","1637","1545","1495","1940","2321","1417","1100","1421","1230","2050","1727","1956","618","952","1103","1573","1942","2105","2520","1646","1622","2210","1774","1636","1365","1283","1378","1691","1142","875","1677","1253","1163","1970","966","2065","1371","1692","1147","1112","1283","1196","929","1279","1080","1344","936","876","1158","41","1256","1172","775","1475","1080","1589","614","1132","1576","1159","2095","1710","1580","1247","1323","872","1335","1106","751","1060","1004","1307","1442","500","956","1539","547","1234","1239","779","1264","951","766","1556","1174","845","1477","983","1187","1773","537","119","1228","2084","1054","912","1109","1049","1101","1267","1445","1091","1337","1143","1455","642","1517","1909","1086","1505","1157","1061","1268","1366","1685","893","946","1377","1260","1457","1634","1491","437","22","1792","1695","1854","1639","1384","1458","1277","1167","1375","1241","1167","1699","1412","923","608","920","886","861","1146","1309","1511","1833","1614","1061","622","1298","1124","1198","1984","1373","1313","1243","850","1038","1276","954","1011","24","1187","149","1040","838","1218","970","1531","1036","1866","1517","771","1239","2200","1542","1736","1877","891","950","1922","1576","1015","1358"],"pdf:unmappedUnicodeCharsPerPage":["1","0","1","1","1","1","0","0","0","0","3","0","0","2","2","0","0","0","0","3","31","30","0","9","0","2","20","15","23","25","25","10","9","1","0","1","2","4","6","4","3","1","0","0","0","1","14","0","3","26","0","37","0","0","0","3","8","2","0","1","3","22","83","47","51","0","6","11","0","1","11","13","7","2","10","7","7","7","11","11","0","7","16","3","20","16","0","15","15","6","3","0","0","0","6","6","0","0","0","0","0","2","2","1","1","0","1","3","9","1","1","3","10","3","0","1","2","7","25","32","22","6","9","0","0","11","14","7","2","1","0","6","0","5","8","0","6","12","11","7","0","20","28","2","2","0","10","0","7","9","5","3","33","16","40","88","55","0","7","0","9","10","14","17","8","3","0","1","6","15","3","39","10","15","38","3","15","24","5","0","1","4","3","8","13","3","2","11","6","0","21","27","9","12","23","23","0","80","48","7","8","7","2","25","1","2","4","9","5","0","3","29","19","1","46","56","0","0","0","0","0","0","0","2","1","0","0","1","5","0","0","8","8","10","16","4","3","20","2","7"],"xmpTPg:NPages":["1962"],"pdf:hasXMP":["true"]},"metadata_field_count":39,"attempts":1,"timestamp":1754064949.0430613,"platform":"Linux","python_version":"3.13.5"},{"file_path":"test_documents/pdfs/Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf","file_size":35493143,"file_type":"pdf","category":"large","framework":"extractous","iteration":1,"extraction_time":29.008267164230347,"startup_time":null,"peak_memory_mb":472.0078125,"avg_memory_mb":487.2796875,"peak_cpu_percent":32.0,"avg_cpu_percent":6.4,"total_io_mb":null,"status":"success","character_count":500000,"word_count":81636,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":81636,"sentence_count":5185,"paragraph_count":3135,"avg_word_length":5.082439119996081,"avg_sentence_length":15.972999035679846,"extraction_completeness":1.0,"text_coherence":0.751147842056933,"noise_ratio":0.31442000000000003,"gibberish_ratio":0.021897810218978103,"flesch_reading_ease":40.31429394808873,"gunning_fog_index":15.859068527521277,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":false,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5587323279493441,"extracted_text":"\nThis electronic edition is for non-commercial purposes only.\n\nBayesian Data Analysis\n\nThird edition\n\n(with errors fixed as of 13 February 2020)\n\nAndrew Gelman\n\nColumbia University\n\nJohn B. Carlin\n\nUniversity of Melbourne\n\nHal S. Stern\n\nUniversity of California, Irvine\n\nDavid B. Dunson\n\nDuke University\n\nAki Vehtari\n\nAalto University\n\nDonald B. Rubin\n\nHarvard University\n\nCopyright c©1995–2020, by Andrew Gelman, John Carlin, Hal Stern,\n\nDonald Rubin, David Dunson, and Aki Vehtari\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nContents\n\nPreface xiii\n\nPart I: Fundamentals of Bayesian Inference 1\n\n1 Probability and inference 3\n1.1 The three steps of Bayesian data analysis 3\n1.2 General notation for statistical inference 4\n1.3 Bayesian inference 6\n1.4 Discrete examples: genetics and spell checking 8\n1.5 Probability as a measure of uncertainty 11\n1.6 Example: probabilities from football point spreads 13\n1.7 Example: calibration for record linkage 16\n1.8 Some useful results from probability theory 19\n1.9 Computation and software 22\n1.10 Bayesian inference in applied statistics 24\n1.11 Bibliographic note 25\n1.12 Exercises 27\n\n2 Single-parameter models 29\n2.1 Estimating a probability from binomial data 29\n2.2 Posterior as compromise between data and prior information 32\n2.3 Summarizing posterior inference 32\n2.4 Informative prior distributions 34\n2.5 Normal distribution with known variance 39\n2.6 Other standard single-parameter models 42\n2.7 Example: informative prior distribution for cancer rates 46\n2.8 Noninformative prior distributions 51\n2.9 Weakly informative prior distributions 55\n2.10 Bibliographic note 56\n2.11 Exercises 57\n\n3 Introduction to multiparameter models 63\n3.1 Averaging over ‘nuisance parameters’ 63\n3.2 Normal data with a noninformative prior distribution 64\n3.3 Normal data with a conjugate prior distribution 67\n3.4 Multinomial model for categorical data 69\n3.5 Multivariate normal model with known variance 70\n3.6 Multivariate normal with unknown mean and variance 72\n3.7 Example: analysis of a bioassay experiment 74\n3.8 Summary of elementary modeling and computation 78\n3.9 Bibliographic note 78\n3.10 Exercises 79\n\nvii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nviii CONTENTS\n\n4 Asymptotics and connections to non-Bayesian approaches 83\n4.1 Normal approximations to the posterior distribution 83\n4.2 Large-sample theory 87\n4.3 Counterexamples to the theorems 89\n4.4 Frequency evaluations of Bayesian inferences 91\n4.5 Bayesian interpretations of other statistical methods 92\n4.6 Bibliographic note 97\n4.7 Exercises 98\n\n5 Hierarchical models 101\n5.1 Constructing a parameterized prior distribution 102\n5.2 Exchangeability and hierarchical models 104\n5.3 Bayesian analysis of conjugate hierarchical models 108\n5.4 Normal model with exchangeable parameters 113\n5.5 Example: parallel experiments in eight schools 119\n5.6 Hierarchical modeling applied to a meta-analysis 124\n5.7 Weakly informative priors for variance parameters 128\n5.8 Bibliographic note 132\n5.9 Exercises 134\n\nPart II: Fundamentals of Bayesian Data Analysis 139\n\n6 Model checking 141\n6.1 The place of model checking in applied Bayesian statistics 141\n6.2 Do the inferences from the model make sense? 142\n6.3 Posterior predictive checking 143\n6.4 Graphical posterior predictive checks 153\n6.5 Model checking for the educational testing example 159\n6.6 Bibliographic note 161\n6.7 Exercises 163\n\n7 Evaluating, comparing, and expanding models 165\n7.1 Measures of predictive accuracy 166\n7.2 Information criteria and cross-validation 169\n7.3 Model comparison based on predictive performance 178\n7.4 Model comparison using Bayes factors 182\n7.5 Continuous model expansion 184\n7.6 Implicit assumptions and model expansion: an example 187\n7.7 Bibliographic note 192\n7.8 Exercises 194\n\n8 Modeling accounting for data collection 197\n8.1 Bayesian inference requires a model for data collection 197\n8.2 Data-collection models and ignorability 199\n8.3 Sample surveys 205\n8.4 Designed experiments 214\n8.5 Sensitivity and the role of randomization 218\n8.6 Observational studies 220\n8.7 Censoring and truncation 224\n8.8 Discussion 229\n8.9 Bibliographic note 229\n8.10 Exercises 230\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS ix\n\n9 Decision analysis 237\n9.1 Bayesian decision theory in different contexts 237\n9.2 Using regression predictions: survey incentives 239\n9.3 Multistage decision making: medical screening 245\n9.4 Hierarchical decision analysis for home radon 246\n9.5 Personal vs. institutional decision analysis 256\n9.6 Bibliographic note 257\n9.7 Exercises 257\n\nPart III: Advanced Computation 259\n\n10 Introduction to Bayesian computation 261\n10.1 Numerical integration 261\n10.2 Distributional approximations 262\n10.3 Direct simulation and rejection sampling 263\n10.4 Importance sampling 265\n10.5 How many simulation draws are needed? 267\n10.6 Computing environments 268\n10.7 Debugging Bayesian computing 270\n10.8 Bibliographic note 271\n10.9 Exercises 272\n\n11 Basics of Markov chain simulation 275\n11.1 Gibbs sampler 276\n11.2 Metropolis and Metropolis-Hastings algorithms 278\n11.3 Using Gibbs and Metropolis as building blocks 280\n11.4 Inference and assessing convergence 281\n11.5 Effective number of simulation draws 286\n11.6 Example: hierarchical normal model 288\n11.7 Bibliographic note 291\n11.8 Exercises 291\n\n12 Computationally efficient Markov chain simulation 293\n12.1 Efficient Gibbs samplers 293\n12.2 Efficient Metropolis jumping rules 295\n12.3 Further extensions to Gibbs and Metropolis 297\n12.4 Hamiltonian Monte Carlo 300\n12.5 Hamiltonian Monte Carlo for a hierarchical model 305\n12.6 Stan: developing a computing environment 307\n12.7 Bibliographic note 308\n12.8 Exercises 309\n\n13 Modal and distributional approximations 311\n13.1 Finding posterior modes 311\n13.2 Boundary-avoiding priors for modal summaries 313\n13.3 Normal and related mixture approximations 318\n13.4 Finding marginal posterior modes using EM 320\n13.5 Conditional and marginal posterior approximations 325\n13.6 Example: hierarchical normal model (continued) 326\n13.7 Variational inference 331\n13.8 Expectation propagation 338\n13.9 Other approximations 343\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nx CONTENTS\n\n13.10 Unknown normalizing factors 345\n\n13.11 Bibliographic note 348\n\n13.12 Exercises 349\n\nPart IV: Regression Models 351\n\n14 Introduction to regression models 353\n\n14.1 Conditional modeling 353\n\n14.2 Bayesian analysis of classical regression 354\n\n14.3 Regression for causal inference: incumbency and voting 358\n\n14.4 Goals of regression analysis 364\n\n14.5 Assembling the matrix of explanatory variables 365\n\n14.6 Regularization and dimension reduction 367\n\n14.7 Unequal variances and correlations 369\n\n14.8 Including numerical prior information 376\n\n14.9 Bibliographic note 378\n\n14.10 Exercises 378\n\n15 Hierarchical linear models 381\n\n15.1 Regression coefficients exchangeable in batches 382\n\n15.2 Example: forecasting U.S. presidential elections 383\n\n15.3 Interpreting a normal prior distribution as extra data 388\n\n15.4 Varying intercepts and slopes 390\n\n15.5 Computation: batching and transformation 392\n\n15.6 Analysis of variance and the batching of coefficients 395\n\n15.7 Hierarchical models for batches of variance components 398\n\n15.8 Bibliographic note 400\n\n15.9 Exercises 402\n\n16 Generalized linear models 405\n\n16.1 Standard generalized linear model likelihoods 406\n\n16.2 Working with generalized linear models 407\n\n16.3 Weakly informative priors for logistic regression 412\n\n16.4 Overdispersed Poisson regression for police stops 420\n\n16.5 State-level opinons from national polls 422\n\n16.6 Models for multivariate and multinomial responses 423\n\n16.7 Loglinear models for multivariate discrete data 428\n\n16.8 Bibliographic note 431\n\n16.9 Exercises 432\n\n17 Models for robust inference 435\n\n17.1 Aspects of robustness 435\n\n17.2 Overdispersed versions of standard models 437\n\n17.3 Posterior inference and computation 439\n\n17.4 Robust inference for the eight schools 441\n\n17.5 Robust regression using t-distributed errors 444\n\n17.6 Bibliographic note 445\n\n17.7 Exercises 446\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS xi\n\n18 Models for missing data 449\n18.1 Notation 449\n18.2 Multiple imputation 451\n18.3 Missing data in the multivariate normal and t models 454\n18.4 Example: multiple imputation for a series of polls 456\n18.5 Missing values with counted data 462\n18.6 Example: an opinion poll in Slovenia 463\n18.7 Bibliographic note 466\n18.8 Exercises 467\n\nPart V: Nonlinear and Nonparametric Models 469\n\n19 Parametric nonlinear models 471\n19.1 Example: serial dilution assay 471\n19.2 Example: population toxicokinetics 477\n19.3 Bibliographic note 485\n19.4 Exercises 486\n\n20 Basis function models 487\n20.1 Splines and weighted sums of basis functions 487\n20.2 Basis selection and shrinkage of coefficients 490\n20.3 Non-normal models and regression surfaces 494\n20.4 Bibliographic note 498\n20.5 Exercises 498\n\n21 Gaussian process models 501\n21.1 Gaussian process regression 501\n21.2 Example: birthdays and birthdates 505\n21.3 Latent Gaussian process models 510\n21.4 Functional data analysis 512\n21.5 Density estimation and regression 513\n21.6 Bibliographic note 516\n21.7 Exercises 516\n\n22 Finite mixture models 519\n22.1 Setting up and interpreting mixture models 519\n22.2 Example: reaction times and schizophrenia 524\n22.3 Label switching and posterior computation 533\n22.4 Unspecified number of mixture components 536\n22.5 Mixture models for classification and regression 539\n22.6 Bibliographic note 542\n22.7 Exercises 543\n\n23 Dirichlet process models 545\n23.1 Bayesian histograms 545\n23.2 Dirichlet process prior distributions 546\n23.3 Dirichlet process mixtures 549\n23.4 Beyond density estimation 557\n23.5 Hierarchical dependence 560\n23.6 Density regression 568\n23.7 Bibliographic note 571\n23.8 Exercises 573\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxii CONTENTS\n\nAppendixes 575\n\nA Standard probability distributions 577\nA.1 Continuous distributions 577\nA.2 Discrete distributions 585\nA.3 Bibliographic note 586\n\nB Outline of proofs of limit theorems 587\nB.1 Bibliographic note 590\n\nC Computation in R and Stan 591\nC.1 Getting started with R and Stan 591\nC.2 Fitting a hierarchical model in Stan 592\nC.3 Direct simulation, Gibbs, and Metropolis in R 596\nC.4 Programming Hamiltonian Monte Carlo in R 603\nC.5 Further comments on computation 607\nC.6 Bibliographic note 608\n\nReferences 609\n\nAuthor Index 643\n\nSubject Index 654\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPreface\n\nThis book is intended to have three roles and to serve three associated audiences: an\nintroductory text on Bayesian inference starting from first principles, a graduate text on\neffective current approaches to Bayesian modeling and computation in statistics and related\nfields, and a handbook of Bayesian methods in applied statistics for general users of and\nresearchers in applied statistics. Although introductory in its early sections, the book is\ndefinitely not elementary in the sense of a first text in statistics. The mathematics used\nin our book is basic probability and statistics, elementary calculus, and linear algebra. A\nreview of probability notation is given in Chapter 1 along with a more detailed list of topics\nassumed to have been studied. The practical orientation of the book means that the reader’s\nprevious experience in probability, statistics, and linear algebra should ideally have included\nstrong computational components.\n\nTo write an introductory text alone would leave many readers with only a taste of the\nconceptual elements but no guidance for venturing into genuine practical applications, be-\nyond those where Bayesian methods agree essentially with standard non-Bayesian analyses.\nOn the other hand, we feel it would be a mistake to present the advanced methods with-\nout first introducing the basic concepts from our data-analytic perspective. Furthermore,\ndue to the nature of applied statistics, a text on current Bayesian methodology would be\nincomplete without a variety of worked examples drawn from real applications. To avoid\ncluttering the main narrative, there are bibliographic notes at the end of each chapter and\nreferences at the end of the book.\n\nExamples of real statistical analyses appear throughout the book, and we hope thereby\nto give an applied flavor to the entire development. Indeed, given the conceptual simplicity\nof the Bayesian approach, it is only in the intricacy of specific applications that novelty\narises. Non-Bayesian approaches dominated statistical theory and practice for most of the\nlast century, but the last few decades have seen a re-emergence of Bayesian methods. This\nhas been driven more by the availability of new computational techniques than by what\nmany would see as the theoretical and logical advantages of Bayesian thinking.\n\nIn our treatment of Bayesian inference, we focus on practice rather than philosophy. We\ndemonstrate our attitudes via examples that have arisen in the applied research of ourselves\nand others. Chapter 1 presents our views on the foundations of probability as empirical\nand measurable; see in particular Sections 1.4–1.7.\n\nChanges for the third edition\n\nThe biggest change for this new edition is the addition of Chapters 20–23 on nonparametric\nmodeling. Other major changes include weakly informative priors in Chapters 2, 5, and\nelsewhere; boundary-avoiding priors in Chapter 13; an updated discussion of cross-validation\nand predictive information criteria in the new Chapter 7; improved convergence monitoring\nand effective sample size calculations for iterative simulation in Chapter 11; presentations of\nHamiltonian Monte Carlo, variational Bayes, and expectation propagation in Chapters 12\nand 13; and new and revised code in Appendix C. We have made other changes throughout.\n\nDuring the eighteen years since completing the first edition of Bayesian Data Analysis,\nwe have worked on dozens of interesting applications which, for reasons of space, we are not\nable to add to this new edition. Many of these examples appear in our book, Data Analysis\n\nxiii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxiv PREFACE\n\nUsing Regression and Hierarchical/Multilevel Models, as well as in our published research\narticles.\n\nWe have made some small corrections and updates for the second printing of the third\nedition.\n\nOnline information\n\nAdditional materials, including the data used in the examples, solutions to many of the\nend-of-chapter exercises, and any errors found after the book goes to press, are posted at\nhttp://www.stat.columbia.edu/∼gelman/book/. Feel free to send any comments to us\ndirectly.\n\nAcknowledgments\n\nWe thank many students, colleagues, and friends for comments and advice and also ac-\nknowledge the public funding that made much of this work possible.\n\nIn particular, we thank Stephen Ansolabehere, Adriano Azevedo, Jarrett Barber, Richard\nBarker, Tom Belin, Michael Betancourt, Suzette Blanchard, Rob Calver, Brad Carlin, Bob\nCarpenter, Alicia Carriquiry, Samantha Cook, Alex Damour, Victor De Oliveira, Vince\nDorie, David Draper, Greg Dropkin, John Emerson, Steve Fienberg, Alex Franks, Byron\nGajewski, Yuanjun Gao, Daniel Gianola, Yuri Goegebeur, David Hammill, Chad Heilig,\nMatt Hoffman, Chuanpu Hu, Zaiying Huang, Shane Jensen, Yoon-Sook Jeon, Pasi Jy-\nlanki, Jay Kadane, Jouni Kerman, Gary King, Lucien Le Cam, Yew Jin Lim, Rod Little,\nTom Little, Chuanhai Liu, Xuecheng Liu, Tomoki Matsumoto, Peter McCullagh, Mary\nSara McPeek, Xiao-Li Meng, Baback Moghaddam, Sergei Morozov, Jarad Niemi, Olivier\nNimeskern, Peter Norvig, Ali Rahimi, Thomas Richardson, Christian Robert, Scott Schmi-\ndler, Matt Schofield, Andrea Siegel, Raghav Singal, Sandip Sinharay, Elizabeth Stuart,\nDwight Sunada, Andrew Swift, Eric Tassone, Francis Tuerlinckx, Iven Van Mechelen,\nAmos Waterland, Rob Weiss, Lo-Hua Yuan, and Alan Zaslavsky. We especially thank\nJohn Boscardin, Jessica Hwang, Daniel Lee, Phillip Price, and Radford Neal.\n\nThis work was partially supported by research grants from the National Science Foun-\ndation, National Institutes of Health, Institute of Education Sciences, National Security\nAgency, Department of Energy, and Academy of Finland.\n\nMany of our examples have appeared in books and articles written by ourselves and\nothers, as we indicate in the bibliographic notes and exercises in the chapters where they\nappear.1\n\nFinally, we thank Caroline, Nancy, Hara, Amy, Ilona, and other family and friends for\ntheir love and support during the writing and revision of this book.\n\n1In particular: Figures 1.3–1.5 are adapted from the Journal of the American Statistical Association 90\n(1995), pp. 696, 702, and 703, and are reprinted with permission of the American Statistical Association.\nFigures 2.6 and 2.7 come from Gelman, A., and Nolan, D., Teaching Statistics: A Bag of Tricks, Oxford\nUniversity Press (1992), pp. 14 and 15, and are reprinted with permission of Oxford University Press.\nFigures 19.8–19.10 come from the Journal of the American Statistical Association 91 (1996), pp. 1407 and\n1409, and are reprinted with permission of the American Statistical Association. Table 19.1 comes from\nBerry, D., Statistics: A Bayesian Perspective, first edition, copyright 1996 Wadsworth, a part of Cengage\nLearning, Inc. Reproduced by permission. www.cengage.com/permissions. Figures 18.1 and 18.2 come\nfrom the Journal of the American Statistical Association 93 (1998), pp. 851 and 853, and are reprinted\nwith permission of the American Statistical Association. Figures 9.1–9.3 are adapted from the Journal of\n\nBusiness and Economic Statistics 21 (2003), pp. 219 and 223, and are reprinted with permission of the\nAmerican Statistical Association. We thank Jack Taylor for the data used to produce Figure 23.4.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart I: Fundamentals of Bayesian Inference\n\nBayesian inference is the process of fitting a probability model to a set of data and sum-\nmarizing the result by a probability distribution on the parameters of the model and on\nunobserved quantities such as predictions for new observations. In Chapters 1–3, we in-\ntroduce several useful families of models and illustrate their application in the analysis of\nrelatively simple data structures. Some mathematics arises in the analytical manipulation of\nthe probability distributions, notably in transformation and integration in multiparameter\nproblems. We differ somewhat from other introductions to Bayesian inference by emphasiz-\ning stochastic simulation, and the combination of mathematical analysis and simulation, as\ngeneral methods for summarizing distributions. Chapter 4 outlines the fundamental con-\nnections between Bayesian and other approaches to statistical inference. The early chapters\nfocus on simple examples to develop the basic ideas of Bayesian inference; examples in which\nthe Bayesian approach makes a practical difference relative to more traditional approaches\nbegin to appear in Chapter 3. The major practical advantages of the Bayesian approach\nappear in Chapter 5, where we introduce hierarchical models, which allow the parameters\nof a prior, or population, distribution themselves to be estimated from data.\n\n1\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 1\n\nProbability and inference\n\n1.1 The three steps of Bayesian data analysis\n\nThis book is concerned with practical methods for making inferences from data using prob-\nability models for quantities we observe and for quantities about which we wish to learn.\nThe essential characteristic of Bayesian methods is their explicit use of probability for quan-\ntifying uncertainty in inferences based on statistical data analysis.\n\nThe process of Bayesian data analysis can be idealized by dividing it into the following\nthree steps:\n\n1. Setting up a full probability model—a joint probability distribution for all observable and\nunobservable quantities in a problem. The model should be consistent with knowledge\nabout the underlying scientific problem and the data collection process.\n\n2. Conditioning on observed data: calculating and interpreting the appropriate posterior\ndistribution—the conditional probability distribution of the unobserved quantities of ul-\ntimate interest, given the observed data.\n\n3. Evaluating the fit of the model and the implications of the resulting posterior distribution:\nhow well does the model fit the data, are the substantive conclusions reasonable, and\nhow sensitive are the results to the modeling assumptions in step 1? In response, one\ncan alter or expand the model and repeat the three steps.\n\nGreat advances in all these areas have been made in the last forty years, and many\nof these are reviewed and used in examples throughout the book. Our treatment covers\nall three steps, the second involving computational methodology and the third a delicate\nbalance of technique and judgment, guided by the applied context of the problem. The first\nstep remains a major stumbling block for much Bayesian analysis: just where do our models\ncome from? How do we go about constructing appropriate probability specifications? We\nprovide some guidance on these issues and illustrate the importance of the third step in\nretrospectively evaluating the fit of models. Along with the improved techniques available\nfor computing conditional probability distributions in the second step, advances in carrying\nout the third step alleviate to some degree the need to assume correct model specification at\nthe first attempt. In particular, the much-feared dependence of conclusions on ‘subjective’\nprior distributions can be examined and explored.\n\nA primary motivation for Bayesian thinking is that it facilitates a common-sense in-\nterpretation of statistical conclusions. For instance, a Bayesian (probability) interval for\nan unknown quantity of interest can be directly regarded as having a high probability of\ncontaining the unknown quantity, in contrast to a frequentist (confidence) interval, which\nmay strictly be interpreted only in relation to a sequence of similar inferences that might\nbe made in repeated practice. Recently in applied statistics, increased emphasis has been\nplaced on interval estimation rather than hypothesis testing, and this provides a strong im-\npetus to the Bayesian viewpoint, since it seems likely that most users of standard confidence\nintervals give them a common-sense Bayesian interpretation. One of our aims in this book\n\n3\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4 1. PROBABILITY AND INFERENCE\n\nis to indicate the extent to which Bayesian interpretations of common simple statistical\nprocedures are justified.\n\nRather than argue the foundations of statistics—see the bibliographic note at the end\nof this chapter for references to foundational debates—we prefer to concentrate on the\npragmatic advantages of the Bayesian framework, whose flexibility and generality allow\nit to cope with complex problems. The central feature of Bayesian inference, the direct\nquantification of uncertainty, means that there is no impediment in principle to fitting\nmodels with many parameters and complicated multilayered probability specifications. In\npractice, the problems are ones of setting up and computing with such large models, and\na large part of this book focuses on recently developed and still developing techniques\nfor handling these modeling and computational challenges. The freedom to set up complex\nmodels arises in large part from the fact that the Bayesian paradigm provides a conceptually\nsimple method for coping with multiple parameters, as we discuss in detail from Chapter 3\non.\n\n1.2 General notation for statistical inference\n\nStatistical inference is concerned with drawing conclusions, from numerical data, about\nquantities that are not observed. For example, a clinical trial of a new cancer drug might\nbe designed to compare the five-year survival probability in a population given the new drug\nto that in a population under standard treatment. These survival probabilities refer to a\nlarge population of patients, and it is neither feasible nor ethically acceptable to experiment\non an entire population. Therefore inferences about the true probabilities and, in particular,\ntheir differences must be based on a sample of patients. In this example, even if it were\npossible to expose the entire population to one or the other treatment, it is never possible to\nexpose anyone to both treatments, and therefore statistical inference would still be needed to\nassess the causal inference—the comparison between the observed outcome in each patient\nand that patient’s unobserved outcome if exposed to the other treatment.\n\nWe distinguish between two kinds of estimands—unobserved quantities for which sta-\ntistical inferences are made—first, potentially observable quantities, such as future obser-\nvations of a process, or the outcome under the treatment not received in the clinical trial\nexample; and second, quantities that are not directly observable, that is, parameters that\ngovern the hypothetical process leading to the observed data (for example, regression coef-\nficients). The distinction between these two kinds of estimands is not always precise, but is\ngenerally useful as a way of understanding how a statistical model for a particular problem\nfits into the real world.\n\nParameters, data, and predictions\n\nAs general notation, we let θ denote unobservable vector quantities or population parameters\nof interest (such as the probabilities of survival under each treatment for randomly chosen\nmembers of the population in the example of the clinical trial), y denote the observed\ndata (such as the numbers of survivors and deaths in each treatment group), and ỹ denote\nunknown, but potentially observable, quantities (such as the outcomes of the patients under\nthe other treatment, or the outcome under each of the treatments for a new patient similar\nto those already in the trial). In general these symbols represent multivariate quantities.\nWe generally use Greek letters for parameters, lower case Roman letters for observed or\nobservable scalars and vectors (and sometimes matrices), and upper case Roman letters\nfor observed or observable matrices. When using matrix notation, we consider vectors as\ncolumn vectors throughout; for example, if u is a vector with n components, then uTu is a\nscalar and uuT an n× n matrix.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.2. GENERAL NOTATION FOR STATISTICAL INFERENCE 5\n\nObservational units and variables\n\nIn many statistical studies, data are gathered on each of a set of n objects or units, and\nwe can write the data as a vector, y = (y1, . . . , yn). In the clinical trial example, we might\nlabel yi as 1 if patient i is alive after five years or 0 if the patient dies. If several variables\nare measured on each unit, then each yi is actually a vector, and the entire dataset y is a\nmatrix (usually taken to have n rows). The y variables are called the ‘outcomes’ and are\nconsidered ‘random’ in the sense that, when making inferences, we wish to allow for the\npossibility that the observed values of the variables could have turned out otherwise, due\nto the sampling process and the natural variation of the population.\n\nExchangeability\n\nThe usual starting point of a statistical analysis is the (often tacit) assumption that the\nn values yi may be regarded as exchangeable, meaning that we express uncertainty as a\njoint probability density p(y1, . . . , yn) that is invariant to permutations of the indexes. A\nnonexchangeable model would be appropriate if information relevant to the outcome were\nconveyed in the unit indexes rather than by explanatory variables (see below). The idea of\nexchangeability is fundamental to statistics, and we return to it repeatedly throughout the\nbook.\n\nWe commonly model data from an exchangeable distribution as independently and iden-\ntically distributed (iid) given some unknown parameter vector θ with distribution p(θ). In\nthe clinical trial example, we might model the outcomes yi as iid, given θ, the unknown\nprobability of survival.\n\nExplanatory variables\n\nIt is common to have observations on each unit that we do not bother to model as random.\nIn the clinical trial example, such variables might include the age and previous health status\nof each patient in the study. We call this second class of variables explanatory variables, or\ncovariates, and label them x. We use X to denote the entire set of explanatory variables\nfor all n units; if there are k explanatory variables, then X is a matrix with n rows and k\ncolumns. Treating X as random, the notion of exchangeability can be extended to require\nthe distribution of the n values of (x, y)i to be unchanged by arbitrary permutations of\nthe indexes. It is always appropriate to assume an exchangeable model after incorporating\nsufficient relevant information inX that the indexes can be thought of as randomly assigned.\nIt follows from the assumption of exchangeability that the distribution of y, given x, is the\nsame for all units in the study in the sense that if two units have the same value of x, then\ntheir distributions of y are the same. Any of the explanatory variables x can be moved into\nthe y category if we wish to model them. We discuss the role of explanatory variables (also\ncalled predictors) in detail in Chapter 8 in the context of analyzing surveys, experiments,\nand observational studies, and in the later parts of this book in the context of regression\nmodels.\n\nHierarchical modeling\n\nIn Chapter 5 and subsequent chapters, we focus on hierarchical models (also called mul-\ntilevel models), which are used when information is available on several different levels of\nobservational units. In a hierarchical model, it is possible to speak of exchangeability at\neach level of units. For example, suppose two medical treatments are applied, in separate\nrandomized experiments, to patients in several different cities. Then, if no other information\nwere available, it would be reasonable to treat the patients within each city as exchangeable\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6 1. PROBABILITY AND INFERENCE\n\nand also treat the results from different cities as themselves exchangeable. In practice it\nwould make sense to include, as explanatory variables at the city level, whatever relevant\ninformation we have on each city, as well as the explanatory variables mentioned before at\nthe individual level, and then the conditional distributions given these explanatory variables\nwould be exchangeable.\n\n1.3 Bayesian inference\n\nBayesian statistical conclusions about a parameter θ, or unobserved data ỹ, are made in\nterms of probability statements. These probability statements are conditional on the ob-\nserved value of y, and in our notation are written simply as p(θ|y) or p(ỹ|y). We also\nimplicitly condition on the known values of any covariates, x. It is at the fundamental\nlevel of conditioning on observed data that Bayesian inference departs from the approach\nto statistical inference described in many textbooks, which is based on a retrospective eval-\nuation of the procedure used to estimate θ (or ỹ) over the distribution of possible y values\nconditional on the true unknown value of θ. Despite this difference, it will be seen that\nin many simple analyses, superficially similar conclusions result from the two approaches\nto statistical inference. However, analyses obtained using Bayesian methods can be easily\nextended to more complex problems. In this section, we present the basic mathematics and\nnotation of Bayesian inference, followed in the next section by an example from genetics.\n\nProbability notation\nSome comments on notation are needed at this point. First, p(·|·) denotes a conditional\nprobability density with the arguments determined by the context, and similarly for\np(·), which denotes a marginal distribution. We use the terms ‘distribution’ and\n‘density’ interchangeably. The same notation is used for continuous density functions\nand discrete probability mass functions. Different distributions in the same equation\n(or expression) will each be denoted by p(·), as in (1.1) below, for example. Although\nan abuse of standard mathematical notation, this method is compact and similar to\nthe standard practice of using p(·) for the probability of any discrete event, where\nthe sample space is also suppressed in the notation. Depending on context, to avoid\nconfusion, we may use the notation Pr(·) for the probability of an event; for example,\nPr(θ > 2) =\n\n∫\nθ>2\n\np(θ)dθ. When using a standard distribution, we use a notation based\non the name of the distribution; for example, if θ has a normal distribution with mean\nµ and variance σ2, we write θ ∼ N(µ, σ2) or p(θ) = N(θ|µ, σ2) or, to be even more\nexplicit, p(θ|µ, σ2) = N(θ|µ, σ2). Throughout, we use notation such as N(µ, σ2) for\nrandom variables and N(θ|µ, σ2) for density functions. Notation and formulas for\nseveral standard distributions appear in Appendix A.\nWe also occasionally use the following expressions for random variables θ: the coeffi-\ncient of variation is defined as sd(θ)/E(θ), the geometric mean is exp(E[log(θ)]), and\nthe geometric standard deviation is exp(sd[log(θ)]).\n\nBayes’ rule\n\nIn order to make probability statements about θ given y, we must begin with a model\nproviding a joint probability distribution for θ and y. The joint probability mass or density\nfunction can be written as a product of two densities that are often referred to as the prior\ndistribution p(θ) and the sampling distribution (or data distribution) p(y|θ), respectively:\n\np(θ, y) = p(θ)p(y|θ).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.3. BAYESIAN INFERENCE 7\n\nSimply conditioning on the known value of the data y, using the basic property of conditional\nprobability known as Bayes’ rule, yields the posterior density:\n\np(θ|y) = p(θ, y)\n\np(y)\n=\np(θ)p(y|θ)\np(y)\n\n, (1.1)\n\nwhere p(y) =\n∑\n\nθp(θ)p(y|θ), and the sum is over all possible values of θ (or p(y) =∫\np(θ)p(y|θ)dθ in the case of continuous θ). An equivalent form of (1.1) omits the fac-\n\ntor p(y), which does not depend on θ and, with fixed y, can thus be considered a constant,\nyielding the unnormalized posterior density, which is the right side of (1.2):\n\np(θ|y) ∝ p(θ)p(y|θ). (1.2)\n\nThe second term in this expression, p(y|θ), is taken here as a function of θ, not of y. These\nsimple formulas encapsulate the technical core of Bayesian inference: the primary task of\nany specific application is to develop the model p(θ, y) and perform the computations to\nsummarize p(θ|y) in appropriate ways.\n\nPrediction\n\nTo make inferences about an unknown observable, often called predictive inferences, we\nfollow a similar logic. Before the data y are considered, the distribution of the unknown\nbut observable y is\n\np(y) =\n\n∫\np(y, θ)dθ =\n\n∫\np(θ)p(y|θ)dθ. (1.3)\n\nThis is often called the marginal distribution of y, but a more informative name is the prior\npredictive distribution: prior because it is not conditional on a previous observation of the\nprocess, and predictive because it is the distribution for a quantity that is observable.\n\nAfter the data y have been observed, we can predict an unknown observable, ỹ, from\nthe same process. For example, y = (y1, . . . , yn) may be the vector of recorded weights of\nan object weighed n times on a scale, θ = (µ, σ2) may be the unknown true weight of the\nobject and the measurement variance of the scale, and ỹ may be the yet to be recorded\nweight of the object in a planned new weighing. The distribution of ỹ is called the posterior\npredictive distribution, posterior because it is conditional on the observed y and predictive\nbecause it is a prediction for an observable ỹ:\n\np(ỹ|y) =\n\n∫\np(ỹ, θ|y)dθ\n\n=\n\n∫\np(ỹ|θ, y)p(θ|y)dθ\n\n=\n\n∫\np(ỹ|θ)p(θ|y)dθ. (1.4)\n\nThe second and third lines display the posterior predictive distribution as an average of\nconditional predictions over the posterior distribution of θ. The last step follows from the\nassumed conditional independence of y and ỹ given θ.\n\nLikelihood\n\nUsing Bayes’ rule with a chosen probability model means that the data y affect the posterior\ninference (1.2) only through p(y|θ), which, when regarded as a function of θ, for fixed y, is\ncalled the likelihood function. In this way Bayesian inference is obeying what is sometimes\ncalled the likelihood principle.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n8 1. PROBABILITY AND INFERENCE\n\nThe likelihood principle is reasonable, but only within the framework of the model or\nfamily of models adopted for a particular analysis. In practice, one can rarely be confident\nthat the chosen model is correct. We shall see in Chapter 6 that sampling distributions\n(imagining repeated realizations of our data) can play an important role in checking model\nassumptions. In fact, our view of an applied Bayesian statistician is one who is willing to\napply Bayes’ rule under a variety of possible models.\n\nLikelihood and odds ratios\n\nThe ratio of the posterior density p(θ|y) evaluated at the points θ1 and θ2 under a given\nmodel is called the posterior odds for θ1 compared to θ2. The most familiar application of\nthis concept is with discrete parameters, with θ2 taken to be the complement of θ1. Odds\nprovide an alternative representation of probabilities and have the attractive property that\nBayes’ rule takes a particularly simple form when expressed in terms of them:\n\np(θ1|y)\np(θ2|y)\n\n=\np(θ1)p(y|θ1)/p(y)\np(θ2)p(y|θ2)/p(y)\n\n=\np(θ1)\n\np(θ2)\n\np(y|θ1)\np(y|θ2)\n\n. (1.5)\n\nIn words, the posterior odds are equal to the prior odds multiplied by the likelihood ratio,\np(y|θ1)/p(y|θ2).\n\n1.4 Discrete examples: genetics and spell checking\n\nWe next demonstrate Bayes’ theorem with two examples in which the immediate goal is\ninference about a particular discrete quantity rather than with the estimation of a parameter\nthat describes an entire population. These discrete examples allow us to see the prior,\nlikelihood, and posterior probabilities directly.\n\nInference about a genetic status\n\nHuman males have one X-chromosome and one Y-chromosome, whereas females have two\nX-chromosomes, each chromosome being inherited from one parent. Hemophilia is a disease\nthat exhibits X-chromosome-linked recessive inheritance, meaning that a male who inherits\nthe gene that causes the disease on the X-chromosome is affected, whereas a female carrying\nthe gene on only one of her two X-chromosomes is not affected. The disease is generally fatal\nfor women who inherit two such genes, and this is rare, since the frequency of occurrence\nof the gene is low in human populations.\n\nPrior distribution. Consider a woman who has an affected brother, which implies that her\nmother must be a carrier of the hemophilia gene with one ‘good’ and one ‘bad’ hemophilia\ngene. We are also told that her father is not affected; thus the woman herself has a fifty-fifty\nchance of having the gene. The unknown quantity of interest, the state of the woman, has\njust two values: the woman is either a carrier of the gene (θ = 1) or not (θ = 0). Based on\nthe information provided thus far, the prior distribution for the unknown θ can be expressed\nsimply as Pr(θ = 1) = Pr(θ = 0) = 1\n\n2 .\n\nData model and likelihood. The data used to update the prior information consist of the\naffection status of the woman’s sons. Suppose she has two sons, neither of whom is affected.\nLet yi=1 or 0 denote an affected or unaffected son, respectively. The outcomes of the two\nsons are exchangeable and, conditional on the unknown θ, are independent; we assume the\nsons are not identical twins. The two items of independent data generate the following\nlikelihood function:\n\nPr(y1=0, y2 = 0 | θ=1) = (0.5)(0.5) = 0.25\n\nPr(y1=0, y2 = 0 | θ=0) = (1)(1) = 1.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.4. DISCRETE EXAMPLES: GENETICS AND SPELL CHECKING 9\n\nThese expressions follow from the fact that if the woman is a carrier, then each of her sons\nwill have a 50% chance of inheriting the gene and so being affected, whereas if she is not a\ncarrier then there is a probability close to 1 that a son of hers will be unaffected. (In fact,\nthere is a nonzero probability of being affected even if the mother is not a carrier, but this\nrisk—the mutation rate—is small and can be ignored for this example.)\n\nPosterior distribution. Bayes’ rule can now be used to combine the information in the\ndata with the prior probability; in particular, interest is likely to focus on the posterior\nprobability that the woman is a carrier. Using y to denote the joint data (y1, y2), this is\nsimply\n\nPr(θ = 1|y) =\np(y|θ = 1)Pr(θ = 1)\n\np(y|θ = 1)Pr(θ = 1) + p(y|θ = 0)Pr(θ = 0)\n\n=\n(0.25)(0.5)\n\n(0.25)(0.5) + (1.0)(0.5)\n=\n\n0.125\n\n0.625\n= 0.20.\n\nIntuitively it is clear that if a woman has unaffected children, it is less probable that she is\na carrier, and Bayes’ rule provides a formal mechanism for determining the extent of the\ncorrection. The results can also be described in terms of prior and posterior odds. The\nprior odds of the woman being a carrier are 0.5/0.5 = 1. The likelihood ratio based on\nthe information about her two unaffected sons is 0.25/1 = 0.25, so the posterior odds are\n1 ·0.25 = 0.25. Converting back to a probability, we obtain 0.25/(1+0.25) = 0.2, as before.\n\nAdding more data. A key aspect of Bayesian analysis is the ease with which sequential\nanalyses can be performed. For example, suppose that the woman has a third son, who\nis also unaffected. The entire calculation does not need to be redone; rather we use the\nprevious posterior distribution as the new prior distribution, to obtain:\n\nPr(θ = 1|y1, y2, y3) =\n(0.5)(0.20)\n\n(0.5)(0.20) + (1)(0.8)\n= 0.111.\n\nAlternatively, if we suppose that the third son is affected, it is easy to check that the\nposterior probability of the woman being a carrier becomes 1 (again ignoring the possibility\nof a mutation).\n\nSpelling correction\n\nClassification of words is a problem of managing uncertainty. For example, suppose someone\ntypes ‘radom.’ How should that be read? It could be a misspelling or mistyping of ‘random’\nor ‘radon’ or some other alternative, or it could be the intentional typing of ‘radom’ (as\nin its first use in this paragraph). What is the probability that ‘radom’ actually means\nrandom? If we label y as the data and θ as the word that the person was intending to type,\nthen\n\nPr(θ | y=‘radom’) ∝ p(θ) Pr(y=‘radom’ | θ). (1.6)\n\nThis product is the unnormalized posterior density. In this case, if for simplicity we consider\nonly three possibilities for the intended word, θ (random, radon, or radom), we can compute\nthe posterior probability of interest by first computing the unnormalized density for all three\nvalues of theta and then normalizing:\n\np(random|‘radom’) =\np(θ1)p(‘radom’|θ1)∑3\nj=1 p(θj)p(‘radom’|θj)\n\n,\n\nwhere θ1=random, θ2=radon, and θ3=radom. The prior probabilities p(θj) can most simply\ncome from frequencies of these words in some large database, ideally one that is adapted\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n10 1. PROBABILITY AND INFERENCE\n\nto the problem at hand (for example, a database of recent student emails if the word in\nquestion is appearing in such a document). The likelihoods p(y|θj) can come from some\nmodeling of spelling and typing errors, perhaps fit using some study in which people were\nfollowed up after writing emails to identify any questionable words.\n\nPrior distribution. Without any other context, it makes sense to assign the prior proba-\nbilities p(θj) based on the relative frequencies of these three words in some databases. Here\nare probabilities supplied by researchers at Google:\n\nθ p(θ)\nrandom 7.60× 10−5\n\nradon 6.05× 10−6\n\nradom 3.12× 10−7\n\nSince we are considering only these possibilities, we could renormalize the three numbers to\nsum to 1 (p(random) = 760\n\n760+60.5+3.12 , etc.) but there is no need, as the adjustment would\nmerely be absorbed into the proportionality constant in (1.6).\n\nReturning to the table above, we were surprised to see the probability of ‘radom’ in the\ncorpus being as high as it was. We looked up the word in Wikipedia and found that it is a\nmedium-sized city: home to ‘the largest and best-attended air show in Poland . . . also the\npopular unofficial name for a semiautomatic 9 mm Para pistol of Polish design . . . ’ For\nthe documents that we encounter, the relative probability of ‘radom’ seems much too high.\nIf the probabilities above do not seem appropriate for our application, this implies that we\nhave prior information or beliefs that have not yet been included in the model. We shall\nreturn to this point after first working out the model’s implications for this example.\n\nLikelihood. Here are some conditional probabilities from Google’s model of spelling and\ntyping errors:\n\nθ p(‘radom’|θ)\nrandom 0.00193\nradon 0.000143\nradom 0.975\n\nWe emphasize that this likelihood function is not a probability distribution. Rather, it is a\nset of conditional probabilities of a particular outcome (‘radom’) from three different proba-\nbility distributions, corresponding to three different possibilities for the unknown parameter\nθ.\n\nThese particular values look reasonable enough—a 97% chance that this particular five-\nletter word will be typed correctly, a 0.2% chance of obtaining this character string by\nmistakenly dropping a letter from ‘random,’ and a much lower chance of obtaining it by\nmistyping the final letter of ‘radon.’ We have no strong intuition about these probabilities\nand will trust the Google engineers here.\n\nPosterior distribution. We multiply the prior probability and the likelihood to get joint\nprobabilities and then renormalize to get posterior probabilities:\n\nθ p(θ)p(‘radom’|θ) p(θ|‘radom’)\nrandom 1.47× 10−7 0.325\nradon 8.65× 10−10 0.002\nradom 3.04× 10−7 0.673\n\nThus, conditional on the model, the typed word ‘radom’ is about twice as likely to be correct\nas to be a typographical error for ‘random,’ and it is very unlikely to be a mistaken instance\nof ‘radon.’ A fuller analysis would include possibilities beyond these three words, but the\nbasic idea is the same.\n\nDecision making, model checking, and model improvement. We can envision two directions\nto go from here. The first approach is to accept the two-thirds probability that the word\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.5. PROBABILITY AS A MEASURE OF UNCERTAINTY 11\n\nwas typed correctly or even to simply declare ‘radom’ as correct on first pass. The second\noption would be to question this probability by saying, for example, that ‘radom’ looks like\na typo and that the estimated probability of it being correct seems much too high.\n\nWhen we dispute the claims of a posterior distribution, we are saying that the model\ndoes not fit the data or that we have additional prior information not included in the model\nso far. In this case, we are only examining one word so lack of fit is not the issue; thus a\ndispute over the posterior must correspond to a claim of additional information, either in\nthe prior or the likelihood.\n\nFor this problem we have no particular grounds on which to criticize the likelihood. The\nprior probabilities, on the other hand, are highly context dependent. The word ‘random’ is\nof course highly frequent in our own writing on statistics, ‘radon’ occurs occasionally (see\nSection 9.4), while ‘radom’ was entirely new to us. Our surprise at the high probability of\n‘radom’ represents additional knowledge relevant to our particular problem.\n\nThe model can be elaborated most immediately by including contextual information in\nthe prior probabilities. For example, if the document under study is a statistics book, then\nit becomes more likely that the person intended to type ‘random.’ If we label x as the\ncontextual information used by the model, the Bayesian calculation then becomes,\n\np(θ|x, y) ∝ p(θ|x)p(y|θ, x).\n\nTo first approximation, we can simplify that last term to p(y|θ), so that the probability\nof any particular error (that is, the probability of typing a particular string y given the\nintended word θ) does not depend on context. This is not a perfect assumption but could\nreduce the burden of modeling and computation.\n\nThe practical challenges in Bayesian inference involve setting up models to estimate all\nthese probabilities from data. At that point, as shown above, Bayes’ rule can be easily\napplied to determine the implications of the model for the problem at hand.\n\n1.5 Probability as a measure of uncertainty\n\nWe have already used concepts such as probability density, and indeed we assume that the\nreader has a fair degree of familiarity with basic probability theory (although in Section\n1.8 we provide a brief technical review of some probability calculations that often arise\nin Bayesian analysis). But since the uses of probability within a Bayesian framework are\nmuch broader than within non-Bayesian statistics, it is important to consider at least briefly\nthe foundations of the concept of probability before considering more detailed statistical\nexamples. We take for granted a common understanding on the part of the reader of the\nmathematical definition of probability: that probabilities are numerical quantities, defined\non a set of ‘outcomes,’ that are nonnegative, additive over mutually exclusive outcomes,\nand sum to 1 over all possible mutually exclusive outcomes.\n\nIn Bayesian statistics, probability is used as the fundamental measure or yardstick of\nuncertainty. Within this paradigm, it is equally legitimate to discuss the probability of\n‘rain tomorrow’ or of a Brazilian victory in the soccer World Cup as it is to discuss the\nprobability that a coin toss will land heads. Hence, it becomes as natural to consider the\nprobability that an unknown estimand lies in a particular range of values as it is to consider\nthe probability that the mean of a random sample of 10 items from a known fixed population\nof size 100 will lie in a certain range. The first of these two probabilities is of more interest\nafter data have been acquired whereas the second is more relevant beforehand. Bayesian\nmethods enable statements to be made about the partial knowledge available (based on\ndata) concerning some situation or ‘state of nature’ (unobservable or as yet unobserved) in\na systematic way, using probability as the yardstick. The guiding principle is that the state\nof knowledge about anything unknown is described by a probability distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n12 1. PROBABILITY AND INFERENCE\n\nWhat is meant by a numerical measure of uncertainty? For example, the probability of\n‘heads’ in a coin toss is widely agreed to be 1\n\n2 . Why is this so? Two justifications seem to\nbe commonly given:\n\n1. Symmetry or exchangeability argument:\n\nprobability =\nnumber of favorable cases\n\nnumber of possibilities\n,\n\nassuming equally likely possibilities. For a coin toss this is really a physical argument,\nbased on assumptions about the forces at work in determining the manner in which the\ncoin will fall, as well as the initial physical conditions of the toss.\n\n2. Frequency argument: probability = relative frequency obtained in a long sequence of\ntosses, assumed to be performed in an identical manner, physically independently of\neach other.\n\nBoth the above arguments are in a sense subjective, in that they require judgments about\nthe nature of the coin and the tossing procedure, and both involve semantic arguments\nabout the meaning of equally likely events, identical measurements, and independence.\nThe frequency argument may be perceived to have certain special difficulties, in that it\ninvolves the hypothetical notion of a long sequence of identical tosses. If taken strictly, this\npoint of view does not allow a statement of probability for a single coin toss that does not\nhappen to be embedded, at least conceptually, in a long sequence of identical events.\n\nThe following examples illustrate how probability judgments can be increasingly subjec-\ntive. First, consider the following modified coin experiment. Suppose that a particular coin\nis stated to be either double-headed or double-tailed, with no further information provided.\nCan one still talk of the probability of heads? It seems clear that in common parlance one\ncertainly can. It is less clear, perhaps, how to assess this new probability, but many would\nagree on the same value of 1\n\n2 , perhaps based on the exchangeability of the labels ‘heads’\nand ‘tails.’\n\nNow consider some further examples. Suppose Colombia plays Brazil in soccer to-\nmorrow: what is the probability of Colombia winning? What is the probability of rain\ntomorrow? What is the probability that Colombia wins, if it rains tomorrow? What is\nthe probability that a specified rocket launch will fail? Although each of these questions\nseems reasonable in a common-sense way, it is difficult to contemplate strong frequency\ninterpretations for the probabilities being referenced. Frequency interpretations can usually\nbe constructed, however, and this is an extremely useful tool in statistics. For example, one\ncan consider the future rocket launch as a sample from the population of potential launches\nof the same type, and look at the frequency of past launches that have failed (see the bib-\nliographic note at the end of this chapter for more details on this example). Doing this\nsort of thing scientifically means creating a probability model (or, at least, a ‘reference set’\nof comparable events), and this brings us back to a situation analogous to the simple coin\ntoss, where we must consider the outcomes in question as exchangeable and thus equally\nlikely.\n\nWhy is probability a reasonable way of quantifying uncertainty? The following reasons\nare often advanced.\n\n1. By analogy: physical randomness induces uncertainty, so it seems reasonable to describe\nuncertainty in the language of random events. Common speech uses many terms such\nas ‘probably’ and ‘unlikely,’ and it appears consistent with such usage to extend a more\nformal probability calculus to problems of scientific inference.\n\n2. Axiomatic or normative approach: related to decision theory, this approach places all sta-\ntistical inference in the context of decision-making with gains and losses. Then reasonable\naxioms (ordering, transitivity, and so on) imply that uncertainty must be represented in\nterms of probability. We view this normative rationale as suggestive but not compelling.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 13\n\n3. Coherence of bets. Define the probability p attached (by you) to an event E as the\nfraction (p ∈ [0, 1]) at which you would exchange (that is, bet) $p for a return of $1 if E\noccurs. That is, if E occurs, you gain $(1− p); if the complement of E occurs, you lose\n$p. For example:\n\n• Coin toss: thinking of the coin toss as a fair bet suggests even odds corresponding to\np = 1\n\n2 .\n\n• Odds for a game: if you are willing to bet on team A to win a game at 10 to 1 odds\nagainst team B (that is, you bet 1 to win 10), your ‘probability’ for team A winning\nis at least 1\n\n11 .\n\nThe principle of coherence states that your assignment of probabilities to all possible\nevents should be such that it is not possible to make a definite gain by betting with you.\nIt can be proved that probabilities constructed under this principle must satisfy the basic\naxioms of probability theory.\nThe betting rationale has some fundamental difficulties:\n\n• Exact odds are required, on which you would be willing to bet in either direction, for\nall events. How can you assign exact odds if you are not sure?\n\n• If a person is willing to bet with you, and has information you do not, it might not\nbe wise for you to take the bet. In practice, probability is an incomplete (necessary\nbut not sufficient) guide to betting.\n\nAll of these considerations suggest that probabilities may be a reasonable approach to\nsummarizing uncertainty in applied statistics, but the ultimate proof is in the success of the\napplications. The remaining chapters of this book demonstrate that probability provides a\nrich and flexible framework for handling uncertainty in statistical applications.\n\nSubjectivity and objectivity\n\nAll statistical methods that use probability are subjective in the sense of relying on math-\nematical idealizations of the world. Bayesian methods are sometimes said to be especially\nsubjective because of their reliance on a prior distribution, but in most problems, scientific\njudgment is necessary to specify both the ‘likelihood’ and the ‘prior’ parts of the model. For\nexample, linear regression models are generally at least as suspect as any prior distribution\nthat might be assumed about the regression parameters. A general principle is at work\nhere: whenever there is replication, in the sense of many exchangeable units observed, there\nis scope for estimating features of a probability distribution from data and thus making the\nanalysis more ‘objective.’ If an experiment as a whole is replicated several times, then the\nparameters of the prior distribution can themselves be estimated from data, as discussed in\nChapter 5. In any case, however, certain elements requiring scientific judgment will remain,\nnotably the choice of data included in the analysis, the parametric forms assumed for the\ndistributions, and the ways in which the model is checked.\n\n1.6 Example: probabilities from football point spreads\n\nAs an example of how probabilities might be assigned using empirical data and plausible\nsubstantive assumptions, we consider methods of estimating the probabilities of certain\noutcomes in professional (American) football games. This is an example only of probability\nassignment, not of Bayesian inference. A number of approaches to assigning probabilities\nfor football game outcomes are illustrated: making subjective assessments, using empirical\nprobabilities based on observed data, and constructing a parametric probability model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n14 1. PROBABILITY AND INFERENCE\n\nFigure 1.1 Scatterplot of actual outcome vs. point spread for each of 672 professional football games.\nThe x and y coordinates are jittered by adding uniform random numbers to each point’s coordinates\n(between −0.1 and 0.1 for the x coordinate; between −0.2 and 0.2 for the y coordinate) in order to\ndisplay multiple values but preserve the discrete-valued nature of each.\n\nFootball point spreads and game outcomes\n\nFootball experts provide a point spread for every football game as a measure of the difference\nin ability between the two teams. For example, team A might be a 3.5-point favorite to\ndefeat team B. The implication of this point spread is that the proposition that team A,\nthe favorite, defeats team B, the underdog, by 4 or more points is considered a fair bet; in\nother words, the probability that A wins by more than 3.5 points is 1\n\n2 . If the point spread\nis an integer, then the implication is that team A is as likely to win by more points than\nthe point spread as it is to win by fewer points than the point spread (or to lose); there is\npositive probability that A will win by exactly the point spread, in which case neither side\nis paid off. The assignment of point spreads is itself an interesting exercise in probabilistic\nreasoning; one interpretation is that the point spread is the median of the distribution of\nthe gambling population’s beliefs about the possible outcomes of the game. For the rest\nof this example, we treat point spreads as given and do not worry about how they were\nderived.\n\nThe point spread and actual game outcome for 672 professional football games played\nduring the 1981, 1983, and 1984 seasons are graphed in Figure 1.1. (Much of the 1982\nseason was canceled due to a labor dispute.) Each point in the scatterplot displays the\npoint spread, x, and the actual outcome (favorite’s score minus underdog’s score), y. (In\ngames with a point spread of zero, the labels ‘favorite’ and ‘underdog’ were assigned at\nrandom.) A small random jitter is added to the x and y coordinate of each point on the\ngraph so that multiple points do not fall exactly on top of each other.\n\nAssigning probabilities based on observed frequencies\n\nIt is of interest to assign probabilities to particular events: Pr(favorite wins), Pr(favorite\nwins | point spread is 3.5 points), Pr(favorite wins by more than the point spread), Pr(favorite\nwins by more than the point spread | point spread is 3.5 points), and so forth. We might\nreport a subjective probability based on informal experience gathered by reading the news-\npaper and watching football games. The probability that the favored team wins a game\nshould certainly be greater than 0.5, perhaps between 0.6 and 0.75? More complex events\nrequire more intuition or knowledge on our part. A more systematic approach is to assign\nprobabilities based on the data in Figure 1.1. Counting a tied game as one-half win and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 15\n\nFigure 1.2 (a) Scatterplot of (actual outcome − point spread) vs. point spread for each of 672\nprofessional football games (with uniform random jitter added to x and y coordinates). (b) His-\ntogram of the differences between the game outcome and the point spread, with the N(0, 142) density\nsuperimposed.\n\none-half loss, and ignoring games for which the point spread is zero (and thus there is no\nfavorite), we obtain empirical estimates such as:\n\n• Pr(favorite wins) = 410.5\n655 = 0.63\n\n• Pr(favorite wins |x = 3.5) = 36\n59 = 0.61\n\n• Pr(favorite wins by more than the point spread) = 308\n655 = 0.47\n\n• Pr(favorite wins by more than the point spread |x = 3.5) = 32\n59 = 0.54.\n\nThese empirical probability assignments all seem sensible in that they match the intu-\nition of knowledgeable football fans. However, such probability assignments are problematic\nfor events with few directly relevant data points. For example, 8.5-point favorites won five\nout of five times during this three-year period, whereas 9-point favorites won thirteen out of\ntwenty times. However, we realistically expect the probability of winning to be greater for\na 9-point favorite than for an 8.5-point favorite. The small sample size with point spread\n8.5 leads to imprecise probability assignments. We consider an alternative method using a\nparametric model.\n\nA parametric model for the difference between outcome and point spread\n\nFigure 1.2a displays the differences y−x between the observed game outcome and the point\nspread, plotted versus the point spread, for the games in the football dataset. (Once again,\nrandom jitter was added to both coordinates.) This plot suggests that it may be roughly\nreasonable to model the distribution of y − x as independent of x. (See Exercise 6.10.)\nFigure 1.2b is a histogram of the differences y − x for all the football games, with a fitted\nnormal density superimposed. This plot suggests that it may be reasonable to approximate\nthe marginal distribution of the random variable d = y − x by a normal distribution. The\nsample mean of the 672 values of d is 0.07, and the sample standard deviation is 13.86,\nsuggesting that the results of football games are approximately normal with mean equal to\nthe point spread and standard deviation nearly 14 points (two converted touchdowns). For\nthe remainder of the discussion we take the distribution of d to be independent of x and\nnormal with mean zero and standard deviation 14 for each x; that is,\n\nd|x ∼ N(0, 142),\n\nas displayed in Figure 1.2b. The assigned probability model is not perfect: it does not fit\nthe data exactly, and, as is often the case with real data, neither football scores nor point\nspreads are continuous-valued quantities.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n16 1. PROBABILITY AND INFERENCE\n\nAssigning probabilities using the parametric model\n\nNevertheless, the model provides a convenient approximation that can be used to assign\nprobabilities to events. If d has a normal distribution with mean zero and is independent of\nthe point spread, then the probability that the favorite wins by more than the point spread\nis 1\n\n2 , conditional on any value of the point spread, and therefore unconditionally as well.\nDenoting probabilities obtained by the normal model as Prnorm, the probability that an\nx-point favorite wins the game can be computed, assuming the normal model, as follows:\n\nPrnorm(y>0 |x) = Prnorm(d>−x |x) = 1− Φ\n(\n− x\n\n14\n\n)\n,\n\nwhere Φ is the standard normal cumulative distribution function. For example,\n\n• Prnorm(favorite wins |x = 3.5) = 0.60\n\n• Prnorm(favorite wins |x = 8.5) = 0.73\n\n• Prnorm(favorite wins |x = 9.0) = 0.74.\n\nThe probability for a 3.5-point favorite agrees with the empirical value given earlier, whereas\nthe probabilities for 8.5- and 9-point favorites make more intuitive sense than the empirical\nvalues based on small samples.\n\n1.7 Example: calibration for record linkage\n\nWe emphasize the essentially empirical (not ‘subjective’ or ‘personal’) nature of probabilities\nwith another example in which they are estimated from data.\n\nRecord linkage refers to the use of an algorithmic technique to identify records from\ndifferent databases that correspond to the same individual. Record-linkage techniques are\nused in a variety of settings. The work described here was formulated and first applied in\nthe context of record linkage between the U.S. Census and a large-scale post-enumeration\nsurvey, which is the first step of an extensive matching operation conducted to evaluate\ncensus coverage for subgroups of the population. The goal of this first step is to declare as\nmany records as possible ‘matched’ by computer without an excessive rate of error, thereby\navoiding the cost of the resulting manual processing for all records not declared ‘matched.’\n\nExisting methods for assigning scores to potential matches\n\nMuch attention has been paid in the record-linkage literature to the problem of assigning\n‘weights’ to individual fields of information in a multivariate record and obtaining a com-\nposite ‘score,’ which we call y, that summarizes the closeness of agreement between two\nrecords. Here, we assume that this step is complete in the sense that these rules have been\nchosen. The next step is the assignment of candidate matched pairs, where each pair of\nrecords consists of the best potential match for each other from the respective databases.\nThe specified weighting rules then order the candidate matched pairs. In the motivating\nproblem at the Census Bureau, a binary choice is made between the alternatives ‘declare\nmatched’ vs. ‘send to followup,’ where a cutoff score is needed above which records are\ndeclared matched. The false-match rate is then defined as the number of falsely matched\npairs divided by the number of declared matched pairs.\n\nParticularly relevant for any such decision problem is an accurate method for assessing\nthe probability that a candidate matched pair is a correct match as a function of its score.\nSimple methods exist for converting the scores into probabilities, but these lead to extremely\ninaccurate, typically grossly optimistic, estimates of false-match rates. For example, a\nmanual check of a set of records with nominal false-match probabilities ranging from 10−3\n\nto 10−7 (that is, pairs deemed almost certain to be matches) found actual false-match rates\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.7. EXAMPLE: CALIBRATION FOR RECORD LINKAGE 17\n\nFigure 1.3 Histograms of weight scores y for true and false matches in a sample of records from\nthe 1988 test Census. Most of the matches in the sample are true (because a pre-screening process\nhas already picked these as the best potential match for each case), and the two distributions are\nmostly, but not completely, separated.\n\ncloser to the 1% range. Records with nominal false-match probabilities of 1% had an actual\nfalse-match rate of 5%.\n\nWe would like to use Bayesian methods to recalibrate these to obtain objective proba-\nbilities of matching for a given decision rule—in the same way that in the football example,\nwe used past data to estimate the probabilities of different game outcomes conditional on\nthe point spread. Our approach is to work with the scores y and empirically estimate the\nprobability of a match as a function of y.\n\nEstimating match probabilities empirically\n\nWe obtain accurate match probabilities using mixture modeling, a topic we discuss in detail\nin Chapter 22. The distribution of previously obtained scores for the candidate matches\nis considered a ‘mixture’ of a distribution of scores for true matches and a distribution for\nnon-matches. The parameters of the mixture model are estimated from the data. The\nestimated parameters allow us to calculate an estimate of the probability of a false match\n(a pair declared matched that is not a true match) for any given decision threshold on the\nscores. In the procedure that was actually used, some elements of the mixture model (for\nexample, the optimal transformation required to allow a mixture of normal distributions\nto apply) were fit using ‘training’ data with known match status (separate from the data\nto which we apply our calibration procedure), but we do not describe those details here.\nInstead we focus on how the method would be used with a set of data with unknown match\nstatus.\n\nSupport for this approach is provided in Figure 1.3, which displays the distribution of\nscores for the matches and non-matches in a particular dataset obtained from 2300 records\nfrom a ‘test Census’ survey conducted in a single local area two years before the 1990 Census.\nThe two distributions, p(y|match) and p(y|non-match), are mostly distinct—meaning that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n18 1. PROBABILITY AND INFERENCE\n\nFigure 1.4 Lines show expected false-match rate (and 95% bounds) as a function of the proportion\nof cases declared matches, based on the mixture model for record linkage. Dots show the actual\nfalse-match rate for the data.\n\nin most cases it is possible to identify a candidate as a match or not given the score alone—\nbut with some overlap.\n\nIn our application dataset, we do not know the match status. Thus we are faced with a\nsingle combined histogram from which we estimate the two component distributions and the\nproportion of the population of scores that belong to each component. Under the mixture\nmodel, the distribution of scores can be written as,\n\np(y) = Pr(match) p(y|match) + Pr(non-match) p(y|non-match). (1.7)\n\nThe mixture probability (Pr(match)) and the parameters of the distributions of matches\n(p(y|match)) and non-matches (p(y|non-match)) are estimated using the mixture model\napproach (as described in Chapter 22) applied to the combined histogram from the data\nwith unknown match status.\n\nTo use the method to make record-linkage decisions, we construct a curve giving the\nfalse-match rate as a function of the decision threshold, the score above which pairs will\nbe ‘declared’ a match. For a given decision threshold, the probability distributions in (1.7)\ncan be used to estimate the probability of a false match, a score y above the threshold\noriginating from the distribution p(y|non-match). The lower the threshold, the more pairs\nwe will declare as matches. As we declare more matches, the proportion of errors increases.\nThe approach described here should provide an objective error estimate for each threshold.\n(See the validation in the next paragraph.) Then a decision maker can determine the\nthreshold that provides an acceptable balance between the goals of declaring more matches\nautomatically (thus reducing the clerical labor) and making fewer mistakes.\n\nExternal validation of the probabilities using test data\n\nThe approach described above was externally validated using data for which the match\nstatus is known. The method was applied to data from three different locations of the 1988\ntest Census, and so three tests of the methods were possible. We provide detailed results\nfor one; results for the other two were similar. The mixture model was fitted to the scores\nof all the candidate pairs at a test site. Then the estimated model was used to create the\nlines in Figure 1.4, which show the expected false-match rate (and uncertainty bounds) in\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 19\n\nFigure 1.5 Expansion of Figure 1.4 in the region where the estimated and actual match rates change\nrapidly. In this case, it would seem a good idea to match about 88% of the cases and send the rest\nto followup.\n\nterms of the proportion of cases declared matched, as the threshold varies from high (thus\nallowing no matches) to low (thus declaring almost all the candidate pairs to be matches).\nThe false-match proportion is an increasing function of the number of declared matches,\nwhich makes sense: as we move rightward on the graph, we are declaring weaker and weaker\ncases to be matches.\n\nThe lines on Figure 1.4 display the expected proportion of false matches and 95% pos-\nterior bounds for the false-match rate as estimated from the model. (These bounds give\nthe estimated range within which there is 95% posterior probability that the false-match\nrate lies. The concept of posterior intervals is discussed in more detail in the next chapter.)\nThe dots in the graph display the actual false-match proportions, which track well with the\nmodel. In particular, the model would suggest a recommendation of declaring something\nless than 90% of cases as matched and giving up on the other 10% or so, so as to avoid\nmost of the false matches, and the dots show a similar pattern.\n\nIt is clearly possible to match large proportions of the files with little or no error. Also,\nthe quality of candidate matches becomes dramatically worse at some point where the\nfalse-match rate accelerates. Figure 1.5 takes a magnifying glass to the previous display\nto highlight the behavior of the calibration procedure in the region of interest where the\nfalse-match rate accelerates. The predicted false-match rate curves bend upward, close to\nthe points where the observed false-match rate curves rise steeply, which is a particularly\nencouraging feature of the calibration method. The calibration procedure performs well\nfrom the standpoint of providing predicted probabilities that are close to the true probabili-\nties and interval estimates that are informative and include the true values. By comparison,\nthe original estimates of match probabilities, constructed by multiplying weights without\nempirical calibration, were highly inaccurate.\n\n1.8 Some useful results from probability theory\n\nWe assume the reader is familiar with elementary manipulations involving probabilities\nand probability distributions. In particular, basic probability background that must be\nwell understood for key parts of the book includes the manipulation of joint densities, the\ndefinition of simple moments, the transformation of variables, and methods of simulation. In\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n20 1. PROBABILITY AND INFERENCE\n\nthis section we briefly review these assumed prerequisites and clarify some further notational\nconventions used in the remainder of the book. Appendix A provides information on some\ncommonly used probability distributions.\n\nAs introduced in Section 1.3, we generally represent joint distributions by their joint\nprobability mass or density function, with dummy arguments reflecting the name given to\neach variable being considered. Thus for two quantities u and v, we write the joint density\nas p(u, v); if specific values need to be referenced, this notation will be further abused as\nwith, for example, p(u, v=1).\n\nIn Bayesian calculations relating to a joint density p(u, v), we will often refer to a\nconditional distribution or density function such as p(u|v) and a marginal density such as\np(u) =\n\n∫\np(u, v)dv. In this notation, either or both u and v can be vectors. Typically\n\nit will be clear from the context that the range of integration in the latter expression\nrefers to the entire range of the variable being integrated out. It is also often useful to\nfactor a joint density as a product of marginal and conditional densities; for example,\np(u, v, w) = p(u|v, w)p(v|w)p(w).\n\nSome authors use different notations for distributions on parameters and observables—\nfor example, π(θ), f(y|θ)—but this obscures the fact that all probability distributions have\nthe same logical status in Bayesian inference. We must always be careful, though, to in-\ndicate appropriate conditioning; for example, p(y|θ) is different from p(y). In the inter-\nests of conciseness, however, our notation hides the conditioning on hypotheses that hold\nthroughout—no probability judgments can be made in a vacuum—and to be more explicit\none might use a notation such as the following:\n\np(θ, y|H) = p(θ|H)p(y|θ,H),\n\nwhere H refers to the set of hypotheses or assumptions used to define the model. Also, we\nsometimes suppress explicit conditioning on known explanatory variables, x.\n\nWe use the standard notations, E(·) and var(·), for mean and variance, respectively:\n\nE(u) =\n\n∫\nup(u)du, var(u) =\n\n∫\n(u− E(u))2p(u)du.\n\nFor a vector parameter u, the expression for the mean is the same, and the covariance\nmatrix is defined as\n\nvar(u) =\n\n∫\n(u− E(u))(u− E(u))T p(u)du,\n\nwhere u is considered a column vector. (We use the terms ‘variance matrix’ and ‘covariance\nmatrix’ interchangeably.) This notation is slightly imprecise, because E(u) and var(u) are\nreally functions of the distribution function, p(u), not of the variable u. In an expression\ninvolving an expectation, any variable that does not appear explicitly as a conditioning\nvariable is assumed to be integrated out in the expectation; for example, E(u|v) refers to\nthe conditional expectation of u with v held fixed—that is, the conditional expectation as\na function of v—whereas E(u) is the expectation of u, averaging over v (as well as u).\n\nModeling using conditional probability\n\nUseful probability models often express the distribution of observables conditionally or hier-\narchically rather than through more complicated unconditional distributions. For example,\nsuppose y is the height of a university student selected at random. The marginal distri-\nbution p(y) is (essentially) a mixture of two approximately normal distributions centered\naround 160 and 175 centimeters. A more useful description of the distribution of y would\nbe based on the joint distribution of height and sex: p(male) ≈ p(female) ≈ 1\n\n2 , along with\nthe conditional specifications that p(y|female) and p(y|male) are each approximately normal\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(u|v)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over v. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nE(u) =\n\n∫ ∫\nup(u, v)dudv =\n\n∫ ∫\nu p(u|v)du p(v)dv =\n\n∫\nE(u|v)p(v)dv.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(u|v)) + var(E(u|v)). (1.9)\n\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var (E(u|v)) = E\n(\nE(u2|v)− (E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E (E(u|v)))2\n\n= E(u2)− E\n(\n(E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E(u))2\n\n= E(u2)− (E(u))2\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if u is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(·). Suppose pu(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf pu is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\npv(v) = pu(f\n−1(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for pv(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf pu is a continuous distribution, and v = f(u) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\npv(v) = |J | pu(f−1(v))\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(uly)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over vu. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nu)= / / up(u, v)dudv = / / up(ulv)dup(v)dv = / E(ulv)p(w)do.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(ulv)) + var(E(ulv)). (1.9)\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var(E(ulv)) = E(E(u*|v) — (E(ulv))*) + E ((E(ulv))*) — (E (E(u|v)))°\nE(u’) — E ((E(u|v))*) + E ((E(ulv))*) = (E(u)?\n= E(u*) — (E(u)?\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if wu is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(-). Suppose p,(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf p, is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\nPo(v) = Pu(f(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for p,(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf p,, is a continuous distribution, and v = f(wu) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\nPo(v) = |J| pul(f~*(v))\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n22 1. PROBABILITY AND INFERENCE\n\nwhere |J | is the absolute value of the determinant of the Jacobian of the transformation\nu = f−1(v) as a function of v; the Jacobian J is the square matrix of partial derivatives\n(with dimension given by the number of components of u), with the (i, j)th entry equal to\n∂ui/∂vj. Once again, if f is many-to-one, then pv(v) is a sum or integral of terms.\n\nIn one dimension, we commonly use the logarithm to transform the parameter space\nfrom (0,∞) to (−∞,∞). When working with parameters defined on the open unit interval,\n(0, 1), we often use the logistic transformation:\n\nlogit(u) = log\n\n(\nu\n\n1− u\n\n)\n, (1.10)\n\nwhose inverse transformation is\n\nlogit−1(v) =\nev\n\n1 + ev\n.\n\nAnother common choice is the probit transformation, Φ−1(u), where Φ is the standard\nnormal cumulative distribution function, to transform from (0, 1) to (−∞,∞).\n\n1.9 Computation and software\n\nAt the time of writing, the authors rely primarily on the software package R for graphs and\nbasic simulations, fitting of classical simple models (including regression, generalized linear\nmodels, and nonparametric methods such as locally weighted regression), optimization, and\nsome simple programming. We use the Bayesian inference package Stan (see Appendix C)\nfor fitting most models, but for teaching purposes in this book we describe how to perform\nmost of the computations from first principles. Even when using Stan, we typically work\nwithin R to plot and transform the data before model fitting, and to display inferences and\nmodel checks afterwards.\n\nSpecific computational tasks that arise in Bayesian data analysis include:\n\n• Vector and matrix manipulations (see Table 1.1)\n\n• Computing probability density functions (see Appendix A)\n\n• Drawing simulations from probability distributions (see Appendix A for standard distri-\nbutions and Exercise 1.9 for an example of a simple stochastic process)\n\n• Structured programming (including looping and customized functions)\n\n• Calculating the linear regression estimate and variance matrix (see Chapter 14)\n\n• Graphics, including scatterplots with overlain lines and multiple graphs per page (see\nChapter 6 for examples).\n\nOur general approach to computation is to fit many models, gradually increasing the\ncomplexity. We do not recommend the strategy of writing a model and then letting the\ncomputer run overnight to estimate it perfectly. Rather, we prefer to fit each model rela-\ntively quickly, using inferences from the previously fitted simpler models as starting values,\nand displaying inferences and comparing to data before continuing.\n\nWe discuss computation in detail in Part III of this book after first introducing the\nfundamental concepts of Bayesian modeling, inference, and model checking. Appendix C\nillustrates how to perform computations in R and Stan in several different ways for a single\nexample.\n\nSummarizing inferences by simulation\n\nSimulation forms a central part of much applied Bayesian analysis, because of the relative\nease with which samples can often be generated from a probability distribution, even when\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.9. COMPUTATION AND SOFTWARE 23\n\nthe density function cannot be explicitly integrated. In performing simulations, it is helpful\nto consider the duality between a probability density function and a histogram of a set of\nrandom draws from the distribution: given a large enough sample, the histogram can pro-\nvide practically complete information about the density, and in particular, various sample\nmoments, percentiles, and other summary statistics provide estimates of any aspect of the\ndistribution, to a level of precision that can be estimated. For example, to estimate the\n95th percentile of the distribution of θ, draw a random sample of size S from p(θ) and use\nthe 0.95Sth order statistic. For most purposes, S = 1000 is adequate for estimating the\n95th percentile in this way.\n\nAnother advantage of simulation is that extremely large or small simulated values often\nflag a problem with model specification or parameterization (for example, see Figure 4.2)\nthat might not be noticed if estimates and probability statements were obtained in analytic\nform.\n\nGenerating values from a probability distribution is often straightforward with modern\ncomputing techniques based on (pseudo)random number sequences. A well-designed pseu-\ndorandom number generator yields a deterministic sequence that appears to have the same\nproperties as a sequence of independent random draws from the uniform distribution on\n[0, 1]. Appendix A describes methods for drawing random samples from some commonly\nused distributions.\n\nSampling using the inverse cumulative distribution function\n\nAs an introduction to the ideas of simulation, we describe a method for sampling from\ndiscrete and continuous distributions using the inverse cumulative distribution function.\nThe cumulative distribution function, or cdf, F , of a one-dimensional distribution, p(v), is\ndefined by\n\nF (v∗) = Pr(v ≤ v∗)\n\n=\n\n{ ∑\nv≤v∗ p(v) if p is discrete∫ v∗\n\n−∞ p(v)dv if p is continuous.\n\nThe inverse cdf can be used to obtain random samples from the distribution p, as\nfollows. First draw a random value, U , from the uniform distribution on [0, 1], using a table\nof random numbers or, more likely, a random number function on the computer. Now let\nv = F−1(U). The function F is not necessarily one-to-one—certainly not if the distribution\nis discrete—but F−1(U) is unique with probability 1. The value v will be a random draw\nfrom p, and is easy to compute as long as F−1(U) is simple. For a discrete distribution,\nF−1 can simply be tabulated.\n\nFor a continuous example, suppose v has an exponential distribution with parameter λ\n(see Appendix A); then its cdf is F (v) = 1− e−λv, and the value of v for which U=F (v) is\n\nv = − log(1−U)\nλ . Then, recognizing that 1−U also has the uniform distribution on [0, 1], we\n\nsee we can obtain random draws from the exponential distribution as − logU\nλ . We discuss\n\nother methods of simulation in Part III of the book and Appendix A.\n\nSimulation of posterior and posterior predictive quantities\n\nIn practice, we are most often interested in simulating draws from the posterior distribu-\ntion of the model parameters θ, and perhaps from the posterior predictive distribution of\nunknown observables ỹ. Results from a set of S simulation draws can be stored in the\ncomputer in an array, as illustrated in Table 1.1. We use the notation s = 1, . . . , S to in-\ndex simulation draws; (θs, ỹs) is the corresponding joint draw of parameters and predicted\nquantities from their joint posterior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n24 1. PROBABILITY AND INFERENCE\n\nSimulation Parameters Predictive\ndraw quantities\n\nθ1 . . . θk ỹ1 . . . ỹn\n1 θ11 . . . θ1k ỹ11 . . . ỹ1n\n...\n\n...\n. . .\n\n...\n...\n\n. . .\n...\n\nS θS1 . . . θSk ỹS1 . . . ỹSn\n\nTable 1.1 Structure of posterior and posterior predictive simulations. The superscripts are indexes,\nnot powers.\n\nFrom these simulated values, we can estimate the posterior distribution of any quantity\nof interest, such as θ1/θ3, by just computing a new column in Table 1.1 using the existing S\ndraws of (θ, ỹ). We can estimate the posterior probability of any event, such as Pr(ỹ1+ ỹ2 >\neθ1), by the proportion of the S simulations for which it is true. We are often interested in\nposterior intervals; for example, the central 95% posterior interval [a, b] for the parameter\nθj , for which Pr(θj < a) = 0.025 and Pr(θj > b) = 0.025. These values can be directly\nestimated by the appropriate simulated values of θj , for example, the 25th and 976th order\nstatistics if S=1000. We commonly summarize inferences by 50% and 95% intervals.\n\nWe return to the accuracy of simulation inferences in Section 10.5 after we have gained\nsome experience using simulations of posterior distributions in some simple examples.\n\n1.10 Bayesian inference in applied statistics\n\nA pragmatic rationale for the use of Bayesian methods is the inherent flexibility introduced\nby their incorporation of multiple levels of randomness and the resultant ability to combine\ninformation from different sources, while incorporating all reasonable sources of uncertainty\nin inferential summaries. Such methods naturally lead to smoothed estimates in complicated\ndata structures and consequently have the ability to obtain better real-world answers.\n\nAnother reason for focusing on Bayesian methods is more psychological, and involves the\nrelationship between the statistician and the client or specialist in the subject matter area\nwho is the consumer of the statistician’s work. In many practical cases, clients will interpret\ninterval estimates provided by statisticians as Bayesian intervals, that is, as probability\nstatements about the likely values of unknown quantities conditional on the evidence in\nthe data. Such direct probability statements require prior probability specifications for\nunknown quantities (or more generally, probability models for vectors of unknowns), and\nthus the kinds of answers clients will assume are being provided by statisticians, Bayesian\nanswers, require full probability models—explicit or implicit.\n\nFinally, Bayesian inferences are conditional on probability models that invariably contain\napproximations in their attempt to represent complicated real-world relationships. If the\nBayesian answers vary dramatically over a range of scientifically reasonable assumptions\nthat are unassailable by the data, then the resultant range of possible conclusions must be\nentertained as legitimate, and we believe that the statistician has the responsibility to make\nthe client aware of this fact.\n\nIn this book, we focus on the construction of models (especially hierarchical ones, as\ndiscussed in Chapter 5 onward) to relate complicated data structures to scientific questions,\nchecking the fit of such models, and investigating the sensitivity of conclusions to reasonable\nmodeling assumptions. From this point of view, the strength of the Bayesian approach lies in\n(1) its ability to combine information from multiple sources (thereby in fact allowing greater\n‘objectivity’ in final conclusions), and (2) its more encompassing accounting of uncertainty\nabout the unknowns in a statistical problem.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.11. BIBLIOGRAPHIC NOTE 25\n\nOther important themes, many of which are common to much modern applied statistical\npractice, whether formally Bayesian or not, are the following:\n\n• a willingness to use many parameters\n\n• hierarchical structuring of models, which is the essential tool for achieving partial pool-\ning of estimates and compromising in a scientific way between alternative sources of\ninformation\n\n• model checking—not only by examining the internal goodness of fit of models to ob-\nserved and possible future data, but also by comparing inferences about estimands and\npredictions of interest to substantive knowledge\n\n• an emphasis on inference in the form of distributions or at least interval estimates rather\nthan simple point estimates\n\n• the use of simulation as the primary method of computation; the modern computational\ncounterpart to a ‘joint probability distribution’ is a set of randomly drawn values, and a\nkey tool for dealing with missing data is the method of multiple imputation (computation\nand multiple imputation are discussed in more detail in later chapters)\n\n• the use of probability models as tools for understanding and possibly improving data-\nanalytic techniques that may not explicitly invoke a Bayesian model\n\n• the importance of including in the analysis as much background information as possible,\nso as to approximate the goal that data can be viewed as a random sample, conditional\non all the variables in the model\n\n• the importance of designing studies to have the property that inferences for estimands\nof interest will be robust to model assumptions.\n\n1.11 Bibliographic note\n\nSeveral good introductory books have been written on Bayesian statistics, beginning with\nLindley (1965), and continuing through Hoff (2009). Berry (1996) presents, from a Bayesian\nperspective, many of the standard topics for an introductory statistics textbook. Gill\n(2002) and Jackman (2009) introduce applied Bayesian statistics for social scientists, Kr-\nuschke (2011) introduces Bayesian methods for psychology researchers, and Christensen et\nal. (2010) supply a general introduction. Carlin and Louis (2008) cover the theory and\napplications of Bayesian inference, focusing on biological applications and connections to\nclassical methods. Some resources for teaching Bayesian statistics include Sedlmeier and\nGigerenzer (2001) and Gelman (1998, 2008b).\n\nThe bibliographic notes at the ends of the chapters in this book refer to a variety of\nspecific applications of Bayesian data analysis. Several review articles in the statistical\nliterature, such as Breslow (1990) and Racine et al. (1986), have appeared that discuss,\nin general terms, areas of application in which Bayesian methods have been useful. The\nvolumes edited by Gatsonis et al. (1993–2002) are collections of Bayesian analyses, including\nextensive discussions about choices in the modeling process and the relations between the\nstatistical methods and the applications.\n\nThe foundations of probability and Bayesian statistics are an important topic that we\ntreat only briefly. Bernardo and Smith (1994) give a thorough review of the foundations\nof Bayesian models and inference with a comprehensive list of references. Jeffreys (1961) is\na self-contained book about Bayesian statistics that comprehensively presents an inductive\nview of inference; Good (1950) is another important early work. Jaynes (1983) is a collection\nof reprinted articles that present a deductive view of Bayesian inference that we believe is\nsimilar to ours. Both Jeffreys and Jaynes focus on applications in the physical sciences.\nJaynes (2003) focuses on connections between statistical inference and the philosophy of\nscience and includes several examples of physical probability.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n26 1. PROBABILITY AND INFERENCE\n\nGigerenzer and Hoffrage (1995) discuss the connections between Bayesian probability\nand frequency probabilities from a perspective similar to ours, and provide evidence that\npeople can typically understand and compute best with probabilities that are expressed\nin the form of relative frequency. Gelman (1998) presents some classroom activities for\nteaching Bayesian ideas.\n\nDe Finetti (1974) is an influential work that focuses on the crucial role of exchange-\nability. More approachable discussions of the role of exchangeability in Bayesian inference\nare provided by Lindley and Novick (1981) and Rubin (1978a, 1987a). The non-Bayesian\narticle by Draper et al. (1993) makes an interesting attempt to explain how exchangeable\nprobability models can be justified in data analysis. Berger and Wolpert (1984) give a\ncomprehensive discussion and review of the likelihood principle, and Berger (1985, Sections\n1.6, 4.1, and 4.12) reviews a range of philosophical issues from the perspective of Bayesian\ndecision theory.\n\nOur own philosophy of Bayesian statistics appears in Gelman (2011) and Gelman and\nShalizi (2013); for some contrasting views, see the discussion of that article, along with\nEfron (1986) and the discussions following Gelman (2008a).\n\nPratt (1965) and Rubin (1984) discuss the relevance of Bayesian methods for applied\nstatistics and make many connections between Bayesian and non-Bayesian approaches to\ninference. Further references on the foundations of statistical inference appear in Shafer\n(1982) and the accompanying discussion. Kahneman and Tversky (1972) and Alpert and\nRaiffa (1982) present the results of psychological experiments that assess the meaning of\n‘subjective probability’ as measured by people’s stated beliefs and observed actions. Lindley\n(1971a) surveys many different statistical ideas, all from the Bayesian perspective. Box and\nTiao (1973) is an early book on applied Bayesian methods. They give an extensive treatment\nof inference based on normal distributions, and their first chapter, a broad introduction to\nBayesian inference, provides a good counterpart to Chapters 1 and 2 of this book.\n\nThe iterative process involving modeling, inference, and model checking that we present\nin Section 1.1 is discussed at length in the first chapter of Box and Tiao (1973) and also\nin Box (1980). Cox and Snell (1981) provide a more introductory treatment of these ideas\nfrom a less model-based perspective.\n\nMany good books on the mathematical aspects of probability theory are available, such\nas Feller (1968) and Ross (1983); these are useful when constructing probability models\nand working with them. O’Hagan (1988) has written an interesting introductory text on\nprobability from an explicitly Bayesian point of view.\n\nPhysical probability models for coin tossing are discussed by Keller (1986), Jaynes\n(2003), and Gelman and Nolan (2002b). The football example of Section 1.6 is discussed\nin more detail in Stern (1991); see also Harville (1980) and Glickman (1993) and Glickman\nand Stern (1998) for analyses of football scores not using the point spread. Related analyses\nof sports scores and betting odds appear in Stern (1997, 1998). For more background on\nsports betting, see Snyder (1975) and Rombola (1984).\n\nAn interesting real-world example of probability assignment arose with the explosion\nof the Challenger space shuttle in 1986; Martz and Zimmer (1992), Dalal, Fowlkes, and\nHoadley (1989), and Lavine (1991) present and compare various methods for assigning\nprobabilities for space shuttle failures. (At the time of writing we are not aware of similar\ncontributions relating to the more recent space accident in 2003.) The record-linkage ex-\nample in Section 1.7 appears in Belin and Rubin (1995b), who discuss the mixture models\nand calibration techniques in more detail. The Census problem that motivated the record\nlinkage is described by Hogan (1992).\n\nIn all our examples, probabilities are assigned using statistical modeling and estimation,\nnot by ‘subjective’ assessment. Dawid (1986) provides a general discussion of probability\nassignment, and Dawid (1982) discusses the connections between calibration and Bayesian\nprobability assignment.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.12. EXERCISES 27\n\nThe graphical method of jittering, used in Figures 1.1 and 1.2 and elsewhere in this\nbook, is discussed in Chambers et al. (1983). For information on the statistical packages R\nand Bugs, see Becker, Chambers, and Wilks (1988), R Project (2002), Fox (2002), Venables\nand Ripley (2002), and Spiegelhalter et al. (1994, 2003).\n\nNorvig (2007) describes the principles and details of the Bayesian spelling corrector.\n\n1.12 Exercises\n\n1. Conditional probability: suppose that if θ = 1, then y has a normal distribution with\nmean 1 and standard deviation σ, and if θ = 2, then y has a normal distribution with\nmean 2 and standard deviation σ. Also, suppose Pr(θ = 1) = 0.5 and Pr(θ = 2) = 0.5.\n\n(a) For σ = 2, write the formula for the marginal probability density for y and sketch it.\n\n(b) What is Pr(θ = 1|y = 1), again supposing σ = 2?\n\n(c) Describe how the posterior density of θ changes in shape as σ is increased and as it is\ndecreased.\n\n2. Conditional means and variances: show that (1.8) and (1.9) hold if u is a vector.\n\n3. Probability calculation for genetics (from Lindley, 1965): suppose that in each individual\nof a large population there is a pair of genes, each of which can be either x or X, that\ncontrols eye color: those with xx have blue eyes, while heterozygotes (those with Xx or\nxX) and those with XX have brown eyes. The proportion of blue-eyed individuals is p2\n\nand of heterozygotes is 2p(1 − p), where 0 < p < 1. Each parent transmits one of its\nown genes to the child; if a parent is a heterozygote, the probability that it transmits the\ngene of type X is 1\n\n2 . Assuming random mating, show that among brown-eyed children\nof brown-eyed parents, the expected proportion of heterozygotes is 2p/(1+2p). Suppose\nJudy, a brown-eyed child of brown-eyed parents, marries a heterozygote, and they have\nn children, all brown-eyed. Find the posterior probability that Judy is a heterozygote\nand the probability that her first grandchild has blue eyes.\n\n4. Probability assignment: we will use the football dataset to estimate some conditional\nprobabilities about professional football games. There were twelve games with point\nspreads of 8 points; the outcomes in those games were: −7, −5, −3, −3, 1, 6, 7, 13, 15,\n16, 20, and 21, with positive values indicating wins by the favorite and negative values\nindicating wins by the underdog. Consider the following conditional probabilities:\n\nPr(favorite wins | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8 and favorite wins).\n\n(a) Estimate each of these using the relative frequencies of games with a point spread of\n8.\n\n(b) Estimate each using the normal approximation for the distribution of (outcome −\npoint spread).\n\n5. Probability assignment: the 435 U.S. Congressmembers are elected to two-year terms;\nthe number of voters in an individual congressional election varies from about 50,000 to\n350,000. We will use various sources of information to estimate roughly the probability\nthat at least one congressional election is tied in the next national election.\n\n(a) Use any knowledge you have about U.S. politics. Specify clearly what information you\nare using to construct this conditional probability, even if your answer is just a guess.\n\n(b) Use the following information: in the period 1900–1992, there were 20,597 congres-\nsional elections, out of which 6 were decided by fewer than 10 votes and 49 decided\nby fewer than 100 votes.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n28 1. PROBABILITY AND INFERENCE\n\nSee Gelman, King, and Boscardin (1998), Mulligan and Hunter (2001), and Gelman,\nKatz, and Tuerlinckx (2002) for more on this topic.\n\n6. Conditional probability: approximately 1/125 of all births are fraternal twins and 1/300\nof births are identical twins. Elvis Presley had a twin brother (who died at birth). What\nis the probability that Elvis was an identical twin? (You may approximate the probability\nof a boy or girl birth as 1\n\n2 .)\n\n7. Conditional probability: the following problem is loosely based on the television game\nshow Let’s Make a Deal. At the end of the show, a contestant is asked to choose one of\nthree large boxes, where one box contains a fabulous prize and the other two boxes contain\nlesser prizes. After the contestant chooses a box, Monty Hall, the host of the show,\nopens one of the two boxes containing smaller prizes. (In order to keep the conclusion\nsuspenseful, Monty does not open the box selected by the contestant.) Monty offers the\ncontestant the opportunity to switch from the chosen box to the remaining unopened box.\nShould the contestant switch or stay with the original choice? Calculate the probability\nthat the contestant wins under each strategy. This is an exercise in being clear about the\ninformation that should be conditioned on when constructing a probability judgment.\nSee Selvin (1975) and Morgan et al. (1991) for further discussion of this problem.\n\n8. Subjective probability: discuss the following statement. ‘The probability of event E is\nconsidered “subjective” if two rational persons A and B can assign unequal probabilities\nto E, PA(E) and PB(E). These probabilities can also be interpreted as “conditional”:\nPA(E) = P (E|IA) and PB(E) = P (E|IB), where IA and IB represent the knowledge\navailable to persons A and B, respectively.’ Apply this idea to the following examples.\n\n(a) The probability that a ‘6’ appears when a fair die is rolled, where A observes the\noutcome of the die roll and B does not.\n\n(b) The probability that Brazil wins the next World Cup, where A is ignorant of soccer\nand B is a knowledgeable sports fan.\n\n9. Simulation of a queuing problem: a clinic has three doctors. Patients come into the\nclinic at random, starting at 9 a.m., according to a Poisson process with time parameter\n10 minutes: that is, the time after opening at which the first patient appears follows an\nexponential distribution with expectation 10 minutes and then, after each patient arrives,\nthe waiting time until the next patient is independently exponentially distributed, also\nwith expectation 10 minutes. When a patient arrives, he or she waits until a doctor\nis available. The amount of time spent by each doctor with each patient is a random\nvariable, uniformly distributed between 5 and 20 minutes. The office stops admitting\nnew patients at 4 p.m. and closes when the last patient is through with the doctor.\n\n(a) Simulate this process once. How many patients came to the office? How many had to\nwait for a doctor? What was their average wait? When did the office close?\n\n(b) Simulate the process 100 times and estimate the median and 50% interval for each of\nthe summaries in (a).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 2\n\nSingle-parameter models\n\nOur first detailed discussion of Bayesian inference is in the context of statistical models\nwhere only a single scalar parameter is to be estimated; that is, the estimand θ is one-\ndimensional. In this chapter, we consider four fundamental and widely used one-dimensional\nmodels—the binomial, normal, Poisson, and exponential—and at the same time introduce\nimportant concepts and computational methods for Bayesian data analysis.\n\n2.1 Estimating a probability from binomial data\n\nIn the simple binomial model, the aim is to estimate an unknown population proportion\nfrom the results of a sequence of ‘Bernoulli trials’; that is, data y1, . . . , yn, each of which is\neither 0 or 1. This problem provides a relatively simple but important starting point for\nthe discussion of Bayesian inference. By starting with the binomial model, our discussion\nalso parallels the very first published Bayesian analysis by Thomas Bayes in 1763, and his\nseminal contribution is still of interest.\n\nThe binomial distribution provides a natural model for data that arise from a sequence\nof n exchangeable trials or draws from a large population where each trial gives rise to\none of two possible outcomes, conventionally labeled ‘success’ and ‘failure.’ Because of the\nexchangeability, the data can be summarized by the total number of successes in the n\ntrials, which we denote here by y. Converting from a formulation in terms of exchangeable\ntrials to one using independent and identically distributed random variables is achieved\nnaturally by letting the parameter θ represent the proportion of successes in the population\nor, equivalently, the probability of success in each trial. The binomial sampling model is,\n\np(y|θ) = Bin(y|n, θ) =\n(\nn\n\ny\n\n)\nθy(1 − θ)n−y, (2.1)\n\nwhere on the left side we suppress the dependence on n because it is regarded as part of the\nexperimental design that is considered fixed; all the probabilities discussed for this problem\nare assumed to be conditional on n.\n\nExample. Estimating the probability of a female birth\nAs a specific application of the binomial model, we consider the estimation of the\nsex ratio within a population of human births. The proportion of births that are\nfemale has long been a topic of interest both scientifically and to the lay public. Two\nhundred years ago it was established that the proportion of female births in European\npopulations was less than 0.5 (see Historical Note below), while in this century interest\nhas focused on factors that may influence the sex ratio. The currently accepted value\nof the proportion of female births in large European-race populations is 0.485.\nFor this example we define the parameter θ to be the proportion of female births, but\nan alternative way of reporting this parameter is as a ratio of male to female birth\nrates, φ = (1− θ)/θ.\nLet y be the number of girls in n recorded births. By applying the binomial model\n\n29\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n30 2. SINGLE-PARAMETER MODELS\n\nFigure 2.1 Unnormalized posterior density for binomial parameter θ, based on uniform prior dis-\ntribution and y successes out of n trials. Curves displayed for several values of n and y.\n\n(2.1), we are assuming that the n births are conditionally independent given θ, with\nthe probability of a female birth equal to θ for all cases. This modeling assumption\nis motivated by the exchangeability that may be judged to arise when we have no\nexplanatory information (for example, distinguishing multiple births or births within\nthe same family) that might affect the sex of the baby.\n\nTo perform Bayesian inference in the binomial model, we must specify a prior distribu-\ntion for θ. We will discuss issues associated with specifying prior distributions many times\nthroughout this book, but for simplicity at this point, we assume that the prior distribution\nfor θ is uniform on the interval [0, 1].\n\nElementary application of Bayes’ rule as displayed in (1.2), applied to (2.1), then gives\nthe posterior density for θ as\n\np(θ|y) ∝ θy(1 − θ)n−y. (2.2)\n\nWith fixed n and y, the factor\n(\nn\ny\n\n)\ndoes not depend on the unknown parameter θ, and so it\n\ncan be treated as a constant when calculating the posterior distribution of θ. As is typical\nof many examples, the posterior density can be written immediately in closed form, up to a\nconstant of proportionality. In single-parameter problems, this allows immediate graphical\npresentation of the posterior distribution. For example, in Figure 2.1, the unnormalized\ndensity (2.2) is displayed for several different experiments, that is, different values of n and\ny. Each of the four experiments has the same proportion of successes, but the sample sizes\nvary. In the present case, we can recognize (2.2) as the unnormalized form of the beta\ndistribution (see Appendix A),\n\nθ|y ∼ Beta(y + 1, n− y + 1). (2.3)\n\nHistorical note: Bayes and Laplace\nMany early writers on probability dealt with the elementary binomial model. The first\ncontributions of lasting significance, in the 17th and early 18th centuries, concentrated\non the ‘pre-data’ question: given θ, what are the probabilities of the various possible\noutcomes of the random variable y? For example, the ‘weak law of large numbers’ of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.1. ESTIMATING A PROBABILITY FROM BINOMIAL DATA 31\n\nJacob Bernoulli states that if y ∼ Bin(n, θ), then Pr( | yn − θ|>ǫ\n∣∣ θ) → 0 as n → ∞,\n\nfor any θ and any fixed value of ǫ > 0. The Reverend Thomas Bayes, an English\npart-time mathematician whose work was unpublished during his lifetime, and Pierre\nSimon Laplace, an inventive and productive mathematical scientist whose massive\noutput spanned the Napoleonic era in France, receive independent credit as the first\nto invert the probability statement and obtain probability statements about θ, given\nobserved y.\nIn his famous paper, published in 1763, Bayes sought, in our notation, the probability\nPr(θ∈ (θ1, θ2)|y); his solution was based on a physical analogy of a probability space\nto a rectangular table (such as a billiard table):\n\n1. (Prior distribution) A ballW is randomly thrown (according to a uniform distribu-\ntion on the table). The horizontal position of the ball on the table is θ, expressed\nas a fraction of the table width.\n\n2. (Likelihood) A ball O is randomly thrown n times. The value of y is the number\nof times O lands to the right of W .\n\nThus, θ is assumed to have a (prior) uniform distribution on [0, 1]. Using direct\nprobability calculations which he derived in the paper, Bayes then obtained\n\nPr(θ∈(θ1, θ2)|y) =\nPr(θ∈(θ1, θ2), y)\n\np(y)\n\n=\n\n∫ θ2\nθ1\np(y|θ)p(θ)dθ\np(y)\n\n=\n\n∫ θ2\nθ1\n\n(\nn\ny\n\n)\nθy(1− θ)n−ydθ\n\np(y)\n. (2.4)\n\nBayes succeeded in evaluating the denominator, showing that\n\np(y) =\n\n∫ 1\n\n0\n\n(\nn\n\ny\n\n)\nθy(1− θ)n−ydθ (2.5)\n\n=\n1\n\nn+ 1\nfor y = 0, . . . , n.\n\nThis calculation shows that all possible values of y are equally likely a priori.\nThe numerator of (2.4) is an incomplete beta integral with no closed-form expression\nfor large values of y and (n− y), a fact that apparently presented some difficulties for\nBayes.\nLaplace, however, independently ‘discovered’ Bayes’ theorem, and developed new ana-\nlytic tools for computing integrals. For example, he expanded the function θy(1− θ)n−y\naround its maximum at θ = y/n and evaluated the incomplete beta integral using what\nwe now know as the normal approximation.\nIn analyzing the binomial model, Laplace also used the uniform prior distribution. His\nfirst serious application was to estimate the proportion of girl births in a population.\nA total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770. Letting\nθ be the probability that any birth is female, Laplace showed that\n\nPr(θ ≥ 0.5|y = 241,945, n = 251,527+ 241,945) ≈ 1.15× 10−42,\n\nand so he was ‘morally certain’ that θ < 0.5.\n\nPrediction\n\nIn the binomial example with the uniform prior distribution, the prior predictive distribution\ncan be evaluated explicitly, as we have already noted in (2.5). Under the model, all possible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n32 2. SINGLE-PARAMETER MODELS\n\nvalues of y are equally likely, a priori. For posterior prediction from this model, we might\nbe more interested in the outcome of one new trial, rather than another set of n new trials.\nLetting ỹ denote the result of a new trial, exchangeable with the first n,\n\nPr(ỹ = 1|y) =\n\n∫ 1\n\n0\n\nPr(ỹ = 1|θ, y)p(θ|y)dθ\n\n=\n\n∫ 1\n\n0\n\nθp(θ|y)dθ = E(θ|y) = y + 1\n\nn+ 2\n, (2.6)\n\nfrom the properties of the beta distribution (see Appendix A). It is left as an exercise to\nreproduce this result using direct integration of (2.6). This result, based on the uniform\nprior distribution, is known as ‘Laplace’s law of succession.’ At the extreme observations\ny = 0 and y = n, Laplace’s law predicts probabilities of 1\n\nn+2 and n+1\nn+2 , respectively.\n\n2.2 Posterior as compromise between data and prior information\n\nThe process of Bayesian inference involves passing from a prior distribution, p(θ), to a\nposterior distribution, p(θ|y), and it is natural to expect that some general relations might\nhold between these two distributions. For example, we might expect that, because the\nposterior distribution incorporates the information from the data, it will be less variable than\nthe prior distribution. This notion is formalized in the second of the following expressions:\n\nE(θ) = E(E(θ|y)) (2.7)\n\nand\nvar(θ) = E(var(θ|y)) + var(E(θ|y)), (2.8)\n\nwhich are obtained by substituting (θ, y) for the generic (u, v) in (1.8) and (1.9). The result\nexpressed by Equation (2.7) is scarcely surprising: the prior mean of θ is the average of all\npossible posterior means over the distribution of possible data. The variance formula (2.8)\nis more interesting because it says that the posterior variance is on average smaller than\nthe prior variance, by an amount that depends on the variation in posterior means over\nthe distribution of possible data. The greater the latter variation, the more the potential\nfor reducing our uncertainty with regard to θ, as we shall see in detail for the binomial\nand normal models in the next chapter. The mean and variance relations only describe\nexpectations, and in particular situations the posterior variance can be similar to or even\nlarger than the prior variance (although this can be an indication of conflict or inconsistency\nbetween the sampling model and prior distribution).\n\nIn the binomial example with the uniform prior distribution, the prior mean is 1\n2 , and\n\nthe prior variance is 1\n12 . The posterior mean, y+1\n\nn+2 , is a compromise between the prior mean\nand the sample proportion, yn , where clearly the prior mean has a smaller and smaller role\nas the size of the data sample increases. This is a general feature of Bayesian inference: the\nposterior distribution is centered at a point that represents a compromise between the prior\ninformation and the data, and the compromise is controlled to a greater extent by the data\nas the sample size increases.\n\n2.3 Summarizing posterior inference\n\nThe posterior probability distribution contains all the current information about the pa-\nrameter θ. Ideally one might report the entire posterior distribution p(θ|y); as we have seen\nin Figure 2.1, a graphical display is useful. In Chapter 3, we use contour plots and scat-\nterplots to display posterior distributions in multiparameter problems. A key advantage of\nthe Bayesian approach, as implemented by simulation, is the flexibility with which posterior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.3. SUMMARIZING POSTERIOR INFERENCE 33\n\nFigure 2.2 Hypothetical density for which the 95% central interval and 95% highest posterior density\nregion dramatically differ: (a) central posterior interval, (b) highest posterior density region.\n\ninferences can be summarized, even after complicated transformations. This advantage is\nmost directly seen through examples, some of which will be presented shortly.\n\nFor many practical purposes, however, various numerical summaries of the distribu-\ntion are desirable. Commonly used summaries of location are the mean, median, and\nmode(s) of the distribution; variation is commonly summarized by the standard deviation,\nthe interquartile range, and other quantiles. Each summary has its own interpretation: for\nexample, the mean is the posterior expectation of the parameter, and the mode may be\ninterpreted as the single ‘most likely’ value, given the data (and the model). Furthermore,\nas we shall see, much practical inference relies on the use of normal approximations, often\nimproved by applying a symmetrizing transformation to θ, and here the mean and the stan-\ndard deviation play key roles. The mode is important in computational strategies for more\ncomplex problems because it is often easier to compute than the mean or median.\n\nWhen the posterior distribution has a closed form, such as the beta distribution in\nthe current example, summaries such as the mean, median, and standard deviation of\nthe posterior distribution are often available in closed form. For example, applying the\ndistributional results in Appendix A, the mean of the beta distribution in (2.3) is y+1\n\nn+2 , and\nthe mode is y\n\nn , which is well known from different points of view as the maximum likelihood\nand (minimum variance) unbiased estimate of θ.\n\nPosterior quantiles and intervals\n\nIn addition to point summaries, it is nearly always important to report posterior uncertainty.\nOur usual approach is to present quantiles of the posterior distribution of estimands of\ninterest or, if an interval summary is desired, a central interval of posterior probability,\nwhich corresponds, in the case of a 100(1− α)% interval, to the range of values above and\nbelow which lies exactly 100(α/2)% of the posterior probability. Such interval estimates\nare referred to as posterior intervals. For simple models, such as the binomial and normal,\nposterior intervals can be computed directly from cumulative distribution functions, often\nusing calls to standard computer functions, as we illustrate in Section 2.4 with the example\nof the human sex ratio. In general, intervals can be computed using computer simulations\nfrom the posterior distribution, as described at the end of Section 1.9.\n\nA slightly different summary of posterior uncertainty is the highest posterior density\nregion: the set of values that contains 100(1 − α)% of the posterior probability and also\nhas the characteristic that the density within the region is never lower than that outside.\nSuch a region is identical to a central posterior interval if the posterior distribution is\nunimodal and symmetric. In current practice, the central posterior interval is in common\nuse, partly because it has a direct interpretation as the posterior α/2 and 1−α/2 quantiles,\nand partly because it is directly computed using posterior simulations. Figure 2.2 shows\na case where different posterior summaries look much different: the 95% central interval\nincludes the area of zero probability in the center of the distribution, whereas the 95%\nhighest posterior density region comprises two disjoint intervals. In this situation, the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n34 2. SINGLE-PARAMETER MODELS\n\nhighest posterior density region is more cumbersome but conveys more information than\nthe central interval; however, it is probably better not to try to summarize this bimodal\ndensity by any single interval. The central interval and the highest posterior density region\ncan also differ substantially when the posterior density is highly skewed.\n\n2.4 Informative prior distributions\n\nIn the binomial example, we have so far considered only the uniform prior distribution for\nθ. How can this specification be justified, and how in general do we approach the problem\nof constructing prior distributions?\n\nWe consider two basic interpretations that can be given to prior distributions. In the\npopulation interpretation, the prior distribution represents a population of possible parame-\nter values, from which the θ of current interest has been drawn. In the more subjective state\nof knowledge interpretation, the guiding principle is that we must express our knowledge\n(and uncertainty) about θ as if its value could be thought of as a random realization from\nthe prior distribution. For many problems, such as estimating the probability of failure in\na new industrial process, there is no perfectly relevant population of θ’s from which the\ncurrent θ has been drawn, except in hypothetical contemplation. Typically, the prior distri-\nbution should include all plausible values of θ, but the distribution need not be realistically\nconcentrated around the true value, because often the information about θ contained in the\ndata will far outweigh any reasonable prior probability specification.\n\nIn the binomial example, we have seen that the uniform prior distribution for θ im-\nplies that the prior predictive distribution for y (given n) is uniform on the discrete set\n{0, 1, . . . , n}, giving equal probability to the n+1 possible values. In his original treatment\nof this problem (described in the Historical Note in Section 2.1), Bayes’ justification for the\nuniform prior distribution appears to have been based on this observation; the argument\nis appealing because it is expressed entirely in terms of the observable quantities y and n.\nLaplace’s rationale for the uniform prior density was less clear, but subsequent interpre-\ntations ascribe to him the so-called ‘principle of insufficient reason,’ which claims that a\nuniform specification is appropriate if nothing is known about θ. We shall discuss in Section\n2.8 the weaknesses of the principle of insufficient reason as a general approach for assigning\nprobability distributions.\n\nAt this point, we discuss some of the issues that arise in assigning a prior distribution\nthat reflects substantive information.\n\nBinomial example with different prior distributions\n\nWe first pursue the binomial model in further detail using a parametric family of prior\ndistributions that includes the uniform as a special case. For mathematical convenience, we\nconstruct a family of prior densities that lead to simple posterior densities.\n\nConsidered as a function of θ, the likelihood (2.1) is of the form,\n\np(y|θ) ∝ θa(1 − θ)b.\n\nThus, if the prior density is of the same form, with its own values a and b, then the posterior\ndensity will also be of this form. We will parameterize such a prior density as\n\np(θ) ∝ θα−1(1− θ)β−1,\n\nwhich is a beta distribution with parameters α and β: θ ∼ Beta(α, β). Comparing p(θ) and\np(y|θ) suggests that this prior density is equivalent to α− 1 prior successes and β − 1 prior\nfailures. The parameters of the prior distribution are often referred to as hyperparameters.\nThe beta prior distribution is indexed by two hyperparameters, which means we can specify\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 35\n\na particular prior distribution by fixing two features of the distribution, for example its mean\nand variance; see (A.3) on page 585.\n\nFor now, assume that we can select reasonable values α and β. Appropriate methods\nfor working with unknown hyperparameters in certain problems are described in Chapter\n5. The posterior density for θ is\n\np(θ|y) ∝ θy(1− θ)n−yθα−1(1− θ)β−1\n\n= θy+α−1(1− θ)n−y+β−1\n\n= Beta(θ|α + y, β + n− y).\n\nThe property that the posterior distribution follows the same parametric form as the\nprior distribution is called conjugacy; the beta prior distribution is a conjugate family for\nthe binomial likelihood. The conjugate family is mathematically convenient in that the\nposterior distribution follows a known parametric form. If information is available that\ncontradicts the conjugate parametric family, it may be necessary to use a more realistic, if\ninconvenient, prior distribution (just as the binomial likelihood may need to be replaced by\na more realistic likelihood in some cases).\n\nTo continue with the binomial model with beta prior distribution, the posterior mean of\nθ, which may be interpreted as the posterior probability of success for a future draw from\nthe population, is now\n\nE(θ|y) = α+ y\n\nα+ β + n\n,\n\nwhich always lies between the sample proportion, y/n, and the prior mean, α/(α+ β); see\nExercise 2.5b. The posterior variance is\n\nvar(θ|y) = (α+ y)(β + n− y)\n(α+ β + n)2(α+ β + n+ 1)\n\n=\nE(θ|y)[1− E(θ|y)]\nα+ β + n+ 1\n\n.\n\nAs y and n− y become large with fixed α and β, E(θ|y) ≈ y/n and var(θ|y) ≈ 1\nn\ny\nn (1−\n\ny\nn ),\n\nwhich approaches zero at the rate 1/n. In the limit, the parameters of the prior distribution\nhave no influence on the posterior distribution.\n\nIn fact, as we shall see in more detail in Chapter 4, the central limit theorem of proba-\nbility theory can be put in a Bayesian context to show:\n\n(\nθ − E(θ|y)√\n\nvar(θ|y)\n\n∣∣∣∣∣ y\n)\n→ N(0, 1).\n\nThis result is often used to justify approximating the posterior distribution with a normal\ndistribution. For the binomial parameter θ, the normal distribution is a more accurate\napproximation in practice if we transform θ to the logit scale; that is, performing inference\nfor log(θ/(1 − θ)) instead of θ itself, thus expanding the probability space from [0, 1] to\n(−∞,∞), which is more fitting for a normal approximation.\n\nConjugate prior distributions\n\nConjugacy is formally defined as follows. If F is a class of sampling distributions p(y|θ),\nand P is a class of prior distributions for θ, then the class P is conjugate for F if\n\np(θ|y) ∈ P for all p(·|θ) ∈ F and p(·) ∈ P .\n\nThis definition is formally vague since if we choose P as the class of all distributions, then\nP is always conjugate no matter what class of sampling distributions is used. We are most\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n36 2. SINGLE-PARAMETER MODELS\n\ninterested in natural conjugate prior families, which arise by taking P to be the set of all\ndensities having the same functional form as the likelihood.\n\nConjugate prior distributions have the practical advantage, in addition to computational\nconvenience, of being interpretable as additional data, as we have seen for the binomial\nexample and will also see for the normal and other standard models in Sections 2.5 and 2.6.\n\nNonconjugate prior distributions\n\nThe basic justification for the use of conjugate prior distributions is similar to that for using\nstandard models (such as binomial and normal) for the likelihood: it is easy to understand\nthe results, which can often be put in analytic form, they are often a good approximation,\nand they simplify computations. Also, they will be useful later as building blocks for more\ncomplicated models, including in many dimensions, where conjugacy is typically impossible.\nFor these reasons, conjugate models can be good starting points; for example, mixtures of\nconjugate families can sometimes be useful when simple conjugate distributions are not\nreasonable (see Exercise 2.4).\n\nAlthough they can make interpretations of posterior inferences less transparent and\ncomputation more difficult, nonconjugate prior distributions do not pose any new conceptual\nproblems. In practice, for complicated models, conjugate prior distributions may not even\nbe possible. Section 2.4 and Exercises 2.10 and 2.11 present examples of nonconjugate\ncomputation; a more extensive nonconjugate example, an analysis of a bioassay experiment,\nappears in Section 3.7.\n\nConjugate prior distributions, exponential families, and sufficient statistics\n\nWe close this section by relating conjugate families of distributions to the classical concepts\nof exponential families and sufficient statistics. Readers who are unfamiliar with these\nconcepts can skip ahead to the example with no loss.\n\nProbability distributions that belong to an exponential family have natural conjugate\nprior distributions, so we digress at this point to review the definition of exponential families;\nfor complete generality in this section, we allow data points yi and parameters θ to be\nmultidimensional. The class F is an exponential family if all its members have the form,\n\np(yi|θ) = f(yi)g(θ)e\nφ(θ)Tu(yi).\n\nThe factors φ(θ) and u(yi) are, in general, vectors of equal dimension to that of θ. The\nvector φ(θ) is called the ‘natural parameter’ of the family F . The likelihood corresponding\nto a sequence y = (y1, . . . , yn) of independent and identically distributed observations is\n\np(y|θ) =\n(\n\nn∏\n\ni=1\n\nf(yi)\n\n)\ng(θ)n exp\n\n(\nφ(θ)T\n\nn∑\n\ni=1\n\nu(yi)\n\n)\n.\n\nFor all n and y, this has a fixed form (as a function of θ):\n\np(y|θ) ∝ g(θ)neφ(θ)T t(y), where t(y) =\n\nn∑\n\ni=1\n\nu(yi).\n\nThe quantity t(y) is said to be a sufficient statistic for θ, because the likelihood for θ\ndepends on the data y only through the value of t(y). Sufficient statistics are useful in\nalgebraic manipulations of likelihoods and posterior distributions. If the prior density is\nspecified as\n\np(θ) ∝ g(θ)ηeφ(θ)T ν ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 37\n\nthen the posterior density is\n\np(θ|y) ∝ g(θ)η+neφ(θ)T (ν+t(y)),\n\nwhich shows that this choice of prior density is conjugate. It has been shown that, in general,\nthe exponential families are the only classes of distributions that have natural conjugate\nprior distributions, since, apart from certain irregular cases, the only distributions having\na fixed number of sufficient statistics for all n are of the exponential type. We have already\ndiscussed the binomial distribution, where for the likelihood p(y|θ, n) = Bin(y|n, θ) with n\nknown, the conjugate prior distributions on θ are beta distributions. It is left as an exercise\nto show that the binomial is an exponential family with natural parameter logit(θ).\n\nExample. Probability of a girl birth given placenta previa\nAs a specific example of a factor that may influence the sex ratio, we consider the\nmaternal condition placenta previa, an unusual condition of pregnancy in which the\nplacenta is implanted low in the uterus, obstructing the fetus from a normal vaginal\ndelivery. An early study concerning the sex of placenta previa births in Germany found\nthat of a total of 980 births, 437 were female. How much evidence does this provide\nfor the claim that the proportion of female births in the population of placenta previa\nbirths is less than 0.485, the proportion of female births in the general population?\n\nAnalysis using a uniform prior distribution. Under a uniform prior distribution for\nthe probability of a girl birth, the posterior distribution is Beta(438, 544). Exact\nsummaries of the posterior distribution can be obtained from the properties of the\nbeta distribution (Appendix A): the posterior mean of θ is 0.446 and the posterior\nstandard deviation is 0.016. Exact posterior quantiles can be obtained using numerical\nintegration of the beta density, which in practice we perform by a computer function\ncall; the median is 0.446 and the central 95% posterior interval is [0.415, 0.477]. This\n95% posterior interval matches, to three decimal places, the interval that would be\nobtained by using a normal approximation with the calculated posterior mean and\nstandard deviation. Further discussion of the approximate normality of the posterior\ndistribution is given in Chapter 4.\nIn many situations it is not feasible to perform calculations on the posterior density\nfunction directly. In such cases it can be particularly useful to use simulation from the\nposterior distribution to obtain inferences. The first histogram in Figure 2.3 shows the\ndistribution of 1000 draws from the Beta(438, 544) posterior distribution. An estimate\nof the 95% posterior interval, obtained by taking the 25th and 976th of the 1000\nordered draws, is [0.415, 0.476], and the median of the 1000 draws from the posterior\ndistribution is 0.446. The sample mean and standard deviation of the 1000 draws are\n0.445 and 0.016, almost identical to the exact results. A normal approximation to the\n95% posterior interval is [0.445 ± 1.96 · 0.016] = [0.414, 0.476]. Because of the large\nsample and the fact that the distribution of θ is concentrated away from zero and one,\nthe normal approximation works well in this example.\nAs already noted, when estimating a proportion, the normal approximation is gener-\nally improved by applying it to the logit transform, log( θ\n\n1−θ ), which transforms the\nparameter space from the unit interval to the real line. The second histogram in Figure\n2.3 shows the distribution of the transformed draws. The estimated posterior mean\nand standard deviation on the logit scale based on 1000 draws are −0.220 and 0.065.\nA normal approximation to the 95% posterior interval for θ is obtained by inverting\nthe 95% interval on the logit scale [−0.220± 1.96 · 0.065], which yields [0.414, 0.477]\non the original scale. The improvement from using the logit scale is most noticeable\nwhen the sample size is small or the distribution of θ includes values near zero or one.\nIn any real data analysis, it is important to keep the applied context in mind. The pa-\nrameter of interest in this example is traditionally expressed as the ‘sex ratio,’ (1−θ)/θ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n38 2. SINGLE-PARAMETER MODELS\n\nFigure 2.3 Draws from the posterior distribution of (a) the probability of female birth, θ; (b) the\nlogit transform, logit(θ); (c) the male-to-female sex ratio, φ = (1− θ)/θ.\n\nParameters of the Summaries of the\nprior distribution posterior distribution\n\nPosterior 95% posterior\nα\n\nα+β\nα+ β median of θ interval for θ\n\n0.500 2 0.446 [0.415, 0.477]\n0.485 2 0.446 [0.415, 0.477]\n0.485 5 0.446 [0.415, 0.477]\n0.485 10 0.446 [0.415, 0.477]\n0.485 20 0.447 [0.416, 0.478]\n0.485 100 0.450 [0.420, 0.479]\n0.485 200 0.453 [0.424, 0.481]\n\nTable 2.1 Summaries of the posterior distribution of θ, the probability of a girl birth given placenta\nprevia, under a variety of conjugate prior distributions.\n\nthe ratio of male to female births. The posterior distribution of the ratio is illustrated\nin the third histogram. The posterior median of the sex ratio is 1.24, and the 95%\nposterior interval is [1.10, 1.41]. The posterior distribution is concentrated on values\nfar above the usual European-race sex ratio of 1.06, implying that the probability of\na female birth given placenta previa is less than in the general population.\n\nAnalysis using different conjugate prior distributions. The sensitivity of posterior\ninference about θ to the proposed prior distribution is exhibited in Table 2.1. The\nfirst row corresponds to the uniform prior distribution, α=1, β=1, and subsequent\nrows of the table use prior distributions that are increasingly concentrated around\n0.485, the proportion of female births in the general population. The first column\nshows the prior mean for θ, and the second column indexes the amount of prior\ninformation, as measured by α+ β; recall that α+ β − 2 is, in some sense, equivalent\nto the number of prior observations. Posterior inferences based on a large sample are\nnot particularly sensitive to the prior distribution. Only at the bottom of the table,\nwhere the prior distribution contains information equivalent to 100 or 200 births, are\nthe posterior intervals pulled noticeably toward the prior distribution, and even then,\nthe 95% posterior intervals still exclude the prior mean.\n\nAnalysis using a nonconjugate prior distribution. As an alternative to the conjugate\nbeta family for this problem, we might prefer a prior distribution that is centered\naround 0.485 but is flat far away from this value to admit the possibility that the\ntruth is far away. The piecewise linear prior density in Figure 2.4a is an example\nof a prior distribution of this form; 40% of the probability mass is outside the inter-\nval [0.385, 0.585]. This prior distribution has mean 0.493 and standard deviation 0.21,\nsimilar to the standard deviation of a beta distribution with α+β = 5. The unnormal-\nized posterior distribution is obtained at a grid of θ values, (0.000, 0.001, . . . , 1.000),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 39\n\nFigure 2.4 (a) Prior density for θ in an example nonconjugate analysis of birth ratio example; (b)\nhistogram of 1000 draws from a discrete approximation to the posterior density. Figures are plotted\non different scales.\n\nby multiplying the prior density and the binomial likelihood at each point. Poste-\nrior simulations can be obtained by normalizing the distribution on the discrete grid\nof θ values. Figure 2.4b is a histogram of 1000 draws from the discrete posterior\ndistribution. The posterior median is 0.448, and the 95% central posterior interval is\n[0.419, 0.480]. Because the prior distribution is overwhelmed by the data, these results\nmatch those in Table 2.1 based on beta distributions. In taking the grid approach, it\nis important to avoid grids that are too coarse and distort a significant portion of the\nposterior mass.\n\n2.5 Normal distribution with known variance\n\nThe normal distribution is fundamental to most statistical modeling. The central limit\ntheorem helps to justify using the normal likelihood in many statistical problems, as an\napproximation to a less analytically convenient actual likelihood. Also, as we shall see in\nlater chapters, even when the normal distribution does not itself provide a good model fit,\nit can be useful as a component of a more complicated model involving t or finite mixture\ndistributions. For now, we simply work through the Bayesian results assuming the normal\nmodel is appropriate. We derive results first for a single data point and then for the general\ncase of many data points.\n\nLikelihood of one data point\n\nAs the simplest first case, consider a single scalar observation y from a normal distribution\nparameterized by a mean θ and variance σ2, where for this initial development we assume\nthat σ2 is known. The sampling distribution is\n\np(y|θ) = 1√\n2πσ\n\ne−\n1\n\n2σ2 (y−θ)2.\n\nConjugate prior and posterior distributions\n\nConsidered as a function of θ, the likelihood is an exponential of a quadratic form in θ, so\nthe family of conjugate prior densities looks like\n\np(θ) = eAθ\n2+Bθ+C .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n40 2. SINGLE-PARAMETER MODELS\n\nWe parameterize this family as\n\np(θ) ∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n)\n;\n\nthat is, θ ∼ N(µ0, τ\n2\n0 ), with hyperparameters µ0 and τ20 . As usual in this preliminary\n\ndevelopment, we assume that the hyperparameters are known.\nThe conjugate prior density implies that the posterior distribution for θ is the exponential\n\nof a quadratic form and thus normal, but some algebra is required to reveal its specific\nform. In the posterior density, all variables except θ are regarded as constants, giving the\nconditional density,\n\np(θ|y) ∝ exp\n\n(\n−1\n\n2\n\n(\n(y − θ)2\nσ2\n\n+\n(θ − µ0)\n\n2\n\nτ20\n\n))\n.\n\nExpanding the exponents, collecting terms and then completing the square in θ (see Exercise\n2.14(a) for details) gives\n\np(θ|y) ∝ exp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\n, (2.9)\n\nthat is, θ|y ∼ N(µ1, τ\n2\n1 ), where\n\nµ1 =\n\n1\nτ2\n0\nµ0 +\n\n1\nσ2 y\n\n1\nτ2\n0\n+ 1\n\nσ2\n\nand\n1\n\nτ21\n=\n\n1\n\nτ20\n+\n\n1\n\nσ2\n. (2.10)\n\nPrecisions of the prior and posterior distributions. In manipulating normal distributions,\nthe inverse of the variance plays a prominent role and is called the precision. The algebra\nabove demonstrates that for normal data and normal prior distribution (each with known\nprecision), the posterior precision equals the prior precision plus the data precision.\n\nThere are several different ways of interpreting the form of the posterior mean, µ1. In\n(2.10), the posterior mean is expressed as a weighted average of the prior mean and the\nobserved value, y, with weights proportional to the precisions. Alternatively, we can express\nµ1 as the prior mean adjusted toward the observed y,\n\nµ1 = µ0 + (y − µ0)\nτ20\n\nσ2 + τ20\n,\n\nor as the data ‘shrunk’ toward the prior mean,\n\nµ1 = y − (y − µ0)\nσ2\n\nσ2 + τ20\n.\n\nEach formulation represents the posterior mean as a compromise between the prior mean\nand the observed value.\n\nAt the extremes, the posterior mean equals the prior mean or the observed data:\n\nµ1 = µ0 if y = µ0 or τ20 = 0;\n\nµ1 = y if y = µ0 or σ2 = 0.\n\nIf τ20 = 0, the prior distribution is infinitely more precise than the data, and so the posterior\nand prior distributions are identical and concentrated at the value µ0. If σ2 = 0, the data\nare perfectly precise, and the posterior distribution is concentrated at the observed value,\ny. If y = µ0, the prior and data means coincide, and the posterior mean must also fall at\nthis point.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\n\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, ỹ, p(ỹ|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ỹ|y) =\n\n∫\np(ỹ|θ)p(θ|y)dθ\n\n∝\n∫\n\nexp\n\n(\n− 1\n\n2σ2\n(ỹ − θ)2\n\n)\nexp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\ndθ.\n\nThe first line above holds because the distribution of the future observation, ỹ, given θ,\ndoes not depend on the past data, y. We can determine the distribution of ỹ more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (ỹ, θ); hence ỹ and θ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of ỹ is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(ỹ|θ) = θ and var(ỹ|θ) = σ2, along\nwith identities (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|θ, y)|y) = E(θ|y) = µ1,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|θ, y)|y) + var(E(ỹ|θ, y)|y)\n= E(σ2|y) + var(θ|y)\n= σ2 + τ21 .\n\nThus, the posterior predictive distribution of ỹ has mean equal to the posterior mean of θ\nand two components of variance: the predictive variance σ2 from the model and the variance\nτ21 due to posterior uncertainty in θ.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (y1, . . . , yn) is available. Proceeding formally, the posterior density is\n\np(θ|y) ∝ p(θ)p(y|θ)\n\n= p(θ)\n\nn∏\n\ni=1\n\np(yi|θ)\n\n∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n) n∏\n\ni=1\n\nexp\n\n(\n− 1\n\n2σ2\n(yi − θ)2\n\n)\n\n∝ exp\n\n(\n−1\n\n2\n\n(\n1\n\nτ20\n(θ − µ0)\n\n2 +\n1\n\nσ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n))\n\n.\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = 1\n\nn\n\n∑\ni yi; that is, y is a sufficient statistic\n\nin this model. In fact, since y|θ, σ2 ∼ N(θ, σ2/n), the results derived for the single normal\nobservation apply immediately (treating y as the single observation) to give\n\np(θ|y1 . . . , yn) = p(θ|y) = N(θ|µn, τ2n), (2.11)\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, y, p(y|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ily) = / p(Gl0)p(6ly)a6\n\nx few(-daa-0\") an (-y0-mr)an\n\nThe first line above holds because the distribution of the future observation, y, given 6,\ndoes not depend on the past data, y. We can determine the distribution of y more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (g, 8); hence 7 and @ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of y is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(g|@) = 6 and var(g|9) = 07, along\nwith identities (2.7) and (2.8):\n\nE(gly) = E(E(g|@, y)ly) = Ely) = 11,\nand\n\nvar(yly) = E(var(g|@,y)|y) + var(E(g/6, y)|y)\nE(o?|y) + var(4|y)\n\nor +77.\n\nThus, the posterior predictive distribution of y has mean equal to the posterior mean of 0\nand two components of variance: the predictive variance a? from the model and the variance\n7? due to posterior uncertainty in 0.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (yi,---, Yn) is available. Proceeding formally, the posterior density is\n\nPly) x p(O)p(yl@)\n\nR\noO)\n%\nso}\n“~\nwo |\naN\nOw]\nsS\n|\n=\nS\n“—\"\nbo\n+4\nSI —\nil =\nmn\nS\n|\nS\niw)\nNe\nNe\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = + 2, wii that is, 7 is a sufficient statistic\nin this model. In fact, since 9|9,07 ~ N(0,07/n), the results derived for the single normal\nobservation apply immediately (treating 7 as the single observation) to give\n\nP(O\\y1 -- Yn) = PO) = N(O|kins Tr): (2.11)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n42 2. SINGLE-PARAMETER MODELS\n\nwhere\n\nµn =\n\n1\nτ2\n0\nµ0 +\n\nn\nσ2 y\n\n1\nτ2\n0\n+ n\n\nσ2\n\nand\n1\n\nτ2n\n=\n\n1\n\nτ20\n+\n\nn\n\nσ2\n. (2.12)\n\nIncidentally, the same result is obtained by adding information for the data y1, y2, . . . , yn\none point at a time, using the posterior distribution at each step as the prior distribution\nfor the next (see Exercise 2.14(c)).\n\nIn the expressions for the posterior mean and variance, the prior precision, 1/τ20 , and\nthe data precision, n/σ2, play equivalent roles, so if n is large, the posterior distribution\nis largely determined by σ2 and the sample value y. For example, if τ20 = σ2, then the\nprior distribution has the same weight as one extra observation with the value µ0. More\nspecifically, as τ0 →∞ with n fixed, or as n→∞ with τ20 fixed, we have:\n\np(θ|y) ≈ N(θ|y, σ2/n), (2.13)\n\nwhich is, in practice, a good approximation whenever prior beliefs are relatively diffuse over\nthe range of θ where the likelihood is substantial.\n\n2.6 Other standard single-parameter models\n\nRecall that, in general, the posterior density, p(θ|y), has no closed-form expression; the\nnormalizing constant, p(y), is often especially difficult to compute due to the integral (1.3).\nMuch formal Bayesian analysis concentrates on situations where closed forms are available;\nsuch models are sometimes unrealistic, but their analysis often provides a useful starting\npoint when it comes to constructing more realistic models.\n\nThe standard distributions—binomial, normal, Poisson, and exponential—have natural\nderivations from simple probability models. As we have already discussed, the binomial\ndistribution is motivated from counting exchangeable outcomes, and the normal distribu-\ntion applies to a random variable that is the sum of many exchangeable or independent\nterms. We will also have occasion to apply the normal distribution to the logarithm of all-\npositive data, which would naturally apply to observations that are modeled as the product\nof many independent multiplicative factors. The Poisson and exponential distributions arise\nas the number of counts and the waiting times, respectively, for events modeled as occur-\nring exchangeably in all time intervals; that is, independently in time, with a constant rate\nof occurrence. We will generally construct realistic probability models for more compli-\ncated outcomes by combinations of these basic distributions. For example, in Section 22.2,\nwe model the reaction times of schizophrenic patients in a psychological experiment as a\nbinomial mixture of normal distributions on the logarithmic scale.\n\nEach of these standard models has an associated family of conjugate prior distributions,\nwhich we discuss in turn.\n\nNormal distribution with known mean but unknown variance\n\nThe normal model with known mean θ and unknown variance is an important example,\nnot necessarily for its direct applied value, but as a building block for more complicated,\nuseful models, most immediately the normal distribution with unknown mean and variance,\nwhich we cover in Section 3.2. In addition, the normal distribution with known mean but\nunknown variance provides an introductory example of the estimation of a scale parameter.\n\nFor p(y|θ, σ2) = N(y|θ, σ2), with θ known and σ2 unknown, the likelihood for a vector\ny of n independent and identically distributed observations is\n\np(y|σ2) ∝ σ−n exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43\n\n= (σ2)−n/2 exp\n(\n− n\n\n2σ2\nv\n)\n.\n\nThe sufficient statistic is\n\nv =\n1\n\nn\n\nn∑\n\ni=1\n\n(yi − θ)2.\n\nThe corresponding conjugate prior density is the inverse-gamma,\n\np(σ2) ∝ (σ2)−(α+1)e−β/σ\n2\n\n,\n\nwhich has hyperparameters (α, β). A convenient parameterization is as a scaled inverse-χ2\n\ndistribution with scale σ2\n0 and ν0 degrees of freedom (see Appendix A); that is, the prior\n\ndistribution of σ2 is taken to be the distribution of σ2\n0ν0/X , where X is a χ2\n\nν0 random\nvariable. We use the convenient but nonstandard notation, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0).\n\nThe resulting posterior density for σ2 is\n\np(σ2|y) ∝ p(σ2)p(y|σ2)\n\n∝\n(\nσ2\n0\n\nσ2\n\n)ν0/2+1\n\nexp\n\n(\n−ν0σ\n\n2\n0\n\n2σ2\n\n)\n· (σ2)−n/2 exp\n\n(\n−n\n2\n\nv\n\nσ2\n\n)\n\n∝ (σ2)−((n+ν0)/2+1) exp\n\n(\n− 1\n\n2σ2\n(ν0σ\n\n2\n0 + nv)\n\n)\n.\n\nThus,\n\nσ2|y ∼ Inv-χ2\n\n(\nν0 + n,\n\nν0σ\n2\n0 + nv\n\nν0 + n\n\n)\n,\n\nwhich is a scaled inverse-χ2 distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to ν0 observations with average squared deviation σ2\n\n0 .\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate θ, then the probability\ndistribution of a single observation y is\n\np(y|θ) = θye−θ\n\ny!\n, for y = 0, 1, 2, . . . ,\n\nand for a vector y = (y1, . . . , yn) of independent and identically distributed observations,\nthe likelihood is\n\np(y|θ) =\n\nn∏\n\ni=1\n\n1\n\nyi!\nθyie−θ\n\n∝ θt(y)e−nθ,\n\nwhere t(y) =\n∑n\ni=1 yi is the sufficient statistic. We can rewrite the likelihood in exponential\n\nfamily form as\n\np(y|θ) ∝ e−nθet(y) log θ,\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43,\nThe sufficient statistic is\n\nThe corresponding conjugate prior density is the inverse-gamma,\npo?) x (0?) De Ble\",\n\nwhich has hyperparameters (a, 3). A convenient parameterization is as a scaled inverse-y?\ndistribution with scale 2 and vo degrees of freedom (see Appendix A); that is, the prior\ndistribution of 0? is taken to be the distribution of o§vo/X, where X is a x7, random\nvariable. We use the convenient but nonstandard notation, 0? ~ Inv-x?(vo, 08).\n\nThe resulting posterior density for a? is\n\np(o|y) x p(o)p(y|o”)\n2 Vo /2+1 2\n90 Y009 2\\—n/2 nv\n(3) exp ( pa) (a) exp (Fa)\n\n—Un+V 1\nx (a?) (( + 0) /2+1) exp (— spate? + nv) .\n\nThus,\n\nYoo + nu\nYo +n ,\n\no* ly ~ Inv-x? (» +n,\n\nwhich is a scaled inverse-? distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to vp observations with average squared deviation o@.\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate 0, then the probability\ndistribution of a single observation y is\n\nWe?\np(ylO) = Tn for y =0,1,2,...,\n\nand for a vector y = (y1,...,Yn) of independent and identically distributed observations,\nthe likelihood is\n\nnm\n\nLoy\np(y) = [[—e%e~®\n\nja Yt\n\n« Gy e—nF\n\nwhere t(y) = >>\", y; is the sufficient statistic. We can rewrite the likelihood in exponential\nfamily form as\n\np(y|0) x e Mel lee?\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n44 2. SINGLE-PARAMETER MODELS\n\nrevealing that the natural parameter is φ(θ) = log θ, and the natural conjugate prior distri-\nbution is\n\np(θ) ∝ (e−θ)ηeν log θ,\n\nindexed by hyperparameters (η, ν). To put this argument another way, the likelihood is of\nthe form θae−bθ, and so the conjugate prior density must be of the form p(θ) ∝ θAe−Bθ. In\na more conventional parameterization,\n\np(θ) ∝ e−βθθα−1,\n\nwhich is a gamma density with parameters α and β, Gamma(α, β); see Appendix A. Com-\nparing p(y|θ) and p(θ) reveals that the prior density is, in some sense, equivalent to a total\ncount of α− 1 in β prior observations. With this conjugate prior distribution, the posterior\ndistribution is\n\nθ|y ∼ Gamma(α + ny, β + n).\n\nThe negative binomial distribution. With conjugate families, the known form of the prior\nand posterior densities can be used to find the marginal distribution, p(y), using the formula\n\np(y) =\np(y|θ)p(θ)\np(θ|y) .\n\nFor instance, the Poisson model for a single observation, y, has prior predictive distribution\n\np(y) =\nPoisson(y|θ)Gamma(θ|α, β)\nGamma(θ|α+ y, 1 + β)\n\n=\nΓ(α+ y)βα\n\nΓ(α)y!(1 + β)α+y\n,\n\nwhich reduces to\n\np(y) =\n\n(\nα+ y − 1\n\ny\n\n)(\nβ\n\nβ + 1\n\n)α(\n1\n\nβ + 1\n\n)y\n,\n\nwhich is known as the negative binomial density:\n\ny ∼ Neg-bin(α, β).\n\nThe above derivation shows that the negative binomial distribution is a mixture of Poisson\ndistributions with rates, θ, that follow the gamma distribution:\n\nNeg-bin(y|α, β) =\n∫\n\nPoisson(y|θ)Gamma(θ|α, β)dθ.\n\nWe return to the negative binomial distribution in Section 17.2 as a robust alternative to\nthe Poisson distribution.\n\nPoisson model parameterized in terms of rate and exposure\n\nIn many applications, it is convenient to extend the Poisson model for data points y1, . . . , yn\nto the form\n\nyi ∼ Poisson(xiθ), (2.14)\n\nwhere the values xi are known positive values of an explanatory variable, x, and θ is the\nunknown parameter of interest. In epidemiology, the parameter θ is often called the rate,\nand xi is called the exposure of the ith unit. This model is not exchangeable in the yi’s but\nis exchangeable in the pairs (x, y)i. The likelihood for θ in the extended Poisson model is\n\np(y|θ) ∝ θ(\n∑n\n\ni=1\nyi)e−(\n\n∑n\n\ni=1\nxi)θ\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 45\n\n(ignoring factors that do not depend on θ), and so the gamma distribution for θ is conjugate.\nWith prior distribution\n\nθ ∼ Gamma(α, β),\n\nthe resulting posterior distribution is\n\nθ|y ∼ Gamma\n\n(\nα+\n\nn∑\n\ni=1\n\nyi, β +\nn∑\n\ni=1\n\nxi\n\n)\n. (2.15)\n\nEstimating a rate from Poisson data: an idealized example\nSuppose that causes of death are reviewed in detail for a city in the United States for a\nsingle year. It is found that 3 persons, out of a population of 200,000, died of asthma,\ngiving a crude estimated asthma mortality rate in the city of 1.5 cases per 100,000\npersons per year. A Poisson sampling model is often used for epidemiological data of\nthis form. The Poisson model derives from an assumption of exchangeability among\nall small intervals of exposure. Under the Poisson model, the sampling distribution\nof y, the number of deaths in a city of 200,000 in one year, may be expressed as\nPoisson(2.0θ), where θ represents the true underlying long-term asthma mortality rate\nin our city (measured in cases per 100,000 persons per year). In the above notation,\ny = 3 is a single observation with exposure x = 2.0 (since θ is defined in units of\n100,000 people) and unknown rate θ. We can use knowledge about asthma mortality\nrates around the world to construct a prior distribution for θ and then combine the\ndatum y = 3 with that prior distribution to obtain a posterior distribution.\n\nSetting up a prior distribution. What is a sensible prior distribution for θ? Reviews\nof asthma mortality rates around the world suggest that mortality rates above 1.5\nper 100,000 people are rare in Western countries, with typical asthma mortality rates\naround 0.6 per 100,000. Trial-and-error exploration of the properties of the gamma dis-\ntribution, the conjugate prior family for this problem, reveals that a Gamma(3.0, 5.0)\ndensity provides a plausible prior density for the asthma mortality rate in this example\nif we assume exchangeability between this city and other cities and this year and other\nyears. The mean of this prior distribution is 0.6 (with a mode of 0.4), and 97.5% of\nthe mass of the density lies below 1.44. In practice, specifying a prior mean sets the\nratio of the two gamma parameters, and then the shape parameter can be altered by\ntrial and error to match the prior knowledge about the tail of the distribution.\n\nPosterior distribution. The result in (2.15) shows that the posterior distribution\nof θ for a Gamma(α, β) prior distribution is Gamma(α + y, β + x) in this case.\nWith the prior distribution and data described, the posterior distribution for θ is\nGamma(6.0, 7.0), which has mean 0.86—substantial shrinkage has occurred toward\nthe prior distribution. A histogram of 1000 draws from the posterior distribution for\nθ is shown as Figure 2.5a. For example, the posterior probability that the long-term\ndeath rate from asthma in our city is more than 1.0 per 100,000 per year, computed\nfrom the gamma posterior density, is 0.30.\n\nPosterior distribution with additional data. To consider the effect of additional data,\nsuppose that ten years of data are obtained for the city in our example, instead of just\none, and it is found that the mortality rate of 1.5 per 100,000 is maintained; we find\ny = 30 deaths over 10 years. Assuming the population is constant at 200,000, and\nassuming the outcomes in the ten years are independent with constant long-term rate\nθ, the posterior distribution of θ is then Gamma(33.0, 25.0); Figure 2.5b displays 1000\ndraws from this distribution. The posterior distribution is much more concentrated\nthan before, and it still lies between the prior distribution and the data. After ten\nyears of data, the posterior mean of θ is 1.32, and the posterior probability that θ\nexceeds 1.0 is 0.93.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n46 2. SINGLE-PARAMETER MODELS\n\nFigure 2.5 Posterior density for θ, the asthma mortality rate in cases per 100,000 persons per year,\nwith a Gamma(3.0, 5.0) prior distribution: (a) given y = 3 deaths out of 200,000 persons; (b) given\ny = 30 deaths in 10 years for a constant population of 200,000. The histograms appear jagged\nbecause they are constructed from only 1000 random draws from the posterior distribution in each\ncase.\n\nExponential model\n\nThe exponential distribution is commonly used to model ‘waiting times’ and other continu-\nous, positive, real-valued random variables, often measured on a time scale. The sampling\ndistribution of an outcome y, given parameter θ, is\n\np(y|θ) = θ exp(−yθ), for y > 0,\n\nand θ = 1/E(y|θ) is called the ‘rate.’ Mathematically, the exponential is a special case of the\ngamma distribution with the parameters (α, β) = (1, θ). In this case, however, it is being\nused as a sampling distribution for an outcome y, not a prior distribution for a parameter\nθ, as in the Poisson example.\n\nThe exponential distribution has a ‘memoryless’ property that makes it a natural model\nfor survival or lifetime data; the probability that an object survives an additional length of\ntime t is independent of the time elapsed to this point: Pr(y>t+s | y>s, θ) = Pr(y>t | θ) for\nany s, t. The conjugate prior distribution for the exponential parameter θ, as for the Poisson\nmean, is Gamma(θ|α, β) with corresponding posterior distribution Gamma(θ|α+1, β+y).\nThe sampling distribution of n independent exponential observations, y = (y1, . . . , yn), with\nconstant rate θ is\n\np(y|θ) = θn exp(−nyθ), for y ≥ 0,\n\nwhich when viewed as the likelihood of θ, for fixed y, is proportional to a Gamma(n+1, ny)\ndensity. Thus the Gamma(α, β) prior distribution for θ can be viewed as α−1 exponential\nobservations with total waiting time β (see Exercise 2.19).\n\n2.7 Example: informative prior distribution for cancer rates\n\nAt the end of Section 2.4, we considered the effect of the prior distribution on inference\ngiven a fixed quantity of data. Here, in contrast, we consider a large set of inferences, each\nbased on different data but with a common prior distribution. In addition to illustrating\nthe role of the prior distribution, this example introduces hierarchical modeling, to which\nwe return in Chapter 5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 47\n\nFigure 2.6 The counties of the United States with the highest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Why are most of the shaded counties in\nthe middle of the country? See Section 2.7 for discussion.\n\nFigure 2.7 The counties of the United States with the lowest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Surprisingly, the pattern is somewhat\nsimilar to the map of the highest rates, shown in Figure 2.6.\n\nA puzzling pattern in a map\n\nFigure 2.6 shows the counties in the United States with the highest kidney cancer death\nrates during the 1980s.1 The most noticeable pattern in the map is that many of the\ncounties in the Great Plains in the middle of the country, but relatively few counties near\nthe coasts, are shaded.\n\nWhen shown the map, people come up with many theories to explain the dispropor-\ntionate shading in the Great Plains: perhaps the air or the water is polluted, or the people\ntend not to seek medical care so the cancers get detected too late to treat, or perhaps their\ndiet is unhealthy . . . These conjectures may all be true but they are not actually needed\nto explain the patterns in Figure 2.6. To see this, look at Figure 2.7, which plots the 10%\nof counties with the lowest kidney cancer death rates. These are also mostly in the middle\nof the country. So now we need to explain why these areas have the lowest, as well as the\nhighest, rates.\n\n1The rates are age-adjusted and restricted to white males, issues which need not concern us here.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n48 2. SINGLE-PARAMETER MODELS\n\nThe issue is sample size. Consider a county of population 1000. Kidney cancer is a\nrare disease, and, in any ten-year period, a county of 1000 will probably have zero kidney\ncancer deaths, so that it will be tied for the lowest rate in the country and will be shaded\nin Figure 2.7. However, there is a chance the county will have one kidney cancer death\nduring the decade. If so, it will have a rate of 1 per 10,000 per year, which is high enough\nto put it in the top 10% so that it will be shaded in Figure 2.6. The Great Plains has many\nlow-population counties, and so it is overrepresented in both maps. There is no evidence\nfrom these maps that cancer rates are particularly high there.\n\nBayesian inference for the cancer death rates\n\nThe misleading patterns in the maps of raw rates suggest that a model-based approach to\nestimating the true underlying rates might be helpful. In particular, it is natural to estimate\nthe underlying cancer death rate in each county j using the model\n\nyj ∼ Poisson(10njθj), (2.16)\n\nwhere yj is the number of kidney cancer deaths in county j from 1980–1989, nj is the\npopulation of the county, and θj is the underlying rate in units of deaths per person per\nyear. In this notation, the maps in Figures 2.6 and 2.7 are plotting the raw rates,\n\nyj\n10nj\n\n.\n\n(Here we are ignoring the age-standardization, although a generalization of the model to\nallow for this would be possible.)\n\nThis model differs from (2.14) in that θj varies between counties, so that (2.16) is a\nseparate model for each of the counties in the U.S. We use the subscript j (rather than i)\nin (2.16) to emphasize that these are separate parameters, each being estimated from its\nown data. Were we performing inference for just one of the counties, we would simply write\ny ∼ Poisson(10nθ).\n\nTo perform Bayesian inference, we need a prior distribution for the unknown rate θj .\nFor convenience we use a gamma distribution, which is conjugate to the Poisson. As we\nshall discuss later, a gamma distribution with parameters α = 20 and β = 430,000 is a\nreasonable prior distribution for underlying kidney cancer death rates in the counties of\nthe U.S. during this period. This prior distribution has a mean of α\n\nβ = 4.65 × 10−5 and\n\nstandard deviation\n√\nα\nβ = 1.04× 10−5.\n\nThe posterior distribution of θj is then,\n\nθj |yj ∼ Gamma(20 + yj , 430,000+ 10nj),\n\nwhich has mean and variance,\n\nE(θj |yj) =\n20 + yj\n\n430,000 + 10nj\n\nvar(θj |yj) =\n20 + yj\n\n(430,000 + 10nj)2\n.\n\nThe posterior mean can be viewed as a weighted average of the raw rate,\nyj\n\n10nj\n, and the\n\nprior mean, αβ = 4.65× 10−5. (For a similar calculation, see Exercise 2.5.)\n\nRelative importance of the local data and the prior distribution\n\nInference for a small county. The relative weighting of prior information and data depends\non the population size nj . For example, consider a small county with nj = 1000:\n\n• For this county, if yj = 0, then the raw death rate is 0 but the posterior mean is\n20\n\n440,000 = 4.55× 10−5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 49\n\nFigure 2.8 (a) Kidney cancer death rates yj/(10nj) vs. population size nj. (b) Replotted on the\nscale of log10 population to see the data more clearly. The patterns come from the discreteness of\nthe data (nj = 0, 1, 2, . . .).\n\n• If yj = 1, then the raw death rate is 1 per 1000 per 10 years, or 10−4 per person-year\n(about twice as high as the national mean), but the posterior mean is only 21\n\n440,000 =\n\n4.77× 10−5.\n\n• If yj = 2, then the raw death rate is an extremely high 2 × 10−4 per person-year, but\nthe posterior mean is still only 22\n\n440,000 = 5.00× 10−5.\n\nWith such a small population size, the data are dominated by the prior distribution.\n\nBut how likely, a priori, is it that yj will equal 0, 1, 2, and so forth, for this county with\nnj = 1000? This is determined by the predictive distribution, the marginal distribution\nof yj , averaging over the prior distribution of θj . As discussed in Section 2.6, the Poisson\nmodel with gamma prior distribution has a negative binomial predictive distribution:\n\nyj ∼ Neg-bin\n\n(\nα,\n\nβ\n\n10nj\n\n)\n.\n\nIt is perhaps even simpler to simulate directly the predictive distribution of yj as follows:\n(1) draw 500 (say) values of θj from the Gamma(20, 430,000) distribution; (2) for each of\nthese, draw one value yj from the Poisson distribution with parameter 10,000 θj. Of 500\nsimulations of yj produced in this way, 319 were 0’s, 141 were 1’s, 33 were 2’s, and 5 were\n3’s.\n\nInference for a large county. Now consider a large county with nj = 1 million. How\nmany cancer deaths yj might we expect to see in a ten-year period? Again we can use\nthe Gamma(20, 430,000) and Poisson(107 θj) distributions to simulate 500 values yj from\nthe predictive distribution. Doing this we found a median of 473 and a 50% interval of\n[393, 545]. The raw death rate in such a county is then as likely or not to fall between\n3.93× 10−5 and 5.45× 10−5.\n\nWhat about the Bayesianly estimated or ‘Bayes-adjusted’ death rate? For example, if\nyj takes on the low value of 393, then the raw death rate is 3.93× 10−5 and the posterior\nmean of θj is 20+393\n\n107+430,000 = 3.96× 10−5, and if yj = 545, then the raw rate is 5.45× 10−5\n\nand the posterior mean is 5.41 × 10−5. In this large county, the data dominate the prior\ndistribution.\n\nComparing counties of different sizes. In the Poisson model (2.16), the variance of\nyj\n\n10nj\n\nis inversely proportional to the exposure parameter nj , which can thus be considered a\n‘sample size’ for county j. Figure 2.8 shows how the raw kidney cancer death rates vary by\npopulation. The extremely high and extremely low rates are all in low-population counties.\nBy comparison, Figure 2.9a shows that the Bayes-estimated rates are much less variable.\nFinally, Figure 2.9b displays 50% interval estimates for a sample of counties (chosen because\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n50 2. SINGLE-PARAMETER MODELS\n\nFigure 2.9 (a) Bayes-estimated posterior mean kidney cancer death rates, E(θj |yj) = 20+yj\n\n430,000+10nj\n\nvs. logarithm of population size nj , the 3071 counties in the U.S. (b) Posterior medians and 50%\nintervals for θj for a sample of 100 counties j. The scales on the y-axes differ from the plots in\nFigure 2.8b.\n\nit would be hard to display all 3071 in a single plot). The smaller counties supply less\ninformation and thus have wider posterior intervals.\n\nConstructing a prior distribution\n\nWe now step back and discuss where we got the Gamma(20, 430,000) prior distribution for\nthe underlying rates. As we discussed when introducing the model, we picked the gamma\ndistribution for mathematical convenience. We now explain how the two parameters α, β\ncan be estimated from data to match the distribution of the observed cancer death rates\nyj\n\n10nj\n. It might seem inappropriate to use the data to set the prior distribution, but we\n\nview this as a useful approximation to our preferred approach of hierarchical modeling\n(introduced in Chapter 5), in which distributional parameters such as α, β in this example\nare treated as unknowns to be estimated.\n\nUnder the model, the observed count yj for any county j comes from the predictive dis-\n\ntribution, p(yj) =\n∫\np(yj |θj)p(θj)dθj , which in this case is Neg-bin(α, β\n\n10nj\n). From Appendix\n\nA, we can find the mean and variance of this distribution:\n\nE(yj) = 10nj\nα\n\nβ\n\nvar(yj) = 10nj\nα\n\nβ\n+ (10nj)\n\n2 α\n\nβ2\n. (2.17)\n\nThese can also be derived directly using the mean and variance formulas (1.8) and (1.9);\nsee Exercise 2.6.\n\nMatching the observed mean and variance to their expectations and solving for α and β\nyields the parameters of the prior distribution. The actual computation is more complicated\nbecause we must deal with the age adjustment and it also is more efficient to work with the\nmean and variance of the rates\n\nyj\n10nj\n\n:\n\nE\n\n(\nyj\n\n10nj\n\n)\n=\n\nα\n\nβ\n\nvar\n\n(\nyj\n\n10nj\n\n)\n=\n\n1\n\n10nj\n\nα\n\nβ\n+\n\nα\n\nβ2\n. (2.18)\n\nAfter dealing with the age adjustments, we equate the observed and theoretical moments,\nsetting the mean of the values of\n\nyj\n10nj\n\nto α\nβ and setting the variance of the values of\n\nyj\n10nj\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 51\n\nFigure 2.10 Empirical distribution of the age-adjusted kidney cancer death rates,\nyj\n\n10nj\n, for the 3071\n\ncounties in the U.S., along with the Gamma(20, 430,000) prior distribution for the underlying cancer\nrates θj.\n\nto E\n(\n\n1\n10nj\n\n)\nα\nβ + α\n\nβ2 , using the sample average of the values 1\n10nj\n\nin place of E\n(\n\n1\n10nj\n\n)\nin that\n\nlast expression.\nFigure 2.10 shows the empirical distribution of the raw cancer rates, along with the\n\nestimated Gamma(20, 430,000) prior distribution for the underlying cancer rates θj . The\ndistribution of the raw rates is much broader, which makes sense since they include the\nPoisson variability as well as the variation between counties.\n\nOur prior distribution is reasonable in this example, but this method of constructing\nit—by matching moments—is somewhat sloppy and can be difficult to apply in general. In\nChapter 5, we discuss how to estimate this and other prior distributions in a more direct\nBayesian manner, using hierarchical models.\n\nA more important way this model could be improved is by including information at the\ncounty level that could predict variation in the cancer rates. This would move the model\ntoward a hierarchical Poisson regression of the sort discussed in Chapter 16.\n\n2.8 Noninformative prior distributions\n\nWhen prior distributions have no population basis, they can be difficult to construct, and\nthere has long been a desire for prior distributions that can be guaranteed to play a minimal\nrole in the posterior distribution. Such distributions are sometimes called ‘reference prior\ndistributions,’ and the prior density is described as vague, flat, diffuse or noninformative.\nThe rationale for using noninformative prior distributions is often said to be ‘to let the\ndata speak for themselves,’ so that inferences are unaffected by information external to the\ncurrent data.\n\nA related idea is the weakly informative prior distribution, which contains some informa-\ntion—enough to ‘regularize’ the posterior distribution, that is, to keep it roughly within rea-\nsonable bounds—but without attempting to fully capture one’s scientific knowledge about\nthe underlying parameter.\n\nProper and improper prior distributions\n\nWe return to the problem of estimating the mean θ of a normal model with known variance\nσ2, with a N(µ0, τ\n\n2\n0 ) prior distribution on θ. If the prior precision, 1/τ20 , is small relative to\n\nthe data precision, n/σ2, then the posterior distribution is approximately as if τ20 =∞:\n\np(θ|y) ≈ N(θ|y, σ2/n).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(θ) is proportional to a constant for θ ∈ (−∞,∞). Such a distribution is\nnot strictly possible, since the integral of the assumed p(θ) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(θ) proper if it\ndoes not depend on data and integrates to 1. (If p(θ) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-χ2 prior distri-\nbution. If the prior degrees of freedom, ν0, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if ν0 = 0:\n\np(σ2|y) ≈ Inv-χ2(σ2|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for σ2 as p(σ2) ∝ 1/σ2, which is improper, having an infinite integral over the range\n(0,∞).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, θ). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(θ|y) ∝ p(y|θ)p(θ).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,∫\np(θ|y)dθ is finite for all y. Posterior distributions obtained from improper prior distri-\n\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis as approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\nφ = h(θ). By transformation of variables, the prior density p(θ) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on φ:\n\np(φ) = p(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣ = p(θ)|h′(θ)|−1. (2.19)\n\nJeffreys’ general principle is that any rule for determining the prior density p(θ) should\nyield an equivalent result if applied to the transformed parameter; that is, p(φ) computed\nby determining p(θ) and applying (2.19) should match the distribution that is obtained by\ndetermining p(φ) directly using the transformed model, p(y, φ) = p(φ)p(y|φ).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(θ) ∝ [J(θ)]1/2,\nwhere J(θ) is the Fisher information for θ:\n\nJ(θ) = E\n\n((\nd log p(y|θ)\n\ndθ\n\n)2\n∣∣∣∣∣ θ\n)\n\n= −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n. (2.20)\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(9) is proportional to a constant for 6 € (—oo, 00). Such a distribution is\nnot strictly possible, since the integral of the assumed p(@) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(0) proper if it\ndoes not depend on data and integrates to 1. (If p(@) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-y? prior distri-\nbution. If the prior degrees of freedom, vo, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if vp = 0:\n\np(o\"|y) © Inv-x?(o*|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for 0? as p(a?) « 1/07, which is improper, having an infinite integral over the range\n(0, co).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, @). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(y) x p(y|O)p(9).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,\n{p(Oly)d@ is finite for all y. Posterior distributions obtained from improper prior distri-\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis aS approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\n@ = h(@). By transformation of variables, the prior density p(@) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on ¢:\n\ndo\n\ndd| —\nJeffreys’ general principle is that any rule for determining the prior density p(@) should\nyield an equivalent result if applied to the transformed parameter; that is, p(¢@) computed\nby determining p(@) and applying (2.19) should match the distribution that is obtained by\ndetermining p(#) directly using the transformed model, p(y, ¢) = p(¢)p(yl¢).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(6) « [J(6)]\nwhere J(@) is the Fisher information for 0:\n\n_ dlog p(y|9) \\”\nsay = (et\n\nrd) = v(0) p()|h!(a)|—2. (2.19)\n\n1/2\n?\n\ndé?\n\n) =-5 (eeu) 0). (2.20)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\n\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(φ) at θ =\nh−1(φ):\n\nJ(φ) = −E\n(\nd2 log p(y|φ)\n\ndφ2\n\n)\n\n= −E\n(\nd2 log p(y|θ=h−1(φ))\n\ndθ2\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n)\n\n= J(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n\n;\n\nthus, J(φ)1/2 = J(θ)1/2\n∣∣∣ dθdφ\n∣∣∣, as required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter θ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ∼ Bin(n, θ), which has log-likelihood\n\nlog p(y|θ) = constant + y log θ + (n− y) log(1− θ).\n\nRoutine evaluation of the second derivative and substitution of E(y|θ) = nθ yields the\nFisher information:\n\nJ(θ) = −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n\n=\nn\n\nθ(1 − θ) .\n\nJeffreys’ prior density is then p(θ) ∝ θ−1/2(1 − θ)−1/2, which is a Beta(12 ,\n1\n2 ) density. By\n\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\nθ ∼ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(θ)) ∝ constant (see\nExercise 2.7), which corresponds to the improper Beta(0, 0) density on θ. In practice,\nthe difference between these alternatives is often small, since to get from θ ∼ Beta(0, 0)\nto θ ∼ Beta(1, 1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0, 0) prior distribution—if\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y− θ|θ) is a function that is free of θ and y, say,\nf(u), where u = y − θ, then y − θ is a pivotal quantity, and θ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for θ\nwould give f(y−θ) for the posterior distribution, p(y−θ|y). That is, under the posterior\ndistribution, y − θ should still be a pivotal quantity, whose distribution is free of both\nθ and y. Under this condition, using Bayes’ rule, p(y−θ|y) ∝ p(θ)p(y−θ|θ), thereby\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(¢) at 0 =\nh-*(¢):\n@ log p(yl¢)\n= —K a\nH(0) Ger\n_ _p{ Plogpyl@=h\\(6)) | d0 |?\ndo? do\ndo |\n= J(0)|/—| ;\n0)\n\nagp |> 2 required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter @ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nthus, J(¢)!/2 = J(@)!/2 \\\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ~ Bin(n, 6), which has log-likelihood\nlog p(y|@) = constant + y log 6 + (n — y) log(1 — @).\nRoutine evaluation of the second derivative and substitution of E(y|0) = n@ yields the\n\nFisher information: 5 (v0)\nd° log p(y|@ n\n0) = —E | ——~——_] 0 ) = —_~..\n10) = 8 ee) = aaa\n\nJeffreys’ prior density is then p(@) « 6~'/2(1 — 0)~1/?, which is a Beta(4, 4) density. By\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\n@ ~ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(@)) o constant (see\nExercise 2.7), which corresponds to the improper Beta(0,0) density on @. In practice,\nthe difference between these alternatives is often small, since to get from 0 ~ Beta(0, 0)\nto @ ~ Beta(1,1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0,0) prior distribution—if\n\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y—0|@) is a function that is free of 0 and y, say,\nf(u), where u = y — 0, then y — 6 is a pivotal quantity, and @ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for 0\nwould give f(y—6@) for the posterior distribution, p(y—9|y). That is, under the posterior\ndistribution, y — 0 should still be a pivotal quantity, whose distribution is free of both\n6 and y. Under this condition, using Bayes’ rule, p(y—9@ly) « p(@)p(y—6|@), thereby\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n54 2. SINGLE-PARAMETER MODELS\n\nimplying that the noninformative prior density is uniform on θ; that is, p(θ) ∝ constant\nover the range (−∞,∞).\n\n2. If the density of y is such that p(yθ |θ) is a function that is free of θ and y—say, g(u), where\nu = y\n\nθ—then u = y\nθ is a pivotal quantity and θ is called a pure scale parameter. In such\n\na case, it is reasonable that a noninformative prior distribution for θ would give g(yθ )\nfor the posterior distribution, p(yθ |y). By transformation of variables, the conditional\ndistribution of y given θ can be expressed in terms of the distribution of u given θ,\n\np(y|θ) = 1\n\nθ\np(u|θ),\n\nand similarly,\n\np(θ|y) = y\n\nθ2\np(u|y).\n\nAfter letting both p(u|θ) and p(u|y) equal g(u), we have the identity p(θ|y) = y\nθ p(y|θ).\n\nThus, in this case, the reference prior distribution is p(θ) ∝ 1\nθ or, equivalently, p(log θ) ∝ 1\n\nor p(θ2) ∝ 1\nθ2 .\n\nThis approach, in which the sampling distribution of the pivot is used as its posterior\ndistribution, can be applied to sufficient statistics in more complicated examples, such as\nhierarchical normal models.\n\nEven these principles can be misleading in some problems, in the critical sense of suggest-\ning prior distributions that can lead to improper posterior distributions. For example, the\nuniform prior density does not work for the logarithm of a hierarchical variance parameter,\nas we discuss in Section 5.4.\n\nDifficulties with noninformative prior distributions\n\nThe search for noninformative priors has several problems, including:\n\n1. Searching for a prior distribution that is always vague seems misguided: if the likelihood\nis truly dominant in a given problem, then the choice among a range of relatively flat\nprior densities cannot matter. Establishing a particular specification as the reference\nprior distribution seems to encourage its automatic, and possibly inappropriate, use.\n\n2. For many problems, there is no clear choice for a vague prior distribution, since a density\nthat is flat or uniform in one parameterization will not be in another. This is the\nessential difficulty with Laplace’s principle of insufficient reason—on what scale should\nthe principle apply? For example, the ‘reasonable’ prior density on the normal mean θ\nabove is uniform, while for σ2, the density p(σ2) ∝ 1/σ2 seems reasonable. However, if\nwe define φ = log σ2, then the prior density on φ is\n\np(φ) = p(σ2)\n\n∣∣∣∣\ndσ2\n\ndφ\n\n∣∣∣∣ ∝\n1\n\nσ2\nσ2 = 1;\n\nthat is, uniform on φ = log σ2. With discrete distributions, there is the analogous\ndifficulty of deciding how to subdivide outcomes into ‘atoms’ of equal probability.\n\n3. Further difficulties arise when averaging over a set of competing models that have im-\nproper prior distributions, as we discuss in Section 7.3.\n\nNevertheless, noninformative and reference prior densities are often useful when it does\nnot seem to be worth the effort to quantify one’s real prior knowledge as a probability\ndistribution, as long as one is willing to perform the mathematical work to check that\nthe posterior density is proper and to determine the sensitivity of posterior inferences to\nmodeling assumptions of convenience.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.9. WEAKLY INFORMATIVE PRIOR DISTRIBUTIONS 55\n\n2.9 Weakly informative prior distributions\n\nWe characterize a prior distribution as weakly informative if it is proper but is set up so that\nthe information it does provide is intentionally weaker than whatever actual prior knowledge\nis available. We will discuss this further in the context of a specific example, but in general\nany problem has some natural constraints that would allow a weakly informative model.\nFor example, for regression models on the logarithmic or logistic scale, with predictors that\nare binary or scaled to have standard deviation 1, we can be sure for most applications that\neffect sizes will be less than 10, given that a difference of 10 on the log scale changes the\nexpected value by a factor of exp(10) = 20,000, and on the logit scale shifts a probability\nof logit−1(−5) = 0.01 to logit−1(5) = 0.99.\n\nRather than trying to model complete ignorance, we prefer in most problems to use\nweakly informative prior distributions that include a small amount of real-world information,\nenough to ensure that the posterior distribution makes sense. For example, in the sex\nratio example from Sections 2.1 and 2.4, one could use a prior distribution concentrated\nbetween 0.4 and 0.6, for example N(0.5, 0.12) or, to keep the mathematical convenience of\nconjugacy, Beta(20, 20).2 In the general problem of estimating a normal mean from Section\n2.5, a N(0, A2) prior distribution is weakly informative, with A set to some large value that\ndepends on the context of the problem.\n\nIn almost every real problem, the data analyst will have more information than can\nbe conveniently included in the statistical model. This is an issue with the likelihood as\nwell as the prior distribution. In practice, there is always compromise for a number of\nreasons: to describe the model more conveniently; because it may be difficult to express\nknowledge accurately in probabilistic form; to simplify computations; or perhaps to avoid\nusing a possibly unreliable source of information. Except for the last reason, these are all\narguments for convenience and are best justified by the claim that the answer would not\nhave changed much had we been more accurate. If so few data are available that the choice\nof noninformative prior distribution makes a difference, one should put relevant information\ninto the prior distribution, perhaps using a hierarchical model, as we discuss in Chapter 5.\nWe return to the issue of accuracy vs. convenience in likelihoods and prior distributions in\nthe examples of the later chapters.\n\nConstructing a weakly informative prior distribution\n\nOne might argue that virtually all statistical models are weakly informative: a model always\nconveys some information, if only in its choice of inputs and the functional form of how\nthey are combined, but it is not possible or perhaps even desirable to encode all of one’s\nprior beliefs about a subject into a set of probability distributions. With that in mind, we\noffer two principles for setting up weakly informative priors, going at the problem from two\ndifferent directions:\n\n• Start with some version of a noninformative prior distribution and then add enough\ninformation so that inferences are constrained to be reasonable.\n\n• Start with a strong, highly informative prior and broaden it to account for uncertainty\nin one’s prior beliefs and in the applicability of any historically based prior distribution\nto new data.\n\nNeither of these approaches is pure. In the first case, it can happen that the purportedly\nnoninformative prior distribution used as a starting point is in fact too strong. For example,\nif a U(0, 1) prior distribution is assigned to the probability of some rare disease, then in\nthe presence of weak data the probability can be grossly overestimated (suppose y = 0\n\n2A quick R calculation, pbeta(.6,20,20) - pbeta(.4,20,20), reveals that 80% of the probability mass\nin the Beta(20, 20) falls between 0.4 and 0.6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n56 2. SINGLE-PARAMETER MODELS\n\nincidences out of n = 100 cases, and the true prevalence is known to be less than 1 in\n10,000), and an appropriate weakly informative prior will be such that the posterior in this\ncase will be concentrated in that low range. In the second case, a prior distribution that is\nbelieved to be strongly informative may in fact be too weak along some direction. This is\nnot to say that priors should be made more precise whenever posterior inferences are vague;\nin many cases, our best strategy is simply to acknowledge whatever posterior uncertainty\nwe have. But we should not feel constrained by default noninformative models when we\nhave substantive prior knowledge available.\n\nThere are settings, however, when it can be recommended to not use relevant informa-\ntion, even when it could clearly improve posterior inferences. The concern here is often\nexpressed in terms of fairness and encoded mathematically as a symmetry principle, that\nthe prior distribution should not pull inferences in any predetermined direction. For exam-\nple, consider an experimenter studying an effect that she is fairly sure is positive; perhaps\nher prior distribution is N(0.5, 0.5) on some appropriate scale. Such an assumption might\nbe pefectly reasonable given current scientific information but seems potentially risky if it\nis part of the analysis of an experiment designed to test the scientist’s theory. If anything,\none might want a prior distribution that leans against an experimenter’s hypothesis in order\nto require a higher standard of proof.\n\nUltimately, such concerns can and should be subsumed into decision analysis and some\nsort of model of the entire scientific process, trading off the gains of early identification of\nlarge and real effects against the losses entailed in overestimating the magnitudes of effects\nand overreacting to patterns that could be attributed to chance. In the meantime, though,\nwe know that statistical inferences are taken as evidence of effects, and as guides to future\ndecision making, and for this purpose it can make sense to require models to have certain\nconstraints such as symmetry about 0 for the prior distribution of a single treatment effect.\n\n2.10 Bibliographic note\n\nA fascinating detailed account of the early development of the idea of ‘inverse probability’\n(Bayesian inference) is provided in the book by Stigler (1986), on which our brief accounts\nof Bayes’ and Laplace’s solutions to the problem of estimating an unknown proportion are\nbased. Bayes’ famous 1763 essay in the Philosophical Transactions of the Royal Society of\nLondon has been reprinted as Bayes (1763); see also Laplace (1785, 1810).\n\nIntroductory textbooks providing complementary discussions of the simple models cov-\nered in this chapter were listed at the end of Chapter 1. In particular, Box and Tiao (1973)\nprovide a detailed treatment of Bayesian analysis with the normal model and also discuss\nhighest posterior density regions in some detail. The theory of conjugate prior distributions\nwas developed in detail by Raiffa and Schlaifer (1961). An interesting account of inference\nfor prediction, which also includes extensive details of particular probability models and\nconjugate prior analyses, appears in Aitchison and Dunsmore (1975).\n\nLiu et al. (2013) discuss how to efficiently compute highest posterior density intervals\nusing simulations.\n\nNoninformative and reference prior distributions have been studied by many researchers.\nJeffreys (1961) and Hartigan (1964) discuss invariance principles for noninformative prior\ndistributions. Chapter 1 of Box and Tiao (1973) presents a straightforward and practically\noriented discussion, a brief but detailed survey is given by Berger (1985), and the article by\nBernardo (1979) is accompanied by a wide-ranging discussion. Bernardo and Smith (1994)\ngive an extensive treatment of this topic along with many other matters relevant to the\nconstruction of prior distributions. Barnard (1985) discusses the relation between pivotal\nquantities and noninformative Bayesian inference. Kass and Wasserman (1996) provide a\nreview of many approaches for establishing noninformative prior densities based on Jeffreys’\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 57\n\nrule, and they also discuss the problems that may arise from uncritical use of purportedly\nnoninformative prior specifications. Dawid, Stone, and Zidek (1973) discuss some difficulties\nthat can arise with noninformative prior distributions; also see Jaynes (1980).\n\nKerman (2011) discusses noninformative and informative conjugate prior distributions\nfor the binomial and Poisson models.\n\nJaynes (1983) discusses in several places the idea of objectively constructing prior dis-\ntributions based on invariance principles and maximum entropy. Appendix A of Bretthorst\n(1988) outlines an objective Bayesian approach to assigning prior distributions, as applied\nto the problem of estimating the parameters of a sinusoid from time series data. More\ndiscussions of maximum entropy models appear in Jaynes (1982), Skilling (1989), and Gull\n(1989a); see Titterington (1984) and Donoho et al. (1992) for other views.\n\nFor more on weakly informative prior distributions, see Gelman (2006a) and Gelman,\nJakulin, et al. (2008). Gelman (2004b) discusses connections between parameterization and\nBayesian modeling. Greenland (2001) discusses informative prior distributions in epidemi-\nology.\n\nThe data for the placenta previa example come from a study from 1922 reported in\nJames (1987). For more on the challenges of estimating sex ratios from small samples,\nsee Gelman and Weakliem (2009). The Bayesian analysis of age-adjusted kidney cancer\ndeath rates in Section 2.7 is adapted from Manton et al. (1989); see also Gelman and Nolan\n(2002a) for more on this particular example and Bernardinelli, Clayton, and Montomoli\n(1995) for a general discussion of prior distributions for disease mapping. Gelman and\nPrice (1999) discuss artifacts in maps of parameter estimates, and Louis (1984), Shen and\nLouis (1998), and Louis and Shen (1999) analyze the general problem of estimation of\nensembles of parameters, a topic to which we return in Chapter 5.\n\n2.11 Exercises\n\n1. Posterior inference: suppose you have a Beta(4, 4) prior distribution on the probability θ\nthat a coin will yield a ‘head’ when spun in a specified manner. The coin is independently\nspun ten times, and ‘heads’ appear fewer than 3 times. You are not told how many heads\nwere seen, only that the number is less than 3. Calculate your exact posterior density\n(up to a proportionality constant) for θ and sketch it.\n\n2. Predictive distributions: consider two coins, C1 and C2, with the following characteristics:\nPr(heads|C1) = 0.6 and Pr(heads|C2) = 0.4. Choose one of the coins at random and\nimagine spinning it repeatedly. Given that the first two spins from the chosen coin are\ntails, what is the expectation of the number of additional spins until a head shows up?\n\n3. Predictive distributions: let y be the number of 6’s in 1000 rolls of a fair die.\n\n(a) Sketch the approximate distribution of y, based on the normal approximation.\n\n(b) Using the normal distribution table, give approximate 5%, 25%, 50%, 75%, and 95%\npoints for the distribution of y.\n\n4. Predictive distributions: let y be the number of 6’s in 1000 independent rolls of a par-\nticular real die, which may be unfair. Let θ be the probability that the die lands on ‘6.’\nSuppose your prior distribution for θ is as follows:\n\nPr(θ = 1/12) = 0.25,\n\nPr(θ = 1/6) = 0.5,\n\nPr(θ = 1/4) = 0.25.\n\n(a) Using the normal approximation for the conditional distributions, p(y|θ), sketch your\napproximate prior predictive distribution for y.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n58 2. SINGLE-PARAMETER MODELS\n\n(b) Give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution of y. (Be\ncareful here: y does not have a normal distribution, but you can still use the normal\ndistribution as part of your analysis.)\n\n5. Posterior distribution as a compromise between prior information and data: let y be the\nnumber of heads in n spins of a coin, whose probability of heads is θ.\n\n(a) If your prior distribution for θ is uniform on the range [0, 1], derive your prior predictive\ndistribution for y,\n\nPr(y = k) =\n\n∫ 1\n\n0\n\nPr(y = k|θ)dθ,\n\nfor each k = 0, 1, . . . , n.\n\n(b) Suppose you assign a Beta(α, β) prior distribution for θ, and then you observe y heads\nout of n spins. Show algebraically that your posterior mean of θ always lies between\nyour prior mean, α\n\nα+β , and the observed relative frequency of heads, yn .\n\n(c) Show that, if the prior distribution on θ is uniform, the posterior variance of θ is\nalways less than the prior variance.\n\n(d) Give an example of a Beta(α, β) prior distribution and data y, n, in which the posterior\nvariance of θ is higher than the prior variance.\n\n6. Predictive distributions: Derive the mean and variance (2.17) of the negative binomial\npredictive distribution for the cancer rate example, using the mean and variance formulas\n(1.8) and (1.9).\n\n7. Noninformative prior densities:\n\n(a) For the binomial likelihood, y ∼ Bin(n, θ), show that p(θ) ∝ θ−1(1 − θ)−1 is the\nuniform prior distribution for the natural parameter of the exponential family.\n\n(b) Show that if y = 0 or n, the resulting posterior distribution is improper.\n\n8. Normal distribution with unknown mean: a random sample of n students is drawn\nfrom a large population, and their weights are measured. The average weight of the n\nsampled students is y = 150 pounds. Assume the weights in the population are normally\ndistributed with unknown mean θ and known standard deviation 20 pounds. Suppose\nyour prior distribution for θ is normal with mean 180 and standard deviation 40.\n\n(a) Give your posterior distribution for θ. (Your answer will be a function of n.)\n\n(b) A new student is sampled at random from the same population and has a weight of\nỹ pounds. Give a posterior predictive distribution for ỹ. (Your answer will still be a\nfunction of n.)\n\n(c) For n = 10, give a 95% posterior interval for θ and a 95% posterior predictive interval\nfor ỹ.\n\n(d) Do the same for n = 100.\n\n9. Setting parameters for a beta prior distribution: suppose your prior distribution for θ,\nthe proportion of Californians who support the death penalty, is beta with mean 0.6 and\nstandard deviation 0.3.\n\n(a) Determine the parameters α and β of your prior distribution. Sketch the prior density\nfunction.\n\n(b) A random sample of 1000 Californians is taken, and 65% support the death penalty.\nWhat are your posterior mean and variance for θ? Draw the posterior density function.\n\n(c) Examine the sensitivity of the posterior distribution to different prior means and\nwidths including a non-informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 59\n\nYear Fatal Passenger Death\naccidents deaths rate\n\n1976 24 734 0.19\n1977 25 516 0.12\n1978 31 754 0.15\n1979 31 877 0.16\n1980 22 814 0.14\n1981 21 362 0.06\n1982 26 764 0.13\n1983 20 809 0.13\n1984 16 223 0.03\n1985 22 1066 0.15\n\nTable 2.2 Worldwide airline fatalities, 1976–1985. Death rate is passenger deaths per 100 million\npassenger miles. Source: Statistical Abstract of the United States.\n\n10. Discrete sample spaces: suppose there are N cable cars in San Francisco, numbered\nsequentially from 1 to N . You see a cable car at random; it is numbered 203. You wish\nto estimate N . (See Goodman, 1952, for a discussion and references to several versions of\nthis problem, and Jeffreys, 1961, Lee, 1989, and Jaynes, 2003, for Bayesian treatments.)\n\n(a) Assume your prior distribution on N is geometric with mean 100; that is,\n\np(N) = (1/100)(99/100)N−1, for N = 1, 2, . . . .\n\nWhat is your posterior distribution for N?\n\n(b) What are the posterior mean and standard deviation of N? (Sum the infinite series\nanalytically or approximate them on the computer.)\n\n(c) Choose a reasonable ‘noninformative’ prior distribution for N and give the resulting\nposterior distribution, mean, and standard deviation for N .\n\n11. Computing with a nonconjugate single-parameter model: suppose y1, . . . , y5 are inde-\npendent samples from a Cauchy distribution with unknown center θ and known scale 1:\np(yi|θ) ∝ 1/(1 + (yi − θ)2). Assume, for simplicity, that the prior distribution for θ is\nuniform on [0, 100]. Given the observations (y1, . . . , y5) = (43, 44, 45, 46.5, 47.5):\n\n(a) Compute the unnormalized posterior density function, p(θ)p(y|θ), on a grid of points\nθ = 0, 1\n\nm ,\n2\nm , . . . , 100, for some large integerm. Using the grid approximation, compute\n\nand plot the normalized posterior density function, p(θ|y), as a function of θ.\n\n(b) Sample 1000 draws of θ from the posterior density and plot a histogram of the draws.\n\n(c) Use the 1000 samples of θ to obtain 1000 samples from the predictive distribution of\na future observation, y6, and plot a histogram of the predictive draws.\n\n12. Jeffreys’ prior distributions: suppose y|θ ∼ Poisson(θ). Find Jeffreys’ prior density for θ,\nand then find α and β for which the Gamma(α, β) density is a close match to Jeffreys’\ndensity.\n\n13. Discrete data: Table 2.2 gives the number of fatal accidents and deaths on scheduled\nairline flights per year over a ten-year period. We use these data as a numerical example\nfor fitting discrete data models.\n\n(a) Assume that the numbers of fatal accidents in each year are independent with a\nPoisson(θ) distribution. Set a prior distribution for θ and determine the posterior\ndistribution based on the data from 1976 through 1985. Under this model, give a 95%\npredictive interval for the number of fatal accidents in 1986. You can use the normal\napproximation to the gamma and Poisson or compute using simulation.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n60 2. SINGLE-PARAMETER MODELS\n\n(b) Assume that the numbers of fatal accidents in each year follow independent Poisson\ndistributions with a constant rate and an exposure in each year proportional to the\nnumber of passenger miles flown. Set a prior distribution for θ and determine the\nposterior distribution based on the data for 1976–1985. (Estimate the number of\npassenger miles flown in each year by dividing the appropriate columns of Table 2.2\nand ignoring round-off errors.) Give a 95% predictive interval for the number of fatal\naccidents in 1986 under the assumption that 8 × 1011 passenger miles are flown that\nyear.\n\n(c) Repeat (a) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(d) Repeat (b) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(e) In which of the cases (a)–(d) above does the Poisson model seem more or less rea-\nsonable? Why? Discuss based on general principles, without specific reference to the\nnumbers in Table 2.2.\n\nIncidentally, in 1986, there were 22 fatal accidents, 546 passenger deaths, and a death\nrate of 0.06 per 100 million miles flown. We return to this example in Exercises 3.12,\n6.2, 6.3, and 8.14.\n\n14. Algebra of the normal model:\n\n(a) Fill in the steps to derive (2.9)–(2.10), and (2.11)–(2.12).\n\n(b) Derive (2.11) and (2.12) by starting with a N(µ0, τ\n2\n0 ) prior distribution and adding\n\ndata points one at a time, using the posterior distribution at each step as the prior\ndistribution for the next.\n\n15. Beta distribution: assume the result, from standard advanced calculus, that\n\n∫ 1\n\n0\n\nuα−1(1− u)β−1du =\nΓ(α)Γ(β)\n\nΓ(α + β)\n.\n\nIf Z has a beta distribution with parameters α and β, find E[Zm(1− Z)n] for any non-\nnegative integers m and n. Hence derive the mean and variance of Z.\n\n16. Beta-binomial distribution and Bayes’ prior distribution: suppose y has a binomial dis-\ntribution for given n and unknown parameter θ, where the prior distribution of θ is\nBeta(α, β).\n\n(a) Find p(y), the marginal distribution of y, for y = 0, . . . , n (unconditional on θ). This\ndiscrete distribution is known as the beta-binomial, for obvious reasons.\n\n(b) Show that if the beta-binomial probability is constant in y, then the prior distribution\nhas to have α = β = 1.\n\n17. Posterior intervals: unlike the central posterior interval, the highest posterior interval\nis not invariant to transformation. For example, suppose that, given σ2, the quantity\nnv/σ2 is distributed as χ2\n\nn, and that σ has the (improper) noninformative prior density\np(σ) ∝ σ−1, σ > 0.\n\n(a) Prove that the corresponding prior density for σ2 is p(σ2) ∝ σ−2.\n\n(b) Show that the 95% highest posterior density region for σ2 is not the same as the region\nobtained by squaring the endpoints of a posterior interval for σ.\n\n18. Poisson model: derive the gamma posterior distribution (2.15) for the Poisson model\nparameterized in terms of rate and exposure with conjugate prior distribution.\n\n19. Exponential model with conjugate prior distribution:\n\n(a) Show that if y|θ is exponentially distributed with rate θ, then the gamma prior dis-\ntribution is conjugate for inferences about θ given an independent and identically\ndistributed sample of y values.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 61\n\n(b) Show that the equivalent prior specification for the mean, φ = 1/θ, is inverse-gamma.\n(That is, derive the latter density function.)\n\n(c) The length of life of a light bulb manufactured by a certain process has an exponential\ndistribution with unknown rate θ. Suppose the prior distribution for θ is a gamma\ndistribution with coefficient of variation 0.5. (The coefficient of variation is defined\nas the standard deviation divided by the mean.) A random sample of light bulbs is\nto be tested and the lifetime of each obtained. If the coefficient of variation of the\ndistribution of θ is to be reduced to 0.1, how many light bulbs need to be tested?\n\n(d) In part (c), if the coefficient of variation refers to φ instead of θ, how would your\nanswer be changed?\n\n20. Censored and uncensored data in the exponential model:\n\n(a) Suppose y|θ is exponentially distributed with rate θ, and the marginal (prior) distri-\nbution of θ is Gamma(α, β). Suppose we observe that y ≥ 100, but do not observe\nthe exact value of y. What is the posterior distribution, p(θ|y≥100), as a function of\nα and β? Write down the posterior mean and variance of θ.\n\n(b) In the above problem, suppose that we are now told that y is exactly 100. Now what\nare the posterior mean and variance of θ?\n\n(c) Explain why the posterior variance of θ is higher in part (b) even though more in-\nformation has been observed. Why does this not contradict identity (2.8) on page\n32?\n\n21. Simple hierarchical modeling:\nThe file pew research center june elect wknd data.dta3 has data from Pew Research\nCenter polls taken during the 2008 election campaign. You can read these data into R\nusing the read.dta() function (after first loading the foreign package into R).\nYour task is to estimate the percentage of the (adult) population in each state (excluding\nAlaska, Hawaii, and the District of Columbia) who label themselves as ‘very liberal,’\nfollowing the general procedure that was used in Section 2.7 to estimate cancer rates,\nbut using the binomial and beta rather than Poisson and gamma distributions. But you\ndo not need to make maps; it will be enough to make scatterplots, plotting the estimate\nvs. Barack Obama’s vote share in 2008 (data available at 2008ElectionResult.csv,\nreadable in R using read.csv()).\nMake the following four graphs on a single page:\n\n• Graph proportion very liberal among the survey respondents in each state vs. Obama\nvote share—that is, a scatterplot using the two-letter state abbreviations (see state.abb()\nin R).\n\n• Graph the Bayes posterior mean in each state vs. Obama vote share.\n\n• Repeat graphs (a) and (b) using the number of respondents in the state on the x-axis.\n\nThis exercise has four challenges: first, manipulating the data in order to get the totals\nby state; second, estimating the parameters of the prior distribution; third, doing the\nBayesian analysis by state; and fourth, making the graphs.\n\n22. Prior distributions:\nA (hypothetical) study is performed to estimate the effect of a simple training program\non basketball free-throw shooting. A random sample of 100 college students is recruited\ninto the study. Each student first shoots 100 free-throws to establish a baseline success\nprobability. Each student then takes 50 practice shots each day for a month. At the end\nof that time, he or she takes 100 shots for a final measurement. Let θ be the average\nimprovement in success probability.\nGive three prior distributions for θ (explaining each in a sentence):\n\n3For data for this and other exercises, go to http://www.stat.columbia.edu/∼gelman/book/.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n62 2. SINGLE-PARAMETER MODELS\n\n(a) A noninformative prior,\n\n(b) A subjective prior based on your best knowledge, and\n\n(c) A weakly informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 3\n\nIntroduction to multiparameter models\n\nVirtually every practical problem in statistics involves more than one unknown or unob-\nservable quantity. It is in dealing with such problems that the simple conceptual framework\nof the Bayesian approach reveals its principal advantages over other methods of inference.\nAlthough a problem can include several parameters of interest, conclusions will often be\ndrawn about one, or only a few, parameters at a time. In this case, the ultimate aim of a\nBayesian analysis is to obtain the marginal posterior distribution of the particular param-\neters of interest. In principle, the route to achieving this aim is clear: we first require the\njoint posterior distribution of all unknowns, and then we integrate this distribution over the\nunknowns that are not of immediate interest to obtain the desired marginal distribution.\nOr equivalently, using simulation, we draw samples from the joint posterior distribution\nand then look at the parameters of interest and ignore the values of the other unknowns.\nIn many problems there is no interest in making inferences about many of the unknown\nparameters, although they are required in order to construct a realistic model. Parameters\nof this kind are often called nuisance parameters. A classic example is the scale of the\nrandom errors in a measurement problem.\n\nWe begin this chapter with a general treatment of nuisance parameters and then cover\nthe normal distribution with unknown mean and variance in Section 3.2. Sections 3.4\nand 3.5 present inference for the multinomial and multivariate normal distributions—the\nsimplest models for discrete and continuous multivariate data, respectively. The chapter\nconcludes with an analysis of a nonconjugate logistic regression model, using numerical\ncomputation of the posterior density on a grid.\n\n3.1 Averaging over ‘nuisance parameters’\n\nTo express the ideas of joint and marginal posterior distributions mathematically, suppose\nθ has two parts, each of which can be a vector, θ = (θ1, θ2), and further suppose that we\nare only interested (at least for the moment) in inference for θ1, so θ2 may be considered a\n‘nuisance’ parameter. For instance, in the simple example,\n\ny|µ, σ2 ∼ N(µ, σ2),\n\nin which both µ (=‘θ1’) and σ\n2 (=‘θ2’) are unknown, interest commonly centers on µ.\n\nWe seek the conditional distribution of the parameter of interest given the observed\ndata; in this case, p(θ1|y). This is derived from the joint posterior density,\n\np(θ1, θ2|y) ∝ p(y|θ1, θ2)p(θ1, θ2),\n\nby averaging over θ2:\n\np(θ1|y) =\n∫\np(θ1, θ2|y)dθ2.\n\n63\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(θ1|y) =\n∫\np(θ1|θ2, y)p(θ2|y)dθ2, (3.1)\n\nwhich shows that the posterior distribution of interest, p(θ1|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, θ2, where p(θ2|y) is a weighting\nfunction for the different possible values of θ2. The weights depend on the posterior density\nof θ2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters θ2 can be interpreted generally; for example, θ2 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing θ2 from\nits marginal posterior distribution and then θ1 from its conditional posterior distribution,\ngiven the drawn value of θ2. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(µ, σ2);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for µ and σ, assuming prior\nindependence of location and scale parameters, is uniform on (µ, log σ) or, equivalently,\n\np(µ, σ2) ∝ (σ2)−1.\n\nThe joint posterior distribution, p(µ, σ2|y)\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/σ2:\n\np(µ, σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − µ)2\n)\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\n[\nn∑\n\ni=1\n\n(yi − y)2 + n(y − µ)2\n])\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\n, (3.2)\n\nwhere\n\ns2 =\n1\n\nn− 1\n\nn∑\n\ni=1\n\n(yi − y)2\n\nis the sample variance of the yi’s. The sufficient statistics are y and s2.\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(Orly) = / p(61|62, y)p(O2|y) dbo, (3.1)\n\nwhich shows that the posterior distribution of interest, p(@i|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, 02, where p(02|y) is a weighting\nfunction for the different possible values of 62. The weights depend on the posterior density\nof #2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters 62 can be interpreted generally; for example, 62 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing 62 from\nits marginal posterior distribution and then 0; from its conditional posterior distribution,\ngiven the drawn value of 02. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(, 07);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for w and o, assuming prior\nindependence of location and scale parameters, is uniform on (y,logo) or, equivalently,\n\np(u,o7) x (a7) *.\n\nThe joint posterior distribution, p(u,07|y)\n\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/c?:\n\nan 1<\nP(u,o*|y) oo\" * exp (-2: Sen -n]\n\nII\nq\n3\ni)\n)\ntal\nue)\n—“~\nto\nPai\nbo\nS\n3\n—\n=\n|\nSl\niw)\n+\n=\nRad]\n|\n=\ni)\n|\nn___”\n\n(3.2)\n\nII\nQ\n3\nbo\nco\ntal\nue)\na ™~\n|\n| H\n=\n3\n|\n—\n“—\nD\niw)\n+\n3\n—\nKa]\n|\n=\n“—\n7\nQe\n\nwhere\n\nis the sample variance of the y;’s. The sufficient statistics are 7 and s?.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(µ|σ2, y), and then the marginal posterior density, p(σ2|y). To determine\nthe posterior distribution of µ, given σ2, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nµ|σ2, y ∼ N(y, σ2/n). (3.3)\n\nThe marginal posterior distribution, p(σ2|y)\n\nTo determine p(σ2|y), we must average the joint distribution (3.2) over µ:\n\np(σ2|y) ∝\n∫\nσ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\ndµ.\n\nIntegrating this expression over µ requires evaluating the integral exp\n(\n− 1\n\n2σ2n(y − µ)2\n)\n,\n\nwhich is a simple normal integral; thus,\n\np(σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n(n−1)s2\n\n)√\n2πσ2/n\n\n∝ (σ2)−(n+1)/2 exp\n\n(\n− (n− 1)s2\n\n2σ2\n\n)\n, (3.4)\n\nwhich is a scaled inverse-χ2 density:\n\nσ2|y ∼ Inv-χ2(n− 1, s2). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(µ, σ2|y) = p(µ|σ2, y)p(σ2|y).\n\nThis marginal posterior distribution for σ2 has a remarkable similarity to the analogous\nsampling theory result: conditional on σ2 (and µ), the distribution of the appropriately\n\nscaled sufficient statistic, (n−1)s2\n\nσ2 , is χ2\nn−1. Considering our derivation of the reference prior\n\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw σ2 from (3.5),\nthen draw µ from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of µ\n\nThe population mean, µ, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of µ, which can be obtained by\nintegrating σ2 out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of µ can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-χ2 distribution for the variance, σ2. We can derive the marginal\nposterior density for µ by integrating the joint posterior density over σ2:\n\np(µ|y) =\n∫ ∞\n\n0\n\np(µ, σ2|y)dσ2.\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\nThe conditional posterior distribution, p(u|o?, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(u|o”, y), and then the marginal posterior density, p(a?|y). To determine\nthe posterior distribution of 4, given 07, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nulo?,y ~ NG,o7/n). (3.3)\n\nThe marginal posterior distribution, p(o?|y)\n\nTo determine p(o?|y), we must average the joint distribution (3.2) over p:\n2 —n-2 1 2 = 2\nploly) x fo\" exp | —sal(n—1s° +n — 4)\"] } du.\n\nIntegrating this expression over fz requires evaluating the integral exp (-sin(y — L)’),\nwhich is a simple normal integral; thus,\n\npoly) oo\" Pexp (—o5(n—1)s*) VBra%]n\n\n20?\n\nx (0?) TY? exp (-“S*) ; (3.4)\n\nwhich is a scaled inverse-y? density:\no*|y ~ Inv-x?(n — 1, s”). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(u,0?|y) = p(ulo?, y)p(o?|y).\n\nThis marginal posterior distribution for ¢? has a remarkable similarity to the analogous\nsampling theory result: conditional on o? (and jy), the distribution of the appropriately\nscaled sufficient statistic, (nas , is x2_,. Considering our derivation of the reference prior\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw o? from (3.5),\nthen draw py from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of ju\n\nThe population mean, ju, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of yu, which can be obtained by\nintegrating 0? out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of 4: can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-x? distribution for the variance, 07. We can derive the marginal\n\nposterior density for 4 by integrating the joint posterior density over o?:\n\nP(Hly) = [ p(b, 07 |y)do”.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\n\nz =\nA\n\n2σ2\n, where A = (n− 1)s2 + n(µ− y)2,\n\nand recognizing that the result is an unnormalized gamma integral:\n\np(µ|y) ∝ A−n/2\n∫ ∞\n\n0\n\nz(n−2)/2 exp(−z)dz\n\n∝ [(n− 1)s2 + n(µ− y)2]−n/2\n\n∝\n[\n1 +\n\nn(µ− y)2\n(n− 1)s2\n\n]−n/2\n.\n\nThis is the tn−1(y, s\n2/n) density (see Appendix A).\n\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (µ, log σ), the posterior distribution of µ has the form\n\nµ− y\ns/\n√\nn\n\n∣∣∣∣ y ∼ tn−1,\n\nwhere tn−1 denotes the standard t density (location 0, scale 1) with n−1 degrees of freedom.\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|µ, σ2), the following relation holds:\n\ny − µ\ns/\n√\nn\n\n∣∣∣∣µ, σ2 ∼ tn−1.\n\nThe sampling distribution of the pivotal quantity (y − µ)/(s/√n) does not depend on the\nnuisance parameter σ2, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, ỹ, can be written as a mixture,\np(ỹ|y) =\n\n∫∫\np(ỹ|µ, σ2, y)p(µ, σ2|y)dµdσ2. The first of the two factors in the integral is just\n\nthe normal distribution for the future observation given the values of (µ, σ2), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw µ, σ2 from\ntheir joint posterior distribution and then simulate ỹ ∼ N(µ, σ2).\n\nIn fact, the posterior predictive distribution of ỹ is a t distribution with location y,\nscale (1 + 1\n\nn )\n1/2s, and n − 1 degrees of freedom. This analytic form is obtained using the\n\nsame techniques as in the derivation of the posterior distribution of µ. Specifically, the\ndistribution can be obtained by integrating out the parameters µ, σ2 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(ỹ|σ2, y) =\n\n∫\np(ỹ|µ, σ2, y)p(µ|σ2, y)dµ leads to p(ỹ|σ2, y) = N(ỹ|y, (1 + 1\n\nn )σ\n2),\n\nwhich is the same, up to a changed scale factor, as the distribution of µ|σ2, y.\n\nExample. Estimating the speed of light\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\nz= AL where A = (n —1)s? + n(pp— 9)?\n202 >) ?\n\nand recognizing that the result is an unnormalized gamma integral:\n\nply) 2 Av? [0-2 exp(—a)de\n10)\nx [(n—1)s? +n(u— 92?\n\n_\\94—n/2\nnu 9)?”\n«x |1+———> .\n(n — 1)s?\nThis is the t,_1(Y, s?/n) density (see Appendix A).\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (js, log a), the posterior distribution of y has the form\n\nMay\nwhere t,,_1 denotes the standard ¢ density (location 0, scale 1) with n—1 degrees of freedom.\n\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|u, 07), the following relation holds:\n\nyYy~ tn—-1;\n\nYH\n\ns/vn\\ i?\n\nThe sampling distribution of the pivotal quantity (y — )/(s/./n) does not depend on the\nnuisance parameter o7, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\n2 ty.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, y, can be written as a mixture,\nply) = f{vG\\u, 0? y)p(u, 07 |y)dudo?. The first of the two factors in the integral is just\nthe normal distribution for the future observation given the values of (1,07), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw 4,0? from\ntheir joint posterior distribution and then simulate 7 ~ N(j, 07).\n\nIn fact, the posterior predictive distribution of y is a t distribution with location ¥,\nscale (1 + 4)1/ 2s, and n — 1 degrees of freedom. This analytic form is obtained using the\nsame techniques as in the derivation of the posterior distribution of jz. Specifically, the\ndistribution can be obtained by integrating out the parameters j1,07 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(glo”, y) = [p(glu,07, y)p(ulo?, yd leads to p(glo?, y) = N(gly, (1+ 4)o”),\nwhich is the same, up to a changed scale factor, as the distribution of pu\\o?, y.\n\nExample. Estimating the speed of light\n\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\n\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\n\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\n\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.3. NORMAL DATA WITH A CONJUGATE PRIOR DISTRIBUTION 67\n\nFigure 3.1 Histogram of Simon Newcomb’s measurements for estimating the speed of light, from\nStigler (1977). The data are recorded as deviations from 24,800 nanoseconds.\n\nµ and variance σ2. The main substantive goal is posterior inference for µ. The outlying\nmeasurements do not fit the normal model; we discuss Bayesian methods for measur-\ning the lack of fit for these data in Section 6.3. The mean of the 66 measurements is\ny = 26.2, and the sample standard deviation is s = 10.8. Assuming the noninformative\nprior distribution p(µ, σ2) ∝ (σ2)−1, a 95% central posterior interval for µ is obtained\nfrom the t65 marginal posterior distribution of µ as y ± 1.997s/\n\n√\n66 = [23.6, 28.8].\n\nThe posterior interval can also be obtained by simulation. Following the factorization\nof the posterior distribution given by (3.5) and (3.3), we first draw a random value of\nσ2 ∼ Inv-χ2(65, s2) as 65s2 divided by a random draw from the χ2\n\n65 distribution (see\nAppendix A). Then given this value of σ2, we draw µ from its conditional posterior\ndistribution, N(26.2, σ2/66). Based on 1000 simulated values of (µ, σ2), we estimate\nthe posterior median of µ to be 26.2 and a 95% central posterior interval for µ to be\n[23.6, 28.9], close to the analytically calculated interval.\nIncidentally, based on the currently accepted value of the speed of light, the ‘true\nvalue’ for µ in Newcomb’s experiment is 33.0, which falls outside our 95% interval.\nThis reinforces the fact that posterior inferences are only as good as the model and\nthe experiment that produced the data.\n\n3.3 Normal data with a conjugate prior distribution\n\nA family of conjugate prior distributions\n\nA first step toward a more general model is to assume a conjugate prior distribution for\nthe two-parameter univariate normal sampling model in place of the noninformative prior\ndistribution just considered. The form of the likelihood displayed in (3.2) and the subse-\nquent discussion shows that the conjugate prior density must also have the product form\np(σ2)p(µ|σ2), where the marginal distribution of σ2 is scaled inverse-χ2 and the conditional\ndistribution of µ given σ2 is normal (so that marginally µ has a t distribution). A convenient\nparameterization is given by the following specification:\n\nµ|σ2 ∼ N(µ0, σ\n2/κ0)\n\nσ2 ∼ Inv-χ2(ν0, σ\n2\n0),\n\nwhich corresponds to the joint prior density\n\np(µ, σ2) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ0 − µ)2]\n\n)\n. (3.6)\n\nWe label this the N-Inv-χ2(µ, σ2|µ0, σ\n2\n0/κ0; ν0, σ\n\n2\n0) density; its four parameters can be iden-\n\ntified as the location and scale of µ and the degrees of freedom and scale of σ2, respectively.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n68 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThe appearance of σ2 in the conditional distribution of µ|σ2 means that µ and σ2 are\nnecessarily dependent in their joint conjugate prior density: for example, if σ2 is large, then\na high-variance prior distribution is induced on µ. This dependence is notable, considering\nthat conjugate prior distributions are used largely for convenience. Upon reflection, however,\nit often makes sense for the prior variance of the mean to be tied to σ2, which is the sampling\nvariance of the observation y. In this way, prior belief about µ is calibrated by the scale of\nmeasurement of y and is equivalent to κ0 prior measurements on this scale.\n\nThe joint posterior distribution, p(µ, σ2|y)\n\nMultiplying the prior density (3.6) by the normal likelihood yields the posterior density\n\np(µ, σ2|y) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ− µ0)\n\n2]\n\n)\n×\n\n× (σ2)−n/2 exp\n\n(\n− 1\n\n2σ2\n[(n− 1)s2 + n(y − µ)2]\n\n)\n(3.7)\n\n= N-Inv-χ2(µ, σ2|µn, σ2\nn/κn; νn, σ\n\n2\nn),\n\nwhere, after some algebra (see Exercise 3.9), it can be shown that\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nνnσ\n2\nn = ν0σ\n\n2\n0 + (n− 1)s2 +\n\nκ0n\n\nκ0 + n\n(y − µ0)\n\n2.\n\nThe parameters of the posterior distribution combine the prior information and the infor-\nmation contained in the data. For example µn is a weighted average of the prior mean and\nthe sample mean, with weights determined by the relative precision of the two pieces of\ninformation. The posterior degrees of freedom, νn, is the prior degrees of freedom plus the\nsample size. The posterior sum of squares, νnσ\n\n2\nn, combines the prior sum of squares, the\n\nsample sum of squares, and the additional uncertainty conveyed by the difference between\nthe sample mean and the prior mean.\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nThe conditional posterior density of µ, given σ2, is proportional to the joint posterior density\n(3.7) with σ2 held constant,\n\nµ|σ2, y ∼ N(µn, σ\n2/κn)\n\n= N\n\n( κ0\n\nσ2µ0 +\nn\nσ2 y\n\nκ0\n\nσ2 + n\nσ2\n\n,\n1\n\nκ0\n\nσ2 + n\nσ2\n\n)\n, (3.8)\n\nwhich agrees, as it must, with the analysis in Section 2.5 of µ with σ considered fixed.\n\nThe marginal posterior distribution, p(σ2|y)\n\nThe marginal posterior density of σ2, from (3.7), is scaled inverse-χ2:\n\nσ2|y ∼ Inv-χ2(νn, σ\n2\nn). (3.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.4. MULTINOMIAL MODEL FOR CATEGORICAL DATA 69\n\nSampling from the joint posterior distribution\n\nTo sample from the joint posterior distribution, just as in the previous section, we first draw\nσ2 from its marginal posterior distribution (3.9), then draw µ from its normal conditional\nposterior distribution (3.8), using the simulated value of σ2.\n\nAnalytic form of the marginal posterior distribution of µ\n\nIntegration of the joint posterior density with respect to σ2, in a precisely analogous way\nto that used in the previous section, shows that the marginal posterior density for µ is\n\np(µ|y) ∝\n(\n1 +\n\nκn(µ− µn)2\nνnσ2\n\nn\n\n)−(νn+1)/2\n\n= tνn(µ|µn, σ2\nn/κn).\n\n3.4 Multinomial model for categorical data\n\nThe binomial distribution that was emphasized in Chapter 2 can be generalized to allow\nmore than two possible outcomes. The multinomial sampling distribution is used to describe\ndata for which each observation is one of k possible outcomes. If y is the vector of counts\nof the number of observations of each outcome, then\n\np(y|θ) ∝\nk∏\n\nj=1\n\nθ\nyj\nj ,\n\nwhere the sum of the probabilities,\n∑k\n\nj=1 θj , is 1. The distribution is typically thought of as\n\nimplicitly conditioning on the number of observations,\n∑k\nj=1 yj = n. The conjugate prior\n\ndistribution is a multivariate generalization of the beta distribution known as the Dirichlet,\n\np(θ|α) ∝\nk∏\n\nj=1\n\nθ\nαj−1\nj ,\n\nwhere the distribution is restricted to nonnegative θj ’s with\n∑k\nj=1 θj = 1; see Appendix\n\nA for details. The resulting posterior distribution for the θj ’s is Dirichlet with parameters\nαj + yj .\n\nThe prior distribution expressed on the scale of α is mathematically equivalent to a\nlikelihood resulting from\n\n∑k\nj=1(αj−1) observations with αj−1 observations of the jth out-\n\ncome category. As in the binomial there are several plausible noninformative Dirichlet prior\ndistributions. A uniform density is obtained by setting αj = 1 for all j; this distribution\n\nassigns equal density to any vector θ satisfying\n∑k\nj=1 θj = 1. Setting αj = 0 for all j results\n\nin an improper prior distribution that is uniform in the log(θj)’s. The resulting posterior\ndistribution is proper if there is at least one observation in each of the k categories, so that\neach component of y is positive. The bibliographic note at the end of this chapter points\nto other suggested noninformative prior distributions for the multinomial model.\n\nExample. Pre-election polling\nFor a simple example of a multinomial model, we consider a sample survey question\nwith three possible responses. In late October, 1988, a survey was conducted by CBS\nNews of 1447 adults in the United States to find out their preferences in the upcoming\npresidential election. Out of 1447 persons, y1 = 727 supported George Bush, y2 = 583\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n70 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.2 Histogram of values of (θ1− θ2) for 1000 simulations from the posterior distribution for\nthe election polling example.\n\nsupported Michael Dukakis, and y3 = 137 supported other candidates or expressed no\nopinion. Assuming no other information on the respondents, the 1447 observations\nare exchangeable. If we also assume simple random sampling (that is, 1447 names\n‘drawn out of a hat’), then the data (y1, y2, y3) follow a multinomial distribution, with\nparameters (θ1, θ2, θ3), the proportions of Bush supporters, Dukakis supporters, and\nthose with no opinion in the survey population. An estimand of interest is θ1 − θ2,\nthe population difference in support for the two major candidates.\nWith a noninformative uniform prior distribution on θ, α1=α2=α3=1, the posterior\ndistribution for (θ1, θ2, θ3) is Dirichlet(728, 584, 138). We could compute the posterior\ndistribution of θ1 − θ2 by integration, but it is simpler just to draw 1000 points\n(θ1, θ2, θ3) from the posterior Dirichlet distribution and then compute θ1 − θ2 for\neach. The result is displayed in Figure 3.2. All of the 1000 simulations had θ1 > θ2;\nthus, the estimated posterior probability that Bush had more support than Dukakis\nin the survey population is over 99.9%.\nIn fact, the CBS survey does not use independent random sampling but rather uses a\nvariant of a stratified sampling plan. We discuss an improved analysis of this survey,\nusing some knowledge of the sampling scheme, in Section 8.3 (see Table 8.2 on page\n207).\n\nIn complicated problems—for example, analyzing the results of many survey questions\nsimultaneously—the number of multinomial categories, and thus parameters, becomes so\nlarge that it is hard to usefully analyze a dataset of moderate size without additional\nstructure in the model. Formally, additional information can enter the analysis through\nthe prior distribution or the sampling model. An informative prior distribution might be\nused to improve inference in complicated problems, using the ideas of hierarchical modeling\nintroduced in Chapter 5. Alternatively, loglinear models can be used to impose structure on\nmultinomial parameters that result from cross-classifying several survey questions; Section\n16.7 provides details and an example.\n\n3.5 Multivariate normal model with known variance\n\nHere we give a somewhat formal account of the distributional results of Bayesian inference\nfor the parameters of a multivariate normal distribution. In many ways, these results\nparallel those already given for the univariate normal model, but there are some important\nnew aspects that play a major role in the analysis of linear models, which is the central\nactivity of much applied statistical work (see Chapters 5, 14, and 15). This section can be\nviewed at this point as reference material for future chapters.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\n\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|µ,Σ ∼ N(µ,Σ), (3.10)\n\nwhere µ is a (column) vector of length d and Σ is a d×d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\np(y|µ,Σ) ∝ |Σ|−1/2 exp\n\n(\n−1\n\n2\n(y − µ)TΣ−1(y − µ)\n\n)\n,\n\nand for a sample of n independent and identically distributed observations, y1, . . . , yn, is\n\np(y1, . . . , yn|µ,Σ) ∝ |Σ|−n/2 exp\n(\n−1\n\n2\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n)\n. (3.11)\n\nConjugate analysis\n\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known Σ.\n\nConjugate prior distribution for µ with known Σ. The log-likelihood is a quadratic form\nin µ, and therefore the conjugate prior distribution for µ is the multivariate normal distri-\nbution, which we parameterize as µ ∼ N(µ0,Λ0).\n\nPosterior distribution for µ with known Σ. The posterior distribution of µ is\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n\n(\n(µ− µ0)\n\nTΛ−1\n0 (µ− µ0) +\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n))\n\n,\n\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n(µ− µn)TΛ−1\n\nn (µ− µn)\n)\n\n= N(µ|µn,Λn),\n\nwhere\n\nµn = (Λ−1\n0 + nΣ−1)−1(Λ−1\n\n0 µ0 + nΣ−1y)\n\nΛ−1\nn = Λ−1\n\n0 + nΣ−1. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nΣ−1 and Λ−1\n\n0 , respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of µ with known Σ. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, µ(1) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\nµn and variance matrix equal to the appropriate submatrix of Λn. Also, the conditional\nposterior distribution of a subset µ(1) given the values of a second subset µ(2) is multivariate\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|w, &~ N(w,%), (3.10)\n\nwhere yu is a (column) vector of length d and © is a dx d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\npul.) x [E12 ep (—S y= w= 1)),\n\nand for a sample of n independent and identically distributed observations, y1,..., Yn, 1S\n\n_n 1X _\nPCY «+++ Yn|ts B) oc [Z|-\"/? exp (-4 Yow w= Mn 0))- (3.11)\n\ni=l\n\nConjugate analysis\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known ™.\n\nConjugate prior distribution for w with known X. The log-likelihood is a quadratic form\nin ys, and therefore the conjugate prior distribution for yz is the multivariate normal distri-\nbution, which we parameterize as pu ~ N({uo, Ao).\n\nPosterior distribution for with known %. The posterior distribution of ju is\n1 _ . _\np(uly, &) x exp (-} (\\: — po)” Ng (= po) + S3(yi = BPE yi - »))\ni=1\n\nwhich is an exponential of a quadratic form in jz. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\nplays) 0 exp (—5(00~ nn) A\" — sn)\n\n= N(u| én, An),\nwhere\nHin = (Ag* + n¥7\")7*(Ag to + nE~\"y)\nAyt = Apt+nzc7. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nu~! and Ag 1 respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of 1 with known %. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, 1“) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\n[in and variance matrix equal to the appropriate submatrix of A,. Also, the conditional\nposterior distribution of a subset p“) given the values of a second subset “?) is multivariate\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n72 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nnormal. If we write superscripts in parentheses to indicate appropriate subvectors and\nsubmatrices, then\n\nµ(1)|µ(2), y ∼ N\n(\nµ(1)\nn + β1|2(µ(2) − µ(2)\n\nn ),Λ1|2\n)\n, (3.13)\n\nwhere the regression coefficients β1|2 and conditional variance matrix Λ1|2 are defined by\n\nβ1|2 = Λ(12)\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ1|2 = Λ(11)\nn − Λ(12)\n\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ(21)\nn .\n\nPosterior predictive distribution for new data. We now work out the analytic form of the\nposterior predictive distribution for a new observation ỹ ∼ N(µ,Σ). As with the univariate\nnormal, we first note that the joint distribution, p(ỹ, µ|y) = N(ỹ|µ,Σ)N(µ|µn,Λn), is the\nexponential of a quadratic form in (ỹ, µ); hence (ỹ, µ) have a joint normal posterior distri-\nbution, and so the marginal posterior distribution of ỹ is (multivariate) normal. We are\nstill assuming the variance matrix Σ is known. As in the univariate case, we can determine\nthe posterior mean and variance of ỹ using (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|µ, y)|y)\n= E(µ|y) = µn,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|µ, y)|y) + var(E(ỹ|µ, y)|y)\n= E(Σ|y) + var(µ|y) = Σ + Λn.\n\nTo sample from the posterior distribution or the posterior predictive distribution, re-\nfer to Appendix A for a method of generating random draws from a multivariate normal\ndistribution with specified mean and variance matrix.\n\nNoninformative prior density for µ. A noninformative uniform prior density for µ is p(µ) ∝\nconstant, obtained in the limit as the prior precision tends to zero in the sense |Λ−1\n\n0 | → 0;\nin the limit of infinite prior variance (zero prior precision), the prior mean is irrelevant.\nThough this choice of prior density does not combine with the likelihood to form a proper\njoint probability model for µ and y, the posterior density obtained by applying Bayes’ rule\nis a proper posterior density. The posterior density is proportional to the likelihood (3.11)\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant terms yields the posterior distribution for µ, given the uniform prior density,\nas µ|Σ, y ∼ N(y,Σ/n).\n\n3.6 Multivariate normal with unknown mean and variance\n\nConjugate inverse-Wishart family of prior distributions\n\nRecall that the conjugate distribution for the univariate normal with unknown mean and\nvariance is the normal-inverse-χ2 distribution (3.6). We can use the inverse-Wishart dis-\ntribution, a multivariate generalization of the scaled inverse-χ2, to describe the prior dis-\ntribution of the matrix Σ. The conjugate prior distribution for (µ,Σ), the normal-inverse-\nWishart, is conveniently parameterized in terms of hyperparameters (µ0,Λ0/κ0; ν0,Λ0):\n\nΣ ∼ Inv-Wishartν0(Λ\n−1\n0 )\n\nµ|Σ ∼ N(µ0,Σ/κ0),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.6. MULTIVARIATE NORMAL WITH UNKNOWN MEAN AND VARIANCE 73\n\nwhich corresponds to the joint prior density\n\np(µ,Σ)∝|Σ|−((ν0+d)/2+1) exp\n\n(\n−1\n\n2\ntr(Λ0Σ\n\n−1)− κ0\n2\n(µ− µ0)\n\nTΣ−1(µ− µ0)\n\n)\n.\n\nThe parameters ν0 and Λ0 describe the degrees of freedom and the scale matrix for the\ninverse-Wishart distribution on Σ. The remaining parameters are the prior mean, µ0, and\nthe number of prior measurements, κ0, on the Σ scale. Multiplying the prior density by the\nnormal likelihood results in a posterior density of the same family with parameters\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nΛn = Λ0 + S +\nκ0n\n\nκ0 + n\n(y − µ0)(y − µ0)\n\nT ,\n\nwhere S is the sum of squares matrix about the sample mean,\n\nS =\n\nn∑\n\ni=1\n\n(yi − y)(yi − y)T .\n\nOther results from the univariate normal easily generalize to the multivariate case. The\nmarginal posterior distribution of µ is multivariate tνn−d+1(µn,Λn/(κn(νn − d+ 1))). The\nposterior predictive distribution of a new observation ỹ is also multivariate t with an ad-\nditional factor of κn+1 in the numerator of the scale matrix. Samples from the joint\nposterior distribution of (µ,Σ) are easily obtained using the following procedure: first,\ndraw Σ|y ∼ Inv-Wishartνn(Λ\n\n−1\nn ), then draw µ|Σ, y ∼ N(µn,Σ/κn). See Appendix A for\n\ndrawing from inverse-Wishart and multivariate normal distributions. To draw from the\nposterior predictive distribution of a new observation, draw ỹ|µ,Σ, y ∼ N(µ,Σ), given the\nalready drawn values of µ and Σ.\n\nDifferent noninformative prior distributions\n\nInverse-Wishart with d + 1 degrees of freedom. Setting Σ ∼ Inv-Wishartd+1(I) has the\nappealing feature that each of the correlations in Σ has, marginally, a uniform prior distri-\nbution. (The joint distribution is not uniform, however, because of the constraint that the\ncorrelation matrix be positive definite.)\n\nInverse-Wishart with d−1 degrees of freedom. Another proposed noninformative prior\ndistribution is the multivariate Jeffreys prior density,\n\np(µ,Σ) ∝ |Σ|−(d+1)/2,\n\nwhich is the limit of the conjugate prior density as κ0 → 0, ν0 → −1, |Λ0| → 0. The\ncorresponding posterior distribution can be written as\n\nΣ|y ∼ Inv-Wishartn−1(S\n−1)\n\nµ|Σ, y ∼ N(y,Σ/n).\n\nResults for the marginal distribution of µ and the posterior predictive distribution of ỹ,\nassuming that the posterior distribution is proper, follow from the previous paragraph. For\nexample, the marginal posterior distribution of µ is multivariate tn−d(y, S/(n(n− d))).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n74 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nDose, xi Number of Number of\n(log g/ml) animals, ni deaths, yi\n\n−0.86 5 0\n−0.30 5 1\n−0.05 5 3\n0.73 5 5\n\nTable 3.1: Bioassay data from Racine et al. (1986).\n\nScaled inverse-Wishart model\n\nWhen modeling covariance matrices it can help to extend the inverse-Wishart model by\nmultiplying by a set of scale parameters that can be modeled separately. This gives flexibility\nin modeling and allows one to set up a uniform or weak prior distribution on correlations\nwithout overly constraining the variance parameters. The scaled inverse-Wishart model for\nΣ has the form,\n\nΣ = Diag(ξ)ΣηDiag(ξ),\n\nwhere Ση is given an inverse-Wishart prior distribution (one choice is Inv-Wishartd+1(I), so\nthat the marginal distributions of the correlations are uniform) and then the scale param-\neters ξ can be given weakly informative priors themselves. We discuss further in Section\n15.4 in the context of varying-intercept, varying-slope hierarchical regression models.\n\n3.7 Example: analysis of a bioassay experiment\n\nBeyond the normal distribution, few multiparameter sampling models allow simple explicit\ncalculation of posterior distributions. Data analysis for such models is possible using the\ncomputational methods described in Part III of this book. Here we present an example\nof a nonconjugate model for a bioassay experiment, drawn from the literature on applied\nBayesian statistics. The model is a two-parameter example from the broad class of general-\nized linear models to be considered more thoroughly in Chapter 16. We use a particularly\nsimple simulation approach, approximating the posterior distribution by a discrete distri-\nbution supported on a two-dimensional grid of points, that provides sufficiently accurate\ninferences for this two-parameter example.\n\nThe scientific problem and the data\n\nIn the development of drugs and other chemical compounds, acute toxicity tests or bioassay\nexperiments are commonly performed on animals. Such experiments proceed by adminis-\ntering various dose levels of the compound to batches of animals. The animals’ responses\nare typically characterized by a dichotomous outcome: for example, alive or dead, tumor\nor no tumor. An experiment of this kind gives rise to data of the form\n\n(xi, ni, yi); i = 1, . . . , k,\n\nwhere xi represents the ith of k dose levels (often measured on a logarithmic scale) given\nto ni animals, of which yi subsequently respond with positive outcome. An example of real\ndata from such an experiment is shown in Table 3.1: twenty animals were tested, five at\neach of four dose levels.\n\nModeling the dose–response relation\n\nGiven what we have seen so far, we must model the outcomes of the five animals within\neach group i as exchangeable, and it seems reasonable to model them as independent with\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 75\n\nequal probabilities, which implies that the data points yi are binomially distributed:\n\nyi|θi ∼ Bin(ni, θi),\n\nwhere θi is the probability of death for animals given dose xi. (An example of a situation\nin which independence and the binomial model would not be appropriate is if the deaths\nwere caused by a contagious disease.) For this experiment, it is also reasonable to treat the\noutcomes in the four groups as independent of each other, given the parameters θ1, . . . , θ4.\n\nThe simplest analysis would treat the four parameters θi as exchangeable in their prior\ndistribution, perhaps using a noninformative density such as p(θ1, . . . , θ4) ∝ 1, in which case\nthe parameters θi would have independent beta posterior distributions. The exchangeable\nprior model for the θi parameters has a serious flaw, however; we know the dose level xi\nfor each group i, and one would expect the probability of death to vary systematically as a\nfunction of dose.\n\nThe simplest model of the dose–response relation—that is, the relation of θi to xi—is\nlinear: θi = α + βxi. Unfortunately, this model has the flaw that at low or high doses,\nxi approaches ±∞ (recall that the dose is measured on the log scale), whereas θi, being a\nprobability, must be constrained to lie between 0 and 1. The standard solution is to use a\ntransformation of the θ’s, such as the logistic, in the dose–response relation:\n\nlogit(θi) = α+ βxi, (3.14)\n\nwhere logit(θi) = log(θi/(1 − θi)) as defined in (1.10). This is called a logistic regression\nmodel.\n\nThe likelihood\n\nUnder the model (3.14), we can write the sampling distribution, or likelihood, for each\ngroup i in terms of the parameters α and β as\n\np(yi|α, β, ni, xi) ∝ [logit−1(α+ βxi)]\nyi [1− logit−1(α+ βxi)]\n\nni−yi .\n\nThe model is characterized by the parameters α and β, whose joint posterior distribution\nis\n\np(α, β|y, n, x) ∝ p(α, β|n, x)p(y|α, β, n, x) (3.15)\n\n∝ p(α, β)\n\nk∏\n\ni=1\n\np(yi|α, β, ni, xi).\n\nWe consider the sample sizes ni and dose levels xi as fixed for this analysis and suppress\nthe conditioning on (n, x) in subsequent notation.\n\nThe prior distribution\n\nWe present an analysis based on a prior distribution for (α, β) that is independent and locally\nuniform in the two parameters; that is, p(α, β) ∝ 1. In practice, we might use a uniform\nprior distribution if we really have no prior knowledge about the parameters, or if we want to\npresent a simple analysis of this experiment alone. If the analysis using the noninformative\nprior distribution is insufficiently precise, we may consider using other sources of substantive\ninformation (for example, from other bioassay experiments) to construct an informative\nprior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n76 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.3 (a) Contour plot for the posterior density of the parameters in the bioassay example.\nContour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of 1000 draws\nfrom the posterior distribution.\n\nA rough estimate of the parameters\n\nWe will compute the joint posterior distribution (3.15) at a grid of points (α, β), but before\ndoing so, it is a good idea to get a rough estimate of (α, β) so we know where to look. To\nobtain the rough estimate, we use existing software to perform a logistic regression; that\nis, finding the maximum likelihood estimate of (α, β) in (3.15) for the four data points in\n\nTable 3.1. The estimate is (α̂, β̂) = (0.8, 7.7), with standard errors of 1.0 and 4.9 for α and\nβ, respectively.\n\nObtaining a contour plot of the joint posterior density\n\nWe are now ready to compute the posterior density at a grid of points (α, β). After some\nexperimentation, we use the range (α, β) ∈ [−5, 10]× [−10, 40], which captures almost all\nthe mass of the posterior distribution. The resulting contour plot appears in Figure 3.3a;\na general justification for setting the lowest contour level at 0.05 for two-dimensional plots\nappears in Section 4.1.\n\nSampling from the joint posterior distribution\n\nHaving computed the unnormalized posterior density at a grid of values that cover the\neffective range of (α, β), we can normalize by approximating the distribution as a step\nfunction over the grid and setting the total probability in the grid to 1. We sample 1000\nrandom draws (αs, βs) from the posterior distribution using the following procedure.\n\n1. Compute the marginal posterior distribution of α by numerically summing over β in the\ndiscrete distribution computed on the grid of Figure 3.3a.\n\n2. For s = 1, . . . , 1000:\n\n(a) Draw αs from the discretely computed p(α|y); this can be viewed as a discrete version\nof the inverse cdf method described in Section 1.9.\n\n(b) Draw βs from the discrete conditional distribution, p(β|α, y), given the just-sampled\nvalue of α.\n\n(c) For each of the sampled α and β, add a uniform random jitter centered at zero with\na width equal to the spacing of the sampling grid. This gives the simulation draws a\ncontinuous distribution.\n\nThe 1000 draws (αs, βs) are displayed on a scatterplot in Figure 3.3b. The scale of the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 77\n\nFigure 3.4 Histogram of the draws from the posterior distribution of the LD50 (on the scale of log\ndose in g/ml) in the bioassay example, conditional on the parameter β being positive.\n\nplot, which is the same as the scale of Figure 3.3a, has been set large enough that all the\n1000 draws would fit on the graph.\n\nThere are a number of practical considerations when applying this two-dimensional grid\napproximation. There can be difficulty finding the correct location and scale for the grid\npoints. A grid that is defined on too small an area may miss important features of the\nposterior distribution that fall outside the grid. A grid defined on a large area with wide\nintervals between points can miss important features that fall between the grid points. It\nis also important to avoid overflow and underflow operations when computing the poste-\nrior distribution. It is usually a good idea to compute the logarithm of the unnormalized\nposterior distribution and subtract off the maximum value before exponentiating. This\ncreates an unnormalized discrete approximation with maximum value 1, which can then be\nnormalized (by setting the total probability in the grid to 1).\n\nThe posterior distribution of the LD50\n\nA parameter of common interest in bioassay studies is the LD50—the dose level at which\nthe probability of death is 50%. In our logistic model, a 50% survival rate means\n\nLD50: E\n\n(\nyi\nni\n\n)\n= logit−1(α+ βxi) = 0.5;\n\nthus, α + βxi = logit(0.5) = 0, and the LD50 is xi = −α/β. Computing the posterior\ndistribution of any summaries in the Bayesian approach is straightforward, as discussed at\nthe end of Section 1.9. Given what we have done so far, simulating the posterior distribution\nof the LD50 is trivial: we just compute −α/β for the 1000 draws of (α, β) pictured in Figure\n3.3b.\n\nDifficulties with the LD50 parameterization if the drug is beneficial. In the context of this\nexample, LD50 is a meaningless concept if β ≤ 0, in which case increasing the dose does not\ncause the probability of death to increase. If we were certain that the drug could not cause\nthe tumor rate to decrease, we should constrain the parameter space to exclude values of β\nless than 0. However, it seems more reasonable here to allow the possibility of β ≤ 0 and\njust note that LD50 is hard to interpret in this case.\n\nWe summarize the inference on the LD50 scale by reporting two results: (1) the posterior\nprobability that β > 0—that is, that the drug is harmful—and (2) the posterior distribution\nfor the LD50 conditional on β > 0. All of the 1000 simulation draws had positive values of\nβ, so the posterior probability that β > 0 is roughly estimated to exceed 0.999. We compute\nthe LD50 for the simulation draws with positive values of β (which happen to be all 1000\ndraws for this example); a histogram is displayed in Figure 3.4. This example illustrates that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n78 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe marginal posterior mean is not always a good summary of inference about a parameter.\nWe are not, in general, interested in the posterior mean of the LD50, because the posterior\nmean includes the cases in which the dose–response relation is negative.\n\n3.8 Summary of elementary modeling and computation\n\nThe lack of multiparameter models permitting easy calculation of posterior distributions is\nnot a major practical handicap for three main reasons. First, when there are few parame-\nters, posterior inference in nonconjugate multiparameter models can be obtained by simple\nsimulation methods, as we have seen in the bioassay example. Second, sophisticated models\ncan often be represented in a hierarchical or conditional manner, as we shall see in Chapter\n5, for which effective computational strategies are available (as we discuss in general in Part\nIII). Finally, as we discuss in Chapter 4, we can often apply a normal approximation to\nthe posterior distribution, and therefore the conjugate structure of the normal model can\nplay an important role in practice, well beyond its application to explicitly normal sampling\nmodels.\n\nOur successful analysis of the bioassay example suggests the following strategy for com-\nputation of simple Bayesian posterior distributions. What follows is not truly a general ap-\nproach, but it summarizes what we have done so far and foreshadows the general methods—\nbased on successive approximations—presented in Part III.\n\n1. Write the likelihood part of the model, p(y|θ), ignoring any factors that are free of θ.\n\n2. Write the posterior density, p(θ|y) ∝ p(θ)p(y|θ). If prior information is well-formulated,\ninclude it in p(θ). Otherwise use a weakly informative prior distribution or temporarily\nset p(θ) ∝ constant, with the understanding that the prior density can be altered later\nto include additional information or structure.\n\n3. Create a crude estimate of the parameters, θ, for use as a starting point and a comparison\nto the computation in the next step.\n\n4. Draw simulations θ1, . . . , θS , from the posterior distribution. Use the sample draws to\ncompute the posterior density of any functions of θ that may be of interest.\n\n5. If any predictive quantities, ỹ, are of interest, simulate ỹ1, . . . , ỹS by drawing each ỹs\n\nfrom the sampling distribution conditional on the drawn value θs, p(ỹ|θs). In Chapter\n6, we discuss how to use posterior simulations of θ and ỹ to check the fit of the model to\ndata and substantive knowledge.\n\nFor nonconjugate models, step 4 above can be difficult. Various methods have been\ndeveloped to draw posterior simulations in complicated models, as we discuss in Part III.\nOccasionally, high-dimensional problems can be solved by combining analytical and nu-\nmerical simulation methods. If θ has only one or two components, it is possible to draw\nsimulations by computing on a grid, as we illustrated in the previous section for the bioassay\nexample.\n\n3.9 Bibliographic note\n\nChapter 2 of Box and Tiao (1973) thoroughly treats the univariate and multivariate normal\ndistribution problems and also some related problems such as estimating the difference\nbetween two means and the ratio between two variances. At the time that book was\nwritten, computer simulation methods were much less convenient than they are now, and\nso Box and Tiao, and other Bayesian authors of the period, restricted their attention to\nconjugate families and devoted much effort to deriving analytic forms of marginal posterior\ndensities.\n\nMany textbooks on multivariate analysis discuss the unique mathematical features of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 79\n\nSurvey Bush Dukakis No opinion/other Total\n\npre-debate 294 307 38 639\npost-debate 288 332 19 639\n\nTable 3.2 Number of respondents in each preference category from ABC News pre- and post-debate\nsurveys in 1988.\n\nthe multivariate normal distribution, such as the property that all marginal and conditional\ndistributions of components of a multivariate normal vector are normal; for example, see\nMardia, Kent, and Bibby (1979).\n\nSimon Newcomb’s data, along with a discussion of his experiment, appear in Stigler\n(1977).\n\nThe multinomial model and corresponding informative and noninformative prior distri-\nbutions are discussed by Good (1965) and Fienberg (1977); also see the bibliographic note\non loglinear models at the end of Chapter 16.\n\nThe data and model for the bioassay example appear in Racine et al. (1986), an article\nthat presents several examples of simple Bayesian analyses that have been useful in the\npharmaceutical industry.\n\n3.10 Exercises\n\n1. Binomial and multinomial models: suppose data (y1, . . . , yJ) follow a multinomial distri-\nbution with parameters (θ1, . . . , θJ). Also suppose that θ = (θ1, . . . , θJ ) has a Dirichlet\nprior distribution. Let α = θ1\n\nθ1+θ2\n.\n\n(a) Write the marginal posterior distribution for α.\n\n(b) Show that this distribution is identical to the posterior distribution for α obtained by\ntreating y1 as an observation from the binomial distribution with probability α and\nsample size y1 + y2, ignoring the data y3, . . . , yJ .\n\nThis result justifies the application of the binomial distribution to multinomial problems\nwhen we are only interested in two of the categories; for example, see the next problem.\n\n2. Comparison of two multinomial observations: on September 25, 1988, the evening of a\npresidential campaign debate, ABC News conducted a survey of registered voters in the\nUnited States; 639 persons were polled before the debate, and 639 different persons were\npolled after. The results are displayed in Table 3.2. Assume the surveys are independent\nsimple random samples from the population of registered voters. Model the data with\ntwo different multinomial distributions. For j = 1, 2, let αj be the proportion of voters\nwho preferred Bush, out of those who had a preference for either Bush or Dukakis at\nthe time of survey j. Plot a histogram of the posterior density for α2 − α1. What is the\nposterior probability that there was a shift toward Bush?\n\n3. Estimation from two independent experiments: an experiment was performed on the\neffects of magnetic fields on the flow of calcium out of chicken brains. Two groups\nof chickens were involved: a control group of 32 chickens and an exposed group of 36\nchickens. One measurement was taken on each chicken, and the purpose of the experiment\nwas to measure the average flow µc in untreated (control) chickens and the average flow\nµt in treated chickens. The 32 measurements on the control group had a sample mean of\n1.013 and a sample standard deviation of 0.24. The 36 measurements on the treatment\ngroup had a sample mean of 1.173 and a sample standard deviation of 0.20.\n\n(a) Assuming the control measurements were taken at random from a normal distribution\nwith mean µc and variance σ2\n\nc , what is the posterior distribution of µc? Similarly, use\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n80 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe treatment group measurements to determine the marginal posterior distribution\nof µt. Assume a uniform prior distribution on (µc, µt, log σc, log σt).\n\n(b) What is the posterior distribution for the difference, µt − µc? To get this, you may\nsample from the independent t distributions you obtained in part (a) above. Plot a\nhistogram of your samples and give an approximate 95% posterior interval for µt−µc.\n\nThe problem of estimating two normal means with unknown ratio of variances is called\nthe Behrens–Fisher problem.\n\n4. Inference for a 2× 2 table: an experiment was performed to estimate the effect of beta-\nblockers on mortality of cardiac patients. A group of patients were randomly assigned\nto treatment and control groups: out of 674 patients receiving the control, 39 died, and\nout of 680 receiving the treatment, 22 died. Assume that the outcomes are independent\nand binomially distributed, with probabilities of death of p0 and p1 under the control\nand treatment, respectively. We return to this example in Section 5.6.\n\n(a) Set up a noninformative prior distribution on (p0, p1) and obtain posterior simulations.\n\n(b) Summarize the posterior distribution for the odds ratio, (p1/(1− p1))/(p0/(1− p0)).\n(c) Discuss the sensitivity of your inference to your choice of noninformative prior density.\n\n5. Rounded data: it is a common problem for measurements to be observed in rounded\nform (for a review, see Heitjan, 1989). For a simple example, suppose we weigh an\nobject five times and measure weights, rounded to the nearest pound, of 10, 10, 12, 11,\n9. Assume the unrounded measurements are normally distributed with a noninformative\nprior distribution on the mean µ and variance σ2.\n\n(a) Give the posterior distribution for (µ, σ2) obtained by pretending that the observations\nare exact unrounded measurements.\n\n(b) Give the correct posterior distribution for (µ, σ2) treating the measurements as rounded.\n\n(c) How do the incorrect and correct posterior distributions differ? Compare means,\nvariances, and contour plots.\n\n(d) Let z = (z1, . . . , z5) be the original, unrounded measurements corresponding to the five\nobservations above. Draw simulations from the posterior distribution of z. Compute\nthe posterior mean of (z1 − z2)2.\n\n6. Binomial with unknown probability and sample size: some of the difficulties with setting\nprior distributions in multiparameter models can be illustrated with the simple binomial\ndistribution. Consider data y1, . . . , yn modeled as independent Bin(N, θ), with both N\nand θ unknown. Defining a convenient family of prior distributions on (N, θ) is difficult,\npartly because of the discreteness of N .\nRaftery (1988) considers a hierarchical approach based on assigning the parameter N\na Poisson distribution with unknown mean µ. To define a prior distribution on (θ,N),\nRaftery defines λ = µθ and specifies a prior distribution on (λ, θ). The prior distribution\nis specified in terms of λ rather than µ because ‘it would seem easier to formulate prior\ninformation about λ, the unconditional expectation of the observations, than about µ,\nthe mean of the unobserved quantity N .’\n\n(a) A suggested noninformative prior distribution is p(λ, θ) ∝ λ−1. What is a motivation\nfor this noninformative distribution? Is the distribution improper? Transform to\ndetermine p(N, θ).\n\n(b) The Bayesian method is illustrated on counts of waterbuck obtained by remote pho-\ntography on five separate days in Kruger Park in South Africa. The counts were\n53, 57, 66, 67, and 72. Perform the Bayesian analysis on these data and display a\nscatterplot of posterior simulations of (N, θ). What is the posterior probability that\nN > 100?\n\n(c) Why not simply use a Poisson with fixed µ as a prior distribution for N?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 81\n\nType of Bike Counts of bicycles/other vehicles\nstreet route?\n\nResidential yes 16/58, 9/90, 10/48, 13/57, 19/103,\n20/57, 18/86, 17/112, 35/273, 55/64\n\nResidential no 12/113, 1/18, 2/14, 4/44, 9/208,\n7/67, 9/29, 8/154\n\nFairly busy yes 8/29, 35/415, 31/425, 19/42, 38/180,\n47/675, 44/620, 44/437, 29/47, 18/462\n\nFairly busy no 10/557, 43/1258, 5/499, 14/601, 58/1163,\n15/700, 0/90, 47/1093, 51/1459, 32/1086\n\nBusy yes 60/1545, 51/1499, 58/1598, 59/503, 53/407,\n68/1494, 68/1558, 60/1706, 71/476, 63/752\n\nBusy no 8/1248, 9/1246, 6/1596, 9/1765, 19/1290,\n61/2498, 31/2346, 75/3101, 14/1918, 25/2318\n\nTable 3.3 Counts of bicycles and other vehicles in one hour in each of 10 city blocks in each of\nsix categories. (The data for two of the residential blocks were lost.) For example, the first block\nhad 16 bicycles and 58 other vehicles, the second had 9 bicycles and 90 other vehicles, and so on.\nStreets were classified as ‘residential,’ ‘fairly busy,’ or ‘busy’ before the data were gathered.\n\n7. Poisson and binomial distributions: a student sits on a street corner for an hour and\nrecords the number of bicycles b and the number of other vehicles v that go by. Two\nmodels are considered:\n\n• The outcomes b and v have independent Poisson distributions, with unknown means\nθb and θv.\n\n• The outcome b has a binomial distribution, with unknown probability p and sample\nsize b+ v.\n\nShow that the two models have the same likelihood if we define p = θb\nθb+θv\n\n.\n\n8. Analysis of proportions: a survey was done of bicycle and other vehicular traffic in the\nneighborhood of the campus of the University of California, Berkeley, in the spring of\n1993. Sixty city blocks were selected at random; each block was observed for one hour,\nand the numbers of bicycles and other vehicles traveling along that block were recorded.\nThe sampling was stratified into six types of city blocks: busy, fairly busy, and residential\nstreets, with and without bike routes, with ten blocks measured in each stratum. Table\n3.3 displays the number of bicycles and other vehicles recorded in the study. For this\nproblem, restrict your attention to the first four rows of the table: the data on residential\nstreets.\n\n(a) Let y1, . . . , y10 and z1, . . . , z8 be the observed proportion of traffic that was on bicycles\nin the residential streets with bike lanes and with no bike lanes, respectively (so\ny1 = 16/(16 + 58) and z1 = 12/(12 + 113), for example). Set up a model so that the\nyi’s are independent and identically distributed given parameters θy and the zi’s are\nindependent and identically distributed given parameters θz.\n\n(b) Set up a prior distribution that is independent in θy and θz.\n\n(c) Determine the posterior distribution for the parameters in your model and draw 1000\nsimulations from the posterior distribution. (Hint: θy and θz are independent in the\nposterior distribution, so they can be simulated independently.)\n\n(d) Let µy = E(yi|θy) be the mean of the distribution of the yi’s; µy will be a function of\nθy. Similarly, define µz. Using your posterior simulations from (c), plot a histogram of\nthe posterior simulations of µy − µz, the expected difference in proportions in bicycle\ntraffic on residential streets with and without bike lanes.\n\nWe return to this example in Exercise 5.13.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n82 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\n9. Conjugate normal model: suppose y is an independent and identically distributed sam-\nple of size n from the distribution N(µ, σ2), where the prior distribution for (µ, σ2) is\nN-Inv-χ2(µ, σ2|µ0, σ\n\n2\n0/κ0; ν0, σ\n\n2\n0); that is, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0) and µ|σ2 ∼ N(µ0, σ\n\n2/κ0).\nThe posterior distribution, p(µ, σ2|y), is also normal-inverse-χ2; derive explicitly its pa-\nrameters in terms of the prior parameters and the sufficient statistics of the data.\n\n10. Comparison of normal variances: for j = 1, 2, suppose that\n\nyj1, . . . , yjnj |µj , σ2\nj ∼ iid N(µj , σ\n\n2\nj ),\n\np(µj , σ\n2\nj ) ∝ σ−2\n\nj ,\n\nand (µ1, σ\n2\n1) are independent of (µ2, σ\n\n2\n2) in the prior distribution. Show that the posterior\n\ndistribution of (s21/s\n2\n2)/(σ\n\n2\n1/σ\n\n2\n2) is F with (n1−1) and (n2−1) degrees of freedom. (Hint:\n\nto show the required form of the posterior density, you do not need to carry along all the\nnormalizing constants.)\n\n11. Computation: in the bioassay example, replace the uniform prior density by a joint nor-\nmal prior distribution on (α, β), with α ∼ N(0, 22), β ∼ N(10, 102), and corr(α, β)=0.5.\n\n(a) Repeat all the computations and plots of Section 3.7 with this new prior distribution.\n\n(b) Check that your contour plot and scatterplot look like a compromise between the prior\ndistribution and the likelihood (as displayed in Figure 3.3).\n\n(c) Discuss the effect of this hypothetical prior information on the conclusions in the\napplied context.\n\n12. Poisson regression model: expand the model of Exercise 2.13(a) by assuming that the\nnumber of fatal accidents in year t follows a Poisson distribution with mean α+ βt. You\nwill estimate α and β, following the example of the analysis in Section 3.7.\n\n(a) Discuss various choices for a ‘noninformative’ prior for (α, β). Choose one.\n\n(b) Discuss what would be a realistic informative prior distribution for (α, β). Sketch its\ncontours and then put it aside. Do parts (c)–(h) of this problem using your noninfor-\nmative prior distribution from (a).\n\n(c) Write the posterior density for (α, β). What are the sufficient statistics?\n\n(d) Check that the posterior density is proper.\n\n(e) Calculate crude estimates and uncertainties for (α, β) using linear regression.\n\n(f) Plot the contours and take 1000 draws from the joint posterior density of (α, β).\n\n(g) Using your samples of (α, β), plot a histogram of the posterior density for the expected\nnumber of fatal accidents in 1986, α+ 1986β.\n\n(h) Create simulation draws and obtain a 95% predictive interval for the number of fatal\naccidents in 1986.\n\n(i) How does your hypothetical informative prior distribution in (b) differ from the pos-\nterior distribution in (f) and (g), obtained from the noninformative prior distribution\nand the data? If they disagree, discuss.\n\n13. Multivariate normal model: derive equations (3.12) by completing the square in vector-\nmatrix notation.\n\n14. Improper prior and proper posterior distributions: prove that the posterior density (3.15)\nfor the bioassay example has a finite integral over the range (α, β) ∈ (−∞,∞)×(−∞,∞).\n\n15. Joint distributions: The autoregressive time-series model y1, y2, . . . with mean level 0,\nautocorrelation 0.8, residual standard deviation 1, and normal errors can be written as\n(yt|yt−1, yt−2, . . .) ∼ N(0.8yt−1, 1) for all t.\n\n(a) Prove that the distribution of yt, given the observations at all other integer time points\nt, depends only on yt−1 and yt+1.\n\n(b) What is the distribution of yt given yt−1 and yt+1?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 4\n\nAsymptotics and connections to non-Bayesian\n\napproaches\n\nWe have seen that many simple Bayesian analyses based on noninformative prior distribu-\ntions give similar results to standard non-Bayesian approaches (for example, the posterior t\ninterval for the normal mean with unknown variance). The extent to which a noninforma-\ntive prior distribution can be justified as an objective assumption depends on the amount\nof information available in the data: in the simple cases discussed in Chapters 2 and 3,\nit was clear that as the sample size n increases, the influence of the prior distribution on\nposterior inferences decreases. These ideas, sometimes referred to as asymptotic theory,\nbecause they refer to properties that hold in the limit as n becomes large, will be reviewed\nin the present chapter, along with some more explicit discussion of the connections between\nBayesian and non-Bayesian methods. The large-sample results are not actually necessary\nfor performing Bayesian data analysis but are often useful as approximations and as tools\nfor understanding.\n\nWe begin this chapter with a discussion of the various uses of the normal approximation\nto the posterior distribution. Theorems about consistency and normality of the posterior\ndistribution in large samples are outlined in Section 4.2, followed by several counterexamples\nin Section 4.3; proofs of the theorems are sketched in Appendix B. Finally, we discuss how\nthe methods of frequentist statistics can be used to evaluate the properties of Bayesian\ninferences.\n\n4.1 Normal approximations to the posterior distribution\n\nNormal approximation to the joint posterior distribution\n\nIf the posterior distribution p(θ|y) is unimodal and roughly symmetric, it can be convenient\nto approximate it by a normal distribution; that is, the logarithm of the posterior density\nis approximated by a quadratic function of θ.\n\nHere we consider a quadratic approximation to the log-posterior density that is centered\nat the posterior mode (which in general is easy to compute using off-the-shelf optimization\nroutines); in Chapter 13 we discuss more elaborate approximations which can be effective\nin settings where simple mode-based approximations fail.\n\nA Taylor series expansion of log p(θ|y) centered at the posterior mode, θ̂ (where θ can\n\nbe a vector and θ̂ is assumed to be in the interior of the parameter space), gives\n\nlog p(θ|y) = log p(θ̂|y) + 1\n\n2\n(θ − θ̂)T\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n(θ − θ̂) + · · · , (4.1)\n\nwhere the linear term in the expansion is zero because the log-posterior density has zero\nderivative at its mode. As we discuss in Section 4.2, the remainder terms of higher order fade\nin importance relative to the quadratic term when θ is close to θ̂ and n is large. Considering\n\n83\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n84 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of θ, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\np(θ|y) ≈ N(θ̂, [I(θ̂)]−1), (4.2)\n\nwhere I(θ) is the observed information,\n\nI(θ) = − d2\n\ndθ2\nlog p(θ|y).\n\nIf the mode, θ̂, is in the interior of parameter space, then the matrix I(θ̂) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1, . . . , yn be independent observations from a N(µ, σ2) distribution, and,\nfor simplicity, we assume a uniform prior density for (µ, log σ). We set up a normal\napproximation to the posterior distribution of (µ, log σ), which has the virtue of re-\nstricting σ to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\nlog p(µ, log σ|y) = constant− n log σ − 1\n\n2σ2\n((n− 1)s2 + n(y − µ)2).\n\nThe first derivatives are\n\nd\n\ndµ\nlog p(µ, log σ|y) =\n\nn(y − µ)\nσ2\n\n,\n\nd\n\nd(log σ)\nlog p(µ, log σ|y) = −n+\n\n(n− 1)s2 + n(y − µ)2\nσ2\n\n,\n\nfrom which the posterior mode is readily obtained as\n\n(µ̂, log σ̂) =\n\n(\ny, log\n\n(√\nn− 1\n\nn\ns\n\n))\n.\n\nThe second derivatives of the log posterior density are\n\nd2\n\ndµ2\nlog p(µ, log σ|y) = − n\n\nσ2\n\nd2\n\ndµd(log σ)\nlog p(µ, log σ|y) = −2ny − µ\n\nσ2\n\nd2\n\nd(log σ)2\nlog p(µ, log σ|y) = − 2\n\nσ2\n((n− 1)s2 + n(y − µ)2).\n\nThe matrix of second derivatives at the mode is then\n\n(\n−n/σ̂2 0\n\n0 −2n\n\n)\n. From (4.2),\n\nthe posterior distribution can be approximated as\n\np(µ, log σ|y) ≈ N\n\n((\nµ\n\nlog σ\n\n) ∣∣∣∣\n(\n\ny\nlog σ̂\n\n)\n,\n\n(\nσ̂2/n 0\n0 1/(2n)\n\n))\n.\n\nIf we had instead constructed the normal approximation in terms of p(µ, σ2), the sec-\nond derivative matrix would be multiplied by the Jacobian of the transformation from\nlog σ to σ2 and the mode would change slightly, to σ̃2 = n\n\nn+2 σ̂\n2. The two components,\n\n(µ, σ2), would still be independent in their approximate posterior distribution, and\np(σ2|y) ≈ N(σ2|σ̃2, 2σ̃4/(n+ 2)).\n\n84\n\nThis electronic edition is for non-commercial purposes only.\n\n4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of 0, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\nply) = N(8, [L(8)|~*), (4.2)\n\nwhere I(0) is the observed information,\n\n2\n\nd\nI(0) =— 762 log p(4|y).\n\nIf the mode, 6, is in the interior of parameter space, then the matrix I (6) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\n\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1,-.-,Ym be independent observations from a N(,07) distribution, and,\nfor simplicity, we assume a uniform prior density for (u,loga). We set up a normal\napproximation to the posterior distribution of (~,loga), which has the virtue of re-\nstricting o to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\n—((n— 1s? +n(9 — )?).\n\nlog p(y, log oly) = constant — nlog a — 5\no\n\nThe first derivatives are\n\nd ny = LL\nda log p(u,logaly) = mow)\nd (n —1)s* + n(¥ — pw)?\nl | —\nTogo) og p(t, log oly) n+ 53 ;\n\nfrom which the posterior mode is readily obtained as\n\nvue (ne (=).\n\nThe second derivatives of the log posterior density are\n\n2\n\nd n\nae srt logely) = —Ta\na? y-u\n—— | I = -2\nTud(logo) og p(t, log aly) 13\nd? 2 ne:\nTloga)2 SPH log aly) = ~aa((n— 1s + n(y — 1)”).\n. ar . —n/6* 0\nThe matrix of second derivatives at the mode is then 6 on | From (4.2),\n\nthe posterior distribution can be approximated as\n\nP(L, log oly) ~n(( logo ) ( lone ).( -_ 1/(2n) ))\n\nIf we had instead constructed the normal approximation in terms of p(j1,07), the sec-\n\nond derivative matrix would be multiplied by the Jacobian of the transformation from\n\nlog o to o? and the mode would change slightly, to ¢? = age. The two components,\n\n(u,07), would still be independent in their approximate posterior distribution, and\np(o?|y) © N(o?|a?, 264/(n + 2)).\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.1. NORMAL APPROXIMATIONS TO THE POSTERIOR DISTRIBUTION 85\n\nInterpretation of the posterior density function relative to its maximum\n\nIn addition to its direct use as an approximation, the multivariate normal distribution pro-\nvides a benchmark for interpreting the posterior density function and contour plots. In the d-\ndimensional normal distribution, the logarithm of the density function is a constant plus a χ2\n\nd\n\ndistribution divided by −2. For example, the 95th percentile of the χ2\n10 density is 18.31, so if\n\na problem has d = 10 parameters, then approximately 95% of the posterior probability mass\nis associated with the values of θ for which p(θ|y) is no less than exp(−18.31/2) = 1.1×10−4\n\ntimes the density at the mode. Similarly, with d = 2 parameters, approximately 95% of the\nposterior mass corresponds to densities above exp(−5.99/2) = 0.05, relative to the density\nat the mode. In a two-dimensional contour plot of a posterior density (for example, Figure\n3.3a), the 0.05 contour line thus includes approximately 95% of the probability mass.\n\nSummarizing posterior distributions by point estimates and standard errors\n\nThe asymptotic theory outlined in Section 4.2 shows that if n is large enough, a posterior\ndistribution can be approximated by a normal distribution. In many areas of application, a\nstandard inferential summary is the 95% interval obtained by computing a point estimate,\nθ̂, such as the maximum likelihood estimate (which is the posterior mode under a uniform\nprior density), plus or minus two standard errors, with the standard error estimated from\n\nthe information at the estimate, I(θ̂). A different asymptotic argument justifies the non-\nBayesian, frequentist interpretation of this summary, but in many simple situations both\ninterpretations hold. It is difficult to give general guidelines on when the normal approxi-\nmation is likely to be adequate in practice. From the Bayesian point of view, the accuracy\nin any given example can be directly determined by inspecting the posterior distribution.\n\nIn many cases, convergence to normality of the posterior distribution for a parameter\nθ can be dramatically improved by transformation. If φ is a continuous transformation\nof θ, then both p(φ|y) and p(θ|y) approach normal distributions, but the closeness of the\napproximation for finite n can vary substantially with the transformation chosen.\n\nData reduction and summary statistics\n\nUnder the normal approximation, the posterior distribution is summarized by its mode, θ̂,\nand the curvature of the posterior density, I(θ̂); that is, asymptotically, these are sufficient\nstatistics. In the examples at the end of the next chapter, we shall see that it can be\nconvenient to summarize ‘local-level’ or ‘individual-level’ data from a number of sources by\ntheir normal-theory sufficient statistics. This approach using summary statistics allows the\nrelatively easy application of hierarchical modeling techniques to improve each individual\nestimate. For example, in Section 5.5, each of a set of eight experiments is summarized by\na point estimate and a standard error estimated from an earlier linear regression analysis.\nUsing summary statistics is clearly most reasonable when posterior distributions are close\nto normal; the approach can otherwise discard important information and lead to erroneous\ninferences.\n\nLower-dimensional normal approximations\n\nFor a finite sample size n, the normal approximation is typically more accurate for condi-\ntional and marginal distributions of components of θ than for the full joint distribution. For\nexample, if a joint distribution is multivariate normal, all its margins are normal, but the\nconverse is not true. Determining the marginal distribution of a component of θ is equiva-\nlent to averaging over all the other components of θ, and averaging a family of distributions\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n86 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nFigure 4.1 (a) Contour plot of the normal approximation to the posterior distribution of the pa-\nrameters in the bioassay example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the\nmode. Compare to Figure 3.3a. (b) Scatterplot of 1000 draws from the normal approximation to\nthe posterior distribution. Compare to Figure 3.3b.\n\ngenerally brings them closer to normality, by the same logic that underlies the central limit\ntheorem.\n\nThe normal approximation for the posterior distribution of a low-dimensional θ is often\nperfectly acceptable, especially after appropriate transformation. If θ is high-dimensional,\ntwo situations commonly arise. First, the marginal distributions of many individual com-\nponents of θ can be approximately normal; inference about any one of these parameters,\ntaken individually, can then be well summarized by a point estimate and a standard error.\nSecond, it is possible that θ can be partitioned into two subvectors, θ = (θ1, θ2), for which\np(θ2|y) is not necessarily close to normal, but p(θ1|θ2, y) is, perhaps with mean and variance\nthat are functions of θ2. The approach of approximation using conditional distributions is\noften useful, and we consider it more systematically in Section 13.5. Lower-dimensional\napproximations are increasingly popular, for example in computation for latent Gaussian\nmodels.\n\nFinally, approximations based on the normal distribution are often useful for debugging\na computer program or checking a more elaborate method for approximating the posterior\ndistribution.\n\nExample. Bioassay experiment (continued)\nWe illustrate the normal approximation for the model and data from the bioassay\nexperiment of Section 3.7. The sample size in this experiment is relatively small, only\ntwenty animals in all, and we find that the normal approximation is close to the exact\nposterior distribution but with important differences.\n\nThe normal approximation to the joint posterior distribution of (α, β). To begin, we\ncompute the mode of the posterior distribution (using a logistic regression program)\nand the normal approximation (4.2) evaluated at the mode. The posterior mode of\n(α, β) is the same as the maximum likelihood estimate because we have assumed a\nuniform prior density for (α, β). Figure 4.1 shows a contour plot of the bivariate normal\napproximation and a scatterplot of 1000 draws from this approximate distribution.\nThe plots resemble the plots of the actual posterior distribution in Figure 3.3 but\nwithout the skewness in the upper right corner of the earlier plots. The effect of\nthe skewness is apparent when comparing the mean of the normal approximation,\n(α, β) = (0.8, 7.7), to the mean of the actual posterior distribution, (α, β) = (1.4, 11.9),\ncomputed from the simulations displayed in Figure 3.3b.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.2. LARGE-SAMPLE THEORY 87\n\nFigure 4.2 (a) Histogram of the simulations of LD50, conditional on β > 0, in the bioassay example\nbased on the normal approximation p(α, β|y). The wide tails of the histogram correspond to values\nof β close to 0. Omitted from this histogram are five simulation draws with values of LD50 less\nthan −2 and four draws with values greater than 2; the extreme tails are truncated to make the\nhistogram visible. The values of LD50 for the 950 simulation draws corresponding to β > 0 had a\nrange of [−12.4, 5.4]. Compare to Figure 3.4. (b) Histogram of the central 95% of the distribution.\n\nThe posterior distribution for the LD50 using the normal approximation on (α, β).\nFlaws of the normal approximation. The same set of 1000 draws from the normal\napproximation can be used to estimate the probability that β is positive and the\nposterior distribution of the LD50, conditional on β being positive. Out of the 1000\nsimulation draws, 950 had positive values of β, yielding the estimate Pr(β > 0) = 0.95,\na different result than from the exact distribution, where Pr(β > 0) > 0.999. Con-\ntinuing with the analysis based on the normal approximation, we compute the LD50\nas −α/β for each of the 950 draws with β > 0; Figure 4.2a presents a histogram of\nthe LD50 values, excluding some extreme values in both tails. (If the entire range of\nthe simulations were included, the shape of the distribution would be nearly impos-\nsible to see.) To get a better picture of the center of the distribution, we display in\nFigure 4.2b a histogram of the middle 95% of the 950 simulation draws of the LD50.\nThe histograms are centered in approximately the same place as Figure 3.4 but with\nsubstantially more variation, due to the possibility that β is close to zero.\nIn summary, posterior inferences based on the normal approximation here are roughly\nsimilar to the exact results, but because of the small sample, the actual joint posterior\ndistribution is substantially more skewed than the large-sample approximation, and\nthe posterior distribution of the LD50 actually has much shorter tails than implied by\nusing the joint normal approximation. Whether or not these differences imply that\nthe normal approximation is inadequate for practical use in this example depends on\nthe ultimate aim of the analysis.\n\n4.2 Large-sample theory\n\nTo understand why the normal approximation is often reasonable, we review some theory\nof how the posterior distribution behaves as the amount of data, from some fixed sampling\ndistribution, increases.\n\nNotation and mathematical setup\n\nThe basic tool of large sample Bayesian inference is asymptotic normality of the posterior\ndistribution: as more and more data arrive from the same underlying process, the posterior\ndistribution of the parameter vector approaches multivariate normality, even if the true\ndistribution of the data is not within the parametric family under consideration. Mathe-\nmatically, the results apply most directly to observations y1, . . . , yn that are independent\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n88 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\noutcomes sampled from a common distribution, f(y). In many situations, the notion of a\n‘true’ underlying distribution, f(y), for the data is difficult to interpret, but it is necessary\nin order to develop the asymptotic theory. Suppose the data are modeled by a parametric\nfamily, p(y|θ), with a prior distribution p(θ). In general, the data points yi and the parame-\nter θ can be vectors. If the true data distribution is included in the parametric family—that\nis, if f(y) = p(y|θ0) for some θ0—then, in addition to asymptotic normality, the property\nof consistency holds: the posterior distribution converges to a point mass at the true pa-\nrameter value, θ0, as n→∞. When the true distribution is not included in the parametric\nfamily, there is no longer a true value θ0, but its role in the theoretical result is replaced by\na value θ0 that makes the model distribution, p(y|θ), closest to the true distribution, f(y),\nin a technical sense involving Kullback-Leibler divergence, as is explained in Appendix B.\n\nIn discussing the large-sample properties of posterior distributions, the concept of Fisher\ninformation, J(θ), introduced as (2.20) in Section 2.8 in the context of Jeffreys’ prior dis-\ntributions, plays an important role.\n\nAsymptotic normality and consistency\n\nThe fundamental mathematical result given in Appendix B shows that, under some regu-\nlarity conditions (notably that the likelihood is a continuous function of θ and that θ0 is\nnot on the boundary of the parameter space), as n → ∞, the posterior distribution of θ\napproaches normality with mean θ0 and variance (nJ(θ0))\n\n−1. At its simplest level, this\nresult can be understood in terms of the Taylor series expansion (4.1) of the log posterior\ndensity centered about the posterior mode. A preliminary result shows that the posterior\nmode is consistent for θ0, so that as n → ∞, the mass of the posterior distribution p(θ|y)\nbecomes concentrated in smaller and smaller neighborhoods of θ0, and the distance |θ̂− θ0|\napproaches zero.\n\nFurthermore, we can rewrite the coefficient of the quadratic term in (4.1):\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n=\n\n[\nd2\n\ndθ2\nlog p(θ)\n\n]\n\nθ=θ̂\n\n+\n\nn∑\n\ni=1\n\n[\nd2\n\ndθ2\nlog p(yi|θ)\n\n]\n\nθ=θ̂\n\n.\n\nThis coefficient is a single term for the prior plus the sum of n likelihood terms, each of\nwhose expected value under the true sampling distribution of yi, p(y|θ0), is approximately\n\n−J(θ0), as long as θ̂ is close to θ0 (we are assuming now that f(y) = p(y|θ0) for some θ0).\nTherefore, for large n, the curvature of the log posterior density can be approximated by\nthe Fisher information, evaluated at either θ̂ or θ0 (where only the former is available in\npractice).\n\nIn summary, in the limit of large n, in the context of a specified family of models,\nthe posterior mode, θ̂, approaches θ0, and the curvature (the observed information or the\n\nnegative of the coefficient of the second term in the Taylor expansion) approaches nJ(θ̂) or\nnJ(θ0). In addition, as n → ∞, the likelihood dominates the prior distribution, so we can\njust use the likelihood alone to obtain the mode and curvature for the normal approximation.\nMore precise statements of the theorems and outlines of proofs appear in Appendix B.\n\nLikelihood dominating the prior distribution\n\nThe asymptotic results formalize the notion that the importance of the prior distribution\ndiminishes as the sample size increases. One consequence of this result is that in problems\nwith large sample sizes we need not work especially hard to formulate a prior distribution\nthat accurately reflects all available information. When sample sizes are small, the prior\ndistribution is a critical part of the model specification.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.3. COUNTEREXAMPLES TO THE THEOREMS 89\n\n4.3 Counterexamples to the theorems\n\nA good way to understand the limitations of the large-sample results is to consider cases in\nwhich the theorems fail. The normal distribution is usually helpful as a starting approxi-\nmation, but one must examine deviations, especially with unusual parameter spaces and in\nthe extremes of the distribution. The counterexamples to the asymptotic theorems gener-\nally correspond to situations in which the prior distribution has an impact on the posterior\ninference, even in the limit of infinite sample sizes.\n\nUnderidentified models and nonidentified parameters. The model is underidentified given\ndata y if the likelihood, p(y|θ), is equal for a range of values of θ. This may also be called\na flat likelihood (although that term is sometimes also used for likelihoods for parameters\nthat are only weakly identified by the data—so the likelihood function is not strictly equal\nfor a range of values, only almost so). Under such a model, there is no single point θ0 to\nwhich the posterior distribution can converge.\n\nFor example, consider the model,\n(\nu\nv\n\n)\n∼ N\n\n((\n0\n0\n\n)\n,\n\n(\n1 ρ\nρ 1\n\n))\n,\n\nin which only one of u or v is observed from each pair (u, v). Here, the parameter ρ is\nnonidentified. The data supply no information about ρ, so the posterior distribution of ρ is\nthe same as its prior distribution, no matter how large the dataset is.\n\nThe only solution to a problem of nonidentified or underidentified parameters is to\nrecognize that the problem exists and, if there is a desire to estimate these parameters\nmore precisely, gather further information that can enable the parameters to be estimated\n(either from future data collection or from external information that can inform a prior\ndistribution).\n\nNumber of parameters increasing with sample size. In complicated problems, there can\nbe large numbers of parameters, and then we need to distinguish between different types\nof asymptotics. If, as n increases, the model changes so that the number of parameters\nincreases as well, then the simple results outlined in Sections 4.1 and 4.2, which assume a\nfixed model class p(yi|θ), do not apply. For example, sometimes a parameter is assigned\nfor each sampling unit in a study; for example, yi ∼ N(θi, σ\n\n2). The parameters θi generally\ncannot be estimated consistently unless the amount of data collected from each sampling\nunit increases along with the number of units. In nonparametric models such as Gaussian\nprocesses (see Chapter 21) there can be a new latent parameter corresponding to each data\npoint.\n\nAs with underidentified parameters, the posterior distribution for θi will not converge to\na point mass if new data do not bring enough information about θi. Here, the posterior dis-\ntribution will not in general converge to a point in the expanding parameter space (reflecting\nthe increasing dimensionality of θ), and its projection into any fixed space—for example,\nthe marginal posterior distribution of any particular θi—will not necessarily converge to a\npoint either.\n\nAliasing. Aliasing is a special case of underidentified parameters in which the same likeli-\nhood function repeats at a discrete set of points. For example, consider the following normal\nmixture model with independent and identically distributed data y1, . . . , yn and parameter\nvector θ = (µ1, µ2, σ\n\n2\n1 , σ\n\n2\n2 , λ):\n\np(yi|µ1, µ2, σ\n2\n1 , σ\n\n2\n2 , λ)=λ\n\n1√\n2π σ1\n\ne\n− 1\n\n2σ2\n1\n\n(yi−µ1)\n2\n\n+ (1 − λ) 1√\n2π σ2\n\ne\n− 1\n\n2σ2\n2\n\n(yi−µ2)\n2\n\n.\n\nIf we interchange each of (µ1, µ2) and (σ2\n1 , σ\n\n2\n2), and replace λ by (1 − λ), the likelihood of\n\nthe data remains the same. The posterior distribution of this model generally has at least\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n90 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ntwo modes and consists of a (50%, 50%) mixture of two distributions that are mirror images\nof each other; it does not converge to a single point no matter how large the dataset is.\n\nIn general, the problem of aliasing is eliminated by restricting the parameter space so\nthat no duplication appears; in the above example, the aliasing can be removed by restricting\nµ1 to be less than or equal to µ2.\n\nUnbounded likelihoods. If the likelihood function is unbounded, then there might be no\nposterior mode within the parameter space, invalidating both the consistency results and\nthe normal approximation. For example, consider the previous normal mixture model; for\nsimplicity, assume that λ is known (and not equal to 0 or 1). If we set µ1 = yi for any\narbitrary yi, and let σ2\n\n1 → 0, then the likelihood approaches infinity. As n → ∞, the\nnumber of modes of the likelihood increases. If the prior distribution is uniform on σ2\n\n1 and\nσ2\n2 in the region near zero, there will be likewise an increasing number of posterior modes,\n\nwith no corresponding normal approximations. A prior distribution proportional to σ−2\n1 σ−2\n\n2\n\njust makes things worse because this puts more probability near zero, causing the posterior\ndistribution to explode even faster at zero.\n\nIn general, this problem should arise rarely in practice, because the poles of an un-\nbounded likelihood correspond to unrealistic conditions in a model. The problem can be\nsolved by restricting to a plausible set of distributions. When the problem occurs for\nvariance components near zero, it can be resolved in various ways, such as using a prior\ndistribution that declines to zero at the boundary or by assigning an informative prior\ndistribution to the ratio of the variance parameters.\n\nImproper posterior distributions. If the unnormalized posterior density, obtained by multi-\nplying the likelihood by a ‘formal’ prior density representing an improper prior distribution,\nintegrates to infinity, then the asymptotic results, which rely on probabilities summing to\n1, do not follow. An improper posterior distribution cannot occur except with an improper\nprior distribution.\n\nA simple example arises from combining a Beta(0, 0) prior distribution for a binomial\nproportion with data consisting of n successes and 0 failures. More subtle examples, with\nhierarchical binomial and normal models, are discussed in Sections 5.3 and 5.4.\n\nThe solution to this problem is clear. An improper prior distribution is only a convenient\napproximation, and if it does not give rise to a proper posterior distribution then the sought\nconvenience is lost. In this case a proper prior distribution is needed, or at least an improper\nprior density that when combined with the likelihood has a finite integral.\n\nPrior distributions that exclude the point of convergence. If p(θ0) = 0 for a discrete param-\neter space, or if p(θ) = 0 in a neighborhood about θ0 for a continuous parameter space, then\nthe convergence results, which are based on the likelihood dominating the prior distribution,\ndo not hold. The solution is to give positive probability density in the prior distribution to\nall values of θ that are even remotely plausible.\n\nConvergence to the edge of parameter space. If θ0 is on the boundary of the parameter\nspace, then the Taylor series expansion must be truncated in some directions, and the\nnormal distribution will not necessarily be appropriate, even in the limit.\n\nFor example, consider the model, yi ∼ N(θ, 1), with the restriction θ ≥ 0. Suppose that\nthe model is accurate, with θ = 0 as the true value. The posterior distribution for θ is\nnormal, centered at y, truncated to be positive. The shape of the posterior distribution for\nθ, in the limit as n → ∞, is half of a normal distribution, centered about 0, truncated to\nbe positive.\n\nFor another example, consider the same assumed model, but now suppose that the true\nθ is −1, a value outside the assumed parameter space. The limiting posterior distribution\nfor θ has a sharp spike at 0 with no resemblance to a normal distribution at all. The\nsolution in practice is to recognize the difficulties of applying the normal approximation if\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.4. FREQUENCY EVALUATIONS OF BAYESIAN INFERENCES 91\n\none is interested in parameter values near the edge of parameter space. More important,\none should give positive prior probability density to all values of θ that are even remotely\npossible, or in the neighborhood of remotely possible values.\n\nTails of the distribution. The normal approximation can hold for essentially all the mass\nof the posterior distribution but still not be accurate in the tails. For example, suppose\np(θ|y) is proportional to e−c|θ| as |θ| → ∞, for some constant c; by comparison, the normal\n\ndensity is proportional to e−cθ\n2\n\n. The distribution function still converges to normality, but\nfor any finite sample size n the approximation fails far out in the tail. As another example,\nconsider any parameter that is constrained to be positive. For any finite sample size, the\nnormal approximation will admit the possibility of the parameter being negative, because\nthe approximation is simply not appropriate at that point in the tail of the distribution,\nbut that point becomes farther and farther in the tail as n increases.\n\n4.4 Frequency evaluations of Bayesian inferences\n\nJust as the Bayesian paradigm can be seen to justify simple ‘classical’ techniques, the\nmethods of frequentist statistics provide a useful approach for evaluating the properties of\nBayesian inferences—their operating characteristics—when these are regarded as embedded\nin a sequence of repeated samples. We have already used this notion in discussing the ideas\nof consistency and asymptotic normality. The notion of stable estimation, which says that\nfor a fixed model, the posterior distribution approaches a point as more data arrive—\nleading, in the limit, to inferential certainty—is based on the idea of repeated sampling.\nIt is certainly appealing that if the hypothesized family of probability models contains the\ntrue distribution (and assigns it a nonzero prior density), then as more information about\nθ arrives, the posterior distribution converges to the true value of θ.\n\nLarge-sample correspondence\n\nSuppose that the normal approximation (4.2) for the posterior distribution of θ holds; then\nwe can transform to the standard multivariate normal:\n\n[I(θ̂)]1/2(θ − θ̂) | y ∼ N(0, I), (4.3)\n\nwhere θ̂ is the posterior mode and [I(θ̂)]1/2 is any matrix square root of I(θ̂). In addition,\n\nθ̂ → θ0, and so we could just as well write the approximation in terms of I(θ0). If the true\ndata distribution is included in the class of models, so that f(y) ≡ p(y|θ) for some θ, then\nin repeated sampling with fixed θ, in the limit n→∞, it can be proved that\n\n[I(θ̂)]1/2(θ − θ̂) | θ ∼ N(0, I), (4.4)\n\na result from classical statistical theory that is generally proved for θ̂ equal to the maximum\nlikelihood estimate but is easily extended to the case with θ̂ equal to the posterior mode.\nThese results mean that, for any function of (θ− θ̂), the posterior distribution derived from\n(4.3) is asymptotically the same as the repeated sampling distribution derived from (4.4).\nThus, for example, a 95% central posterior interval for θ will cover the true value 95% of\nthe time under repeated sampling with any fixed true θ.\n\nPoint estimation, consistency, and efficiency\n\nIn the Bayesian framework, obtaining an ‘estimate’ of θ makes most sense in large samples\nwhen the posterior mode, θ̂, is the obvious center of the posterior distribution of θ and the\nuncertainty conveyed by nI(θ̂) is so small as to be practically unimportant. More generally,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n92 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nhowever, in smaller samples, it is inappropriate to summarize inference about θ by one\nvalue, especially when the posterior distribution of θ is more variable or even asymmetric.\nFormally, by incorporating loss functions in a decision-theoretic context (see Section 9.1\nand Exercise 9.6), one can define optimal point estimates; for the purposes of Bayesian data\nanalysis, however, we believe that representation of the full posterior distribution (as, for\nexample, with 50% and 95% central posterior intervals) is more useful. In many problems,\nespecially with large samples, a point estimate and its estimated standard error are adequate\nto summarize a posterior inference, but we interpret the estimate as an inferential summary,\nnot as the solution to a decision problem. In any case, the large-sample frequency properties\nof any estimate can be evaluated, without consideration of whether the estimate was derived\nfrom a Bayesian analysis.\n\nA point estimate is said to be consistent in the sampling theory sense if, as samples\nget larger, it converges to the true value of the parameter that it is asserted to estimate.\nThus, if f(y) ≡ p(y|θ0), then a point estimate θ̂ of θ is consistent if its sampling distribution\nconverges to a point mass at θ0 as the data sample size n increases (that is, considering\n\nθ̂ as a function of y, which is a random variable conditional on θ0). A closely related\n\nconcept is asymptotic unbiasedness, where (E(θ̂|θ0) − θ0)/sd(θ̂|θ0) converges to 0 (once\n\nagain, considering θ̂(y) as a random variable whose distribution is determined by p(y|θ0)).\nWhen the truth is included in the family of models being fitted, the posterior mode θ̂, and\nalso the posterior mean and median, are consistent and asymptotically unbiased under mild\nregularity conditions.\n\nA point estimate θ̂ is said to be efficient if there exists no other function of y that\nestimates θ with lower mean squared error, that is, if the expression E((θ̂ − θ0)2|θ0) is at\n\nits optimal, lowest value. More generally, the efficiency of θ̂ is the optimal mean squared\nerror divided by the mean squared error of θ̂. An estimate is asymptotically efficient if its\nefficiency approaches 1 as the sample size n → ∞. Under mild regularity conditions, the\ncenter of the posterior distribution (defined, for example, by the posterior mean, median,\nor mode) is asymptotically efficient.\n\nConfidence coverage\n\nIf a region C(y) includes θ0 at least 100(1 − α)% of the time (given any value of θ0) in\nrepeated samples, then C(y) is called a 100(1− α)% confidence region for the parameter\nθ. The word ‘confidence’ is carefully chosen to distinguish such intervals from probability\nintervals and to convey the following behavioral meaning: if one chooses α to be small\nenough (for example, 0.05 or 0.01), then since confidence regions cover the truth in at least\n(1 − α) of their applications, one should be confident in each application that the truth\nis within the region and therefore act as if it is. We saw previously that asymptotically a\n100(1− α)% central posterior interval for θ has the property that, in repeated samples of\ny, 100(1− α)% of the intervals include the value θ0.\n\n4.5 Bayesian interpretations of other statistical methods\n\nWe consider three levels at which Bayesian statistical methods can be compared with other\nmethods. First, as we have already indicated, Bayesian methods are often similar to other\nstatistical approaches in problems involving large samples from a fixed probability model.\nSecond, even for small samples, many statistical methods can be considered as approxi-\nmations to Bayesian inferences based on particular prior distributions; as a way of under-\nstanding a statistical procedure, it is often useful to determine the implicit underlying prior\ndistribution. Third, some methods from classical statistics (notably hypothesis testing)\ncan give results that differ greatly from those given by Bayesian methods. In this section,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 93\n\nwe briefly consider several statistical concepts—point and interval estimation, likelihood\ninference, unbiased estimation, frequency coverage of confidence intervals, hypothesis test-\ning, multiple comparisons, nonparametric methods, and the jackknife and bootstrap—and\ndiscuss their relation to Bayesian methods.\n\nOne way to develop possible models is to examine the interpretation of crude data-\nanalytic procedures as approximations to Bayesian inference under specific models. For\nexample, a widely used technique in sample surveys is ratio estimation, in which, for exam-\nple, given data from a simple random sample, one estimates R = y/x by yobs/xobs, in the\nnotation of Chapter 8. It can be shown that this estimate corresponds to a summary of\na Bayesian posterior inference given independent observations yi|xi ∼ N(Rxi, σ\n\n2xi) and a\nnoninformative prior distribution. Ratio estimates can be useful in a wide variety of cases\nin which this model does not hold, but when the data deviate greatly from this model, the\nratio estimate generally is not appropriate.\n\nFor another example, standard methods of selecting regression predictors, based on\n‘statistical significance,’ correspond roughly to Bayesian analyses under exchangeable prior\ndistributions on the coefficients in which the prior distribution of each coefficient is a mixture\nof a peak at zero and a widely spread distribution, as we discuss further in Section 14.6. We\nbelieve that understanding this correspondence suggests when such models can be usefully\napplied and how they can be improved. Often, in fact, such procedures can be improved\nby including additional information, for example, in problems involving large numbers of\npredictors, by clustering regression coefficients that are likely to be similar into batches.\n\nMaximum likelihood and other point estimates\n\nFrom the perspective of Bayesian data analysis, we can often interpret classical point esti-\nmates as exact or approximate posterior summaries based on some implicit full probability\nmodel. In the limit of large sample size, in fact, we can use asymptotic theory to con-\nstruct a theoretical Bayesian justification for classical maximum likelihood inference. In the\nlimit (assuming regularity conditions), the maximum likelihood estimate, θ̂, is a sufficient\nstatistic—and so is the posterior mode, mean, or median. That is, for large enough n,\nthe maximum likelihood estimate (or any of the other summaries) supplies essentially all\nthe information about θ available from the data. The asymptotic irrelevance of the prior\ndistribution can be taken to justify the use of convenient noninformative prior models.\n\nIn repeated sampling with θ = θ0,\n\np(θ̂(y)|θ=θ0) ≈ N(θ̂(y)|θ0, (nJ(θ0))−1);\n\nthat is, the sampling distribution of θ̂(y) is approximately normal with mean θ0 and precision\n\nnJ(θ0), where for clarity we emphasize that θ̂ is a function of y. Assuming that the prior\ndistribution is locally uniform (or continuous and nonzero) near the true θ, the simple\nanalysis of the normal mean (Section 3.5) shows that the posterior Bayesian inference is\n\np(θ|θ̂) ≈ N(θ|θ̂, (nJ(θ̂))−1).\n\nThis result appears directly from the asymptotic normality theorem, but deriving it indi-\nrectly through Bayesian inference given θ̂ gives insight into a Bayesian rationale for classical\nasymptotic inference based on point estimates and standard errors.\n\nFor finite n, the above approach is inefficient or wasteful of information to the extent\nthat θ̂ is not a sufficient statistic. When the number of parameters is large, the consistency\nresult is often not helpful, and noninformative prior distributions are hard to justify. As\ndiscussed in Chapter 5, hierarchical models are preferable when dealing with a large number\nof parameters since then their common distribution can be estimated from data. In addi-\ntion, any method of inference based on the likelihood alone can be improved if real prior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n94 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ninformation is available that is strong enough to contribute substantially to that contained\nin the likelihood function.\n\nUnbiased estimates\n\nSome non-Bayesian statistical methods place great emphasis on unbiasedness as a desirable\nprinciple of estimation, and it is intuitively appealing that, over repeated sampling, the mean\n(or perhaps the median) of a parameter estimate should be equal to its true value. Formally,\n\nan estimate θ̂(y) is called unbiased if E(θ̂(y)|θ) = θ for any value of θ, where this expectation\nis taken over the data distribution, p(y|θ). From a Bayesian perspective, the principle of\nunbiasedness is reasonable in the limit of large samples (see page 92) but otherwise is\npotentially misleading. The major difficulties arise when there are many parameters to be\nestimated and our knowledge or partial knowledge of some of these parameters is clearly\nrelevant to the estimation of others. Requiring unbiased estimates will often lead to relevant\ninformation being ignored (as we discuss with hierarchical models in Chapter 5). In sampling\ntheory terms, minimizing bias will often lead to counterproductive increases in variance.\n\nOne general problem with unbiasedness (and point estimation in general) is that it is\noften not possible to estimate several parameters at once in an even approximately unbiased\nmanner. For example, unbiased estimates of θ1, . . . , θJ yield an upwardly biased estimate\nof the variance of the θj ’s (except in the trivial case in which the θj’s are known exactly).\n\nAnother problem with the principle of unbiasedness arises when treating a future ob-\nservable value as a parameter in prediction problems.\n\nExample. Prediction using regression\nConsider the problem of estimating θ, the height of an adult daughter, given y, her\nmother’s height. For simplicity, assume that the heights of mothers and daughters\nare jointly normally distributed, with known equal means of 160 centimeters, equal\nvariances, and a known correlation of 0.5. Conditioning on the known value of y (in\nother words, using Bayesian inference), the posterior mean of θ is\n\nE(θ|y) = 160 + 0.5(y − 160). (4.5)\n\nThe posterior mean is not, however, an unbiased estimate of θ, in the sense of repeated\nsampling of y given a fixed θ. Given the daughter’s height, θ, the mother’s height, y,\nhas mean E(y|θ) = 160+0.5(θ− 160). Thus, under repeated sampling of y given fixed\nθ, the posterior mean (4.5) has expectation 160+ 0.25(θ− 160) and is biased towards\nthe grand mean of 160. In contrast, the estimate\n\nθ̂ = 160 + 2(y − 160)\n\nis unbiased under repeated sampling of y, conditional on θ. Unfortunately, the esti-\nmate θ̂ makes no sense for values of y not equal to 160; for example, if a mother is 10\ncentimeters taller than average, it estimates her daughter to be 20 centimeters taller\nthan average!\nIn this simple example, in which θ has an accepted population distribution, a sensible\nnon-Bayesian statistician would not use the unbiased estimate θ̂; instead, this problem\nwould be classified as ‘prediction’ rather than ‘estimation,’ and procedures would not\nbe evaluated conditional on the random variable θ. The example illustrates, however,\nthe limitations of unbiasedness as a general principle: it requires unknown quantities to\nbe characterized either as ‘parameters’ or ‘predictions,’ with different implications for\nestimation but no clear substantive distinction. Chapter 5 considers similar situations\nin which the population distribution of θ must be estimated from data rather than\nconditioning on a particular value.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 95\n\nThe important principle illustrated by the example is that of regression to the mean:\nfor any given mother, the expected value of her daughter’s height lies between her\nmother’s height and the population mean. This principle was fundamental to the\noriginal use of the term ‘regression’ for this type of analysis by Galton in the late\n19th century. In many ways, Bayesian analysis can be seen as a logical extension of\nthe principle of regression to the mean, ensuring that proper weighting is made of\ninformation from different sources.\n\nConfidence intervals\n\nEven in small samples, Bayesian (1−α) posterior intervals often have close to (1−α) con-\nfidence coverage under repeated samples conditional on θ. But there are some confidence\nintervals, derived purely from sampling-theory arguments, that differ considerably from\nBayesian probability intervals. From our perspective these intervals are of doubtful value.\nFor example, many authors have shown that a general theory based on unconditional behav-\nior can lead to clearly counterintuitive results, for example, the possibilities of confidence\nintervals with zero or infinite length. A simple example is the confidence interval that is\nempty 5% of the time and contains all of the real line 95% of the time: this always contains\nthe true value (of any real-valued parameter) in 95% of repeated samples. Such examples\ndo not imply that there is no value in the concept of confidence coverage but rather show\nthat coverage alone is not a sufficient basis on which to form reasonable inferences.\n\nHypothesis testing\n\nThe perspective of this book has little role for the non-Bayesian concept of hypothesis\ntests, especially where these relate to point null hypotheses of the form θ = θ0. In order\nfor a Bayesian analysis to yield a nonzero probability for a point null hypothesis, it must\nbegin with a nonzero prior probability for that hypothesis; in the case of a continuous\nparameter, such a prior distribution (comprising a discrete mass, of say 0.5, at θ0 mixed\nwith a continuous density elsewhere) usually seems contrived. In fact, most of the difficulties\nin interpreting hypothesis tests arise from the artificial dichotomy that is required between\nθ = θ0 and θ 6= θ0. Difficulties related to this dichotomy are widely acknowledged from all\nperspectives on statistical inference. In problems involving a continuous parameter θ (say\nthe difference between two means), the hypothesis that θ is exactly zero is rarely reasonable,\nand it is of more interest to estimate a posterior distribution or a corresponding interval\nestimate of θ. For a continuous parameter θ, the question ‘Does θ equal 0?’ can generally\nbe rephrased more usefully as ‘What is the posterior distribution for θ?’\n\nIn various simple one-sided hypothesis tests, conventional p-values may correspond with\nposterior probabilities under noninformative prior distributions. For example, suppose we\nobserve y = 1 from the model y ∼ N(θ, 1), with a uniform prior density on θ. One cannot\n‘reject the hypothesis’ that θ = 0: the one-sided p-value is 0.16 and the two-sided p-value\nis 0.32, both greater than the conventionally accepted cutoff value of 0.05 for ‘statistical\nsignificance.’ On the other hand, the posterior probability that θ > 0 is 84%, which is a\nmore satisfactory and informative conclusion than the dichotomous verdict ‘reject’ or ‘do\nnot reject.’\n\nIn contrast to the problem of making inference about a parameter within a particular\nmodel, we do find a form of hypothesis test to be useful when assessing the goodness of fit of\na probability model. In the Bayesian framework, it is useful to check a model by comparing\nobserved data to possible predictive outcomes, as we discuss in detail in Chapter 6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n96 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nMultiple comparisons and multilevel modeling\n\nConsider a problem with independent measurements, yj ∼ N(θj , 1), on each of J parameters,\nin which the goal is to detect differences among and ordering of the continuous parame-\nters θj . Several competing multiple comparisons procedures have been derived in classical\nstatistics, with rules about when various θj ’s can be declared ‘significantly different.’ In\nthe Bayesian approach, the parameters have a joint posterior distribution. One can com-\npute the posterior probability of each of the J ! orderings if desired. If there is posterior\nuncertainty in the ordering, several permutations will have substantial probabilities, which\nis a more reasonable conclusion than producing a list of θj ’s that can be declared different\n(with the false implication that other θj ’s may be exactly equal). With J large, the exact\nordering is probably not important, and it might be more reasonable to give a posterior\nmedian and interval estimate of the quantile of each θj in the population.\n\nWe prefer to handle multiple comparisons problems using hierarchical models, as we\nshall illustrate in a comparison of treatment effects in eight schools in Section 5.5 (see also\nExercise 5.3). Hierarchical modeling automatically partially pools estimates of different θj ’s\ntoward each other when there is little evidence for real variation. As a result, this Bayesian\nprocedure automatically addresses the key concern of classical multiple comparisons analy-\nsis, which is the possibility of finding large differences as a byproduct of searching through\nso many possibilities. For example, in the educational testing example, the eight schools\ngive 8 · 7/2 = 28 possible comparisons, and none turn out to be close to ‘statistically sig-\nnificant’ (in the sense that zero is contained within the 95% intervals for all the differences\nin effects between pairs of schools), which makes sense since the between-school variation\n(the parameter τ in that model) is estimated to be low.\n\nNonparametric methods, permutation tests, jackknife, bootstrap\n\nMany non-Bayesian methods have been developed that avoid complete probability models,\neven at the sampling level. It is difficult to evaluate many of these from a Bayesian point\nof view. For instance, hypothesis tests for comparing medians based on ranks do not have\ndirect counterparts in Bayesian inference; therefore it is hard to interpret the resulting es-\ntimates and p-values from a Bayesian point of view (for example, as posterior expectations,\nintervals, or probabilities for parameters or predictions of interest). In complicated prob-\nlems, there is often a degree of arbitrariness in the procedures used; for example there is\ngenerally no clear method for constructing a nonparametric inference or an estimator to\njackknife/bootstrap in hypothetical replications. Without a specified probability model,\nit is difficult to see how to test the assumptions underlying a particular nonparametric\nmethod. In such problems, we find it more satisfactory to construct a joint probability\ndistribution and check it against the data (as in Chapter 6) than to construct an estimator\nand evaluate its frequency properties. Nonparametric methods are useful to us as tools for\ndata summary and description that can help us to construct models or help us evaluate\ninferences from a completely different perspective.\n\nFrom a different direction, one might well say that Bayesian methods involve arbitrary\nchoices of models and are difficult to evaluate because in practice there will always be\nimportant aspects of a model that are impossible to check. Our purpose here is not to\ndismiss or disparage classical nonparametric methods but rather to put them in a Bayesian\ncontext to the extent this is possible.\n\nSome nonparametric methods such as permutation tests for experiments and sampling-\ntheory inference for surveys turn out to give similar results in simple problems to Bayesian\ninferences with noninformative prior distributions, if the Bayesian model is constructed\nto fit the data reasonably well. Such simple problems include balanced designs with no\nmissing data and surveys based on simple random sampling. When estimating several pa-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.6. BIBLIOGRAPHIC NOTE 97\n\nrameters at once or including explanatory variables in the analysis (using methods such as\nthe analysis of covariance or regression) or prior information on the parameters, the permu-\ntation/sampling theory methods give no direct answer, and this often provides considerable\npractical incentive to move to a model-based Bayesian approach.\n\nExample. The Wilcoxon rank test\nAnother connection can be made by interpreting nonparametric methods in terms\nof implicit models. For example, the Wilcoxon rank test for comparing two samples\n(y1, . . . , yny ) and (z1, . . . , znz) proceeds by first ranking each of the points in the com-\nbined data from 1 to n = ny +nz, then computing the difference between the average\nranks of the y’s and z’s, and finally computing the p-value of this difference by com-\nparing to a tabulated reference distribution calculated based on the assumption of\nrandom assignment of the n ranks. This can be formulated as a nonlinear transfor-\nmation that replaces each data point by its rank in the combined data, followed by\na comparison of the mean values of the two transformed samples. Even more clear\nwould be to transform the ranks 1, 2, . . . , n to quantiles 1\n\n2n ,\n3\n2n , . . . ,\n\n2n−1\n2n , so that the\n\ndifference between the two means can be interpreted as an average distance in the scale\nof the quantiles of the combined distribution. From the Central Limit Theorem, the\nmean difference is approximately normally distributed, and so classical normal-theory\nconfidence intervals can be interpreted as Bayesian posterior probability statements,\nas discussed at the beginning of this section.\nWe see two major advantages of expressing rank tests as approximate Bayesian infer-\nences. First, the Bayesian framework is more flexible than rank testing for handling\nthe complications that arise, for example, from additional information such as regres-\nsion predictors or from complications such as censored or truncated data. Second,\nsetting up the problem in terms of a nonlinear transformation reveals the general-\nity of the model-based approach—we are free to use any transformation that might\nbe appropriate for the problem, perhaps now treating the combined quantiles as a\nconvenient default choice.\n\n4.6 Bibliographic note\n\nRelatively little has been written on the practical implications of asymptotic theory for\nBayesian analysis. The overview by Edwards, Lindman, and Savage (1963) remains one of\nthe best and includes a detailed discussion of the principle of ‘stable estimation’ or when\nprior information can be satisfactorily approximated by a uniform density function. Much\nmore has been written comparing Bayesian and non-Bayesian approaches to inference, and\nwe have largely ignored the extensive philosophical and logical debates on this subject.\nSome good sources on the topic from the Bayesian point of view include Lindley (1958),\nPratt (1965), and Berger and Wolpert (1984). Jaynes (1976) discusses some disadvantages\nof non-Bayesian methods compared to a particular Bayesian approach.\n\nIn Appendix B we provide references to the asymptotic normality theory. The coun-\nterexamples presented in Section 4.3 have arisen, in various forms, in our own applied\nresearch. Berzuini et al. (1997) discuss Bayesian inference for sequential data problems, in\nwhich the posterior distribution changes as data arrive, thus approaching the asymptotic\nresults dynamically.\n\nAn example of the use of the normal approximation with small samples is provided by\nRubin and Schenker (1987), who approximate the posterior distribution of the logit of the\nbinomial parameter in a real application and evaluate the frequentist operating character-\nistics of their procedure; see also Agresti and Coull (1998). Clogg et al. (1991) provide\nadditional discussion of this approach in a more complicated setting.\n\nMorris (1983) and Rubin (1984) discuss, from two different standpoints, the concept\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n98 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nof evaluating Bayesian procedures by examining long-run frequency properties (such as\ncoverage of 95% confidence intervals). An example of frequency evaluation of Bayesian\nprocedures in an applied problem is given by Zaslavsky (1993).\n\nKrantz (1999) discusses the strengths and weaknesses of p-values as used in statistical\ndata analysis in practice. Discussions of the role of p-values in Bayesian inference appear\nin Bayarri and Berger (1998, 2000). Earlier work on the Bayesian analysis of hypothesis\ntesting and the problems of interpreting conventional p-values is provided by Berger and\nSellke (1987), which contains a lively discussion and many further references. Gelman\n(2008a) and discussants provide a more recent airing of arguments for and against Bayesian\nstatistics. Gelman (2006b) compares Bayesian inference and the more generalized approach\nknown as belief functions (Dempster, 1967, 1968) using a simple toy example.\n\nGreenland and Poole (2013) and Gelman (2013a) present some more recent discussions\nof the relevance of classical p-values in Bayesian inference.\n\nA simple and pragmatic discussion of the need to consider Bayesian ideas in hypothesis\ntesting in a biostatistical context is given by Browner and Newman (1987), and further dis-\ncussion of the role of Bayesian thinking in medical statistics appears in Goodman (1999a, b)\nand Sterne and Smith (2001). Gelman and Tuerlinckx (2000), Efron and Tibshirani (2002),\nand Gelman, Hill, and Yajima (2012) give a Bayesian perspective on multiple comparisons\nin the context of hierarchical modeling.\n\nStigler (1983) discusses the similarity between Bayesian inference and regression predic-\ntion that we mention in our critique of unbiasedness in Section 4.5; Stigler (1986) discusses\nGalton’s use of regression.\n\nSequential monitoring and analysis of clinical trials in medical research is an important\narea of practical application that has been dominated by frequentist thinking but has re-\ncently seen considerable discussion of the merits of a Bayesian approach; recent reviews\nand examples are provided by Freedman, Spiegelhalter, and Parmar (1994), Parmar et al.\n(2001), and Vail et al. (2001). Thall, Simon, and Estey (1995) consider frequency properties\nof Bayesian analyses of sequential trials. More references on sequential designs appear in\nthe bibliographic note at the end of Chapter 8.\n\nThe non-Bayesian principles and methods mentioned in Section 4.5 are covered in many\nbooks, for example, Lehmann (1983, 1986), Cox and Hinkley (1974), Hastie and Tibshi-\nrani (1990), and Efron and Tibshirani (1993). The connection between ratio estimation\nand modeling alluded to in Section 4.5 is discussed by Brewer (1963), Royall (1970), and,\nfrom our Bayesian approach, Rubin (1987a, p. 46). Conover and Iman (1980) discuss the\nconnection between nonparametric tests and data transformations.\n\n4.7 Exercises\n\n1. Normal approximation: suppose that y1, . . . , y5 are independent samples from a Cauchy\ndistribution with unknown center θ and known scale 1: p(yi|θ) ∝ 1/(1 + (yi − θ)2).\nAssume that the prior distribution for θ is uniform on [0, 1]. Given the observations\n(y1, . . . , y5) = (−2,−1, 0, 1.5, 2.5):\n\n(a) Determine the derivative and the second derivative of the log posterior density.\n\n(b) Find the posterior mode of θ by iteratively solving the equation determined by setting\nthe derivative of the log-likelihood to zero.\n\n(c) Construct the normal approximation based on the second derivative of the log posterior\ndensity at the mode. Plot the approximate normal density and compare to the exact\ndensity as computed using the approach described in Exercise 2.11.\n\n2. Normal approximation: derive the analytic form of the information matrix and the nor-\nmal approximation variance for the bioassay example.\n\n3. Normal approximation to the marginal posterior distribution of an estimand: in the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.7. EXERCISES 99\n\nbioassay example, the normal approximation to the joint posterior distribution of (α, β)\nis obtained. The posterior distribution of any estimand, such as the LD50, can be ap-\nproximated by a normal distribution fit to its marginal posterior mode and the curvature\nof the marginal posterior density about the mode. This is sometimes called the ‘delta\nmethod.’ Expand the posterior distribution of the LD50, −α/β, as a Taylor series around\nthe posterior mode and thereby derive the asymptotic posterior median and standard\ndeviation. Compare to the histogram in Figure 4.2.\n\n4. Asymptotic normality: assuming the regularity conditions hold, we know that p(θ|y)\napproaches normality as n → ∞. In addition, if φ = f(θ) is any one-to-one continuous\ntransformation of θ, we can express the Bayesian inference in terms of φ and find that\np(φ|y) also approaches normality. But a nonlinear transformation of a normal distribution\nis no longer normal. How can both limiting normal distributions be valid?\n\n5. Approximate mean and variance:\n\n(a) Suppose x and y are independent normally distributed random variables, where x has\nmean 4 and standard deviation 1, and y has mean 3 and standard deviation 2. What\nare the mean and standard deviation of y/x? Compute this using simulation.\n\n(b) Suppose x and y are independent random variables, where x has mean 4 and standard\ndeviation 1, and y has mean 3 and standard deviation 2. What are the approximate\nmean and standard deviation of y/x? Determine this without using simulation.\n\n(c) What assumptions are required for the approximation in (b) to be reasonable?\n\n6. Statistical decision theory: a decision-theoretic approach to the estimation of an unknown\nparameter θ introduces the loss function L(θ, a) which, loosely speaking, gives the cost of\ndeciding that the parameter has the value a, when it is in fact equal to θ. The estimate\na can be chosen to minimize the posterior expected loss,\n\nE(L(a|y)) =\n∫\nL(θ, a)p(θ|y)dθ.\n\nThis optimal choice of a is called a Bayes estimate for the loss function L. Show that:\n\n(a) If L(θ, a) = (θ − a)2 (squared error loss), then the posterior mean, E(θ|y), if it exists,\nis the unique Bayes estimate of θ.\n\n(b) If L(θ, a) = |θ − a|, then any posterior median of θ is a Bayes estimate of θ.\n\n(c) If k0 and k1 are nonnegative numbers, not both zero, and\n\nL(θ, a) =\n\n{\nk0(θ − a) if θ ≥ a\nk1(a− θ) if θ < a,\n\nthen any k0\nk0+k1\n\nquantile of the posterior distribution p(θ|y) is a Bayes estimate of θ.\n\n7. Unbiasedness: prove that the Bayesian posterior mean, based on a proper prior distri-\nbution, cannot be an unbiased estimator except in degenerate problems (see Bickel and\nBlackwell, 1967, and Lehmann, 1983, p. 244).\n\n8. Regression to the mean: work through the details of the example of mother’s and daugh-\nter’s heights on page 94, illustrating with a sketch of the joint distribution and relevant\nconditional distributions.\n\n9. Point estimation: suppose a measurement y is recorded with a N(θ, σ2) sampling dis-\ntribution, with σ known exactly and θ known to lie in the interval [0, 1]. Consider two\npoint estimates of θ: (1) the maximum likelihood estimate, restricted to the range [0, 1],\nand (2) the posterior mean based on the assumption of a uniform prior distribution on\nθ. Show that if σ is large enough, estimate (1) has a higher mean squared error than\n(2) for any value of θ in [0, 1]. (The unrestricted maximum likelihood estimate has even\nhigher mean squared error.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n100 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n10. Non-Bayesian inference: replicate the analysis of the bioassay example in Section 3.7\nusing non-Bayesian inference. This problem does not have a unique answer, so be clear\non what methods you are using.\n\n(a) Construct an ‘estimator’ of (α, β); that is, a function whose input is a dataset, (x, n, y),\n\nand whose output is a point estimate (α̂, β̂). Compute the value of the estimate for\nthe data given in Table 3.1.\n\n(b) The bias and variance of this estimate are functions of the true values of the parameters\n(α, β) and also of the sampling distribution of the data, given α, β. Assuming the\nbinomial model, estimate the bias and variance of your estimator.\n\n(c) Create approximate 95% confidence intervals for α, β, and the LD50 based on asymp-\ntotic theory and the estimated bias and variance.\n\n(d) Does the inaccuracy of the normal approximation for the posterior distribution (com-\npare Figures 3.3 and 4.1) cast doubt on the coverage properties of your confidence\nintervals in (c)? If so, why?\n\n(e) Create approximate 95% confidence intervals for α, β, and the LD50 using the jack-\nknife or bootstrap (see Efron and Tibshirani, 1993).\n\n(f) Compare your 95% intervals for the LD50 in (c) and (e) to the posterior distribution\ndisplayed in Figure 3.4 and the posterior distribution based on the normal approxima-\ntion, displayed in 4.2b. Comment on the similarities and differences among the four\nintervals. Which do you prefer as an inferential summary about the LD50? Why?\n\n11. Bayesian interpretation of non-Bayesian estimates: consider the following estimation\nprocedure, which is based on classical hypothesis testing. A matched pairs experiment\nis done, and the differences y1, . . . , yn are recorded and modeled as independent draws\nfrom N(θ, σ2). For simplicity, assume σ2 is known. The parameter θ is estimated as the\naverage observed difference if it is ‘statistically significant’ and zero otherwise:\n\nθ̂ =\n\n{\ny if y ≥ 1.96σ/\n\n√\nn\n\n0 otherwise.\n\nCan this be interpreted, in some sense, as an approximate summary (for example, a\nposterior mean or mode) of a Bayesian inference under some prior distribution on θ?\n\n12. Bayesian interpretation of non-Bayesian estimates: repeat the above problem but with\nσ replaced by s, the sample standard deviation of y1, . . . , yn.\n\n13. Objections to Bayesian inference: discuss the criticism, ‘Bayesianism assumes: (a) Either\na weak or uniform prior [distribution], in which case why bother?, (b) Or a strong prior\n[distribution], in which case why collect new data?, (c) Or more realistically, something\nin between, in which case Bayesianism always seems to duck the issue’ (Ehrenberg, 1986).\nFeel free to use any of the examples covered so far to illustrate your points.\n\n14. Objectivity and subjectivity: discuss the statement, ‘People tend to believe results that\nsupport their preconceptions and disbelieve results that surprise them. Bayesian methods\nencourage this undisciplined mode of thinking.’\n\n15. Coverage of posterior intervals:\n\n(a) Consider a model with scalar parameter θ. Prove that, if you draw θ from the prior,\ndraw y|θ from the data model, then perform Bayesian inference for θ given y, that\nthere is a 50% probability that your 50% interval for θ contains the true value.\n\n(b) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is 1. What is the\ncoverage of the posterior 50% interval for θ? (You have to work this one out; it’s not\n50% or any other number you could just guess.)\n\n(c) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is θ0. Make a\nplot showing the coverage of the posterior 50% interval for θ, as a function of θ0.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 5\n\nHierarchical models\n\nMany statistical applications involve multiple parameters that can be regarded as related\nor connected in some way by the structure of the problem, implying that a joint probability\nmodel for these parameters should reflect their dependence. For example, in a study of the\neffectiveness of cardiac treatments, with the patients in hospital j having survival probability\nθj , it might be reasonable to expect that estimates of the θj ’s, which represent a sample of\nhospitals, should be related to each other. We shall see that this is achieved in a natural\nway if we use a prior distribution in which the θj ’s are viewed as a sample from a common\npopulation distribution. A key feature of such applications is that the observed data, yij ,\nwith units indexed by i within groups indexed by j, can be used to estimate aspects of\nthe population distribution of the θj ’s even though the values of θj are not themselves\nobserved. It is natural to model such a problem hierarchically, with observable outcomes\nmodeled conditionally on certain parameters, which themselves are given a probabilistic\nspecification in terms of further parameters, known as hyperparameters. Such hierarchical\nthinking helps in understanding multiparameter problems and also plays an important role\nin developing computational strategies.\n\nPerhaps even more important in practice is that simple nonhierarchical models are usu-\nally inappropriate for hierarchical data: with few parameters, they generally cannot fit large\ndatasets accurately, whereas with many parameters, they tend to ‘overfit’ such data in the\nsense of producing models that fit the existing data well but lead to inferior predictions for\nnew data. In contrast, hierarchical models can have enough parameters to fit the data well,\nwhile using a population distribution to structure some dependence into the parameters,\nthereby avoiding problems of overfitting. As we show in the examples in this chapter, it is\noften sensible to fit hierarchical models with more parameters than there are data points.\n\nIn Section 5.1, we consider the problem of constructing a prior distribution using hierar-\nchical principles but without fitting a formal probability model for the hierarchical structure.\nWe first consider the analysis of a single experiment, using historical data to create a prior\ndistribution, and then we consider a plausible prior distribution for the parameters of a set\nof experiments. The treatment in Section 5.1 is not fully Bayesian, because, for the purpose\nof simplicity in exposition, we work with a point estimate, rather than a complete joint\nposterior distribution, for the parameters of the population distribution (the hyperparam-\neters). In Section 5.2, we discuss how to construct a hierarchical prior distribution in the\ncontext of a fully Bayesian analysis. Sections 5.3–5.4 present a general approach to compu-\ntation with hierarchical models in conjugate families by combining analytical and numerical\nmethods. We defer details of the most general computational methods to Part III in order\nto explore immediately the important practical and conceptual advantages of hierarchical\nBayesian models. The chapter continues with two extended examples: a hierarchical model\nfor an educational testing experiment and a Bayesian treatment of the method of ‘meta-\nanalysis’ as used in medical research to combine the results of separate studies relating to\nthe same research question. We conclude with a discussion of weakly informative priors,\nwhich become important for hierarchical models fit to data from a small number of groups.\n\n101\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n102 5. HIERARCHICAL MODELS\n\nPrevious experiments:\n\n0/20 0/20 0/20 0/20 0/20 0/20 0/20 0/19 0/19 0/19\n0/19 0/18 0/18 0/17 1/20 1/20 1/20 1/20 1/19 1/19\n1/18 1/18 2/25 2/24 2/23 2/20 2/20 2/20 2/20 2/20\n2/20 1/10 5/49 2/19 5/46 3/27 2/17 7/49 7/47 3/20\n3/20 2/13 9/48 10/50 4/20 4/20 4/20 4/20 4/20 4/20\n4/20 10/48 4/19 4/19 4/19 5/22 11/46 12/49 5/20 5/20\n6/23 5/19 6/22 6/20 6/20 6/20 16/52 15/47 15/46 9/24\n\nCurrent experiment:\n4/14\n\nTable 5.1 Tumor incidence in historical control groups and current group of rats, from Tarone\n(1982). The table displays the values of\n\nyj\n\nnj\n: (number of rats with tumors)/(total number of rats).\n\n5.1 Constructing a parameterized prior distribution\n\nAnalyzing a single experiment in the context of historical data\n\nTo begin our description of hierarchical models, we consider the problem of estimating a\nparameter θ using data from a small experiment and a prior distribution constructed from\nsimilar previous (or historical) experiments. Mathematically, we will consider the current\nand historical experiments to be a random sample from a common population.\n\nExample. Estimating the risk of tumor in a group of rats\nIn the evaluation of drugs for possible clinical application, studies are routinely per-\nformed on rodents. For a particular study drawn from the statistical literature, sup-\npose the immediate aim is to estimate θ, the probability of tumor in a population of\nfemale laboratory rats of type ‘F344’ that receive a zero dose of the drug (a control\ngroup). The data show that 4 out of 14 rats developed endometrial stromal polyps (a\nkind of tumor). It is natural to assume a binomial model for the number of tumors,\ngiven θ. For convenience, we select a prior distribution for θ from the conjugate family,\nθ ∼ Beta(α, β).\n\nAnalysis with a fixed prior distribution. From historical data, suppose we knew that\nthe tumor probabilities θ among groups of female lab rats of type F344 follow an\napproximate beta distribution, with known mean and standard deviation. The tumor\nprobabilities θ vary because of differences in rats and experimental conditions among\nthe experiments. Referring to the expressions for the mean and variance of the beta\ndistribution (see Appendix A), we could find values for α, β that correspond to the\ngiven values for the mean and standard deviation. Then, assuming a Beta(α, β) prior\ndistribution for θ yields a Beta(α+ 4, β + 10) posterior distribution for θ.\n\nApproximate estimate of the population distribution using the historical data. Typ-\nically, the mean and standard deviation of underlying tumor risks are not available.\nRather, historical data are available on previous experiments on similar groups of rats.\nIn the rat tumor example, the historical data were in fact a set of observations of tu-\nmor incidence in 70 groups of rats (Table 5.1). In the jth historical experiment, let the\nnumber of rats with tumors be yj and the total number of rats be nj . We model the\nyj ’s as independent binomial data, given sample sizes nj and study-specific means θj .\nAssuming that the beta prior distribution with parameters (α, β) is a good description\nof the population distribution of the θj ’s in the historical experiments, we can display\nthe hierarchical model schematically as in Figure 5.1, with θ71 and y71 corresponding\nto the current experiment.\nThe observed sample mean and standard deviation of the 70 values\n\nyj\nnj\n\nare 0.136 and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.1. CONSTRUCTING A PARAMETERIZED PRIOR DISTRIBUTION 103\n\nα, β\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑✑✰\n\n✚\n✚\n\n✚\n✚\n\n✚\n✚\n\n✚✚❂\n\n�\n�\n\n�\n�\n\n�\n�✠\n\n✁\n✁\n✁\n\n✁\n✁\n✁☛ ❄\n\n❆\n❆\n❆\n❆\n❆\n❆❯\n\n❅\n❅\n❅\n❅\n❅\n❅❘\n\n❩\n❩\n❩\n❩\n❩\n❩\n❩❩⑦\n\n◗\n◗\n◗\n◗\n◗\n◗\n◗\n◗◗s\n\nθ1 θ2 θ3 . . . . . . . . . . . . θ70 θ71\n\n❄ ❄ ❄ ❄ ❄\ny1 y2 y3 . . . . . . . . . . . . y70 y71\n\nFigure 5.1: Structure of the hierarchical model for the rat tumor example.\n\n0.103. If we set the mean and standard deviation of the population distribution to\nthese values, we can solve for α and β—see (A.3) on page 585 in Appendix A. The\nresulting estimate for (α, β) is (1.4, 8.6). This is not a Bayesian calculation because\nit is not based on any specified full probability model. We present a better, fully\nBayesian approach to estimating (α, β) for this example in Section 5.3. The estimate\n(1.4, 8.6) is simply a starting point from which we can explore the idea of estimating\nthe parameters of the population distribution.\nUsing the simple estimate of the historical population distribution as a prior distribu-\ntion for the current experiment yields a Beta(5.4, 18.6) posterior distribution for θ71:\nthe posterior mean is 0.223, and the standard deviation is 0.083. The prior informa-\ntion has resulted in a posterior mean substantially lower than the crude proportion,\n4/14 = 0.286, because the weight of experience indicates that the number of tumors\nin the current experiment is unusually high.\nThese analyses require that the current tumor risk, θ71, and the 70 historical tumor\nrisks, θ1, . . . , θ70, be considered a random sample from a common distribution, an\nassumption that would be invalidated, for example, if it were known that the historical\nexperiments were all done in laboratory A but the current data were gathered in\nlaboratory B, or if time trends were relevant. In practice, a simple, although arbitrary,\nway of accounting for differences between the current and historical data is to inflate\nthe historical variance. For the beta model, inflating the historical variance means\ndecreasing (α+β) while holding α\n\nβ constant. Other systematic differences, such as a\ntime trend in tumor risks, can be incorporated in a more extensive model.\n\nHaving used the 70 historical experiments to form a prior distribution for θ71, we might\nnow like also to use this same prior distribution to obtain Bayesian inferences for the tumor\nprobabilities in the first 70 experiments, θ1, . . . , θ70. There are several logical and practical\nproblems with the approach of directly estimating a prior distribution from existing data:\n\n• If we wanted to use the estimated prior distribution for inference about the first 70\nexperiments, then the data would be used twice: first, all the results together are used to\nestimate the prior distribution, and then each experiment’s results are used to estimate\nits θ. This would seem to cause us to overestimate our precision.\n\n• The point estimate for α and β seems arbitrary, and using any point estimate for α and\nβ necessarily ignores some posterior uncertainty.\n\n• We can also make the opposite point: does it make sense to ‘estimate’ α and β at all?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n104 5. HIERARCHICAL MODELS\n\nThey are part of the ‘prior’ distribution: should they be known before the data are\ngathered, according to the logic of Bayesian inference?\n\nLogic of combining information\n\nDespite these problems, it clearly makes more sense to try to estimate the population\ndistribution from all the data, and thereby to help estimate each θj , than to estimate all 71\nvalues θj separately. Consider the following thought experiment about inference on two of\nthe parameters, θ26 and θ27, each corresponding to experiments with 2 observed tumors out\nof 20 rats. Suppose our prior distribution for both θ26 and θ27 is centered around 0.15; now\nsuppose that you were told after completing the data analysis that θ26 = 0.1 exactly. This\nshould influence your estimate of θ27; in fact, it would probably make you think that θ27\nis lower than you previously believed, since the data for the two parameters are identical,\nand the postulated value of 0.1 is lower than you previously expected for θ26 from the prior\ndistribution. Thus, θ26 and θ27 should be dependent in the posterior distribution, and they\nshould not be analyzed separately.\n\nWe retain the advantages of using the data to estimate prior parameters and eliminate\nall of the disadvantages just mentioned by putting a probability model on the entire set of\nparameters and experiments and then performing a Bayesian analysis on the joint distribu-\ntion of all the model parameters. A complete Bayesian analysis is described in Section 5.3.\nThe analysis using the data to estimate the prior parameters, which is sometimes called\nempirical Bayes, can be viewed as an approximation to the complete hierarchical Bayesian\nanalysis. We prefer to avoid the term ‘empirical Bayes’ because it misleadingly suggests\nthat the full Bayesian method, which we discuss here and use for the rest of the book, is\nnot ‘empirical.’\n\n5.2 Exchangeability and hierarchical models\n\nGeneralizing from the example of the previous section, consider a set of experiments j =\n1, . . . , J , in which experiment j has data (vector) yj and parameter (vector) θj , with like-\nlihood p(yj |θj). (Throughout this chapter we use the word ‘experiment’ for convenience,\nbut the methods can apply equally well to nonexperimental data.) Some of the parameters\nin different experiments may overlap; for example, each data vector yj may be a sample of\nobservations from a normal distribution with mean µj and common variance σ2, in which\ncase θj = (µj , σ\n\n2). In order to create a joint probability model for all the parameters θ, we\nuse the crucial idea of exchangeability introduced in Chapter 1 and used repeatedly since\nthen.\n\nExchangeability\n\nIf no information—other than the data y—is available to distinguish any of the θj ’s from any\nof the others, and no ordering or grouping of the parameters can be made, one must assume\nsymmetry among the parameters in their prior distribution. This symmetry is represented\nprobabilistically by exchangeability; the parameters (θ1, . . . , θJ) are exchangeable in their\njoint distribution if p(θ1, . . . , θJ) is invariant to permutations of the indexes (1, . . . , J). For\nexample, in the rat tumor problem, suppose we have no information to distinguish the 71\nexperiments, other than the sample sizes nj, which presumably are not related to the values\nof θj ; we therefore use an exchangeable model for the θj ’s.\n\nWe have already encountered the concept of exchangeability in constructing independent\nand identically distributed models for direct data. In practice, ignorance implies exchange-\nability. Generally, the less we know about a problem, the more confidently we can make\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 105\n\nclaims of exchangeability. (This is not, we hasten to add, a good reason to limit our knowl-\nedge of a problem before embarking on statistical analysis!) Consider the analogy to a\nroll of a die: we should initially assign equal probabilities to all six outcomes, but if we\nstudy the measurements of the die and weigh the die carefully, we might eventually notice\nimperfections, which might make us favor one outcome over the others and thus eliminate\nthe symmetry among the six outcomes.\n\nThe simplest form of an exchangeable distribution has each of the parameters θj as an\nindependent sample from a prior (or population) distribution governed by some unknown\nparameter vector φ; thus,\n\np(θ|φ) =\nJ∏\n\nj=1\n\np(θj |φ). (5.1)\n\nIn general, φ is unknown, so our distribution for θ must average over our uncertainty in φ:\n\np(θ) =\n\n∫ ( J∏\n\nj=1\n\np(θj |φ)\n)\np(φ)dφ, (5.2)\n\nThis form, the mixture of independent identical distributions, is usually all that we need to\ncapture exchangeability in practice.\n\nA related theoretical result, de Finetti’s theorem, to which we alluded in Section 1.2,\nstates that in the limit as J → ∞, any suitably well-behaved exchangeable distribution\non (θ1, . . . , θJ) can be expressed as a mixture of independent and identical distributions\nas in (5.2). The theorem does not hold when J is finite (see Exercises 5.1, 5.2, and 5.4).\nStatistically, the mixture model characterizes parameters θ as drawn from a common ‘su-\nperpopulation’ that is determined by the unknown hyperparameters, φ. We are already\nfamiliar with exchangeable models for data, y1, . . . , yn, in the form of likelihoods in which\nthe n observations are independent and identically distributed, given some parameter vector\nθ.\n\nAs a simple counterexample to the above mixture model, consider the probabilities of a\ngiven die landing on each of its six faces. The probabilities θ1, . . . , θ6 are exchangeable, but\nthe six parameters θj are constrained to sum to 1 and so cannot be modeled with a mixture\nof independent identical distributions; nonetheless, they can be modeled exchangeably.\n\nExample. Exchangeability and sampling\nThe following thought experiment illustrates the role of exchangeability in inference\nfrom random sampling. For simplicity, we use a nonhierarchical example with ex-\nchangeability at the level of y rather than θ.\nWe, the authors, have selected eight states out of the United States and recorded the\ndivorce rate per 1000 population in each state in 1981. Call these y1, . . . , y8. What\ncan you, the reader, say about y8, the divorce rate in the eighth state?\nSince you have no information to distinguish any of the eight states from the others,\nyou must model them exchangeably. You might use a beta distribution for the eight\nyj ’s, a logit normal, or some other prior distribution restricted to the range [0, 1].\nUnless you are familiar with divorce statistics in the United States, your distribution\non (y1, . . . , y8) should be fairly vague.\nWe now randomly sample seven states from these eight and tell you their divorce\nrates: 5.8, 6.6, 7.8, 5.6, 7.0, 7.1, 5.4, each in numbers of divorces per 1000 population\n(per year). Based primarily on the data, a reasonable posterior (predictive) distri-\nbution for the remaining value, y8, would probably be centered around 6.5 and have\nmost of its mass between 5.0 and 8.0. Changing the indexing does not change the\njoint distribution. If we relabel the remaining value to be any other yj the posterior\nestimate would be the same. yj are exchangeable but they are not independent as we\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n106 5. HIERARCHICAL MODELS\n\nassume that the divorce rate in the eighth unobserved state is probably similar to the\nobserved rates.\nSuppose initially we had given you the further prior information that the eight states\nare Mountain states: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah,\nand Wyoming, but selected in a random order; you still are not told which observed\nrate corresponds to which state. Now, before the seven data points were observed,\nthe eight divorce rates should still be modeled exchangeably. However, your prior\ndistribution (that is, before seeing the data), for the eight numbers should change:\nit seems reasonable to assume that Utah, with its large Mormon population, has a\nmuch lower divorce rate, and Nevada, with its liberal divorce laws, has a much higher\ndivorce rate, than the remaining six states. Perhaps, given your expectation of outliers\nin the distribution, your prior distribution should have wide tails. Given this extra\ninformation (the names of the eight states), when you see the seven observed values\nand note that the numbers are so close together, it might seem a reasonable guess that\nthe missing eighth state is Nevada or Utah. Therefore its value might be expected to\nbe much lower or much higher than the seven values observed. This might lead to a\nbimodal or trimodal posterior distribution to account for the two plausible scenarios.\nThe prior distribution on the eight values yj is still exchangeable, however, because\nyou have no information telling which state corresponds to which index number. (See\nExercise 5.6.)\nFinally, we tell you that the state not sampled (corresponding to y8) was Nevada.\nNow, even before seeing the seven observed values, you cannot assign an exchangeable\nprior distribution to the set of eight divorce rates, since you have information that\ndistinguishes y8 from the other seven numbers, here suspecting it is larger than any\nof the others. Once y1, . . . , y7 have been observed, a reasonable posterior distribution\nfor y8 plausibly should have most of its mass above the largest observed rate, that is,\np(y8 > max(y1, . . . , y7)|y1, . . . , y7) should be large.\nIncidentally, Nevada’s divorce rate in 1981 was 13.9 per 1000 population.\n\nExchangeability when additional information is available on the units\n\nOften observations are not fully exchangeable, but are partially or conditionally exchange-\nable:\n\n• If observations can be grouped, we may make hierarchical model, where each group has its\nown submodel, but the group properties are unknown. If we assume that group properties\nare exchangeable, we can use a common prior distribution for the group properties.\n\n• If yi has additional information xi so that yi are not exchangeable but (yi, xi) still are\nexchangeable, then we can make a joint model for (yi, xi) or a conditional model for\nyi|xi.\nIn the rat tumor example, yj were exchangeable as no additional knowledge was available\n\non experimental conditions. If we knew that specific batches of experiments were made in\ndifferent laboratories we could assume partial exchangeability and use two level hierarchical\nmodel to model variation within each laboratory and between laboratories.\n\nIn the divorce example, if we knew xj , the divorce rate in state j last year, for j =\n1, . . . , 8, but not which index corresponded to which state, then we would certainly be able\nto distinguish the eight values of yj, but the joint prior distribution p(xj , yj) would be the\nsame for each state. For states having the same last year divorce rates xj , we could use\ngrouping and assume partial exchangeability or if there are many possible values for xj (as\nwe would assume for divorce rates) we could assume conditional exchangeability and use xj\nas covariate in regression model.\n\nIn general, the usual way to model exchangeability with covariates is through con-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 107\n\nditional independence: p(θ1, . . . , θJ |x1, . . . , xJ ) =\n∫\n[\n∏J\nj=1 p(θj |φ, xj)]p(φ|x)dφ, with x =\n\n(x1, . . . , xJ ). In this way, exchangeable models become almost universally applicable, be-\ncause any information available to distinguish different units should be encoded in the x\nand y variables.\n\nIn the rat tumor example, we have already noted that the sample sizes nj are the only\navailable information to distinguish the different experiments. It does not seem likely that\nnj would be a useful variable for modeling tumor rates, but if one were interested, one\ncould create an exchangeable model for the J pairs (n, y)j . A natural first step would be\nto plot\n\nyj\nnj\n\nvs. nj to see any obvious relation that could be modeled. For example, perhaps\n\nsome studies j had larger sample sizes nj because the investigators correctly suspected rarer\nevents; that is, smaller θj and thus smaller expected values of\n\nyj\nnj\n. In fact, the plot of\n\nyj\nnj\n\nversus nj , not shown here, shows no apparent relation between the two variables.\n\nObjections to exchangeable models\n\nIn virtually any statistical application, it is natural to object to exchangeability on the\ngrounds that the units actually differ. For example, the 71 rat tumor experiments were\nperformed at different times, on different rats, and presumably in different laboratories.\nSuch information does not, however, invalidate exchangeability. That the experiments differ\nimplies that the θj ’s differ, but it might be perfectly acceptable to consider them as if drawn\nfrom a common distribution. In fact, with no information available to distinguish them, we\nhave no logical choice but to model the θj ’s exchangeably. Objecting to exchangeability for\nmodeling ignorance is no more reasonable than objecting to an independent and identically\ndistributed model for samples from a common population, objecting to regression models\nin general, or, for that matter, objecting to displaying points in a scatterplot without\nindividual labels. As with regression, the valid concern is not about exchangeability, but\nabout encoding relevant knowledge as explanatory variables where possible.\n\nThe full Bayesian treatment of the hierarchical model\n\nReturning to the problem of inference, the key ‘hierarchical’ part of these models is that\nφ is not known and thus has its own prior distribution, p(φ). The appropriate Bayesian\nposterior distribution is of the vector (φ, θ). The joint prior distribution is\n\np(φ, θ) = p(φ)p(θ|φ),\n\nand the joint posterior distribution is\n\np(φ, θ|y) ∝ p(φ, θ)p(y|φ, θ)\n= p(φ, θ)p(y|θ), (5.3)\n\nwith the latter simplification holding because the data distribution, p(y|φ, θ), depends only\non θ; the hyperparameters φ affect y only through θ. Previously, we assumed φ was known,\nwhich is unrealistic; now we include the uncertainty in φ in the model.\n\nThe hyperprior distribution\n\nIn order to create a joint probability distribution for (φ, θ), we must assign a prior distri-\nbution to φ. If little is known about φ, we can assign a diffuse prior distribution, but we\nmust be careful when using an improper prior density to check that the resulting poste-\nrior distribution is proper, and we should assess whether our conclusions are sensitive to\nthis simplifying assumption. In most real problems, one should have enough substantive\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n108 5. HIERARCHICAL MODELS\n\nknowledge about the parameters in φ at least to constrain the hyperparameters into a finite\nregion, if not to assign a substantive hyperprior distribution. As in nonhierarchical models,\nit is often practical to start with a simple, relatively noninformative, prior distribution on φ\nand seek to add more prior information if there remains too much variation in the posterior\ndistribution.\n\nIn the rat tumor example, the hyperparameters are (α, β), which determine the beta\ndistribution for θ. We illustrate one approach to constructing an appropriate hyperprior\ndistribution in the continuation of that example in the next section.\n\nPosterior predictive distributions\n\nHierarchical models are characterized both by hyperparameters, φ, in our notation, and\nparameters θ. There are two posterior predictive distributions that might be of interest to\nthe data analyst: (1) the distribution of future observations ỹ corresponding to an existing\nθj , or (2) the distribution of observations ỹ corresponding to future θj ’s drawn from the\n\nsame superpopulation. We label the future θj’s as θ̃. Both kinds of replications can be used\nto assess model adequacy, as we discuss in Chapter 6. In the rat tumor example, future\nobservations can be (1) additional rats from an existing experiment, or (2) results from a\nfuture experiment. In the former case, the posterior predictive draws ỹ are based on the\nposterior draws of θj for the existing experiment. In the latter case, one must first draw\n\nθ̃ for the new experiment from the population distribution, given the posterior draws of φ,\nand then draw ỹ given the simulated θ̃.\n\n5.3 Bayesian analysis of conjugate hierarchical models\n\nOur inferential strategy for hierarchical models follows the general approach to multiparam-\neter problems presented in Section 3.8 but is more difficult in practice because of the large\nnumber of parameters that commonly appear in a hierarchical model. In particular, we\ncannot generally plot the contours or display a scatterplot of the simulations from the joint\nposterior distribution of (θ, φ). With care, however, we can follow a similar simulation-based\napproach as before.\n\nIn this section, we present an approach that combines analytical and numerical methods\nto obtain simulations from the joint posterior distribution, p(θ, φ|y), for the beta-binomial\nmodel for the rat-tumor example, for which the population distribution, p(θ|φ), is conjugate\nto the likelihood, p(y|θ). For the many nonconjugate hierarchical models that arise in\npractice, more advanced computational methods, presented in Part III of this book, are\nnecessary. Even for more complicated problems, however, the approach using conjugate\ndistributions is useful for obtaining approximate estimates and starting points for more\naccurate computations.\n\nAnalytic derivation of conditional and marginal distributions\n\nWe first perform the following three steps analytically.\n\n1. Write the joint posterior density, p(θ, φ|y), in unnormalized form as a product of the\nhyperprior distribution p(φ), the population distribution p(θ|φ), and the likelihood p(y|θ).\n\n2. Determine analytically the conditional posterior density of θ given the hyperparameters\nφ; for fixed observed y, this is a function of φ, p(θ|φ, y).\n\n3. Estimate φ using the Bayesian paradigm; that is, obtain its marginal posterior distribu-\ntion, p(φ|y).\nThe first step is immediate, and the second step is easy for conjugate models because,\n\nconditional on φ, the population distribution for θ is just the independent and identically\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 109\n\ndistributed model (5.1), so that the conditional posterior density is a product of conjugate\nposterior densities for the components θj .\n\nThe third step can be performed by brute force by integrating the joint posterior distri-\nbution over θ:\n\np(φ|y) =\n∫\np(θ, φ|y)dθ. (5.4)\n\nFor many standard models, however, including the normal distribution, the marginal pos-\nterior distribution of φ can be computed algebraically using the conditional probability\nformula,\n\np(φ|y) = p(θ, φ|y)\np(θ|φ, y) . (5.5)\n\nThis expression is useful because the numerator is just the joint posterior distribution (5.3),\nand the denominator is the posterior distribution for θ if φ were known. The difficulty in\nusing (5.5), beyond a few standard conjugate models, is that the denominator, p(θ|φ, y),\nregarded as a function of both θ and φ for fixed y, has a normalizing factor that depends on\nφ as well as y. One must be careful with the proportionality ‘constant’ in Bayes’ theorem,\nespecially when using hierarchical models, to make sure it is actually constant. Exercise\n5.11 has an example of a nonconjugate model in which the integral (5.4) has no closed-form\nsolution so that (5.5) is no help.\n\nDrawing simulations from the posterior distribution\n\nThe following strategy is useful for simulating a draw from the joint posterior distribution,\np(θ, φ|y), for simple hierarchical models such as are considered in this chapter.\n\n1. Draw the vector of hyperparameters, φ, from its marginal posterior distribution, p(φ|y).\nIf φ is low-dimensional, the methods discussed in Chapter 3 can be used; for high-\ndimensional φ, more sophisticated methods such as described in Part III may be needed.\n\n2. Draw the parameter vector θ from its conditional posterior distribution, p(θ|φ, y), given\nthe drawn value of φ. For the examples we consider in this chapter, the factorization\np(θ|φ, y) =\n\n∏\nj p(θj |φ, y) holds, and so the components θj can be drawn independently,\n\none at a time.\n\n3. If desired, draw predictive values ỹ from the posterior predictive distribution given the\ndrawn θ. Depending on the problem, it might be necessary first to draw a new value θ̃,\ngiven φ, as discussed at the end of the previous section.\n\nAs usual, the above steps are performed L times in order to obtain a set of L draws. From\nthe joint posterior simulations of θ and ỹ, we can compute the posterior distribution of any\nestimand or predictive quantity of interest.\n\nApplication to the model for rat tumors\n\nWe now perform a full Bayesian analysis of the rat tumor experiments described in Section\n5.1. Once again, the data from experiments j = 1, . . . , J , J = 71, are assumed to follow\nindependent binomial distributions:\n\nyj ∼ Bin(nj , θj),\n\nwith the number of rats, nj, known. The parameters θj are assumed to be independent\nsamples from a beta distribution:\n\nθj ∼ Beta(α, β),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n110 5. HIERARCHICAL MODELS\n\nand we shall assign a noninformative hyperprior distribution to reflect our ignorance about\nthe unknown hyperparameters. As usual, the word ‘noninformative’ indicates our attitude\ntoward this part of the model and is not intended to imply that this particular distribution\nhas any special properties. If the hyperprior distribution turns out to be crucial for our\ninference, we should report this and if possible seek further substantive knowledge that\ncould be used to construct a more informative prior distribution. If we wish to assign an\nimproper prior distribution for the hyperparameters, (α, β), we must check that the poste-\nrior distribution is proper. We defer the choice of noninformative hyperprior distribution,\na relatively arbitrary and unimportant part of this particular analysis, until we inspect the\nintegrability of the posterior density.\n\nJoint, conditional, and marginal posterior distributions. We first perform the three steps\nfor determining the analytic form of the posterior distribution. The joint posterior distri-\nbution of all parameters is\n\np(θ, α, β|y) ∝ p(α, β)p(θ|α, β)p(y|θ, α, β)\n\n∝ p(α, β)\n\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\nθα−1\nj (1− θj)β−1\n\nJ∏\n\nj=1\n\nθ\nyj\nj (1− θj)nj−yj . (5.6)\n\nGiven (α, β), the components of θ have independent posterior densities that are of the form\nθAj (1− θj)B—that is, beta densities—and the joint density is\n\np(θ|α, β, y) =\nJ∏\n\nj=1\n\nΓ(α+β+nj)\n\nΓ(α+yj)Γ(β+nj−yj)\nθ\nα+yj−1\nj (1− θj)β+nj−yj−1. (5.7)\n\nWe can determine the marginal posterior distribution of (α, β) by substituting (5.6) and\n(5.7) into the conditional probability formula (5.5):\n\np(α, β|y) ∝ p(α, β)\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\n\nΓ(α+ yj)Γ(β + nj − yj)\nΓ(α+β + nj)\n\n. (5.8)\n\nThe product in equation (5.8) cannot be simplified analytically but is easy to compute for\nany specified values of (α, β) using a standard routine to compute the gamma function.\n\nChoosing a standard parameterization and setting up a ‘noninformative’ hyperprior dis-\ntribution. Because we have no immediately available information about the distribution\nof tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribu-\ntion for (α, β). Before assigning a hyperprior distribution, we reparameterize in terms\nof logit( α\n\nα+β ) = log(αβ ) and log(α+β), which are the logit of the mean and the logarithm\nof the ‘sample size’ in the beta population distribution for θ. It would seem reasonable to\nassign independent hyperprior distributions to the prior mean and ‘sample size,’ and we\nuse the logistic and logarithmic transformations to put each on a (−∞,∞) scale. Unfortu-\nnately, a uniform prior density on these newly transformed parameters yields an improper\nposterior density, with an infinite integral in the limit (α+β)→ ∞, and so this particular\nprior density cannot be used here.\n\nIn a problem such as this with a reasonably large amount of data, it is possible to set up a\n‘noninformative’ hyperprior density that is dominated by the likelihood and yields a proper\nposterior distribution. One reasonable choice of diffuse hyperprior density is uniform on\n( α\nα+β , (α+β)\n\n−1/2), which when multiplied by the appropriate Jacobian yields the following\ndensities on the original scale,\n\np(α, β) ∝ (α+β)−5/2, (5.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 111\n\nFigure 5.2 First try at a contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for\n\nthe rat tumor example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode.\n\nand on the natural transformed scale:\n\np\n\n(\nlog(\n\nα\n\nβ\n), log(α+β)\n\n)\n∝ αβ(α+β)−5/2. (5.10)\n\nSee Exercise 5.9 for a discussion of this prior density.\nWe could avoid the mathematical effort of checking the integrability of the posterior\n\ndensity if we were to use a proper hyperprior distribution. Another approach would be\ntentatively to use a flat hyperprior density, such as p( α\n\nα+β , α+β) ∝ 1, or even p(α, β) ∝ 1,\n\nand then compute the contours and simulations from the posterior density (as detailed\nbelow). The result would clearly show the posterior contours drifting off toward infinity,\nindicating that the posterior density is not integrable in that limit. The prior distribution\nwould then have to be altered to obtain an integrable posterior density.\n\nIncidentally, setting the prior distribution for (log(αβ ), log(α+β)) to uniform in a vague\n\nbut finite range, such as [−1010, 1010]× [−1010, 1010], would not be an acceptable solution\nfor this problem, as almost all the posterior mass in this case would be in the range of α\nand β near ‘infinity,’ which corresponds to a Beta(α, β) distribution with a variance of zero,\nmeaning that all the θj parameters would be essentially equal in the posterior distribution.\nWhen the likelihood is not integrable, setting a faraway finite cutoff to a uniform prior\ndensity does not necessarily eliminate the problem.\n\nComputing the marginal posterior density of the hyperparameters. Now that we have estab-\nlished a full probability model for data and parameters, we compute the marginal posterior\ndistribution of the hyperparameters. Figure 5.2 shows a contour plot of the unnormalized\nmarginal posterior density on a grid of values of (log(αβ ), log(α+β)). To create the plot, we\n\nfirst compute the logarithm of the density function (5.8) with prior density (5.9), multiply-\ning by the Jacobian to obtain the density p(log(αβ ), log(α+β)|y). We set a grid in the range\n\n(log(αβ ), log(α+β)) ∈ [−2.5,−1]× [1.5, 3], which is centered near our earlier point estimate\n\n(−1.8, 2.3) (that is, (α, β) = (1.4, 8.6)) and covers a factor of 4 in each parameter. Then, to\navoid computational overflows, we subtract the maximum value of the log density from each\npoint on the grid and exponentiate, yielding values of the unnormalized marginal posterior\ndensity.\n\nThe most obvious features of the contour plot are (1) the mode is not far from the\npoint estimate (as we would expect), and (2) important parts of the marginal posterior\ndistribution lie outside the range of the graph.\n\nWe recompute p(log(αβ ), log(α+ β)|y), this time in the range (log(αβ ), log(α+ β)) ∈\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n112 5. HIERARCHICAL MODELS\n\nFigure 5.3 (a) Contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for the rat tumor\n\nexample. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of\n1000 draws (log(α\n\nβ\n), log(α+β)) from the numerically computed marginal posterior density.\n\n[−2.3,−1.3]× [1, 5]. The resulting grid, shown in Figure 5.3a, displays essentially all of\nthe marginal posterior distribution. Figure 5.3b displays 1000 random draws from the\nnumerically computed posterior distribution. The graphs show that the marginal poste-\nrior distribution of the hyperparameters, under this transformation, is approximately sym-\nmetric about the mode, roughly (−1.75, 2.8). This corresponds to approximate values of\n(α, β) = (2.4, 14.0), which differs somewhat from the crude estimate obtained earlier.\n\nHaving computed the relative posterior density at a grid that covers the effective range\nof (α, β), we normalize by approximating the distribution as a step function over the grid\nand setting the total probability in the grid to 1.\n\nWe can then compute posterior moments based on the grid of (log(αβ ), log(α+β)); for\nexample,\n\nE(α|y) is estimated by\n∑\n\nlog(α\nβ ),log(α+β)\n\nα · p(log(α\nβ\n), log(α+β)|y).\n\nFrom the grid in Figure 5.3, we compute E(α|y) = 2.4 and E(β|y) = 14.3. This is close to the\nestimate based on the mode of Figure 5.3a, given above, because the posterior distribution is\napproximately symmetric on the scale of (log(αβ ), log(α+β)). A more important consequence\n\nof averaging over the grid is to account for the posterior uncertainty in (α, β), which is not\ncaptured in the point estimate.\n\nSampling from the joint posterior distribution of parameters and hyperparameters. We\ndraw 1000 random samples from the joint posterior distribution of (α, β, θ1, . . . , θJ ), as\nfollows.\n\n1. Simulate 1000 draws of (log(αβ ), log(α+β)) from their posterior distribution displayed\n\nin Figure 5.3, using the same discrete-grid sampling procedure used to draw (α, β) for\nFigure 3.3b in the bioassay example of Section 3.8.\n\n2. For l = 1, . . . , 1000:\n\n(a) Transform the lth draw of (log(αβ ), log(α+β)) to the scale (α, β) to yield a draw of\nthe hyperparameters from their marginal posterior distribution.\n\n(b) For each j = 1, . . . , J , sample θj from its conditional posterior distribution, θj |α, β, y ∼\nBeta(α+ yj, β + nj − yj).\n\nDisplaying the results. Figure 5.4 shows posterior medians and 95% intervals for the θj ’s,\ncomputed by simulation. The rates θj are shrunk from their sample point estimates,\n\nyj\nnj\n,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 113\n\nFigure 5.4 Posterior medians and 95% intervals of rat tumor rates, θj (plotted vs. observed tumor\nrates yj/nj), based on simulations from the joint posterior distribution. The 45◦ line corresponds\nto the unpooled estimates, θ̂i = yi/ni. The horizontal positions of the line have been jittered to\nreduce overlap.\n\ntowards the population distribution, with approximate mean 0.14; experiments with fewer\nobservations are shrunk more and have higher posterior variances. The results are superfi-\ncially similar to what would be obtained based on a point estimate of the hyperparameters,\nwhich makes sense in this example, because of the fairly large number of experiments.\nBut key differences remain, notably that posterior variability is higher in the full Bayesian\nanalysis, reflecting posterior uncertainty in the hyperparameters.\n\n5.4 Normal model with exchangeable parameters\n\nWe now present a full treatment of a simple hierarchical model based on the normal distribu-\ntion, in which observed data are normally distributed with a different mean for each ‘group’\nor ‘experiment,’ with known observation variance, and a normal population distribution\nfor the group means. This model is sometimes termed the one-way normal random-effects\nmodel with known data variance and is widely applicable, being an important special case\nof the hierarchical normal linear model, which we treat in some generality in Chapter 15.\nIn this section, we present a general treatment following the computational approach of\nSection 5.3. The following section presents a detailed example; those impatient with the\nalgebraic details may wish to look ahead at the example for motivation.\n\nThe data structure\n\nConsider J independent experiments, with experiment j estimating the parameter θj from\nnj independent normally distributed data points, yij , each with known error variance σ2;\nthat is,\n\nyij |θj ∼ N(θj , σ\n2), for i = 1, . . . , nj; j = 1, . . . , J. (5.11)\n\nUsing standard notation from the analysis of variance, we label the sample mean of each\ngroup j as\n\ny.j =\n1\n\nnj\n\nnj∑\n\ni=1\n\nyij\n\nwith sampling variance\n\nσ2\nj = σ2/nj.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n114 5. HIERARCHICAL MODELS\n\nWe can then write the likelihood for each θj using the sufficient statistics, y.j :\n\ny.j |θj ∼ N(θj , σ\n2\nj ), (5.12)\n\na notation that will prove useful later because of the flexibility in allowing a separate\nvariance σ2\n\nj for the mean of each group j. For the rest of this chapter, all expressions will\n\nbe implicitly conditional on the known values σ2\nj . The problem of estimating a set of means\n\nwith unknown variances will require some additional computational methods, presented in\nSections 11.6 and 13.6. Although rarely strictly true, the assumption of known variances\nat the sampling level of the model is often an adequate approximation.\n\nThe treatment of the model provided in this section is also appropriate for situations\nin which the variances differ for reasons other than the number of data points in the ex-\nperiment. In fact, the likelihood (5.12) can appear in much more general contexts than\nthat stated here. For example, if the group sizes nj are large enough, then the means y.j\nare approximately normally distributed, given θj , even when the data yij are not. Other\napplications where the actual likelihood is well approximated by (5.12) appear in the next\ntwo sections.\n\nConstructing a prior distribution from pragmatic considerations\n\nRather than considering immediately the problem of specifying a prior distribution for the\nparameter vector θ = (θ1, . . . , θJ ), let us consider what sorts of posterior estimates might\nbe reasonable for θ, given data (yij). A simple natural approach is to estimate θj by y.j, the\naverage outcome in experiment j. But what if, for example, there are J = 20 experiments\nwith only nj = 2 observations per experimental group, and the groups are 20 pairs of\nassays taken from the same strain of rat, under essentially identical conditions? The two\nobservations per group do not permit accurate estimates. Since the 20 groups are from the\nsame strain of rat, we might now prefer to estimate each θj by the pooled estimate,\n\ny.. =\n\n∑J\nj=1\n\n1\nσ2\nj\n\ny.j\n∑J\n\nj=1\n1\nσ2\nj\n\n. (5.13)\n\nTo decide which estimate to use, a traditional approach from classical statistics is to\nperform an analysis of variance F test for differences among means: if the J group means\nappear significantly variable, choose separate sample means, and if the variance between\nthe group means is not significantly greater than what could be explained by individual\nvariability within groups, use y... The theoretical analysis of variance table is as follows,\nwhere τ2 is the variance of θ1, . . . , θJ . For simplicity, we present the analysis of variance for\na balanced design in which nj = n and σ2\n\nj = σ2/n for all j.\n\ndf SS MS E(MS|σ2, τ )\n\nBetween groups J − 1\n∑\n\ni\n\n∑\nj\n(y.j − y..)\n\n2 SS/(J − 1) nτ 2 + σ2\n\nWithin groups J(n− 1)\n∑\n\ni\n\n∑\nj\n(yij − y.j)\n\n2 SS/(J(n− 1)) σ2\n\nTotal Jn− 1\n∑\n\ni\n\n∑\nj\n(yij − y..)\n\n2 SS/(Jn − 1)\n\nIn the classical random-effects analysis of variance, one computes the sum of squares (SS)\nand the mean square (MS) columns of the table and uses the ‘between’ and ‘within’ mean\nsquares to estimate τ . If the ratio of between to within mean squares is significantly greater\nthan 1, then the analysis of variance suggests separate estimates, θ̂j = y.j for each j. If\nthe ratio of mean squares is not ‘statistically significant,’ then the F test cannot ‘reject the\nhypothesis’ that τ = 0, and pooling is reasonable: θ̂j = y.., for all j. We discuss Bayesian\nanalysis of variance in Section 15.6 in the context of hierarchical regression models.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 115\n\nBut we are not forced to choose between complete pooling and none at all. An alternative\nis to use a weighted combination:\n\nθ̂j = λjy.j + (1− λj)y..,\n\nwhere λj is between 0 and 1.\nWhat kind of prior models produce these various posterior estimates?\n\n1. The unpooled estimate θ̂j = y.j is the posterior mean if the J values θj have independent\nuniform prior densities on (−∞,∞).\n\n2. The pooled estimate θ̂ = y.. is the posterior mean if the J values θj are restricted to be\nequal, with a uniform prior density on the common θ.\n\n3. The weighted combination is the posterior mean if the J values θj have independent and\nidentically distributed normal prior densities.\n\nAll three of these options are exchangeable in the θj ’s, and options 1 and 2 are special cases\nof option 3. No pooling corresponds to λj ≡ 1 for all j and an infinite prior variance for\nthe θj ’s, and complete pooling corresponds to λj ≡ 0 for all j and a zero prior variance for\nthe θj ’s.\n\nThe hierarchical model\n\nFor the convenience of conjugacy (more accurately, partial conjugacy), we assume that the\nparameters θj are drawn from a normal distribution with hyperparameters (µ, τ):\n\np(θ1, . . . , θJ |µ, τ) =\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2) (5.14)\n\np(θ1, . . . , θJ ) =\n\n∫ J∏\n\nj=1\n\n[\nN(θj |µ, τ2)\n\n]\np(µ, τ)d(µ, τ).\n\nThat is, the θj ’s are conditionally independent given (µ, τ). The hierarchical model also\npermits the interpretation of the θj ’s as a random sample from a shared population distri-\nbution, as illustrated in Figure 5.1 for the rat tumors.\n\nWe assign a noninformative uniform hyperprior distribution to µ, given τ :\n\np(µ, τ) = p(µ|τ)p(τ) ∝ p(τ). (5.15)\n\nThe uniform prior density for µ is generally reasonable for this problem; because the com-\nbined data from all J experiments are generally highly informative about µ, we can afford\nto be vague about its prior distribution. We defer discussion of the prior distribution of\nτ to later in the analysis, although relevant principles have already been discussed in the\ncontext of the rat tumor example. As usual, we first work out the answer conditional on\nthe hyperparameters and then consider their prior and posterior distributions.\n\nThe joint posterior distribution\n\nCombining the sampling model for the observable yij ’s and the prior distribution yields\nthe joint posterior distribution of all the parameters and hyperparameters, which we can\nexpress in terms of the sufficient statistics, y.j:\n\np(θ, µ, τ |y) ∝ p(µ, τ)p(θ|µ, τ)p(y|θ)\n\n∝ p(µ, τ)\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2)\nJ∏\n\nj=1\n\nN(y.j |θj , σ2\nj ), (5.16)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n116 5. HIERARCHICAL MODELS\n\nwhere we can ignore factors that depend only on y and the parameters σj , which are assumed\nknown for this analysis.\n\nThe conditional posterior distribution of the normal means, given the hyperparameters\n\nAs in the general hierarchical structure, the parameters θj are independent in the prior\ndistribution (given µ and τ) and appear in different factors in the likelihood (5.11); thus,\nthe conditional posterior distribution p(θ|µ, τ, y) factors into J components.\n\nConditional on the hyperparameters, we simply have J independent unknown normal\nmeans, given normal prior distributions, so we can use the methods of Section 2.5 inde-\npendently on each θj . The conditional posterior distributions for the θj’s are independent,\nand\n\nθj |µ, τ, y ∼ N(θ̂j , Vj),\n\nwhere\n\nθ̂j =\n\n1\nσ2\nj\n\ny.j +\n1\nτ2µ\n\n1\nσ2\nj\n\n+ 1\nτ2\n\nand Vj =\n1\n\n1\nσ2\nj\n\n+ 1\nτ2\n\n. (5.17)\n\nThe posterior mean is a precision-weighted average of the prior population mean and the\nsample mean of the jth group; these expressions for θ̂j and Vj are functions of µ and τ as\nwell as the data. The conditional posterior density for each θj given µ, τ is proper.\n\nThe marginal posterior distribution of the hyperparameters\n\nThe solution so far is only partial because it depends on the unknown µ and τ . The next step\nin our approach is a full Bayesian treatment for the hyperparameters. Section 5.3 mentions\nintegration or analytic computation as two approaches for obtaining p(µ, τ |y) from the joint\nposterior density p(θ, µ, τ |y). For the hierarchical normal model, we can simply consider\nthe information supplied by the data about the hyperparameters directly:\n\np(µ, τ |y) ∝ p(µ, τ)p(y|µ, τ).\n\nFor many problems, this decomposition is no help, because the ‘marginal likelihood’ factor,\np(y|µ, τ), cannot generally be written in closed form. For the normal distribution, however,\nthe marginal likelihood has a particularly simple form. The marginal distributions of the\ngroup means y.j , averaging over θ, are independent (but not identically distributed) normal:\n\ny.j |µ, τ ∼ N(µ, σ2\nj + τ2).\n\nThus we can write the marginal posterior density as\n\np(µ, τ |y) ∝ p(µ, τ)\nJ∏\n\nj=1\n\nN(y.j|µ, σ2\nj + τ2). (5.18)\n\nPosterior distribution of µ given τ . We could use (5.18) to compute directly the posterior\ndistribution p(µ, τ |y) as a function of two variables and proceed as in the rat tumor example.\nFor the normal model, however, we can further simplify by integrating over µ, leaving a\nsimple univariate numerical computation of p(τ |y). We factor the marginal posterior density\nof the hyperparameters as we did the prior density (5.15):\n\np(µ, τ |y) = p(µ|τ, y)p(τ |y). (5.19)\n\nThe first factor on the right side of (5.19) is just the posterior distribution of µ if τ were\nknown. From inspection of (5.18) with τ assumed known, and with a uniform conditional\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 117\n\nprior density p(µ|τ), the log posterior distribution is found to be quadratic in µ; thus,\np(µ|τ, y) must be normal. The mean and variance of this distribution can be obtained\nimmediately by considering the group means y.j as J independent estimates of µ with\nvariances (σ2\n\nj + τ2). Combining the data with the uniform prior density p(µ|τ) yields\n\nµ|τ, y ∼ N(µ̂, Vµ),\n\nwhere µ̂ is the precision-weighted average of the y.j-values, and V\n−1\nµ is the total precision:\n\nµ̂ =\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2 y.j\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2\n\nand V −1\nµ =\n\nJ∑\n\nj=1\n\n1\n\nσ2\nj + τ2\n\n. (5.20)\n\nThe result is a proper posterior density for µ, given τ .\n\nPosterior distribution of τ . We can now obtain the posterior distribution of τ analyti-\ncally from (5.19) and substitution of (5.18) and (5.20) for the numerator and denominator,\nrespectively:\n\np(τ |y) =\np(µ, τ |y)\np(µ|τ, y)\n\n∝\np(τ)\n\n∏J\nj=1 N(y.j |µ, σ2\n\nj + τ2)\n\nN(µ|µ̂, Vµ)\n.\n\nThis identity must hold for any value of µ (in other words, all the factors of µ must cancel\nwhen the expression is simplified); in particular, it holds if we set µ to µ̂, which makes\nevaluation of the expression simple:\n\np(τ |y) ∝\np(τ)\n\n∏J\nj=1 N(y.j |µ̂, σ2\n\nj + τ2)\n\nN(µ̂|µ̂, Vµ)\n\n∝ p(τ)V 1/2\nµ\n\nJ∏\n\nj=1\n\n(σ2\nj + τ2)−1/2 exp\n\n(\n−\n\n(y.j − µ̂)2\n2(σ2\n\nj + τ2)\n\n)\n, (5.21)\n\nwith µ̂ and Vµ defined in (5.20). Both expressions are functions of τ , which means that\np(τ |y) is a complicated function of τ .\n\nPrior distribution for τ . To complete our analysis, we must assign a prior distribution to\nτ . For convenience, we use a diffuse noninformative prior density for τ and hence must\nexamine the resulting posterior density to ensure it has a finite integral. For our illustrative\nanalysis, we use the uniform prior distribution, p(τ) ∝ 1. We leave it as an exercise to show\nmathematically that the uniform prior density for τ yields a proper posterior density and\nthat, in contrast, the seemingly reasonable ‘noninformative’ prior distribution for a variance\ncomponent, p(log τ) ∝ 1, yields an improper posterior distribution for τ . Alternatively, in\napplications it involves little extra effort to determine a ‘best guess’ and an upper bound\nfor the population variance τ , and a reasonable prior distribution can then be constructed\nfrom the scaled inverse-χ2 family (the natural choice for variance parameters), matching the\n‘best guess’ to the mean of the scaled inverse-χ2 density and the upper bound to an upper\npercentile such as the 99th. Once an initial analysis is performed using the noninformative\n‘uniform’ prior density, a sensitivity analysis with a more realistic prior distribution is often\ndesirable.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n118 5. HIERARCHICAL MODELS\n\nComputation\n\nFor this model, computation of the posterior distribution of θ is most conveniently performed\nvia simulation, following the factorization used above:\n\np(θ, µ, τ |y) = p(τ |y)p(µ|τ, y)p(θ|µ, τ, y).\n\nThe first step, simulating τ , is easily performed numerically using the inverse cdf method\n(see Section 1.9) on a grid of uniformly spaced values of τ , with p(τ |y) computed from\n(5.21). The second and third steps, simulating µ and then θ, can both be done easily by\nsampling from normal distributions, first (5.20) to obtain µ and then (5.17) to obtain the\nθj ’s independently.\n\nPosterior predictive distributions\n\nSampling from the posterior predictive distribution of new data, either from a current or\nnew batch, is straightforward given draws from the posterior distribution of the parameters.\nWe consider two scenarios: (1) future data ỹ from the current set of batches, with means\nθ = (θ1, . . . , θJ), and (2) future data ỹ from J̃ future batches, with means θ̃ = (θ̃1, . . . , θ̃J̃).\n\nIn the latter case, we must also specify the J̃ individual sample sizes ñj for the future\nbatches.\n\nTo obtain a draw from the posterior predictive distribution of new data ỹ from the\ncurrent batch of parameters, θ, first obtain a draw from p(θ, µ, τ |y) and then draw the\npredictive data ỹ from (5.11).\n\nTo obtain posterior predictive simulations of new data ỹ for J̃ new groups, perform the\nfollowing three steps: first, draw (µ, τ) from their posterior distribution; second, draw J̃\nnew parameters θ̃ = (θ̃1, . . . , θ̃J̃) from the population distribution p(θ̃j |µ, τ), which is the\npopulation, or prior, distribution for θ given the hyperparameters (equation (5.14)); and\nthird, draw ỹ given θ̃ from the data distribution (5.11).\n\nDifficulty with a natural non-Bayesian estimate of the hyperparameters\n\nTo see some advantages of our fully Bayesian approach, we compare it to an approximate\nmethod that is sometimes used based on a point estimate of µ and τ from the data. Unbiased\npoint estimates, derived from the analysis of variance presented earlier, are\n\nµ̂ = y..\n\nτ̂2 = (MSB −MSW )/n. (5.22)\n\nThe terms MSB and MSW are the ‘between’ and ‘within’ mean squares, respectively, from\nthe analysis of variance. In this alternative approach, inference for θ1, . . . , θJ is based on\nthe conditional posterior distribution, p(θ|µ̂, τ̂ ), given the point estimates.\n\nAs we saw in the rat tumor example of the previous section, the main problem with\nsubstituting point estimates for the hyperparameters is that it ignores our real uncertainty\nabout them. The resulting inference for θ cannot be interpreted as a Bayesian posterior\nsummary. In addition, the estimate τ̂2 in (5.22) has the flaw that it can be negative! The\nproblem of a negative estimate for a variance component can be avoided by setting τ̂2 to\nzero in the case that MSW exceeds MSB , but this creates new issues. Estimating τ2 = 0\nwhenever MSW > MSB seems too strong a claim: if MSW > MSB, then the sample size is\ntoo small for τ2 to be distinguished from zero, but this is not the same as saying we know\nthat τ2 = 0. The latter claim, made implicitly by the point estimate, implies that all the\ngroup means θj are absolutely identical, which leads to scientifically indefensible claims, as\nwe shall see in the example in the next section. It is possible to construct a point estimate\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 119\n\nof (µ, τ) to avoid this particular difficulty, but it would still have the problem, common to\nall point estimates, of ignoring uncertainty.\n\n5.5 Example: parallel experiments in eight schools\n\nWe illustrate the hierarchical normal model with a problem in which the Bayesian analysis\ngives conclusions that differ in important respects from other methods.\n\nA study was performed for the Educational Testing Service to analyze the effects of\nspecial coaching programs on test scores. Separate randomized experiments were performed\nto estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test-\nVerbal) in each of eight high schools. The outcome variable in each study was the score on\na special administration of the SAT-V, a standardized multiple choice test administered by\nthe Educational Testing Service and used to help colleges make admissions decisions; the\nscores can vary between 200 and 800, with mean about 500 and standard deviation about\n100. The SAT examinations are designed to be resistant to short-term efforts directed\nspecifically toward improving performance on the test; instead they are designed to reflect\nknowledge acquired and abilities developed over many years of education. Nevertheless,\neach of the eight schools in this study considered its short-term coaching program to be\nsuccessful at increasing SAT scores. Also, there was no prior reason to believe that any of\nthe eight programs was more effective than any other or that some were more similar in\neffect to each other than to any other.\n\nThe results of the experiments are summarized in Table 5.2. All students in the ex-\nperiments had already taken the PSAT (Preliminary SAT), and allowance was made for\ndifferences in the PSAT-M (Mathematics) and PSAT-V test scores between coached and\nuncoached students. In particular, in each school the estimated coaching effect and its\nstandard error were obtained by an analysis of covariance adjustment (that is, a linear\nregression was performed of SAT-V on treatment group, using PSAT-M and PSAT-V as\ncontrol variables) appropriate for a completely randomized experiment. A separate regres-\nsion was estimated for each school. Although not simple sample means (because of the\ncovariance adjustments), the estimated coaching effects, which we label yj , and their sam-\npling variances, σ2\n\nj , play the same role in our model as y.j and σ2\nj in the previous section.\n\nThe estimates yj are obtained by independent experiments and have approximately normal\nsampling distributions with sampling variances that are known, for all practical purposes,\nbecause the sample sizes in all of the eight experiments were relatively large, over thirty\nstudents in each school (recall the discussion of data reduction in Section 4.1). Incidentally,\nan increase of eight points on the SAT-V corresponds to about one more test item correct.\n\nInferences based on nonhierarchical models and their problems\n\nBefore fitting the hierarchical Bayesian model, we first consider two simpler nonhierarchical\nmethods—estimating the effects from the eight experiments independently, and complete\npooling—and discuss why neither of these approaches is adequate for this example.\n\nSeparate estimates. A cursory examination of Table 5.2 may at first suggest that some\ncoaching programs have moderate effects (in the range 18–28 points), most have small\neffects (0–12 points), and two have small negative effects; however, when we take note\nof the standard errors of these estimated effects, we see that it is difficult statistically\nto distinguish between any of the experiments. For example, treating each experiment\nseparately and applying the simple normal analysis in each yields 95% posterior intervals\nthat all overlap substantially.\n\nA pooled estimate. The general overlap in the posterior intervals based on independent\nanalyses suggests that all experiments might be estimating the same quantity. Under the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n120 5. HIERARCHICAL MODELS\n\nEstimated Standard error\ntreatment of effect\n\nSchool effect, yj estimate, σj\nA 28 15\nB 8 10\nC −3 16\nD 7 11\nE −1 9\nF 1 11\nG 18 10\nH 12 18\n\nTable 5.2 Observed effects of special preparation on SAT-V scores in eight randomized experiments.\nEstimates are based on separate analyses for the eight experiments.\n\nhypothesis that all experiments have the same effect and produce independent estimates\nof this common effect, we could treat the data in Table 5.2 as eight normally distributed\nobservations with known variances. With a noninformative prior distribution, the posterior\nmean for the common coaching effect in the schools is y.., as defined in equation (5.13) with\n\nyj in place of y.j . This pooled estimate is 7.7, and the posterior variance is (\n∑8\n\nj=1\n1\nσ2\nj\n\n)−1 =\n\n16.6 because the eight experiments are independent. Thus, we would estimate the common\neffect to be 7.7 points with standard error equal to\n\n√\n16.6 = 4.1, which would lead to the\n\n95% posterior interval [−0.5, 15.9], or approximately [8 ± 8]. Supporting this analysis, the\nclassical test of the hypothesis that all θj ’s are estimating the same quantity yields a χ2\n\nstatistic less than its degrees of freedom (seven, in this case):\n∑8\n\nj=1(yj − y..)2/σ2\ni = 4.6. To\n\nput it another way, the estimate τ̂2 from (5.22) is negative.\n\nWould it be possible to have one school’s observed effect be 28 just by chance, if the\ncoaching effects in all eight schools were really the same? To get a feeling for the natural\nvariation that we would expect across eight studies if this assumption were true, suppose\nthe estimated treatment effects are eight independent draws from a normal distribution\nwith mean 8 points and standard deviation 13 points (the square root of the mean of the\neight variances σ2\n\nj ). Then, based on the expected values of normal order statistics, we\nwould expect the largest observed value of yj to be about 26 points and the others, in\ndiminishing order, to be about 19, 14, 10, 6, 2, −3, and −9 points. These expected effect\nsizes are consistent with the set of observed effect sizes in Table 5.2. Thus, it would appear\nimprudent to believe that school A really has an effect as large as 28 points.\n\nDifficulties with the separate and pooled estimates. To see the problems with the two ex-\ntreme attitudes—the separate analyses that consider each θj separately, and the alternative\nview (a single common effect) that leads to the pooled estimate—consider θ1, the effect in\nschool A. The effect in school A is estimated as 28.4 with a standard error of 14.9 under\nthe separate analysis, versus a pooled estimate of 7.7 with a standard error of 4.1 under\nthe common-effect model. The separate analyses of the eight schools imply the following\nposterior statement: ‘the probability is 1\n\n2 that the true effect in A is more than 28.4,’ a\ndoubtful statement, considering the results for the other seven schools. On the other hand,\nthe pooled model implies the following statement: ‘the probability is 1\n\n2 that the true effect\nin A is less than 7.7,’ which, despite the non-significant χ2 test, seems an inaccurate sum-\nmary of our knowledge. The pooled model also implies the statement: ‘the probability is 1\n\n2\nthat the true effect in A is less than the true effect in C,’ which also is difficult to justify\ngiven the data in Table 5.2. As in the theoretical discussion of the previous section, neither\nestimate is fully satisfactory, and we would like a compromise that combines information\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 121\n\nFigure 5.5 Marginal posterior density, p(τ |y), for standard deviation of the population of school\neffects θj in the educational testing example.\n\nfrom all eight experiments without assuming all the θj ’s to be equal. The Bayesian analysis\nunder the hierarchical model provides exactly that.\n\nPosterior simulation under the hierarchical model\n\nConsequently, we compute the posterior distribution of θ1, . . . , θ8, based on the normal\nmodel presented in Section 5.4. (More discussion of the reasonableness of applying this\nmodel in this problem appears in Sections 6.5 and 17.4.) We draw from the posterior\ndistribution for the Bayesian model by simulating the random variables τ , µ, and θ, in that\norder, from their posterior distribution, as discussed at the end of the previous section. The\nsampling standard deviations, σj , are assumed known and equal to the values in Table 5.2,\nand we assume independent uniform prior densities on µ and τ .\n\nResults\n\nThe marginal posterior density function, p(τ |y) from (5.21), is plotted in Figure 5.5. Values\nof τ near zero are most plausible; zero is the most likely value, values of τ larger than 10\nare less than half as likely as τ = 0, and Pr(τ > 25) ≈ 0. Inference regarding the marginal\ndistributions of the other model parameters and the joint distribution are obtained from the\nsimulated values. Illustrations are provided in the discussion that follows this section. In\nthe normal hierarchical model, however, we learn a great deal by considering the conditional\nposterior distributions given τ (and averaged over µ).\n\nThe conditional posterior means E(θj |τ, y) (averaging over µ) are displayed as functions\nof τ in Figure 5.6; the vertical axis displays the scale for the θj ’s. Comparing Figure 5.6\nto Figure 5.5, which has the same scale on the horizontal axis, we see that for most of the\nlikely values of τ , the estimated effects are relatively close together; as τ becomes larger,\ncorresponding to more variability among schools, the estimates become more like the raw\nvalues in Table 5.2.\n\nThe lines in Figure 5.7 show the conditional standard deviations, sd(θj |τ, y), as a func-\ntion of τ . As τ increases, the population distribution allows the eight effects to be more\ndifferent from each other, and hence the posterior uncertainty in each individual θj increases,\napproaching the standard deviations in Table 5.2 in the limit of τ → ∞. (The posterior\nmeans and standard deviations for the components θj , given τ , are computed using the\nmean and variance formulas (2.7) and (2.8), averaging over µ; see Exercise 5.12.)\n\nThe general conclusion from an examination of Figures 5.5–5.7 is that an effect as large\nas 28.4 points in any school is unlikely. For the likely values of τ , the estimates in all\nschools are substantially less than 28 points. For example, even at τ = 10, the probability\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n122 5. HIERARCHICAL MODELS\n\nFigure 5.6 Conditional posterior means of treatment effects, E(θj |τ, y), as functions of the between-\nschool standard deviation τ , for the educational testing example. The line for school C crosses the\nlines for E and F because C has a higher measurement error (see Table 5.2) and its estimate is\ntherefore shrunk more strongly toward the overall mean in the Bayesian analysis.\n\nFigure 5.7 Conditional posterior standard deviations of treatment effects, sd(θj |τ, y), as functions\nof the between-school standard deviation τ , for the educational testing example.\n\nthat the effect in school A is less than 28 points is Φ[(28 − 14.5)/9.1] = 93%, where Φ is\nthe standard normal cumulative distribution function; the corresponding probabilities for\nthe effects being less than 28 points in the other schools are 99.5%, 99.2%, 98.5%, 99.96%,\n99.8%, 97%, and 98%.\n\nOf substantial importance, we do not obtain an accurate summary of the data if we\ncondition on the posterior mode of τ . The technique of conditioning on a modal value (for\nexample, the maximum likelihood estimate) of a hyperparameter such as τ is often used\nin practice (at least as an approximation), but it ignores the uncertainty conveyed by the\nposterior distribution of the hyperparameter. At τ = 0, the inference is that all experiments\nhave the same size effect, 7.7 points, and the same standard error, 4.1 points. Figures 5.5–\n5.7 certainly suggest that this answer represents too much pulling together of the estimates\nin the eight schools. The problem is especially acute in this example because the posterior\nmode of τ is on the boundary of its parameter space. A joint posterior modal estimate of\n(θ1, . . . , θJ , µ, τ) suffers from even worse problems in general.\n\nDiscussion\n\nTable 5.3 summarizes the 200 simulated effect estimates for all eight schools. In one sense,\nthese results are similar to the pooled 95% interval [8± 8], in that the eight Bayesian 95%\nintervals largely overlap and are median-centered between 5 and 10. In a second sense,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 123\n\nSchool Posterior quantiles\n2.5% 25% median 75% 97.5%\n\nA −2 7 10 16 31\nB −5 3 8 12 23\nC −11 2 7 11 19\nD −7 4 8 11 21\nE −9 1 5 10 18\nF −7 2 6 10 28\nG −1 7 10 15 26\nH −6 3 8 13 33\n\nTable 5.3: Summary of 200 simulations of the treatment effects in the eight schools.\n\nFigure 5.8 Histograms of two quantities of interest computed from the 200 simulation draws: (a)\nthe effect in school A, θ1; (b) the largest effect, max{θj}. The jaggedness of the histograms is just\nan artifact caused by sampling variability from using only 200 random draws.\n\nthe results in the table differ from the pooled estimate in a direction toward the eight\nindependent answers: the 95% Bayesian intervals are each almost twice as wide as the one\ncommon interval and suggest substantially greater probabilities of effects larger than 16\npoints, especially in school A, and greater probabilities of negative effects, especially in\nschool C. If greater precision were required in the posterior intervals, one could simulate\nmore simulation draws; we use only 200 draws here to illustrate that a small simulation\ngives adequate inference for many practical purposes.\n\nThe ordering of the effects in the eight schools as suggested by Table 5.3 is essentially the\nsame as would be obtained by the eight separate estimates. However, there are differences\nin the details; for example, the Bayesian probability that the effect in school A is as large\nas 28 points is less than 10%, which is substantially less than the 50% probability based on\nthe separate estimate for school A.\n\nAs an illustration of the simulation-based posterior results, 200 simulations of school\nA’s effect are shown in Figure 5.8a. Having simulated the parameter θ, it is easy to ask\nmore complicated questions of this model. For example, what is the posterior distribution\nof max{θj}, the effect of the most successful of the eight coaching programs? Figure 5.8b\ndisplays a histogram of 200 values from this posterior distribution and shows that only 22\ndraws are larger than 28.4; thus, Pr(max{θj} > 28.4) ≈ 22\n\n200 . Since Figure 5.8a gives the\nmarginal posterior distribution of the effect in school A, and Figure 5.8b gives the marginal\nposterior distribution of the largest effect no matter which school it is in, the latter figure has\nlarger values. For another example, we can estimate Pr(θ1 > θ3|y), the posterior probability\nthat the coaching program is more effective in school A than in school C, by the proportion\nof simulated draws of θ for which θ1 > θ3; the result is 141\n\n200 = 0.705.\nTo sum up, the Bayesian analysis of this example not only allows straightforward infer-\n\nences about many parameters that may be of interest, but the hierarchical model is flexible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n124 5. HIERARCHICAL MODELS\n\nStudy,\nRaw data\n\n(deaths/total)\nLog-\nodds,\n\nsd,\nPosterior quantiles of effect θj\n\nnormal approx. (on log-odds scale)\nj Control Treated yj σj 2.5% 25% median 75% 97.5%\n\n1 3/39 3/38 0.028 0.850 −0.57 −0.33 −0.24 −0.16 0.12\n2 14/116 7/114 −0.741 0.483 −0.64 −0.37 −0.28 −0.20 −0.00\n3 11/93 5/69 −0.541 0.565 −0.60 −0.35 −0.26 −0.18 0.05\n4 127/1520 102/1533 −0.246 0.138 −0.45 −0.31 −0.25 −0.19 −0.05\n5 27/365 28/355 0.069 0.281 −0.43 −0.28 −0.21 −0.11 0.15\n6 6/52 4/59 −0.584 0.676 −0.62 −0.35 −0.26 −0.18 0.05\n7 152/939 98/945 −0.512 0.139 −0.61 −0.43 −0.36 −0.28 −0.17\n8 48/471 60/632 −0.079 0.204 −0.43 −0.28 −0.21 −0.13 0.08\n9 37/282 25/278 −0.424 0.274 −0.58 −0.36 −0.28 −0.20 −0.02\n\n10 188/1921 138/1916 −0.335 0.117 −0.48 −0.35 −0.29 −0.23 −0.13\n11 52/583 64/873 −0.213 0.195 −0.48 −0.31 −0.24 −0.17 0.01\n12 47/266 45/263 −0.039 0.229 −0.43 −0.28 −0.21 −0.12 0.11\n13 16/293 9/291 −0.593 0.425 −0.63 −0.36 −0.28 −0.20 0.01\n14 45/883 57/858 0.282 0.205 −0.34 −0.22 −0.12 0.00 0.27\n15 31/147 25/154 −0.321 0.298 −0.56 −0.34 −0.26 −0.19 0.01\n16 38/213 33/207 −0.135 0.261 −0.48 −0.30 −0.23 −0.15 0.08\n17 12/122 28/251 0.141 0.364 −0.47 −0.29 −0.21 −0.12 0.17\n18 6/154 8/151 0.322 0.553 −0.51 −0.30 −0.23 −0.13 0.15\n19 3/134 6/174 0.444 0.717 −0.53 −0.31 −0.23 −0.14 0.15\n20 40/218 32/209 −0.218 0.260 −0.50 −0.32 −0.25 −0.17 0.04\n21 43/364 27/391 −0.591 0.257 −0.64 −0.40 −0.31 −0.23 −0.09\n22 39/674 22/680 −0.608 0.272 −0.65 −0.40 −0.31 −0.23 −0.07\n\nTable 5.4 Results of 22 clinical trials of beta-blockers for reducing mortality after myocardial infarc-\ntion, with empirical log-odds and approximate sampling variances. Data from Yusuf et al. (1985).\nPosterior quantiles of treatment effects are based on 5000 draws from a Bayesian hierarchical model\ndescribed here. Negative effects correspond to reduced probability of death under the treatment.\n\nenough to adapt to the data, thereby providing posterior inferences that account for the\npartial pooling as well as the uncertainty in the hyperparameters.\n\n5.6 Hierarchical modeling applied to a meta-analysis\n\nMeta-analysis is an increasingly popular and important process of summarizing and inte-\ngrating the findings of research studies in a particular area. As a method for combining\ninformation from several parallel data sources, meta-analysis is closely connected to hierar-\nchical modeling. In this section we consider a relatively simple application of hierarchical\nmodeling to a meta-analysis in medicine. We consider another meta-analysis problem in\nthe context of a decision problem in Section 9.2.\n\nThe data in our medical example are displayed in the first three columns of Table 5.4,\nwhich summarize mortality after myocardial infarction in 22 clinical trials, each consisting of\ntwo groups of heart attack patients randomly allocated to receive or not receive beta-blockers\n(a family of drugs that affect the central nervous system and can relax the heart muscles).\nMortality varies from 3% to 21% across the studies, most of which show a modest, though\nnot ‘statistically significant,’ benefit from the use of beta-blockers. The aim of a meta-\nanalysis is to provide a combined analysis of the studies that indicates the overall strength\nof the evidence for a beneficial effect of the treatment under study. Before proceeding to a\nformal meta-analysis, it is important to apply rigorous criteria in determining which studies\nare included. (This relates to concerns of ignorability in data collection for observational\nstudies, as discussed in Chapter 8.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 125\n\nDefining a parameter for each study\n\nIn the beta-blocker example, the meta-analysis involves data in the form of several 2 × 2\ntables. If clinical trial j (in the series to be considered for meta-analysis) involves the use\nof n0j subjects in the control group and n1j in the treatment group, giving rise to y0j and\ny1j deaths in control and treatment groups, respectively, then the usual sampling model\ninvolves two independent binomial distributions with probabilities of death p0j and p1j ,\nrespectively. Estimands of interest include the difference in probabilities, p1j − p0j , the\nprobability or risk ratio, p1j/p0j , and the odds ratio, ρj =\n\np1j\n1−p1j /\n\np0j\n1−p0j . For a number of\n\nreasons, including interpretability in a range of study designs (including case-control studies\nas well as clinical trials and cohort studies), and the fact that its posterior distribution is\nclose to normality even for relatively small sample sizes, we concentrate on inference for the\n(natural) logarithm of the odds ratio, which we label θj = log ρj .\n\nA normal approximation to the likelihood\n\nRelatively simple Bayesian meta-analysis is possible using the normal-theory results of the\nprevious sections if we summarize the results of each experiment j with an approximate\nnormal likelihood for the parameter θj . This is possible with a number of standard analytic\napproaches that produce a point estimate and standard errors, which can be regarded as\napproximating a normal mean and standard deviation. One approach is based on empirical\nlogits: for each study j, one can estimate θj by\n\nyj = log\n\n(\ny1j\n\nn1j − y1j\n\n)\n− log\n\n(\ny0j\n\nn0j − y0j\n\n)\n, (5.23)\n\nwith approximate sampling variance\n\nσ2\nj =\n\n1\n\ny1j\n+\n\n1\n\nn1j − y1j\n+\n\n1\n\ny0j\n+\n\n1\n\nn0j − y0j\n. (5.24)\n\nWe use the notation yj and σ2\nj to be consistent with our earlier expressions for the hier-\n\narchical normal model. There are various refinements of these estimates that improve the\nasymptotic normality of the sampling distributions involved (in particular, it is often rec-\nommended to add a fraction such as 0.5 to each of the four counts in the 2× 2 table), but\nwhenever study-specific sample sizes are moderately large, such details do not concern us.\n\nThe estimated log-odds ratios yj and their estimated standard errors σ2\nj are displayed\n\nas the fourth and fifth columns of Table 5.4. We use a hierarchical Bayesian analysis to\ncombine information from the 22 studies and gain improved estimates of each θj , along with\nestimates of the mean and variance of the effects over all studies.\n\nGoals of inference in meta-analysis\n\nDiscussions of meta-analysis are sometimes imprecise about the estimands of interest in the\nanalysis, especially when the primary focus is on testing the null hypothesis of no effect in\nany of the studies to be combined. Our focus is on estimating meaningful parameters, and\nfor this objective there appear to be three possibilities, accepting the overarching assumption\nthat the studies are comparable in some broad sense. The first possibility is that we view\nthe studies as identical replications of each other, in the sense we regard the individuals in\nall the studies as independent samples from a common population, with the same outcome\nmeasures and so on. A second possibility is that the studies are so different that the results\nof any one study provide no information about the results of any of the others. A third, more\ngeneral, possibility is that we regard the studies as exchangeable but not necessarily either\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n126 5. HIERARCHICAL MODELS\n\nidentical or completely unrelated; in other words we allow differences from study to study,\nbut such that the differences are not expected a priori to have predictable effects favoring\none study over another. As we have discussed in detail in this chapter, this third possibility\nrepresents a continuum between the two extremes, and it is this exchangeable model (with\nunknown hyperparameters characterizing the population distribution) that forms the basis\nof our Bayesian analysis.\n\nExchangeability does not dictate the form of the joint distribution of the study effects.\nIn what follows we adopt the convenient assumption of a normal distribution for the varying\nparameters; in practice it is important to check this assumption using some of the techniques\ndiscussed in Chapter 6.\n\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem\nin general, is the mean of the distribution of effect sizes, since this represents the overall\n‘average’ effect across all studies that could be regarded as exchangeable with the observed\nstudies. Other possible estimands are the effect size in any of the observed studies and the\neffect size in another, comparable (exchangeable) unobserved study.\n\nWhat if exchangeability is inappropriate?\n\nWhen assuming exchangeability we assume there are no important covariates that might\nform the basis of a more complex model, and this assumption (perhaps misguidedly) is\nwidely adopted in meta-analysis. What if other information (in addition to the data (n, y))\nis available to distinguish among the J studies in a meta-analysis, so that an exchangeable\nmodel is inappropriate? In this situation, we can expand the framework of the model to be\nexchangeable in the observed data and covariates, for example using a hierarchical regression\nmodel, as in Chapter 15, so as to estimate how the treatment effect behaves as a function\nof the covariates. The real aim might in general be to estimate a response surface so that\none could predict an effect based on known characteristics of a population and its exposure\nto risk.\n\nA hierarchical normal model\n\nA normal population distribution in conjunction with the approximate normal sampling\ndistribution of the study-specific effect estimates allows an analysis of the same form as\nused for the SAT coaching example in the previous section. Let yj represent generically the\npoint estimate of the effect θj in the jth study, obtained from (5.23), where j = 1, . . . , J .\nThe first stage of the hierarchical normal model assumes that\n\nyj |θj , σj ∼ N(θj , σ\n2\nj ),\n\nwhere σj represents the corresponding estimated standard error from (5.24), which is as-\nsumed known without error. The simplification of known variances has little effect here\nbecause, with the large sample sizes (more than 50 persons in each treatment group in\nnearly all of the studies in the beta-blocker example), the binomial variances in each study\nare precisely estimated. At the second stage of the hierarchy, we again use an exchangeable\nnormal prior distribution, with mean µ and standard deviation τ , which are unknown hy-\nperparameters. Finally, a hyperprior distribution is required for µ and τ . For this problem,\nit is reasonable to assume a noninformative or locally uniform prior density for µ, since\neven with a small number of studies (say 5 or 10), the combined data become relatively\ninformative about the center of the population distribution of effect sizes. As with the\nSAT coaching example, we also assume a locally uniform prior density for τ , essentially for\nconvenience, although it is easy to modify the analysis to include prior information.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 127\n\nPosterior quantiles\nEstimand 2.5% 25% median 75% 97.5%\n\nMean, µ −0.37 −0.29 −0.25 −0.20 −0.11\nStandard deviation, τ 0.02 0.08 0.13 0.18 0.31\n\nPredicted effect, θ̃j −0.58 −0.34 −0.25 −0.17 0.11\n\nTable 5.5 Summary of posterior inference for the overall mean and standard deviation of study\neffects, and for the predicted effect in a hypothetical future study, from the meta-analysis of the\nbeta-blocker trials in Table 5.4. All effects are on the log-odds scale.\n\nResults of the analysis and comparison to simpler methods\n\nThe analysis of our meta-analysis model now follows exactly the same methodology as in\nthe previous sections. First, a plot (not shown here) similar to Figure 5.5 shows that the\nmarginal posterior density of τ peaks at a nonzero value, although values near zero are\nclearly plausible, zero having a posterior density only about 25% lower than that at the\nmode. Posterior quantiles for the effects θj for the 22 studies on the logit scale are displayed\nas the last columns of Table 5.4.\n\nSince the posterior distribution of τ is concentrated around values that are small relative\nto the sampling standard deviations of the data (compare the posterior median of τ , 0.13,\nin Table 5.5 to the values of σj in the fourth column of Table 5.4), considerable shrinkage\nis evident in the Bayes estimates, especially for studies with low internal precision (for\nexample, studies 1, 6, and 18). The substantial degree of homogeneity between the studies\nis further reflected in the large reductions in posterior variance obtained when going from\nthe study-specific estimates to the Bayesian ones, which borrow strength from each other.\nUsing an approximate approach fixing τ would yield standard deviations that would be too\nsmall compared to the fully Bayesian ones.\n\nHistograms (not shown) of the simulated posterior densities for each of the individual\neffects exhibit skewness away from the central value of the overall mean, whereas the distri-\nbution of the overall mean has greater symmetry. The imprecise studies, such as 2 and 18,\nexhibit longer-tailed posterior distributions than the more precise ones, such as 7 and 14.\n\nIn meta-analysis, interest often focuses on the estimate of the overall mean effect, µ.\nSuperimposing the graphs (not shown here) of the conditional posterior mean and standard\ndeviation of µ given τ on the posterior density of τ reveals a small range in the plausible\nvalues of E(µ|τ, y), from about −0.26 to just over −0.24, but sd(µ|τ, y) varies by a factor\nof more than 2 across the plausible range of values of τ . The latter feature indicates\nthe importance of averaging over τ in order to account adequately for uncertainty in its\nestimation. In fact, the conditional posterior standard deviation, sd(µ|τ, y) has the value\n0.060 at τ = 0.13, whereas upon averaging over the posterior distribution for τ we find a\nvalue of sd(µ|y) = 0.071.\n\nTable 5.5 gives a summary of posterior inferences for the hyperparameters µ and τ and\nthe predicted effect, θ̃j , in a hypothetical future study. The approximate 95% highest pos-\nterior density interval for µ is [−0.37,−0.11], or [0.69, 0.90] when converted to the odds\nratio scale (that is, exponentiated). In contrast, the 95% posterior interval that results\nfrom complete pooling—that is, assuming τ = 0—is considerably narrower, [0.70, 0.85]. In\nthe original published discussion of these data, it was remarked that the latter seems an\n‘unusually narrow range of uncertainty.’ The hierarchical Bayesian analysis suggests that\nthis was due to the use of an inappropriate model that had the effect of claiming all the\nstudies were identical. In mathematical terms, complete pooling makes the assumption that\nthe parameter τ is exactly zero, whereas the data supply evidence that τ might be close\nto zero, but might also plausibly be as high as 0.3. A related concern is that commonly\nused analyses tend to place undue emphasis on inference for the overall mean effect. Un-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n128 5. HIERARCHICAL MODELS\n\ncertainty about the probable treatment effect in a particular population where a study has\nnot been performed (or indeed in a previously studied population but with a slightly mod-\nified treatment) might be more reasonably represented by inference for a new study effect,\nexchangeable with those for which studies have been performed, rather than for the overall\nmean. In this case, uncertainty is even greater, as exhibited in the ‘Predicted effect’ row of\nTable 5.5; uncertainty for an individual patient includes yet another component of varia-\ntion. In particular, with the beta-blocker data, there is just over 10% posterior probability\nthat the true effect, θ̃j , in a new study would be positive (corresponding to the treatment\nincreasing the probability of death in that study).\n\n5.7 Weakly informative priors for variance parameters\n\nA key element in the analyses above is the prior distribution for the scale parameter, τ .\nWe have used the uniform, but various other noninformative prior distributions have been\nsuggested in the Bayesian literature. It turns out that the choice of ‘noninformative’ prior\ndistribution can have a big effect on inferences, especially for problems where the number\nof groups J is small or the group-level variation τ is small.\n\nWe discuss the options here in the context of the normal model, but the principles apply\nto inferences for group-level variances more generally.\n\nConcepts relating to the choice of prior distribution\n\nImproper limit of a prior distribution. Improper prior densities can, but do not necessarily,\nlead to proper posterior distributions. To avoid confusion it is useful to define improper\ndistributions as particular limits of proper distributions. For the group-level variance pa-\nrameter, two commonly considered improper densities are uniform(0, A) on τ , as A → ∞,\nand inverse-gamma(ǫ, ǫ) on τ2, as ǫ→ 0.\n\nAs we shall see, the uniform(0, A) model yields a limiting proper posterior density as\nA→ ∞, as long as the number of groups J is at least 3. Thus, for a finite but sufficiently\nlarge A, inferences are not sensitive to the choice of A.\n\nIn contrast, the inverse-gamma(ǫ, ǫ) model does not have any proper limiting poste-\nrior distribution. As a result, posterior inferences are sensitive to ǫ—it cannot simply be\ncomfortably set to a low value such as 0.001.\n\nCalibration. Posterior inferences can be evaluated using the concept of calibration of the\nposterior mean, the Bayesian analogue to the classical notion of bias. For any parameter\nθ, if we label the posterior mean as θ̂ = E(θ|y), we can define the miscalibration of the\n\nposterior mean as E(θ|θ̂) − θ̂. If the prior distribution is true—that is, if the data are\nconstructed by first drawing θ from p(θ), then drawing y from p(y|θ)—then the posterior\n\nmean is automatically calibrated; that is, the miscalibration is 0 for all values of θ̂.\nTo restate: in classical bias analysis, we condition on the true θ and look at the distri-\n\nbution of the data-based estimate, θ̂. In a Bayesian calibration analysis, we condition on\nthe data y (and thus also on the estimate, θ̂) and look at the distribution of parameters θ\nthat could have produced these data.\n\nWhen considering improper models, the theory must be expanded, since it is impossible\nfor θ to be drawn from an unnormalized density. To evaluate calibration in this context,\nit is necessary to posit a ‘true prior distribution’ from which θ is drawn along with the\n‘inferential prior distribution’ that is used in the Bayesian inference.\n\nFor the hierarchical model for the 8 schools, we can consider the improper uniform\ndensity on τ as a limit of uniform prior densities on the range (0, A), with A → ∞. For\nany finite value of A, we can then see that the improper uniform density leads to inferences\nwith a positive miscalibration—that is, overestimates (on average) of τ .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 129\n\nWe demonstrate this miscalibration in two steps. First, suppose that both the true\nand inferential prior distributions for τ are uniform on (0, A). Then the miscalibration is\ntrivially zero. Now keep the true prior distribution at U(0, A) and let the inferential prior\n\ndistribution go to U(0,∞). This will necessarily increase θ̂ for any data y (since we are now\naveraging over values of θ in the range [A,∞)) without changing the true θ, thus causing\nthe average value of the miscalibration to become positive.\n\nClasses of noninformative and weakly informative prior distributions for hierarchical\nvariance parameters\n\nGeneral considerations. We view any noninformative or weakly informative prior distribu-\ntion as inherently provisional—after the model has been fit, one should look at the posterior\ndistribution and see if it makes sense. If the posterior distribution does not make sense,\nthis implies that additional prior knowledge is available that has not been included in the\nmodel, and that contradicts the assumptions of the prior distribution that has been used.\nIt is then appropriate to go back and alter the prior distribution to be more consistent with\nthis external knowledge.\n\nUniform prior distributions. We first consider uniform priors while recognizing that we\nmust be explicit about the scale on which the distribution is defined. Various choices have\nbeen proposed for modeling variance parameters. A uniform prior distribution on log τ\nwould seem natural—working with the logarithm of a parameter that must be positive—\nbut it results in an improper posterior distribution. An alternative would be to define the\nprior distribution on a compact set (e.g., in the range [−A,A] for some large value of A),\nbut then the posterior distribution would depend strongly on the lower bound −A of the\nprior support.\n\nThe problem arises because the marginal likelihood, p(y|τ)—after integrating over θ and\nµ in (5.16)—approaches a finite nonzero value as τ → 0. Thus, if the prior density for log τ\nis uniform, the posterior will have infinite mass integrating to the limit log τ → −∞. To put\nit another way, in a hierarchical model the data can never rule out a group-level variance\nof zero, and so the prior distribution cannot put an infinite mass in this area.\n\nAnother option is a uniform prior distribution on τ itself, which has a finite integral\nnear τ = 0 and thus avoids the above problem. We have generally used this noninformative\ndensity in our applied work (as illustrated in Section 5.5), but it has a slightly disagreeable\nmiscalibration toward positive values, with its infinite prior mass in the range τ → ∞.\nWith J = 1 or 2 groups, this actually results in an improper posterior density, essentially\nconcluding τ = ∞ and doing no pooling. In a sense this is reasonable behavior, since it\nwould seem difficult from the data alone to decide how much, if any, pooling should be\ndone with data from only one or two groups. However, from a Bayesian perspective it is\nawkward for the decision to be made ahead of time, as it were, with the data having no say\nin the matter. In addition, for small J , such as 4 or 5, we worry that the heavy right tail of\nthe posterior distribution would lead to overestimates of τ and thus result in pooling that\nis less than optimal for estimating the individual θj ’s.\n\nWe can interpret these improper uniform prior densities as limits of weakly informative\nconditionally conjugate priors. The uniform prior distribution on log τ is equivalent to\np(τ) ∝ τ−1 or p(τ2) ∝ τ−2, which has the form of an inverse-χ2 density with 0 degrees of\nfreedom and can be taken as a limit of proper inverse-gamma priors.\n\nThe uniform density on τ is equivalent to p(τ2) ∝ τ−1, an inverse-χ2 density with −1\ndegrees of freedom. This density cannot easily be seen as a limit of proper inverse-χ2\n\ndensities (since these must have positive degrees of freedom), but it can be interpreted as a\nlimit of the half-t family on τ , where the scale approaches ∞ (and any value of ν).\n\nAnother noninformative prior distribution sometimes proposed in the Bayesian literature\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n130 5. HIERARCHICAL MODELS\n\nis uniform on τ2. We do not recommend this, as it seems to have the miscalibration toward\nhigher values as described above, but more so, and also requires J ≥ 4 groups for a proper\nposterior distribution.\n\nInverse-gamma(ǫ, ǫ) prior distributions. The parameter τ in model (5.21) does not have\nany simple family of conjugate prior distributions because its marginal likelihood depends\nin a complex way on the data from all J groups. However, the inverse-gamma family\nis conditionally conjugate given the other parameters in the model: that is, if τ2 has an\ninverse-gamma prior distribution, then the conditional posterior distribution p(τ2 | θ, µ, y)\nis also inverse-gamma. The inverse-gamma(α, β) model for τ2 can also be expressed as an\ninverse-χ2 distribution with scale s2 = β\n\nα and degrees of freedom ν = 2α. The inverse-\nχ2 parameterization can be helpful in understanding the information underlying various\nchoices of proper prior distributions.\n\nThe inverse-gamma(ǫ, ǫ) prior distribution is an attempt at noninformativeness within\nthe conditionally conjugate family, with ǫ set to a low value such as 1 or 0.01 or 0.001.\nA difficulty of this prior distribution is that in the limit of ǫ → 0 it yields an improper\nposterior density, and thus ǫ must be set to a reasonable value. Unfortunately, for datasets\nin which low values of τ are possible, inferences become very sensitive to ǫ in this model,\nand the prior distribution hardly looks noninformative, as we illustrate in Figure 5.9.\n\nHalf-Cauchy prior distributions. We shall also consider the t family of distributions (actu-\nally, the half-t, since the scale parameter τ is constrained to be positive) as an alternative\nclass that includes normal and Cauchy as edge cases. We first considered the t model for\nthis problem because it can be expressed as a conditionally conjugate prior distribution for\nτ using a reparameterization.\n\nFor our purposes here, however, it is enough to recognize that the half-Cauchy can be a\nconvenient weakly informative family; the distribution has a broad peak at zero and a single\nscale parameter, which we shall label A to indicate that it could be set to some large value.\nIn the limit A → ∞ this becomes a uniform prior density on τ . Large but finite values of\nA represent prior distributions which we consider weakly informative because, even in the\ntail, they have a gentle slope (unlike, for example, a half-normal distribution) and can let\nthe data dominate if the likelihood is strong in that region. We shall consider half-Cauchy\nmodels for variance parameters which are estimated from a small number of groups (so that\ninferences are sensitive to the choice of weakly informative prior distribution).\n\nApplication to the 8-schools example\n\nWe demonstrate the properties of some proposed noninformative prior densities on the\neight-schools example of Section 5.5. Here, the parameters θ1, . . . , θ8 represent the relative\neffects of coaching programs in eight different schools, and τ represents the between-school\nstandard deviations of these effects. The effects are measured as points on the test, which\nwas scored from 200 to 800 with an average of about 500; thus the largest possible range of\neffects could be about 300 points, with a realistic upper limit on τ of 100, say.\n\nNoninformative prior distributions for the 8-schools problem. Figure 5.9 displays the pos-\nterior distributions for the 8-schools model resulting from three different choices of prior\ndistributions that are intended to be noninformative.\n\nThe leftmost histogram shows posterior inference for τ for the model with uniform prior\ndensity. The data show support for a range of values below τ = 20, with a slight tail after\nthat, reflecting the possibility of larger values, which are difficult to rule out given that the\nnumber of groups J is only 8—that is, not much more than the J = 3 required to ensure a\nproper posterior density with finite mass in the right tail.\n\nIn contrast, the middle histogram in Figure 5.9 shows the result with an inverse-\ngamma(1, 1) prior distribution for τ2. This new prior distribution leads to changed in-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 131\n\nFigure 5.9 Histograms of posterior simulations of the between-school standard deviation, τ ,\nfrom models with three different prior distributions: (a) uniform prior distribution on τ , (b)\ninverse-gamma(1, 1) prior distribution on τ 2, (c) inverse-gamma(0.001, 0.001) prior distribution\non τ 2. Overlain on each is the corresponding prior density function for τ . (For models (b) and\n(c), the density for τ is calculated using the gamma density function multiplied by the Jacobian of\nthe 1/τ 2 transformation.) In models (b) and (c), posterior inferences are strongly constrained by\nthe prior distribution.\n\nferences. In particular, the posterior mean and median of τ are lower, and shrinkage of the\nθj ’s is greater than in the previously fitted model with a uniform prior distribution on τ . To\nunderstand this, it helps to graph the prior distribution in the range for which the posterior\ndistribution is substantial. The graph shows that the prior distribution is concentrated in\nthe range [0.5, 5], a narrow zone in which the likelihood is close to flat compared to this prior\n(as we can see because the distribution of the posterior simulations of τ closely matches the\nprior distribution, p(τ)). By comparison, in the left graph, the uniform prior distribution\non τ seems closer to ‘noninformative’ for this problem, in the sense that it does not appear\nto be constraining the posterior inference.\n\nFinally, the rightmost histogram in Figure 5.9 shows the corresponding result with an\ninverse-gamma(0.001, 0.001) prior distribution for τ2. This prior distribution is even more\nsharply peaked near zero and further distorts posterior inferences, with the problem arising\nbecause the marginal likelihood for τ remains high near zero.\n\nIn this example, we do not consider a uniform prior density on log τ , which would yield\nan improper posterior density with a spike at τ = 0, like the rightmost graph in Figure 5.9\nbut more so. We also do not consider a uniform prior density on τ2, which would yield a\nposterior similar to the leftmost graph in Figure 5.9, but with a slightly higher right tail.\n\nThis example is a gratifying case in which the simplest approach—the uniform prior\ndensity on τ—seems to perform well. As detailed in Appendix C, this model is also straight-\nforward to program directly in R or Stan.\n\nThe appearance of the histograms and density plots in Figure 5.9 is crucially affected by\nthe choice to plot them on the scale of τ . If instead they were plotted on the scale of log τ ,\nthe inverse-gamma(0.001, 0.001) prior density would appear to be the flattest. However, the\ninverse-gamma(ǫ, ǫ) prior is not at all ‘noninformative’ for this problem since the resulting\nposterior distribution remains highly sensitive to the choice of ǫ. The hierarchical model\nlikelihood does not constrain log τ in the limit log τ → −∞, and so a prior distribution that\nis noninformative on the log scale will not work.\n\nWeakly informative prior distribution for the 3-schools problem\n\nThe uniform prior distribution seems fine for the 8-school analysis, but problems arise if the\nnumber of groups J is much smaller, in which case the data supply little information about\nthe group-level variance, and a noninformative prior distribution can lead to a posterior\ndistribution that is improper or is proper but unrealistically broad. We demonstrate by\nreanalyzing the 8-schools example using just the data from the first three of the schools.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n132 5. HIERARCHICAL MODELS\n\nFigure 5.10 Histograms of posterior simulations of the between-school standard deviation, τ , from\nmodels for the 3-schools data with two different prior distributions on τ : (a) uniform (0,∞), (b)\nhalf-Cauchy with scale 25, set as a weakly informative prior distribution given that τ was expected\nto be well below 100. The histograms are not on the same scales. Overlain on each histogram is\nthe corresponding prior density function. With only J = 3 groups, the noninformative uniform\nprior distribution is too weak, and the proper Cauchy distribution works better, without appearing\nto distort inferences in the area of high likelihood.\n\nFigure 5.10 displays the inferences for τ based on two different priors. First we continue\nwith the default uniform distribution that worked well with J = 8 (as seen in Figure 5.9).\nUnfortunately, as the left histogram of Figure 5.10 shows, the resulting posterior distribution\nfor the 3-schools dataset has an extremely long right tail, containing values of τ that are\ntoo high to be reasonable. This heavy tail is expected since J is so low (if J were any lower,\nthe right tail would have an infinite integral), and using this as a posterior distribution will\nhave the effect of underpooling the estimates of the school effects θj .\n\nThe right histogram of Figure 5.10 shows the posterior inference for τ resulting from\na half-Cauchy prior distribution with scale parameter A = 25 (a value chosen to be a bit\nhigher than we expect for the standard deviation of the underlying θj ’s in the context of\nthis educational testing example, so that the model will constrain τ only weakly). As the\nline on the graph shows, this prior distribution is high over the plausible range of τ < 50,\nfalling off gradually beyond this point. This prior distribution appears to perform well in\nthis example, reflecting the marginal likelihood for τ at its low end but removing much of\nthe unrealistic upper tail.\n\nThis half-Cauchy prior distribution would also perform well in the 8-schools problem;\nhowever it was unnecessary because the default uniform prior gave reasonable results. With\nonly 3 schools, we went to the trouble of using a weakly informative prior, a distribution\nthat was not intended to represent our actual prior state of knowledge about τ but rather\nto constrain the posterior distribution, to an extent allowed by the data.\n\n5.8 Bibliographic note\n\nThe early non-Bayesian work on shrinkage estimation of Stein (1955) and James and Stein\n(1960) was influential in the development of hierarchical normal models. Efron and Morris\n(1971, 1972) present subsequent theoretical work on the topic. Robbins (1955, 1964) con-\nstructs and justifies hierarchical methods from a decision-theoretic perspective. De Finetti’s\ntheorem is described by de Finetti (1974); Bernardo and Smith (1994) discuss its role in\nBayesian modeling. An early thorough development of the idea of Bayesian hierarchical\nmodeling is given by Good (1965).\n\nMosteller and Wallace (1964) analyzed a hierarchical Bayesian model using the negative\nbinomial distribution for counts of words in a study of authorship. Restricted to the limited\ncomputing power at the time, they used various approximations and point estimates for\nhyperparameters.\n\nOther historically influential papers on ‘empirical Bayes’ (or, in our terminology, hierar-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.8. BIBLIOGRAPHIC NOTE 133\n\nchical Bayes) include Hartley and Rao (1967), Laird and Ware (1982) on longitudinal mod-\neling, and Clayton and Kaldor (1987) and Breslow (1990) on epidemiology and biostatistics.\nMorris (1983) and Deely and Lindley (1981) explored the relation between Bayesian and\nnon-Bayesian ideas for these models.\n\nThe problem of estimating several normal means using an exchangeable hierarchical\nmodel was treated in a fully Bayesian framework by Hill (1965), Tiao and Tan (1965,\n1966), and Lindley (1971b). Box and Tiao (1973) present hierarchical normal models using\nslightly different notation from ours. They compare Bayesian and non-Bayesian methods\nand discuss the analysis of variance table in some detail. More references on hierarchical\nnormal models appear in the bibliographic note at the end of Chapter 15.\n\nThe past few decades have seen the publication of applied Bayesian analyses using hierar-\nchical models in a wide variety of application areas. For example, an important application\nof hierarchical models is ‘small-area estimation,’ in which estimates of population charac-\nteristics for local areas are improved by combining the data from each area with information\nfrom neighboring areas (with important early work from Fay and Herriot, 1979, Dempster\nand Raghunathan, 1987, and Mollie and Richardson, 1991). Other applications that have\nmotivated methodological development include measurement error problems in epidemiol-\nogy (for example, Richardson and Gilks, 1993), multiple comparisons in toxicology (Meng\nand Dempster, 1987), and education research (Bock, 1989). We provide references to a\nnumber of other applications in later chapters dealing with specific model types.\n\nHierarchical models can be viewed as a subclass of ‘graphical models,’ and this connec-\ntion has been elegantly exploited for Bayesian inference in the development of the computer\npackage Bugs, using techniques that will be explained in Chapter 11 (see also Appendix C);\nsee Thomas, Spiegelhalter, and Gilks (1992), and Spiegelhalter et al. (1994, 2003). Related\ndiscussion and theoretical work appears in Lauritzen and Spiegelhalter (1988), Pearl (1988),\nWermuth and Lauritzen (1990), and Normand and Tritchler (1992).\n\nThe rat tumor data were analyzed hierarchically by Tarone (1982) and Dempster, Sel-\nwyn, and Weeks (1983); our approach is close in spirit to the latter paper’s. Leonard (1972)\nand Novick, Lewis, and Jackson (1973) are early examples of hierarchical Bayesian analysis\nof binomial data.\n\nMuch of the material in Sections 5.4 and 5.5, along with much of Section 6.5, originally\nappeared in Rubin (1981a), which is an early example of an applied Bayesian analysis using\nsimulation techniques. For later work on the effects of coaching on Scholastic Aptitude Test\nscores, see Hansen (2004).\n\nThe weakly-informative half-Cauchy prior distribution for the 3-schools problem in Sec-\ntion 5.7 comes from Gelman (2006a). Polson and Scott (2012) provide a theoretical justifi-\ncation for this model.\n\nThe material of Section 5.6 is adapted from Carlin (1992), which contains several key\nreferences on meta-analysis; the original data for the example are from Yusuf et al. (1985);\na similar Bayesian analysis of these data under a slightly different model appears as an\nexample in Spiegelhalter et al. (1994, 2003). Thall et al. (2003) discuss hierarchical models\nfor medical treatments that vary across subtypes of a disease. More general treatments\nof meta-analysis from a Bayesian perspective are provided by DuMouchel (1990), Rubin\n(1989), Skene and Wakefield (1990), and Smith, Spiegelhalter, and Thomas (1995). An ex-\nample of a Bayesian meta-analysis appears in Dominici et al. (1999). DuMouchel and Harris\n(1983) present what is essentially a meta-analysis with covariates on the studies; this article\nis accompanied by some interesting discussion by prominent Bayesian and non-Bayesian\nstatisticians. Higgins and Whitehead (1996) discuss how to construct a prior distribution\nfor the group-level variance in a meta-analysis by considering it as an example from larger\npopulation of meta-analyses. Lau, Ioannidis, and Schmid (1997) provide practical advice\non meta-analysis.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n134 5. HIERARCHICAL MODELS\n\n5.9 Exercises\n\n1. Exchangeability with known model parameters: For each of the following three examples,\nanswer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and y2\nindependent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has one black ball and one white ball. We pick a ball y1 at random, put it back,\nand pick another ball y2 at random.\n\n(b) A box has one black ball and one white ball. We pick a ball y1 at random, we do not\nput it back, then we pick ball y2.\n\n(c) A box has a million black balls and a million white balls. We pick a ball y1 at random,\nwe do not put it back, then we pick ball y2 at random.\n\n2. Exchangeability with unknown model parameters: For each of the following three exam-\nples, answer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and\ny2 independent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, put it back, and pick another ball y2 at random.\n\n(b) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, we do not put it back, then we pick ball y2 at random.\n\n(c) Same as (b) but we know that there are many balls of each color in the box.\n\n3. Hierarchical models and multiple comparisons:\n\n(a) Reproduce the computations in Section 5.5 for the educational testing example. Use\nthe posterior simulations to estimate (i) for each school j, the probability that its\ncoaching program is the best of the eight; and (ii) for each pair of schools, j and k,\nthe probability that the coaching program in school j is better than that in school k.\n\n(b) Repeat (a), but for the simpler model with τ set to ∞ (that is, separate estimation\nfor the eight schools). In this case, the probabilities (ii) can be computed analytically.\n\n(c) Discuss how the answers in (a) and (b) differ.\n\n(d) In the model with τ set to 0, the probabilities (i) and (ii) have degenerate values; what\nare they?\n\n4. Exchangeable prior distributions: suppose it is known a priori that the 2J parameters\nθ1, . . . , θ2J are clustered into two groups, with exactly half being drawn from a N(1, 1)\ndistribution, and the other half being drawn from a N(−1, 1) distribution, but we have\nnot observed which parameters come from which distribution.\n\n(a) Are θ1, . . . , θ2J exchangeable under this prior distribution?\n\n(b) Show that this distribution cannot be written as a mixture of independent and iden-\ntically distributed components.\n\n(c) Why can we not simply take the limit as J → ∞ and get a counterexample to de\nFinetti’s theorem?\n\nSee Exercise 8.10 for a related problem.\n\n5. Mixtures of independent distributions: suppose the distribution of θ = (θ1, . . . , θJ) can\nbe written as a mixture of independent and identically distributed components:\n\np(θ) =\n\n∫ J∏\n\nj=1\n\np(θj |φ)p(φ)dφ.\n\nProve that the covariances cov(θi, θj) are all nonnegative.\n\n6. Exchangeable models:\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 135\n\n(a) In the divorce rate example of Section 5.2, set up a prior distribution for the values\ny1, . . . , y8 that allows for one low value (Utah) and one high value (Nevada), with\nindependent and identical distributions for the other six values. This prior distribution\nshould be exchangeable, because it is not known which of the eight states correspond\nto Utah and Nevada.\n\n(b) Determine the posterior distribution for y8 under this model given the observed values\nof y1, . . . , y7 given in the example. This posterior distribution should probably have\ntwo or three modes, corresponding to the possibilities that the missing state is Utah,\nNevada, or one of the other six.\n\n(c) Now consider the entire set of eight data points, including the value for y8 given at\nthe end of the example. Are these data consistent with the prior distribution you gave\nin part (a) above? In particular, did your prior distribution allow for the possibility\nthat the actual data have an outlier (Nevada) at the high end, but no outlier at the\nlow end?\n\n7. Continuous mixture models:\n\n(a) If y|θ ∼ Poisson(θ), and θ ∼ Gamma(α, β), then the marginal (prior predictive)\ndistribution of y is negative binomial with parameters α and β (or p = β/(1 + β)).\nUse the formulas (2.7) and (2.8) to derive the mean and variance of the negative\nbinomial.\n\n(b) In the normal model with unknown location and scale (µ, σ2), the noninformative\nprior density, p(µ, σ2) ∝ 1/σ2, results in a normal-inverse-χ2 posterior distribution for\n(µ, σ2). Marginally then\n\n√\nn(µ − y)/s has a posterior distribution that is tn−1. Use\n\n(2.7) and (2.8) to derive the first two moments of the latter distribution, stating the\nappropriate condition on n for existence of both moments.\n\n8. Discrete mixture models: if pm(θ), for m = 1, . . . ,M , are conjugate prior densities for\nthe sampling model y|θ, show that the class of finite mixture prior densities given by\n\np(θ) =\n\nM∑\n\nm=1\n\nλmpm(θ)\n\nis also a conjugate class, where the λm’s are nonnegative weights that sum to 1. This\ncan provide a useful extension of the natural conjugate prior family to more flexible\ndistributional forms. As an example, use the mixture form to create a bimodal prior\ndensity for a normal mean, that is thought to be near 1, with a standard deviation of\n0.5, but has a small probability of being near −1, with the same standard deviation. If\nthe variance of each observation y1, . . . , y10 is known to be 1, and their observed mean\nis y = −0.25, derive your posterior distribution for the mean, making a sketch of both\nprior and posterior densities. Be careful: the prior and posterior mixture proportions are\ndifferent.\n\n9. Noninformative hyperprior distributions: consider the hierarchical binomial model in\nSection 5.3. Improper posterior distributions are, in fact, a general problem with hier-\narchical models when a uniform prior distribution is specified for the logarithm of the\npopulation standard deviation of the exchangeable parameters. In the case of the beta\npopulation distribution, the prior variance is approximately (α+β)−1 (see Appendix A),\nand so a uniform distribution on log(α+β) is approximately uniform on the log standard\ndeviation. The resulting unnormalized posterior density (5.8) has an infinite integral\nin the limit as the population standard deviation approaches 0. We encountered the\nproblem again in Section 5.4 for the hierarchical normal model.\n\n(a) Show that, with a uniform prior density on (log(αβ ), log(α+β)), the unnormalized\nposterior density has an infinite integral.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n136 5. HIERARCHICAL MODELS\n\n(b) A simple way to avoid the impropriety is to assign a uniform prior distribution to the\nstandard deviation parameter itself, rather than its logarithm. For the beta population\ndistribution we are considering here, this is achieved approximately by assigning a uni-\nform prior distribution to (α+β)−1/2. Show that combining this with an independent\nuniform prior distribution on α\n\nα+β yields the prior density (5.10).\n\n(c) Show that the resulting posterior density (5.8) is proper as long as 0 < yj < nj for at\nleast one experiment j.\n\n10. Checking the integrability of the posterior distribution: consider the hierarchical normal\nmodel in Section 5.4.\n\n(a) If the hyperprior distribution is p(µ, τ) ∝ τ−1 (that is, p(µ, log τ) ∝ 1), show that the\nposterior density is improper.\n\n(b) If the hyperprior distribution is p(µ, τ) ∝ 1, show that the posterior density is proper\nif J > 2.\n\n(c) How would you analyze SAT coaching data if J = 2 (that is, data from only two\nschools)?\n\n11. Nonconjugate hierarchical models: suppose that in the rat tumor example, we wish to\nuse a normal population distribution on the log-odds scale: logit(θj) ∼ N(µ, τ2), for\nj = 1, . . . , J . As in Section 5.3, you will assign a noninformative prior distribution to the\nhyperparameters and perform a full Bayesian analysis.\n\n(a) Write the joint posterior density, p(θ, µ, τ |y).\n(b) Show that the integral (5.4) has no closed-form expression.\n\n(c) Why is expression (5.5) no help for this problem?\n\nIn practice, we can solve this problem by normal approximation, importance sampling,\nand Markov chain simulation, as described in Part III.\n\n12. Conditional posterior means and variances: derive analytic expressions for E(θj |τ, y) and\nvar(θj |τ, y) in the hierarchical normal model (and used in Figures 5.6 and 5.7). (Hint:\nuse (2.7) and (2.8), averaging over µ.)\n\n13. Hierarchical binomial model: Exercise 3.8 described a survey of bicycle traffic in Berkeley,\nCalifornia, with data displayed in Table 3.3. For this problem, restrict your attention to\nthe first two rows of the table: residential streets labeled as ‘bike routes,’ which we will\nuse to illustrate this computational exercise.\n\n(a) Set up a model for the data in Table 3.3 so that, for j = 1, . . . , 10, the observed number\nof bicycles at location j is binomial with unknown probability θj and sample size equal\nto the total number of vehicles (bicycles included) in that block. The parameter θj\ncan be interpreted as the underlying or ‘true’ proportion of traffic at location j that is\nbicycles. (See Exercise 3.8.) Assign a beta population distribution for the parameters\nθj and a noninformative hyperprior distribution as in the rat tumor example of Section\n5.3. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and draw simulations\nfrom the joint posterior distribution of the parameters and hyperparameters, as in\nSection 5.3.\n\n(c) Compare the posterior distributions of the parameters θj to the raw proportions,\n(number of bicycles / total number of vehicles) in location j. How do the inferences\nfrom the posterior distribution differ from the raw proportions?\n\n(d) Give a 95% posterior interval for the average underlying proportion of traffic that is\nbicycles.\n\n(e) A new city block is sampled at random and is a residential street with a bike route. In\nan hour of observation, 100 vehicles of all kinds go by. Give a 95% posterior interval\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 137\n\nfor the number of those vehicles that are bicycles. Discuss how much you trust this\ninterval in application.\n\n(f) Was the beta distribution for the θj ’s reasonable?\n\n14. Hierarchical Poisson model: consider the dataset in the previous problem, but suppose\nonly the total amount of traffic at each location is observed.\n\n(a) Set up a model in which the total number of vehicles observed at each location j\nfollows a Poisson distribution with parameter θj , the ‘true’ rate of traffic per hour at\nthat location. Assign a gamma population distribution for the parameters θj and a\nnoninformative hyperprior distribution. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and plot its contours.\nSimulate random draws from the posterior distribution of the hyperparameters and\nmake a scatterplot of the simulation draws.\n\n(c) Is the posterior density integrable? Answer analytically by examining the joint pos-\nterior density at the limits or empirically by examining the plots of the marginal\nposterior density above.\n\n(d) If the posterior density is not integrable, alter it and repeat the previous two steps.\n\n(e) Draw samples from the joint posterior distribution of the parameters and hyperpa-\nrameters, by analogy to the method used in the hierarchical binomial model.\n\n15. Meta-analysis: perform the computations for the meta-analysis data of Table 5.4.\n\n(a) Plot the posterior density of τ over an appropriate range that includes essentially all\nof the posterior density, analogous to Figure 5.5.\n\n(b) Produce graphs analogous to Figures 5.6 and 5.7 to display how the posterior means\nand standard deviations of the θj ’s depend on τ .\n\n(c) Produce a scatterplot of the crude effect estimates vs. the posterior median effect\nestimates of the 22 studies. Verify that the studies with smallest sample sizes are\npartially pooled the most toward the mean.\n\n(d) Draw simulations from the posterior distribution of a new treatment effect, θ̃j . Plot\na histogram of the simulations.\n\n(e) Given the simulations just obtained, draw simulated outcomes from replications of\na hypothetical new experiment with 100 persons in each of the treated and control\ngroups. Plot a histogram of the simulations of the crude estimated treatment effect\n(5.23) in the new experiment.\n\n16. Equivalent data: Suppose we wish to apply the inferences from the meta-analysis example\nin Section 5.6 to data on a new study with equal numbers of people in the control and\ntreatment groups. How large would the study have to be so that the prior and data were\nweighted equally in the posterior inference for that study?\n\n17. Informative prior distributions: Continuing the example from Exercise 2.22, consider a\n(hypothetical) study of a simple training program for basketball free-throw shooting. A\nrandom sample of 100 college students is recruited into the study. Each student first\nshoots 100 free-throws to establish a baseline success probability. Each student then\ntakes 50 practice shots each day for a month. At the end of that time, he or she takes\n100 shots for a final measurement.\nLet θi be the improvement in success probability for person i. For simplicity, assume the\nθi’s are normally distributed with mean µ and standard deviation σ.\nGive three joint prior distributions for µ, σ:\n\n(a) A noninformative prior distribution,\n\n(b) A subjective prior distribution based on your best knowledge, and\n\n(c) A weakly informative prior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart II: Fundamentals of Bayesian Data\n\nAnalysis\n\nFor most problems of applied Bayesian statistics, the data analyst must go beyond the\nsimple structure of prior distribution, likelihood, and posterior distribution. In Chapter 6,\nwe discuss methods of assessing the sensitivity of posterior inferences to model assumptions\nand checking the fit of a probability model to data and substantive information. Model\nchecking allows an escape from the tautological aspect of formal approaches to Bayesian\ninference, under which all conclusions are conditional on the truth of the posited model.\nChapter 7 considers evaluating and comparing models using predictive accuracy, adjusting\nfor the parameters being fit to the data. Chapter 8 outlines the role of study design and\nmethods of data collection in probability modeling, focusing on how to set up Bayesian\ninference for sample surveys, designed experiments, and observational studies; this chapter\ncontains some of the most conceptually distinctive and potentially difficult material in\nthe book. Chapter 9 discusses the use of Bayesian inference in applied decision analysis,\nillustrating with examples from social science, medicine, and public health. These four\nchapters explore the creative choices that are required, first to set up a Bayesian model in\na complex problem, then to perform the model checking and confidence building that is\ntypically necessary to make posterior inferences scientifically defensible, and finally to use\nthe inferences in decision making.\n\n139\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 6\n\nModel checking\n\n6.1 The place of model checking in applied Bayesian statistics\n\nOnce we have accomplished the first two steps of a Bayesian analysis—constructing a prob-\nability model and computing the posterior distribution of all estimands—we should not\nignore the relatively easy step of assessing the fit of the model to the data and to our\nsubstantive knowledge. It is difficult to include in a probability distribution all of one’s\nknowledge about a problem, and so it is wise to investigate what aspects of reality are not\ncaptured by the model.\n\nChecking the model is crucial to statistical analysis. Bayesian prior-to-posterior infer-\nences assume the whole structure of a probability model and can yield misleading inferences\nwhen the model is poor. A good Bayesian analysis, therefore, should include at least some\ncheck of the adequacy of the fit of the model to the data and the plausibility of the model\nfor the purposes for which the model will be used. This is sometimes discussed as a problem\nof sensitivity to the prior distribution, but in practice the likelihood model is typically just\nas suspect; throughout, we use ‘model’ to encompass the sampling distribution, the prior\ndistribution, any hierarchical structure, and issues such as which explanatory variables have\nbeen included in a regression.\n\nSensitivity analysis and model improvement\n\nIt is typically the case that more than one reasonable probability model can provide an\nadequate fit to the data in a scientific problem. The basic question of a sensitivity analysis\nis: how much do posterior inferences change when other reasonable probability models\nare used in place of the present model? Other reasonable models may differ substantially\nfrom the present model in the prior specification, the sampling distribution, or in what\ninformation is included (for example, predictor variables in a regression). It is possible that\nthe present model provides an adequate fit to the data, but that posterior inferences differ\nunder plausible alternative models.\n\nIn theory, both model checking and sensitivity analysis can be incorporated into the\nusual prior-to-posterior analysis. Under this perspective, model checking is done by set-\nting up a comprehensive joint distribution, such that any data that might be observed are\nplausible outcomes under the joint distribution. That is, this joint distribution is a mixture\nof all possible ‘true’ models or realities, incorporating all known substantive information.\nThe prior distribution in such a case incorporates prior beliefs about the likelihood of the\ncompeting realities and about the parameters of the constituent models. The posterior dis-\ntribution of such an exhaustive probability model automatically incorporates all ‘sensitivity\nanalysis’ but is still predicated on the truth of some member of the larger class of models.\n\nIn practice, however, setting up such a super-model to include all possibilities and all\nsubstantive knowledge is both conceptually impossible and computationally infeasible in all\nbut the simplest problems. It is thus necessary for us to examine our models in other ways\n\n141\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n142 6. MODEL CHECKING\n\nto see how they fail to fit reality and how sensitive the resulting posterior distributions are\nto arbitrary specifications.\n\nJudging model flaws by their practical implications\n\nWe do not like to ask, ‘Is our model true or false?’, since probability models in most\ndata analyses will not be perfectly true. Even the coin tosses and die rolls ubiquitous in\nprobability theory texts are not truly exchangeable. The more relevant question is, ‘Do the\nmodel’s deficiencies have a noticeable effect on the substantive inferences?’\n\nIn the examples of Chapter 5, the beta population distribution for the tumor rates and\nthe normal distribution for the eight school effects are both chosen partly for convenience.\nIn these examples, making convenient distributional assumptions turns out not to matter,\nin terms of the impact on the inferences of most interest. How to judge when assumptions\nof convenience can be made safely is a central task of Bayesian sensitivity analysis. Failures\nin the model lead to practical problems by creating clearly false inferences about estimands\nof interest.\n\n6.2 Do the inferences from the model make sense?\n\nIn any applied problem, there will be knowledge that is not included formally in either\nthe prior distribution or the likelihood, for reasons of convenience or objectivity. If the\nadditional information suggests that posterior inferences of interest are false, then this\nsuggests a potential for creating a more accurate probability model for the parameters and\ndata collection process. We illustrate with an example of a hierarchical regression model.\n\nExample. Evaluating election predictions by comparing to substantive po-\nlitical knowledge\nFigure 6.1 displays a forecast, made in early October, 1992, of the probability that Bill\nClinton would win each state in the U.S. presidential election that November. The\nestimates are posterior probabilities based on a hierarchical linear regression model.\nFor each state, the height of the shaded part of the box represents the estimated\nprobability that Clinton would win the state. Even before the election occurred, the\nforecasts for some of the states looked wrong; for example, from state polls, Clinton\nwas known in October to be much weaker in Texas and Florida than shown in the\nmap. This does not mean that the forecast is useless, but it is good to know where\nthe weak points are. Certainly, after the election, we can do an even better job of\ncriticizing the model and understanding its weaknesses. We return to this election\nforecasting example in Section 15.2 as an example of a hierarchical linear model.\n\nExternal validation\n\nMore formally, we can check a model by external validation using the model to make predic-\ntions about future data, and then collecting those data and comparing to their predictions.\nPosterior means should be correct on average, 50% intervals should contain the true values\nhalf the time, and so forth. We used external validation to check the empirical probability\nestimates in the record-linkage example in Section 1.7, and we apply the idea again to check\na toxicology model in Section 19.2. In the latter example, the external validation (see Figure\n19.10 on page 484) reveals a generally reasonable fit but with some notable discrepancies\nbetween predictions and external data. Often we need to check the model before obtaining\nnew data or waiting for the future to happen. In this chapter and the next, we discuss\nmethods which can approximate external validation using the data we already have.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 143\n\nFigure 6.1 Summary of a forecast of the 1992 U.S. presidential election performed one month\nbefore the election. For each state, the proportion of the box that is shaded represents the estimated\nprobability of Clinton winning the state; the width of the box is proportional to the number of\nelectoral votes for the state.\n\nChoices in defining the predictive quantities\n\nA single model can be used to make different predictions. For example, in the SAT example\nwe could consider a joint prediction for future data from the 8 schools in the study, p(ỹ|y),\na joint prediction for 8 new schools p(ỹi|y), i = 9, . . . , 16, or any other combination of new\nand existing schools. Other scenarios may have even more different choices in defining the\nfocus of predictions. For example, in analyses of sample surveys and designed experiments,\nit often makes sense to consider hypothetical replications of the experiment with a new\nrandomization of selection or treatment assignment, by analogy to classical randomization\ntests.\n\nSections 6.3 and 6.4 discuss posterior predictive checking, which use global summaries\nto check the joint posterior predictive distribution p(ỹ|y). At the end of Section 6.3 we\nbriefly discuss methods that combine inferences for local quantities to check marginal pre-\ndictive distributions p(ỹi|y), an idea that is related to cross-validation methods considered\nin Chapter 7.\n\n6.3 Posterior predictive checking\n\nIf the model fits, then replicated data generated under the model should look similar to\nobserved data. To put it another way, the observed data should look plausible under\nthe posterior predictive distribution. This is really a self-consistency check: an observed\ndiscrepancy can be due to model misfit or chance.\n\nOur basic technique for checking the fit of a model to data is to draw simulated values\nfrom the joint posterior predictive distribution of replicated data and compare these samples\nto the observed data. Any systematic differences between the simulations and the data\nindicate potential failings of the model.\n\nWe introduce posterior predictive checking with a simple example of an obviously poorly\nfitting model, and then in the rest of this section we lay out the key choices involved in pos-\nterior predictive checking. Sections 6.3 and 6.4 discuss numerical and graphical predictive\nchecks in more detail.\n\nExample. Comparing Newcomb’s speed of light measurements to the pos-\nterior predictive distribution\nSimon Newcomb’s 66 measurements on the speed of light are presented in Section 3.2.\nIn the absence of other information, in Section 3.2 we modeled the measurements as\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n144 6. MODEL CHECKING\n\nFigure 6.2 Twenty replications, yrep, of the speed of light data from the posterior predictive distri-\nbution, p(yrep|y); compare to observed data, y, in Figure 3.1. Each histogram displays the result\nof drawing 66 independent values ỹi from a common normal distribution with mean and variance\n(µ, σ2) drawn from the posterior distribution, p(µ, σ2|y), under the normal model.\n\nFigure 6.3 Smallest observation of Newcomb’s speed of light data (the vertical line at the left of the\ngraph), compared to the smallest observations from each of the 20 posterior predictive simulated\ndatasets displayed in Figure 6.2.\n\nN(µ, σ2), with a noninformative uniform prior distribution on (µ, log σ). However, the\nlowest of Newcomb’s measurements look like outliers compared to the rest of the data.\nCould the extreme measurements have reasonably come from a normal distribution?\nWe address this question by comparing the observed data to what we expect to be\nobserved under our posterior distribution. Figure 6.2 displays twenty histograms,\neach of which represents a single draw from the posterior predictive distribution of\nthe values in Newcomb’s experiment, obtained by first drawing (µ, σ2) from their\njoint posterior distribution, then drawing 66 values from a normal distribution with\nthis mean and variance. All these histograms look different from the histogram of\nactual data in Figure 3.1 on page 67. One way to measure the discrepancy is to\ncompare the smallest value in each hypothetical replicated dataset to Newcomb’s\nsmallest observation, −44. The histogram in Figure 6.3 shows the smallest observation\nin each of the 20 hypothetical replications; all are much larger than Newcomb’s smallest\nobservation, which is indicated by a vertical line on the graph. The normal model\nclearly does not capture the variation that Newcomb observed. A revised model\nmight use an asymmetric contaminated normal distribution or a symmetric long-tailed\ndistribution in place of the normal measurement model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 145\n\nMany other examples of posterior predictive checks appear throughout the book, includ-\ning the educational testing example in Section 6.5, linear regressions examples in Sections\n14.3 and 15.2, and a hierarchical mixture model in Section 22.2.\n\nFor many problems, it is useful to examine graphical comparisons of summaries of the\ndata to summaries from posterior predictive simulations, as in Figure 6.3. In cases with\nless blatant discrepancies than the outliers in the speed of light data, it is often also useful\nto measure the ‘statistical significance’ of the lack of fit, a notion we formalize here.\n\nNotation for replications\n\nLet y be the observed data and θ be the vector of parameters (including all the hyperpa-\nrameters if the model is hierarchical). To avoid confusion with the observed data, y, we\ndefine yrep as the replicated data that could have been observed, or, to think predictively, as\nthe data we would see tomorrow if the experiment that produced y today were replicated\nwith the same model and the same value of θ that produced the observed data.\n\nWe distinguish between yrep and ỹ, our general notation for predictive outcomes: ỹ is\nany future observable value or vector of observable quantities, whereas yrep is specifically a\nreplication just like y. For example, if the model has explanatory variables, x, they will be\nidentical for y and yrep, but ỹ may have its own explanatory variables, x̃.\n\nWe will work with the distribution of yrep given the current state of knowledge, that is,\nwith the posterior predictive distribution\n\np(yrep|y) =\n∫\np(yrep|θ)p(θ|y)dθ. (6.1)\n\nTest quantities\n\nWe measure the discrepancy between model and data by defining test quantities, the aspects\nof the data we wish to check. A test quantity, or discrepancy measure, T (y, θ), is a scalar\nsummary of parameters and data that is used as a standard when comparing data to\npredictive simulations. Test quantities play the role in Bayesian model checking that test\nstatistics play in classical testing. We use the notation T (y) for a test statistic, which is a\ntest quantity that depends only on data; in the Bayesian context, we can generalize test\nstatistics to allow dependence on the model parameters under their posterior distribution.\nThis can be useful in directly summarizing discrepancies between model and data. We\ndiscuss options for graphical test quantities in Section 6.4. The test quantities in this\nsection are usually functions of y or replicated data yrep. In the end of this section we\nbriefly discuss a different sort of test quantities used for calibration that are functions of\nboth yi and y\n\nrep\ni (or ỹi). In Chapter 7 we discuss measures of discrepancy between model\n\nand data, that is, measures of predictive accuracy that are also functions of both yi and\nyrepi (or ỹi).\n\nTail-area probabilities\n\nLack of fit of the data with respect to the posterior predictive distribution can be measured\nby the tail-area probability, or p-value, of the test quantity, and computed using posterior\nsimulations of (θ, yrep). We define the p-value mathematically, first for the familiar classical\ntest and then in the Bayesian context.\n\nClassical p-values. The classical p-value for the test statistic T (y) is\n\npC = Pr(T (yrep)≥T (y)|θ), (6.2)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n146 6. MODEL CHECKING\n\nwhere the probability is taken over the distribution of yrep with θ fixed. (The distribution of\nyrep given y and θ is the same as its distribution given θ alone","extracted_metadata":{"xmp:CreatorTool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"access_permission:modify_annotations":["true"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"dcterms:created":["2020-04-27T08:01:52Z"],"resourceName":["Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf"],"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"access_permission:can_print_degraded":["true"],"pdf:charsPerPage":["396","53","1436","1728","1697","1636","1492","528","3071","3140","1234","53","2742","3330","3069","2988","2256","2803","2648","2740","3465","3348","2958","2470","2340","3049","1963","2161","2318","3056","2205","2627","2963","3087","3225","3644","2929","2677","2494","1927","2281","2873","3093","3116","2331","2646","3530","2407","1793","1903","1918","2860","1541","1822","3183","1972","1401","2495","2382","1813","2158","2677","2369","2617","3359","3595","2953","2521","2429","2675","3013","177","2211","2155","2102","2430","2259","1865","2320","2463","1936","2317","2074","2392","2285","1981","2439","3093","2762","3198","2770","2935","2362","1684","3302","2463","2576","3133","3078","3559","2965","3399","3413","3082","3222","3652","3361","3382","2915","3107","3106","2884","2549","3091","3033","3268","2839","3066","2445","2758","2420","2181","1850","2903","2149","2374","2207","2834","3400","2905","2465","1749","2319","3019","2985","3110","3392","3275","3498","3496","2971","2867","3626","2513","2989","2931","2971","53","1343","53","2696","3022","2284","1753","2796"],"access_permission:assemble_document":["true"],"xmpTPg:NPages":["677"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"xmpMM:DocumentID":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"pdf:docinfo:title":["book.dvi"],"pdf:producer":["GPL Ghostscript 9.26"],"xmp:ModifyDate":["2020-04-27T08:01:52Z"],"pdf:encrypted":["false"],"dc:title":["book.dvi"],"Content-Type":["application/pdf"],"xmp:About":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"Content-Length":["35493143"],"pdf:PDFVersion":["1.4"],"pdf:hasXMP":["true"],"access_permission:extract_content":["true"],"pdf:unmappedUnicodeCharsPerPage":["1","0","0","0","0","0","0","0","0","0","0","0","0","0","0","1","7","0","1","0","0","0","0","0","0","2","0","0","0","4","13","2","3","0","0","0","0","0","2","2","9","2","0","0","8","7","0","0","0","8","18","3","15","9","4","0","0","0","2","5","4","24","32","8","0","0","0","1","0","1","0","0","1","12","11","14","2","6","9","0","12","6","3","0","1","0","2","0","0","0","0","0","2","19","0","0","0","7","8","0","0","0","0","0","1","0","0","0","2","1","0","0","7","0","5","0","2","0","2","4","2","1","1","8","7","1","8","0","0","2","0","0","0","0","4","0","0","0","0","0","0","0","0","2","1","0","0","0","0","0","0","0","0","0","1"],"pdf:hasXFA":["false"],"access_permission:can_print":["true"],"pdf:docinfo:created":["2020-04-27T08:01:52Z"],"pdf:docinfo:modified":["2020-04-27T08:01:52Z"],"pdf:docinfo:creator_tool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"pdf:hasMarkedContent":["false"],"access_permission:extract_for_accessibility":["true"],"dcterms:modified":["2020-04-27T08:01:52Z"],"access_permission:fill_in_form":["true"],"xmp:CreateDate":["2020-04-27T08:01:52Z"],"access_permission:can_modify":["true"],"dc:format":["application/pdf; version=1.4"],"pdf:docinfo:producer":["GPL Ghostscript 9.26"],"pdf:hasCollection":["false"]},"metadata_field_count":38,"attempts":1,"timestamp":1754064983.1587095,"platform":"Linux","python_version":"3.13.5"},{"file_path":"test_documents/pdfs/Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf","file_size":20812100,"file_type":"pdf","category":"large","framework":"extractous","iteration":1,"extraction_time":132.84068989753723,"startup_time":null,"peak_memory_mb":522.64453125,"avg_memory_mb":512.5171875,"peak_cpu_percent":29.9,"avg_cpu_percent":5.9799999999999995,"total_io_mb":null,"status":"success","character_count":500000,"word_count":112307,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":112307,"sentence_count":7563,"paragraph_count":6926,"avg_word_length":3.38541675941838,"avg_sentence_length":13.423773634801005,"extraction_completeness":1.0,"text_coherence":0.30375980524749796,"noise_ratio":0.5307600000000001,"gibberish_ratio":0.01694915254237288,"flesch_reading_ease":70.7747961398366,"gunning_fog_index":10.59532347580288,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":true,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5120034464866704,"extracted_text":"\nAlgebra, Topology, Differential Calculus, and\nOptimization Theory\n\nFor Computer Science and Machine Learning\n\nJean Gallier and Jocelyn Quaintance\nDepartment of Computer and Information Science\n\nUniversity of Pennsylvania\nPhiladelphia, PA 19104, USA\ne-mail: jean@cis.upenn.edu\n\nc© Jean Gallier\n\nAugust 2, 2019\n\n\n\n2\n\n\n\n\n\nContents\n\nContents 3\n\n1 Introduction 17\n\n2 Groups, Rings, and Fields 19\n2.1 Groups, Subgroups, Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.2 Cyclic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.3 Rings and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nI Linear Algebra 43\n\n3 Vector Spaces, Bases, Linear Maps 45\n3.1 Motivations: Linear Combinations, Linear Independence, Rank . . . . . . . 45\n3.2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.3 Indexed Families; the Sum Notation\n\n∑\ni∈I ai . . . . . . . . . . . . . . . . . . 60\n\n3.4 Linear Independence, Subspaces . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.5 Bases of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.6 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n3.7 Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n3.8 Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.9 Linear Forms and the Dual Space . . . . . . . . . . . . . . . . . . . . . . . . 94\n3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n4 Matrices and Linear Maps 107\n4.1 Representation of Linear Maps by Matrices . . . . . . . . . . . . . . . . . . 107\n4.2 Composition of Linear Maps and Matrix Multiplication . . . . . . . . . . . 112\n4.3 Change of Basis Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n4.4 The Effect of a Change of Bases on Matrices . . . . . . . . . . . . . . . . . 120\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\n5 Haar Bases, Haar Wavelets, Hadamard Matrices 131\n\n3\n\n\n\n4 CONTENTS\n\n5.1 Introduction to Signal Compression Using Haar Wavelets . . . . . . . . . . 131\n5.2 Haar Matrices, Scaling Properties of Haar Wavelets . . . . . . . . . . . . . . 133\n5.3 Kronecker Product Construction of Haar Matrices . . . . . . . . . . . . . . 138\n5.4 Multiresolution Signal Analysis with Haar Bases . . . . . . . . . . . . . . . 140\n5.5 Haar Transform for Digital Images . . . . . . . . . . . . . . . . . . . . . . . 142\n5.6 Hadamard Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n5.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\n6 Direct Sums 155\n6.1 Sums, Direct Sums, Direct Products . . . . . . . . . . . . . . . . . . . . . . 155\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation . . . . . . . . . . . . . . 165\n6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n6.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n7 Determinants 181\n7.1 Permutations, Signature of a Permutation . . . . . . . . . . . . . . . . . . . 181\n7.2 Alternating Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n7.3 Definition of a Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n7.4 Inverse Matrices and Determinants . . . . . . . . . . . . . . . . . . . . . . . 197\n7.5 Systems of Linear Equations and Determinants . . . . . . . . . . . . . . . . 200\n7.6 Determinant of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.7 The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.8 Permanents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n7.10 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n7.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n\n8 Gaussian Elimination, LU, Cholesky, Echelon Form 219\n8.1 Motivating Example: Curve Interpolation . . . . . . . . . . . . . . . . . . . 219\n8.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n8.3 Elementary Matrices and Row Operations . . . . . . . . . . . . . . . . . . . 228\n8.4 LU -Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n8.5 PA = LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n8.6 Proof of Theorem 8.5 ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n8.7 Dealing with Roundoff Errors; Pivoting Strategies . . . . . . . . . . . . . . . 251\n8.8 Gaussian Elimination of Tridiagonal Matrices . . . . . . . . . . . . . . . . . 252\n8.9 SPD Matrices and the Cholesky Decomposition . . . . . . . . . . . . . . . . 254\n8.10 Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n8.11 RREF, Free Variables, Homogeneous Systems . . . . . . . . . . . . . . . . . 269\n8.12 Uniqueness of RREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n8.13 Solving Linear Systems Using RREF . . . . . . . . . . . . . . . . . . . . . . 274\n8.14 Elementary Matrices and Columns Operations . . . . . . . . . . . . . . . . 281\n\n\n\nCONTENTS 5\n\n8.15 Transvections and Dilatations ~ . . . . . . . . . . . . . . . . . . . . . . . . 282\n8.16 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n8.17 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n\n9 Vector Norms and Matrix Norms 301\n9.1 Normed Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n9.2 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n9.3 Subordinate Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n9.4 Inequalities Involving Subordinate Norms . . . . . . . . . . . . . . . . . . . 324\n9.5 Condition Numbers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 326\n9.6 An Application of Norms: Inconsistent Linear Systems . . . . . . . . . . . . 335\n9.7 Limits of Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . 336\n9.8 The Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n9.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n\n10 Iterative Methods for Solving Linear Systems 351\n10.1 Convergence of Sequences of Vectors and Matrices . . . . . . . . . . . . . . 351\n10.2 Convergence of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . 354\n10.3 Methods of Jacobi, Gauss–Seidel, and Relaxation . . . . . . . . . . . . . . . 356\n10.4 Convergence of the Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n10.5 Convergence Methods for Tridiagonal Matrices . . . . . . . . . . . . . . . . 367\n10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n10.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n\n11 The Dual Space and Duality 375\n11.1 The Dual Space E∗ and Linear Forms . . . . . . . . . . . . . . . . . . . . . 375\n11.2 Pairing and Duality Between E and E∗ . . . . . . . . . . . . . . . . . . . . 382\n11.3 The Duality Theorem and Some Consequences . . . . . . . . . . . . . . . . 387\n11.4 The Bidual and Canonical Pairings . . . . . . . . . . . . . . . . . . . . . . . 393\n11.5 Hyperplanes and Linear Forms . . . . . . . . . . . . . . . . . . . . . . . . . 395\n11.6 Transpose of a Linear Map and of a Matrix . . . . . . . . . . . . . . . . . . 396\n11.7 Properties of the Double Transpose . . . . . . . . . . . . . . . . . . . . . . . 403\n11.8 The Four Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . 405\n11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n11.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n\n12 Euclidean Spaces 413\n12.1 Inner Products, Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . . 413\n12.2 Orthogonality and Duality in Euclidean Spaces . . . . . . . . . . . . . . . . 422\n12.3 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n12.4 Existence and Construction of Orthonormal Bases . . . . . . . . . . . . . . 432\n12.5 Linear Isometries (Orthogonal Transformations) . . . . . . . . . . . . . . . . 439\n\n\n\n6 CONTENTS\n\n12.6 The Orthogonal Group, Orthogonal Matrices . . . . . . . . . . . . . . . . . 442\n12.7 The Rodrigues Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\n12.8 QR-Decomposition for Invertible Matrices . . . . . . . . . . . . . . . . . . . 447\n12.9 Some Applications of Euclidean Geometry . . . . . . . . . . . . . . . . . . . 452\n12.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\n12.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n\n13 QR-Decomposition for Arbitrary Matrices 467\n13.1 Orthogonal Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467\n13.2 QR-Decomposition Using Householder Matrices . . . . . . . . . . . . . . . . 472\n13.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n13.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n\n14 Hermitian Spaces 489\n14.1 Hermitian Spaces, Pre-Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 489\n14.2 Orthogonality, Duality, Adjoint of a Linear Map . . . . . . . . . . . . . . . 498\n14.3 Linear Isometries (Also Called Unitary Transformations) . . . . . . . . . . . 503\n14.4 The Unitary Group, Unitary Matrices . . . . . . . . . . . . . . . . . . . . . 505\n14.5 Hermitian Reflections and QR-Decomposition . . . . . . . . . . . . . . . . . 508\n14.6 Orthogonal Projections and Involutions . . . . . . . . . . . . . . . . . . . . 513\n14.7 Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523\n14.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n\n15 Eigenvectors and Eigenvalues 529\n15.1 Eigenvectors and Eigenvalues of a Linear Map . . . . . . . . . . . . . . . . . 529\n15.2 Reduction to Upper Triangular Form . . . . . . . . . . . . . . . . . . . . . . 537\n15.3 Location of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541\n15.4 Conditioning of Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . 544\n15.5 Eigenvalues of the Matrix Exponential . . . . . . . . . . . . . . . . . . . . . 547\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n15.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n\n16 Unit Quaternions and Rotations in SO(3) 561\n16.1 The Group SU(2) and the Skew Field H of Quaternions . . . . . . . . . . . 561\n16.2 Representation of Rotation in SO(3) By Quaternions in SU(2) . . . . . . . 563\n16.3 Matrix Representation of the Rotation rq . . . . . . . . . . . . . . . . . . . 568\n16.4 An Algorithm to Find a Quaternion Representing a Rotation . . . . . . . . 570\n16.5 The Exponential Map exp: su(2)→ SU(2) . . . . . . . . . . . . . . . . . . 573\n16.6 Quaternion Interpolation ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 575\n16.7 Nonexistence of a “Nice” Section from SO(3) to SU(2) . . . . . . . . . . . . 577\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n16.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n\n\n\nCONTENTS 7\n\n17 Spectral Theorems 583\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n17.2 Normal Linear Maps: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . 583\n17.3 Spectral Theorem for Normal Linear Maps . . . . . . . . . . . . . . . . . . . 589\n17.4 Self-Adjoint and Other Special Linear Maps . . . . . . . . . . . . . . . . . . 594\n17.5 Normal and Other Special Matrices . . . . . . . . . . . . . . . . . . . . . . . 600\n17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlacing . . . . . . . . . . . . . 603\n17.7 The Courant–Fischer Theorem; Perturbation Results . . . . . . . . . . . . . 608\n17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611\n17.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n\n18 Computing Eigenvalues and Eigenvectors 619\n18.1 The Basic QR Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n18.2 Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627\n18.3 Making the QR Method More Efficient Using Shifts . . . . . . . . . . . . . 633\n18.4 Krylov Subspaces; Arnoldi Iteration . . . . . . . . . . . . . . . . . . . . . . 638\n18.5 GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642\n18.6 The Hermitian Case; Lanczos Iteration . . . . . . . . . . . . . . . . . . . . . 643\n18.7 Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644\n18.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646\n18.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647\n\n19 Introduction to The Finite Elements Method 649\n19.1 A One-Dimensional Problem: Bending of a Beam . . . . . . . . . . . . . . . 649\n19.2 A Two-Dimensional Problem: An Elastic Membrane . . . . . . . . . . . . . 660\n19.3 Time-Dependent Boundary Problems . . . . . . . . . . . . . . . . . . . . . . 663\n\n20 Graphs and Graph Laplacians; Basic Facts 671\n20.1 Directed Graphs, Undirected Graphs, Weighted Graphs . . . . . . . . . . . 674\n20.2 Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 681\n20.3 Normalized Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . 685\n20.4 Graph Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . 689\n20.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691\n20.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692\n\n21 Spectral Graph Drawing 695\n21.1 Graph Drawing and Energy Minimization . . . . . . . . . . . . . . . . . . . 695\n21.2 Examples of Graph Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . 698\n21.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702\n\n22 Singular Value Decomposition and Polar Form 705\n22.1 Properties of f ∗ ◦ f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705\n22.2 Singular Value Decomposition for Square Matrices . . . . . . . . . . . . . . 709\n\n\n\n8 CONTENTS\n\n22.3 Polar Form for Square Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 712\n22.4 Singular Value Decomposition for Rectangular Matrices . . . . . . . . . . . 715\n22.5 Ky Fan Norms and Schatten Norms . . . . . . . . . . . . . . . . . . . . . . 718\n22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n22.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n\n23 Applications of SVD and Pseudo-Inverses 723\n23.1 Least Squares Problems and the Pseudo-Inverse . . . . . . . . . . . . . . . . 723\n23.2 Properties of the Pseudo-Inverse . . . . . . . . . . . . . . . . . . . . . . . . 730\n23.3 Data Compression and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . 735\n23.4 Principal Components Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . 737\n23.5 Best Affine Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\n23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752\n23.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\n\nII Affine and Projective Geometry 757\n\n24 Basics of Affine Geometry 759\n24.1 Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759\n24.2 Examples of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\n24.3 Chasles’s Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769\n24.4 Affine Combinations, Barycenters . . . . . . . . . . . . . . . . . . . . . . . . 770\n24.5 Affine Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775\n24.6 Affine Independence and Affine Frames . . . . . . . . . . . . . . . . . . . . . 781\n24.7 Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787\n24.8 Affine Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794\n24.9 Affine Geometry: A Glimpse . . . . . . . . . . . . . . . . . . . . . . . . . . 796\n24.10 Affine Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 800\n24.11 Intersection of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 802\n\n25 Embedding an Affine Space in a Vector Space 805\n25.1 The “Hat Construction,” or Homogenizing . . . . . . . . . . . . . . . . . . . 805\n25.2 Affine Frames of E and Bases of Ê . . . . . . . . . . . . . . . . . . . . . . . 812\n25.3 Another Construction of Ê . . . . . . . . . . . . . . . . . . . . . . . . . . . 815\n25.4 Extending Affine Maps to Linear Maps . . . . . . . . . . . . . . . . . . . . . 818\n\n26 Basics of Projective Geometry 823\n26.1 Why Projective Spaces? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823\n26.2 Projective Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828\n26.3 Projective Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n26.4 Projective Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836\n26.5 Projective Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850\n\n\n\nCONTENTS 9\n\n26.6 Finding a Homography Between Two Projective Frames . . . . . . . . . . . 856\n26.7 Affine Patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869\n26.8 Projective Completion of an Affine Space . . . . . . . . . . . . . . . . . . . 872\n26.9 Making Good Use of Hyperplanes at Infinity . . . . . . . . . . . . . . . . . 877\n26.10 The Cross-Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880\n26.11 Fixed Points of Homographies and Homologies . . . . . . . . . . . . . . . . 884\n26.12 Duality in Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 898\n26.13 Cross-Ratios of Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . 902\n26.14 Complexification of a Real Projective Space . . . . . . . . . . . . . . . . . . 904\n26.15 Similarity Structures on a Projective Space . . . . . . . . . . . . . . . . . . 906\n26.16 Some Applications of Projective Geometry . . . . . . . . . . . . . . . . . . . 915\n\nIII The Geometry of Bilinear Forms 921\n\n27 The Cartan–Dieudonné Theorem 923\n27.1 The Cartan–Dieudonné Theorem for Linear Isometries . . . . . . . . . . . . 923\n27.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 935\n27.3 Fixed Points of Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 937\n27.4 Affine Isometries and Fixed Points . . . . . . . . . . . . . . . . . . . . . . . 939\n27.5 The Cartan–Dieudonné Theorem for Affine Isometries . . . . . . . . . . . . 945\n\n28 Isometries of Hermitian Spaces 949\n28.1 The Cartan–Dieudonné Theorem, Hermitian Case . . . . . . . . . . . . . . . 949\n28.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 958\n\n29 The Geometry of Bilinear Forms; Witt’s Theorem 963\n29.1 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963\n29.2 Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 971\n29.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 975\n29.4 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 980\n29.5 Isometries Associated with Sesquilinear Forms . . . . . . . . . . . . . . . . . 982\n29.6 Totally Isotropic Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 986\n29.7 Witt Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992\n29.8 Symplectic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000\n29.9 Orthogonal Groups and the Cartan–Dieudonné Theorem . . . . . . . . . . . 1004\n29.10 Witt’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1011\n\nIV Algebra: PID’s, UFD’s, Noetherian Rings, Tensors,\nModules over a PID, Normal Forms 1017\n\n30 Polynomials, Ideals and PID’s 1019\n\n\n\n10 CONTENTS\n\n30.1 Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1019\n30.2 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1020\n30.3 Euclidean Division of Polynomials . . . . . . . . . . . . . . . . . . . . . . . 1026\n30.4 Ideals, PID’s, and Greatest Common Divisors . . . . . . . . . . . . . . . . . 1028\n30.5 Factorization and Irreducible Factors in K[X] . . . . . . . . . . . . . . . . . 1036\n30.6 Roots of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1040\n30.7 Polynomial Interpolation (Lagrange, Newton, Hermite) . . . . . . . . . . . . 1047\n\n31 Annihilating Polynomials; Primary Decomposition 1055\n31.1 Annihilating Polynomials and the Minimal Polynomial . . . . . . . . . . . . 1057\n31.2 Minimal Polynomials of Diagonalizable Linear Maps . . . . . . . . . . . . . 1059\n31.3 Commuting Families of Linear Maps . . . . . . . . . . . . . . . . . . . . . . 1062\n31.4 The Primary Decomposition Theorem . . . . . . . . . . . . . . . . . . . . . 1065\n31.5 Jordan Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1071\n31.6 Nilpotent Linear Maps and Jordan Form . . . . . . . . . . . . . . . . . . . . 1074\n31.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1080\n31.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1081\n\n32 UFD’s, Noetherian Rings, Hilbert’s Basis Theorem 1085\n32.1 Unique Factorization Domains (Factorial Rings) . . . . . . . . . . . . . . . . 1085\n32.2 The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . 1099\n32.3 Noetherian Rings and Hilbert’s Basis Theorem . . . . . . . . . . . . . . . . 1105\n32.4 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1109\n\n33 Tensor Algebras 1111\n33.1 Linear Algebra Preliminaries: Dual Spaces and Pairings . . . . . . . . . . . 1113\n33.2 Tensors Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1118\n33.3 Bases of Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1130\n33.4 Some Useful Isomorphisms for Tensor Products . . . . . . . . . . . . . . . . 1131\n33.5 Duality for Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . 1135\n33.6 Tensor Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1141\n33.7 Symmetric Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1148\n33.8 Bases of Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1152\n33.9 Some Useful Isomorphisms for Symmetric Powers . . . . . . . . . . . . . . . 1155\n33.10 Duality for Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . 1155\n33.11 Symmetric Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1159\n33.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1162\n\n34 Exterior Tensor Powers and Exterior Algebras 1165\n34.1 Exterior Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1165\n34.2 Bases of Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1170\n34.3 Some Useful Isomorphisms for Exterior Powers . . . . . . . . . . . . . . . . 1173\n34.4 Duality for Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1173\n\n\n\nCONTENTS 11\n\n34.5 Exterior Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1177\n34.6 The Hodge ∗-Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1181\n34.7 Left and Right Hooks ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1185\n34.8 Testing Decomposability ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 1195\n34.9 The Grassmann-Plücker’s Equations and Grassmannians ~ . . . . . . . . . 1198\n34.10 Vector-Valued Alternating Forms . . . . . . . . . . . . . . . . . . . . . . . . 1201\n34.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205\n\n35 Introduction to Modules; Modules over a PID 1207\n35.1 Modules over a Commutative Ring . . . . . . . . . . . . . . . . . . . . . . . 1207\n35.2 Finite Presentations of Modules . . . . . . . . . . . . . . . . . . . . . . . . . 1216\n35.3 Tensor Products of Modules over a Commutative Ring . . . . . . . . . . . . 1222\n35.4 Torsion Modules over a PID; Primary Decomposition . . . . . . . . . . . . . 1225\n35.5 Finitely Generated Modules over a PID . . . . . . . . . . . . . . . . . . . . 1231\n35.6 Extension of the Ring of Scalars . . . . . . . . . . . . . . . . . . . . . . . . 1247\n\n36 Normal Forms; The Rational Canonical Form 1253\n36.1 The Torsion Module Associated With An Endomorphism . . . . . . . . . . 1253\n36.2 The Rational Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . 1261\n36.3 The Rational Canonical Form, Second Version . . . . . . . . . . . . . . . . . 1268\n36.4 The Jordan Form Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . 1269\n36.5 The Smith Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1272\n\nV Topology, Differential Calculus 1285\n\n37 Topology 1287\n37.1 Metric Spaces and Normed Vector Spaces . . . . . . . . . . . . . . . . . . . 1287\n37.2 Topological Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1294\n37.3 Continuous Functions, Limits . . . . . . . . . . . . . . . . . . . . . . . . . . 1303\n37.4 Connected Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1311\n37.5 Compact Sets and Locally Compact Spaces . . . . . . . . . . . . . . . . . . 1320\n37.6 Second-Countable and Separable Spaces . . . . . . . . . . . . . . . . . . . . 1331\n37.7 Sequential Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1335\n37.8 Complete Metric Spaces and Compactness . . . . . . . . . . . . . . . . . . . 1341\n37.9 Completion of a Metric Space . . . . . . . . . . . . . . . . . . . . . . . . . . 1344\n37.10 The Contraction Mapping Theorem . . . . . . . . . . . . . . . . . . . . . . 1351\n37.11 Continuous Linear and Multilinear Maps . . . . . . . . . . . . . . . . . . . . 1355\n37.12 Completion of a Normed Vector Space . . . . . . . . . . . . . . . . . . . . . 1362\n37.13 Normed Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n37.14 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n\n38 A Detour On Fractals 1367\n\n\n\n12 CONTENTS\n\n38.1 Iterated Function Systems and Fractals . . . . . . . . . . . . . . . . . . . . 1367\n\n39 Differential Calculus 1375\n39.1 Directional Derivatives, Total Derivatives . . . . . . . . . . . . . . . . . . . 1375\n39.2 Jacobian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1389\n39.3 The Implicit and The Inverse Function Theorems . . . . . . . . . . . . . . . 1397\n39.4 Tangent Spaces and Differentials . . . . . . . . . . . . . . . . . . . . . . . . 1401\n39.5 Second-Order and Higher-Order Derivatives . . . . . . . . . . . . . . . . . . 1402\n39.6 Taylor’s formula, Faà di Bruno’s formula . . . . . . . . . . . . . . . . . . . . 1407\n39.7 Vector Fields, Covariant Derivatives, Lie Brackets . . . . . . . . . . . . . . . 1411\n39.8 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1413\n\nVI Preliminaries for Optimization Theory 1415\n\n40 Extrema of Real-Valued Functions 1417\n40.1 Local Extrema and Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . 1417\n40.2 Using Second Derivatives to Find Extrema . . . . . . . . . . . . . . . . . . . 1427\n40.3 Using Convexity to Find Extrema . . . . . . . . . . . . . . . . . . . . . . . 1430\n40.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1440\n\n41 Newton’s Method and Its Generalizations 1441\n41.1 Newton’s Method for Real Functions of a Real Argument . . . . . . . . . . 1441\n41.2 Generalizations of Newton’s Method . . . . . . . . . . . . . . . . . . . . . . 1442\n41.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1448\n\n42 Quadratic Optimization Problems 1449\n42.1 Quadratic Optimization: The Positive Definite Case . . . . . . . . . . . . . 1449\n42.2 Quadratic Optimization: The General Case . . . . . . . . . . . . . . . . . . 1458\n42.3 Maximizing a Quadratic Function on the Unit Sphere . . . . . . . . . . . . 1463\n42.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1468\n\n43 Schur Complements and Applications 1469\n43.1 Schur Complements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1469\n43.2 SPD Matrices and Schur Complements . . . . . . . . . . . . . . . . . . . . . 1472\n43.3 SP Semidefinite Matrices and Schur Complements . . . . . . . . . . . . . . 1473\n\nVII Linear Optimization 1475\n\n44 Convex Sets, Cones, H-Polyhedra 1477\n44.1 What is Linear Programming? . . . . . . . . . . . . . . . . . . . . . . . . . 1477\n44.2 Affine Subsets, Convex Sets, Hyperplanes, Half-Spaces . . . . . . . . . . . . 1479\n44.3 Cones, Polyhedral Cones, and H-Polyhedra . . . . . . . . . . . . . . . . . . 1482\n\n\n\nCONTENTS 13\n\n45 Linear Programs 1489\n45.1 Linear Programs, Feasible Solutions, Optimal Solutions . . . . . . . . . . . 1489\n45.2 Basic Feasible Solutions and Vertices . . . . . . . . . . . . . . . . . . . . . . 1495\n\n46 The Simplex Algorithm 1503\n46.1 The Idea Behind the Simplex Algorithm . . . . . . . . . . . . . . . . . . . . 1503\n46.2 The Simplex Algorithm in General . . . . . . . . . . . . . . . . . . . . . . . 1512\n46.3 How to Perform a Pivoting Step Efficiently . . . . . . . . . . . . . . . . . . 1519\n46.4 The Simplex Algorithm Using Tableaux . . . . . . . . . . . . . . . . . . . . 1523\n46.5 Computational Efficiency of the Simplex Method . . . . . . . . . . . . . . . 1532\n\n47 Linear Programming and Duality 1535\n47.1 Variants of the Farkas Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 1535\n47.2 The Duality Theorem in Linear Programming . . . . . . . . . . . . . . . . . 1540\n47.3 Complementary Slackness Conditions . . . . . . . . . . . . . . . . . . . . . 1548\n47.4 Duality for Linear Programs in Standard Form . . . . . . . . . . . . . . . . 1550\n47.5 The Dual Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 1553\n47.6 The Primal-Dual Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1558\n\nVIII NonLinear Optimization 1569\n\n48 Basics of Hilbert Spaces 1571\n48.1 The Projection Lemma, Duality . . . . . . . . . . . . . . . . . . . . . . . . 1571\n48.2 Farkas–Minkowski Lemma in Hilbert Spaces . . . . . . . . . . . . . . . . . . 1588\n\n49 General Results of Optimization Theory 1591\n49.1 Optimization Problems; Basic Terminology . . . . . . . . . . . . . . . . . . 1591\n49.2 Existence of Solutions of an Optimization Problem . . . . . . . . . . . . . . 1594\n49.3 Minima of Quadratic Functionals . . . . . . . . . . . . . . . . . . . . . . . . 1599\n49.4 Elliptic Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1605\n49.5 Iterative Methods for Unconstrained Problems . . . . . . . . . . . . . . . . 1608\n49.6 Gradient Descent Methods for Unconstrained Problems . . . . . . . . . . . 1612\n49.7 Convergence of Gradient Descent with Variable Stepsize . . . . . . . . . . . 1617\n49.8 Steepest Descent for an Arbitrary Norm . . . . . . . . . . . . . . . . . . . . 1622\n49.9 Newton’s Method For Finding a Minimum . . . . . . . . . . . . . . . . . . . 1624\n49.10 Conjugate Gradient Methods; Unconstrained Problems . . . . . . . . . . . . 1628\n49.11 Gradient Projection for Constrained Optimization . . . . . . . . . . . . . . 1640\n49.12 Penalty Methods for Constrained Optimization . . . . . . . . . . . . . . . . 1642\n49.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1644\n\n50 Introduction to Nonlinear Optimization 1647\n50.1 The Cone of Feasible Directions . . . . . . . . . . . . . . . . . . . . . . . . . 1647\n\n\n\n14 CONTENTS\n\n50.2 Active Constraints and Qualified Constraints . . . . . . . . . . . . . . . . . 1654\n50.3 The Karush–Kuhn–Tucker Conditions . . . . . . . . . . . . . . . . . . . . . 1660\n50.4 Equality Constrained Minimization . . . . . . . . . . . . . . . . . . . . . . . 1672\n50.5 Hard Margin Support Vector Machine; Version I . . . . . . . . . . . . . . . 1677\n50.6 Hard Margin Support Vector Machine; Version II . . . . . . . . . . . . . . . 1681\n50.7 Lagrangian Duality and Saddle Points . . . . . . . . . . . . . . . . . . . . . 1690\n50.8 Weak and Strong Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1699\n50.9 Handling Equality Constraints Explicitly . . . . . . . . . . . . . . . . . . . . 1707\n50.10 Dual of the Hard Margin Support Vector Machine . . . . . . . . . . . . . . 1710\n50.11 Conjugate Function and Legendre Dual Function . . . . . . . . . . . . . . . 1715\n50.12 Some Techniques to Obtain a More Useful Dual Program . . . . . . . . . . 1725\n50.13 Uzawa’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1729\n50.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1735\n\n51 Subgradients and Subdifferentials 1737\n51.1 Extended Real-Valued Convex Functions . . . . . . . . . . . . . . . . . . . . 1739\n51.2 Subgradients and Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . . 1748\n51.3 Basic Properties of Subgradients and Subdifferentials . . . . . . . . . . . . . 1760\n51.4 Additional Properties of Subdifferentials . . . . . . . . . . . . . . . . . . . . 1766\n51.5 The Minimum of a Proper Convex Function . . . . . . . . . . . . . . . . . . 1770\n51.6 Generalization of the Lagrangian Framework . . . . . . . . . . . . . . . . . 1776\n51.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1780\n\n52 Dual Ascent Methods; ADMM 1783\n52.1 Dual Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1785\n52.2 Augmented Lagrangians and the Method of Multipliers . . . . . . . . . . . . 1789\n52.3 ADMM: Alternating Direction Method of Multipliers . . . . . . . . . . . . . 1794\n52.4 Convergence of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1797\n52.5 Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806\n52.6 Some Applications of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . 1807\n52.7 Applications of ADMM to `1-Norm Problems . . . . . . . . . . . . . . . . . 1810\n52.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1815\n\nIX Applications to Machine Learning 1817\n\n53 Ridge Regression and Lasso Regression 1819\n53.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1819\n53.2 Lasso Regression (`1-Regularized Regression) . . . . . . . . . . . . . . . . . 1829\n53.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1835\n\n54 Positive Definite Kernels 1837\n54.1 Basic Properties of Positive Definite Kernels . . . . . . . . . . . . . . . . . . 1837\n\n\n\nCONTENTS 15\n\n54.2 Hilbert Space Representation of a Positive Kernel . . . . . . . . . . . . . . . 1848\n54.3 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1852\n54.4 ν-SV Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1855\n\n55 Soft Margin Support Vector Machines 1865\n55.1 Soft Margin Support Vector Machines; (SVMs1) . . . . . . . . . . . . . . . . 1868\n55.2 Soft Margin Support Vector Machines; (SVMs2) . . . . . . . . . . . . . . . . 1878\n55.3 Soft Margin Support Vector Machines; (SVMs2′) . . . . . . . . . . . . . . . 1885\n55.4 Soft Margin SVM; (SVMs3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1900\n55.5 Soft Margin Support Vector Machines; (SVMs4) . . . . . . . . . . . . . . . . 1903\n55.6 Soft Margin SVM; (SVMs5) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1911\n55.7 Summary and Comparison of the SVM Methods . . . . . . . . . . . . . . . 1914\n\nX Appendices 1927\n\nA Total Orthogonal Families in Hilbert Spaces 1929\nA.1 Total Orthogonal Families, Fourier Coefficients . . . . . . . . . . . . . . . . 1929\nA.2 The Hilbert Space `2(K) and the Riesz-Fischer Theorem . . . . . . . . . . . 1937\n\nB Zorn’s Lemma; Some Applications 1947\nB.1 Statement of Zorn’s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 1947\nB.2 Proof of the Existence of a Basis in a Vector Space . . . . . . . . . . . . . . 1948\nB.3 Existence of Maximal Proper Ideals . . . . . . . . . . . . . . . . . . . . . . 1949\n\nBibliography 1951\n\n\n\n16 CONTENTS\n\n16\n\nCONTENTS\n\n\n\n\nChapter 1\n\nIntroduction\n\n17\n\n\n\n18 CHAPTER 1. INTRODUCTION\n\n\n\nChapter 2\n\nGroups, Rings, and Fields\n\nIn the following four chapters, the basic algebraic structures (groups, rings, fields, vector\nspaces) are reviewed, with a major emphasis on vector spaces. Basic notions of linear alge-\nbra such as vector spaces, subspaces, linear combinations, linear independence, bases, quo-\ntient spaces, linear maps, matrices, change of bases, direct sums, linear forms, dual spaces,\nhyperplanes, transpose of a linear maps, are reviewed.\n\n2.1 Groups, Subgroups, Cosets\n\nThe set R of real numbers has two operations +: R × R → R (addition) and ∗ : R × R →\nR (multiplication) satisfying properties that make R into an abelian group under +, and\nR− {0} = R∗ into an abelian group under ∗. Recall the definition of a group.\n\nDefinition 2.1. A group is a set G equipped with a binary operation · : G × G → G that\nassociates an element a · b ∈ G to every pair of elements a, b ∈ G, and having the following\nproperties: · is associative, has an identity element e ∈ G, and every element in G is invertible\n(w.r.t. ·). More explicitly, this means that the following equations hold for all a, b, c ∈ G:\n\n(G1) a · (b · c) = (a · b) · c. (associativity);\n\n(G2) a · e = e · a = a. (identity);\n\n(G3) For every a ∈ G, there is some a−1 ∈ G such that a · a−1 = a−1 · a = e. (inverse).\n\nA group G is abelian (or commutative) if\n\na · b = b · a for all a, b ∈ G.\n\nA set M together with an operation · : M ×M → M and an element e satisfying only\nConditions (G1) and (G2) is called a monoid . For example, the set N = {0, 1, . . . , n, . . .} of\nnatural numbers is a (commutative) monoid under addition. However, it is not a group.\n\nSome examples of groups are given below.\n\n19\n\n\n\n20 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.1.\n\n1. The set Z = {. . . ,−n, . . . ,−1, 0, 1, . . . , n, . . .} of integers is an abelian group under\naddition, with identity element 0. However, Z∗ = Z − {0} is not a group under\nmultiplication.\n\n2. The set Q of rational numbers (fractions p/q with p, q ∈ Z and q 6= 0) is an abelian\ngroup under addition, with identity element 0. The set Q∗ = Q−{0} is also an abelian\ngroup under multiplication, with identity element 1.\n\n3. Given any nonempty set S, the set of bijections f : S → S, also called permutations\nof S, is a group under function composition (i.e., the multiplication of f and g is the\ncomposition g ◦ f), with identity element the identity function idS. This group is not\nabelian as soon as S has more than two elements. The permutation group of the set\nS = {1, . . . , n} is often denoted Sn and called the symmetric group on n elements.\n\n4. For any positive integer p ∈ N, define a relation on Z, denoted m ≡ n (mod p), as\nfollows:\n\nm ≡ n (mod p) iff m− n = kp for some k ∈ Z.\n\nThe reader will easily check that this is an equivalence relation, and, moreover, it is\ncompatible with respect to addition and multiplication, which means that if m1 ≡ n1\n\n(mod p) and m2 ≡ n2 (mod p), then m1 + m2 ≡ n1 + n2 (mod p) and m1m2 ≡ n1n2\n\n(mod p). Consequently, we can define an addition operation and a multiplication\noperation of the set of equivalence classes (mod p):\n\n[m] + [n] = [m+ n]\n\nand\n[m] · [n] = [mn].\n\nThe reader will easily check that addition of residue classes (mod p) induces an abelian\ngroup structure with [0] as zero. This group is denoted Z/pZ.\n\n5. The set of n×n invertible matrices with real (or complex) coefficients is a group under\nmatrix multiplication, with identity element the identity matrix In. This group is\ncalled the general linear group and is usually denoted by GL(n,R) (or GL(n,C)).\n\n6. The set of n × n invertible matrices A with real (or complex) coefficients such that\ndet(A) = 1 is a group under matrix multiplication, with identity element the identity\nmatrix In. This group is called the special linear group and is usually denoted by\nSL(n,R) (or SL(n,C)).\n\n7. The set of n× n matrices Q with real coefficients such that\n\nQQ> = Q>Q = In\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nwe have Q−1 = Q>. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n× n invertible matrices Q with real coefficients such that\n\nQQ> = Q>Q = In and det(Q) = 1\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nas in (6), we have Q−1 = Q>. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)–(8) are nonabelian for n ≥ 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a−1 of an element a ∈ G is denoted by −a.\n\nThe identity element of a group is unique. In fact, we can prove a more general fact:\n\nProposition 2.1. If a binary operation · : M ×M → M is associative and if e′ ∈ M is a\nleft identity and e′′ ∈M is a right identity, which means that\n\ne′ · a = a for all a ∈M (G2l)\n\nand\na · e′′ = a for all a ∈M, (G2r)\n\nthen e′ = e′′.\n\nProof. If we let a = e′′ in equation (G2l), we get\n\ne′ · e′′ = e′′,\n\nand if we let a = e′ in equation (G2r), we get\n\ne′ · e′′ = e′,\n\nand thus\ne′ = e′ · e′′ = e′′,\n\nas claimed.\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix /,,;\nwe have Q-! = Q'. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n x n invertible matrices Q with real coefficients such that\n\nQQ'=Q'Q=I, and det(Q)=1\n\nis a group under matrix multiplication, with identity element the identity matrix [,,;\nas in (6), we have Q-! = Q'. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)—(8) are nonabelian for n > 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a~! of an element a € G is denoted by —a.\n\nThe identity element of a group is unzque. In fact, we can prove a more general fact:\n\nProposition 2.1. /f a binary operation -: M x M + M is associative and if e’ € M is a\nleft identity and e” € M is a right identity, which means that\n\ne-a=a forall ae M (G21)\n\nand\na-e’=a forall ae M, (G2r)\n\nthen e' = e”.\nProof. If we let a = e” in equation (G21), we get\n/ \" 1\n\ne-e =e,\n\nand if we let a = e’ in equation (G2r), we get\n\nand thus\n\nas claimed. Oo\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n\n\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a ∈M has some\nleft inverse a′ ∈M and some right inverse a′′ ∈M , which means that\n\na′ · a = e (G3l)\n\nand\na · a′′ = e, (G3r)\n\nthen a′ = a′′.\n\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a′ · a) · a′′ = e · a′′ = a′′.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na′ · (a · a′′) = a′ · e = a′.\n\nHowever, since M is monoid, the operation · is associative, so\n\na′ = a′ · (a · a′′) = (a′ · a) · a′′ = a′′,\n\nas claimed.\n\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n(G2l) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G is a group\nof order n. If G is infinite, we say that G has infinite order . The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R, S ⊆ G, we let\n\nRS = {r · s | r ∈ R, s ∈ S}.\n\nIn particular, for any g ∈ G, if R = {g}, we write\n\ngS = {g · s | s ∈ S},\n\nand similarly, if S = {g}, we write\n\nRg = {r · g | r ∈ R}.\n\nFrom now on, we will drop the multiplication sign and write g1g2 for g1 · g2.\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a € M has some\nleft inverse a’ € M and some right inverse a\" € M, which means that\n\na-a=e (G31)\n\nand\na-a\" =e, (G3r)\n\nthen a’ =a\".\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a’-a) ql — e-q’ — a’.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na’-(a-a\")=d'-e=d’.\nHowever, since M is monoid, the operation - is associative, so\na’ =a'-(a-a\") =(a'-a)-a\" =a\",\nas claimed. O\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n\n(G21) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G' is a group\nof order n. If G is infinite, we say that G has infinite order. The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R,S' C G, we let\nRS ={r-s|reR,seS}.\nIn particular, for any g € G, if R = {g}, we write\ngS = {g-s|s € S},\nand similarly, if S = {g}, we write\n\nRg={r-g|reR}.\n\nFrom now on, we will drop the multiplication sign and write gg for gi - go.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 23\n\nDefinition 2.3. Let G be a group. For any g ∈ G, define Lg, the left translation by g, by\nLg(a) = ga, for all a ∈ G, and Rg, the right translation by g, by Rg(a) = ag, for all a ∈ G.\n\nThe following simple fact is often used.\n\nProposition 2.3. Given a group G, the translations Lg and Rg are bijections.\n\nProof. We show this for Lg, the proof for Rg being similar.\n\nIf Lg(a) = Lg(b), then ga = gb, and multiplying on the left by g−1, we get a = b, so Lg\ninjective. For any b ∈ G, we have Lg(g\n\n−1b) = gg−1b = b, so Lg is surjective. Therefore, Lg\nis bijective.\n\nDefinition 2.4. Given a group G, a subset H of G is a subgroup of G iff\n\n(1) The identity element e of G also belongs to H (e ∈ H);\n\n(2) For all h1, h2 ∈ H, we have h1h2 ∈ H;\n\n(3) For all h ∈ H, we have h−1 ∈ H.\n\nThe proof of the following proposition is left as an exercise.\n\nProposition 2.4. Given a group G, a subset H ⊆ G is a subgroup of G iff H is nonempty\nand whenever h1, h2 ∈ H, then h1h\n\n−1\n2 ∈ H.\n\nIf the group G is finite, then the following criterion can be used.\n\nProposition 2.5. Given a finite group G, a subset H ⊆ G is a subgroup of G iff\n\n(1) e ∈ H;\n\n(2) H is closed under multiplication.\n\nProof. We just have to prove that Condition (3) of Definition 2.4 holds. For any a ∈ H,\nsince the left translation La is bijective, its restriction to H is injective, and since H is finite,\nit is also bijective. Since e ∈ H, there is a unique b ∈ H such that La(b) = ab = e. However,\nif a−1 is the inverse of a in G, we also have La(a\n\n−1) = aa−1 = e, and by injectivity of La, we\nhave a−1 = b ∈ H.\n\nExample 2.2.\n\n1. For any integer n ∈ Z, the set\n\nnZ = {nk | k ∈ Z}\n\nis a subgroup of the group Z.\n\n\n\n24 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The set of matrices\n\nGL+(n,R) = {A ∈ GL(n,R) | det(A) > 0}\n\nis a subgroup of the group GL(n,R).\n\n3. The group SL(n,R) is a subgroup of the group GL(n,R).\n\n4. The group O(n) is a subgroup of the group GL(n,R).\n\n5. The group SO(n) is a subgroup of the group O(n), and a subgroup of the group\nSL(n,R).\n\n6. It is not hard to show that every 2× 2 rotation matrix R ∈ SO(2) can be written as\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n, with 0 ≤ θ < 2π.\n\nThen SO(2) can be considered as a subgroup of SO(3) by viewing the matrix\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\nas the matrix\n\nQ =\n\ncos θ − sin θ 0\nsin θ cos θ 0\n\n0 0 1\n\n .\n\n7. The set of 2× 2 upper-triangular matrices of the form(\na b\n0 c\n\n)\na, b, c ∈ R, a, c 6= 0\n\nis a subgroup of the group GL(2,R).\n\n8. The set V consisting of the four matrices(\n±1 0\n0 ±1\n\n)\nis a subgroup of the group GL(2,R) called the Klein four-group.\n\nDefinition 2.5. If H is a subgroup of G and g ∈ G is any element, the sets of the form\ngH are called left cosets of H in G and the sets of the form Hg are called right cosets of H\nin G. The left cosets (resp. right cosets) of H induce an equivalence relation ∼ defined as\nfollows: For all g1, g2 ∈ G,\n\ng1 ∼ g2 iff g1H = g2H\n\n(resp. g1 ∼ g2 iff Hg1 = Hg2). Obviously, ∼ is an equivalence relation.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 25\n\nNow, we claim the following fact:\n\nProposition 2.6. Given a group G and any subgroup H of G, we have g1H = g2H iff\ng−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H, for all g1, g2 ∈ G.\n\nProof. If we apply the bijection Lg−1\n2\n\nto both g1H and g2H we get Lg−1\n2\n\n(g1H) = g−1\n2 g1H\n\nand Lg−1\n2\n\n(g2H) = H, so g1H = g2H iff g−1\n2 g1H = H. If g−1\n\n2 g1H = H, since 1 ∈ H, we get\n\ng−1\n2 g1 ∈ H. Conversely, if g−1\n\n2 g1 ∈ H, since H is a group, the left translation Lg−1\n2 g1\n\nis a\n\nbijection of H, so g−1\n2 g1H = H. Thus, g−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H.\n\nIt follows that the equivalence class of an element g ∈ G is the coset gH (resp. Hg).\nSince Lg is a bijection between H and gH, the cosets gH all have the same cardinality. The\nmap Lg−1 ◦ Rg is a bijection between the left coset gH and the right coset Hg, so they also\nhave the same cardinality. Since the distinct cosets gH form a partition of G, we obtain the\nfollowing fact:\n\nProposition 2.7. (Lagrange) For any finite group G and any subgroup H of G, the order\nh of H divides the order n of G.\n\nDefinition 2.6. Given a finite group G and a subgroup H of G, if n = |G| and h = |H|,\nthen the ratio n/h is denoted by (G : H) and is called the index of H in G.\n\nThe index (G : H) is the number of left (and right) cosets of H in G. Proposition 2.7\ncan be stated as\n\n|G| = (G : H)|H|.\n\nThe set of left cosets of H in G (which, in general, is not a group) is denoted G/H.\nThe “points” of G/H are obtained by “collapsing” all the elements in a coset into a single\nelement.\n\nExample 2.3.\n\n1. Let n be any positive integer, and consider the subgroup nZ of Z (under addition).\nThe coset of 0 is the set {0}, and the coset of any nonzero integer m ∈ Z is\n\nm+ nZ = {m+ nk | k ∈ Z}.\n\nBy dividing m by n, we have m = nq + r for some unique r such that 0 ≤ r ≤ n− 1.\nBut then we see that r is the smallest positive element of the coset m + nZ. This\nimplies that there is a bijection betwen the cosets of the subgroup nZ of Z and the set\nof residues {0, 1, . . . , n− 1} modulo n, or equivalently a bijection with Z/nZ.\n\n\n\n26 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The cosets of SL(n,R) in GL(n,R) are the sets of matrices\n\nASL(n,R) = {AB | B ∈ SL(n,R)}, A ∈ GL(n,R).\n\nSince A is invertible, det(A) 6= 0, and we can write A = (det(A))1/n((det(A))−1/nA)\nif det(A) > 0 and A = (− det(A))1/n((− det(A))−1/nA) if det(A) < 0. But we have\n(det(A))−1/nA ∈ SL(n,R) if det(A) > 0 and −(− det(A))−1/nA ∈ SL(n,R) if det(A) <\n0, so the coset ASL(n,R) contains the matrix\n\n(det(A))1/nIn if det(A) > 0, −(− det(A))1/nIn if det(A) < 0.\n\nIt follows that there is a bijection between the cosets of SL(n,R) in GL(n,R) and R.\n\n3. The cosets of SO(n) in GL+(n,R) are the sets of matrices\n\nASO(n) = {AQ | Q ∈ SO(n)}, A ∈ GL+(n,R).\n\nIt can be shown (using the polar form for matrices) that there is a bijection between\nthe cosets of SO(n) in GL+(n,R) and the set of n × n symmetric, positive, definite\nmatrices; these are the symmetric matrices whose eigenvalues are strictly positive.\n\n4. The cosets of SO(2) in SO(3) are the sets of matrices\n\nQSO(2) = {QR | R ∈ SO(2)}, Q ∈ SO(3).\n\nThe group SO(3) moves the points on the sphere S2 in R3, namely for any x ∈ S2,\n\nx 7→ Qx for any rotation Q ∈ SO(3).\n\nHere,\nS2 = {(x, y, z) ∈ R3 | x2 + y2 + z2 = 1}.\n\nLet N = (0, 0, 1) be the north pole on the sphere S2. Then it is not hard to show that\nSO(2) is precisely the subgroup of SO(3) that leaves N fixed. As a consequence, all\nrotations QR in the coset QSO(2) map N to the same point QN ∈ S2, and it can be\nshown that there is a bijection between the cosets of SO(2) in SO(3) and the points\non S2. The surjectivity of this map has to do with the fact that the action of SO(3)\non S2 is transitive, which means that for any point x ∈ S2, there is some rotation\nQ ∈ SO(3) such that QN = x.\n\nIt is tempting to define a multiplication operation on left cosets (or right cosets) by\nsetting\n\n(g1H)(g2H) = (g1g2)H,\n\nbut this operation is not well defined in general, unless the subgroup H possesses a special\nproperty. In Example 2.3, it is possible to define multiplication of cosets in (1), but it is not\npossible in (2) and (3).\n\nThe property of the subgroup H that allows defining a multiplication operation on left\ncosets is typical of the kernels of group homomorphisms, so we are led to the following\ndefinition.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G′, a function ϕ : G→ G′ is a homomorphism\niff\n\nϕ(g1g2) = ϕ(g1)ϕ(g2), for all g1, g2 ∈ G.\n\nTaking g1 = g2 = e (in G), we see that\n\nϕ(e) = e′,\n\nand taking g1 = g and g2 = g−1, we see that\n\nϕ(g−1) = (ϕ(g))−1.\n\nExample 2.4.\n\n1. The map ϕ : Z→ Z/nZ given by ϕ(m) = m mod n for all m ∈ Z is a homomorphism.\n\n2. The map det : GL(n,R) → R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A,B. Similarly, the map det : O(n)→ R is a homomorphism.\n\nIf ϕ : G → G′ and ψ : G′ → G′′ are group homomorphisms, then ψ ◦ ϕ : G → G′′ is also\na homomorphism. If ϕ : G→ G′ is a homomorphism of groups, and if H ⊆ G, H ′ ⊆ G′ are\ntwo subgroups, then it is easily checked that\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H}\n\nis a subgroup of G′ and\nϕ−1(H ′) = {g ∈ G | ϕ(g) ∈ H ′}\n\nis a subgroup of G. In particular, when H ′ = {e′}, we obtain the kernel , Ker ϕ, of ϕ.\n\nDefinition 2.8. If ϕ : G → G′ is a homomorphism of groups, and if H ⊆ G is a subgroup\nof G, then the subgroup of G′,\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H},\n\nis called the image of H by ϕ, and the subgroup of G,\n\nKer ϕ = {g ∈ G | ϕ(g) = e′},\n\nis called the kernel of ϕ.\n\nExample 2.5.\n\n1. The kernel of the homomorphism ϕ : Z→ Z/nZ is nZ.\n\n2. The kernel of the homomorphism det : GL(n,R)→ R is SL(n,R). Similarly, the kernel\nof the homomorphism det : O(n)→ R is SO(n).\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G’, a function y: G > G’ is a homomorphism\niff\n(9192) = P(m)e(G2), for all gr, g2 € G.\n\nTaking g, = go =e (in G), we see that\n\n! we see that\n\nog\") = (v(g))*.\n\nand taking g; = g and g2 = g~\n\nExample 2.4.\n1. The map y: Z > Z/nZ given by y(m) = m mod n for all m € Z is a homomorphism.\n\n2. The map det: GL(n, R) — R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A, B. Similarly, the map det: O(n) > R is a homomorphism.\n\nIf y: G > G' and wy: G’ > G\"” are group homomorphisms, then yo y: G > G\" is also\na homomorphism. If y: G + G’ is a homomorphism of groups, and if H C G, H’ C G’ are\ntwo subgroups, then it is easily checked that\n\nIm H = 9(H) = {y(9) |g © A}\n\nis a subgroup of G’ and\neg (H')= {9 €G| lg) € H’}\nis a subgroup of G. In particular, when H’ = {e’}, we obtain the kernel, Ker y, of y.\n\nDefinition 2.8. If ¢: G > G’ is a homomorphism of groups, and if H C G is a subgroup\nof G, then the subgroup of G’,\n\nIm H = 9(H) = {y(9) |g € FH},\n\nis called the image of H by vy, and the subgroup of G,\n\nKer p= {9 €G | p(y) =e},\nis called the kernel of y.\nExample 2.5.\n1. The kernel of the homomorphism y: Z > Z/nZ is nZ.\n\n2. The kernel of the homomorphism det: GL(n, R) > Ris SL(n, R). Similarly, the kernel\nof the homomorphism det: O(n) > R is SO(n).\n\n\n\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. If ϕ : G→ G′ is a homomorphism of groups, then ϕ : G→ G′ is injective\niff Ker ϕ = {e}. (We also write Ker ϕ = (0).)\n\nProof. Assume ϕ is injective. Since ϕ(e) = e′, if ϕ(g) = e′, then ϕ(g) = ϕ(e), and by\ninjectivity of ϕ we must have g = e, so Ker ϕ = {e}.\n\nConversely, assume that Ker ϕ = {e}. If ϕ(g1) = ϕ(g2), then by multiplication on the\nleft by (ϕ(g1))−1 we get\n\ne′ = (ϕ(g1))−1ϕ(g1) = (ϕ(g1))−1ϕ(g2),\n\nand since ϕ is a homomorphism (ϕ(g1))−1 = ϕ(g−1\n1 ), so\n\ne′ = (ϕ(g1))−1ϕ(g2) = ϕ(g−1\n1 )ϕ(g2) = ϕ(g−1\n\n1 g2).\n\nThis shows that g−1\n1 g2 ∈ Ker ϕ, but since Ker ϕ = {e} we have g−1\n\n1 g2 = e, and thus g2 = g1,\nproving that ϕ is injective.\n\nDefinition 2.9. We say that a group homomorphism ϕ : G→ G′ is an isomorphism if there\nis a homomorphism ψ : G′ → G, so that\n\nψ ◦ ϕ = idG and ϕ ◦ ψ = idG′ . (†)\n\nIf ϕ is an isomorphism we say that the groups G and G′ are isomorphic. When G′ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nϕ : G→ G′ is an isomorphism, then the homomorphism ψ : G′ → G satisfying Condition (†)\nis unique. This homomorphism is denoted ϕ−1.\n\nThe left translations Lg and the right translations Rg are automorphisms of G.\n\nSuppose ϕ : G → G′ is a bijective homomorphism, and let ϕ−1 be the inverse of ϕ (as a\nfunction). Then for all a, b ∈ G, we have\n\nϕ(ϕ−1(a)ϕ−1(b)) = ϕ(ϕ−1(a))ϕ(ϕ−1(b)) = ab,\n\nand so\n\nϕ−1(ab) = ϕ−1(a)ϕ−1(b),\n\nwhich proves that ϕ−1 is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism ϕ : G→ G′ is an isomorphism.\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. Ifo: G > G’' is a homomorphism of groups, then py: G — G\" is injective\niff Ker y = {e}. (We also write Ker y = (0).)\n\nProof. Assume ¢p is injective. Since y(e) = e’, if y(g) = e’, then y(g) = y(e), and by\ninjectivity of ~ we must have g = e, so Ker y = {e}.\n\nConversely, assume that Ker y = {e}. If y(g1) = v(ge), then by multiplication on the\nleft by (y(g1))~* we get\n\ne’ = (9(m)) yg) = (¢(91)) *9(92),\n\nand since y is a homomorphism (y(g))~! = y(g,\"), so\n\ne' = (y(m1)) *v(g2) = v(97')9(92) = v(9r 92):\n\nThis shows that g>'g2 € Ker y, but since Ker y = {e} we have gj ‘go = e, and thus g = gu,\nproving that y is injective. L\n\nDefinition 2.9. We say that a group homomorphism y: G — G’ is an isomorphism if there\nis a homomorphism w: G’ + G, so that\n\nwop=idg and pow=idq@. (Tt)\n\nIf y is an isomorphism we say that the groups G and G’ are isomorphic. When G’ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nyp: G > G’ is an isomorphism, then the homomorphism ~: G’ + G satisfying Condition (T)\nis unique. This homomorphism is denoted y!.\n\nThe left translations L, and the right translations R, are automorphisms of G.\n\nSuppose vy: G > G’ is a bijective homomorphism, and let y~' be the inverse of y (as a\nfunction). Then for all a,b € G, we have\n\no(y ‘(a)y *(b)) = v(e*(a))e(y *(b)) = ab,\n\nand so\nyp *(ab) =p ‘(a)y (0),\n\nwhich proves that y~! is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism yp: G > G\" is an isomorphism.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\n\ngH = Hg, for all g ∈ G. (∗)\n\nis equivalent by multiplication on the right by g−1 to\n\ngHg−1 = H, for all g ∈ G,\n\nand the above is equivalent to\n\ngHg−1 ⊆ H, for all g ∈ G. (∗∗)\n\nThis is because gHg−1 ⊆ H implies H ⊆ g−1Hg, and this for all g ∈ G.\n\nProposition 2.10. Let ϕ : G → G′ be a group homomorphism. Then H = Ker ϕ satisfies\nProperty (∗∗), and thus Property (∗).\n\nProof. We have\n\nϕ(ghg−1) = ϕ(g)ϕ(h)ϕ(g−1) = ϕ(g)e′ϕ(g)−1 = ϕ(g)ϕ(g)−1 = e′,\n\nfor all h ∈ H = Ker ϕ and all g ∈ G. Thus, by definition of H = Ker ϕ, we have gHg−1 ⊆\nH.\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\n\ngNg−1 = N, for all g ∈ G.\n\nThis is denoted by N CG.\n\nProposition 2.10 shows that the kernel Ker ϕ of a homomorphism ϕ : G→ G′ is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R ∈ SO(2) and A ∈ SL(2,R) be the matrices\n\nR =\n\n(\n0 −1\n1 0\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nThen\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\nand we have\n\nARA−1 =\n\n(\n1 1\n0 1\n\n)(\n0 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n1 −1\n\n)\n,\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\ngH =Hg, forallg €G. (*)\nis equivalent by multiplication on the right by g~! to\ngHg ‘=H, forallg€G,\nand the above is equivalent to\ngHg' CH, forallg€G. (x)\nThis is because gHg~' C H implies H C g7!Hq, and this for all g € G.\n\nProposition 2.10. Let y: G — G’ be a group homomorphism. Then H = Ker y satisfies\nProperty (**), and thus Property (x).\n\nProof. We have\n\n/\n\ne(ghg*) = v(gelh)ye(g\") = v(ae'e(9)* = vla)e(g)* =e,\n\nfor all h € H = Ker y and all g € G. Thus, by definition of H = Ker y, we have gHg™! C\nH. O\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\ngNg |=N, forallg €G.\n\nThis is denoted by N dG.\n\nProposition 2.10 shows that the kernel Ker y of a homomorphism y: G > G\" is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R € SO(2) and A € SL(2,R) be the matrices\n\ne=(1o)) a= i)\n\nThen\n\nand we have\n\nae (00 G6 TG ov) Z)-G 5)\n\n\n\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA−1 /∈ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R ∈ SO(2) and Q ∈ SO(3) be the matrices\n\nR =\n\n0 −1 0\n1 0 0\n0 0 1\n\n , Q =\n\n1 0 0\n0 0 −1\n0 1 0\n\n .\n\nThen\n\nQ−1 = Q> =\n\n1 0 0\n0 0 1\n0 −1 0\n\n\nand we have\n\nQRQ−1 =\n\n1 0 0\n0 0 −1\n0 1 0\n\n0 −1 0\n1 0 0\n0 0 1\n\n1 0 0\n0 0 1\n0 −1 0\n\n =\n\n0 −1 0\n0 0 −1\n1 0 0\n\n1 0 0\n0 0 1\n0 −1 0\n\n\n=\n\n0 0 −1\n0 1 0\n1 0 0\n\n .\n\nObserve that QRQ−1 /∈ SO(2), so SO(2) is not a normal subgroup of SO(3).\n\nLet T and A ∈ GL(2,R) be the following matrices\n\nT =\n\n(\n1 1\n0 1\n\n)\n, A =\n\n(\n0 1\n1 0\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n0 1\n1 0\n\n)\n= A,\n\nand\n\nATA−1 =\n\n(\n0 1\n1 0\n\n)(\n1 1\n0 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n0 1\n1 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n1 0\n1 1\n\n)\n.\n\nThe matrix T is upper triangular, but ATA−1 is not, so the group of 2× 2 upper triangular\nmatrices is not a normal subgroup of GL(2,R).\n\nLet Q ∈ V and A ∈ GL(2,R) be the following matrices\n\nQ =\n\n(\n1 0\n0 −1\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA~! ¢ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R € SO(2) and Q € SO(3) be the matrices\n\n0 -1 O 10 0\nR={1 0 0], Q={0 0 -1\n0 0 1 01 0\nThen\n1 O 0\nQ't=Q'={0 0 1\n0 —l1 O\nand we have\n10 0 0 -1 0 1 O 0 0 -1 O 1 O 0\nQRQ‘'*={0 0 -1 1 0 O 0 0 1y}y=]0 0 -I1 0 0 1\n01 0 0 0 1 0 —-l1 O 1 0 0O 0 —-1 O\n00 -l\n=|{|0 1 O\n10 0\n\nObserve that QRQ7! ¢ SO(2), so SO(2) is not a normal subgroup of SO(3).\nLet T and A € GL(2,R) be the following matrices\n\n(0)\nAt= (9 )) =4\naera 0 CO a) =0 9):\n\nThe matrix T is upper triangular, but ATA! is not, so the group of 2 x 2 upper triangular\nmatrices is not a normal subgroup of GL(2, R).\n\nLet Q € V and A € GL(2,R) be the following matrices\n\na=(5 4). 4-0)\n\nWe have\n\nWe have\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\nand\n\nAQA−1 =\n\n(\n1 1\n0 1\n\n)(\n1 0\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n0 −1\n\n)\n.\n\nClearly AQA−1 /∈ V , which shows that the Klein four group is not a normal subgroup of\nGL(2,R).\n\nThe reader should check that the subgroups nZ, GL+(n,R), SL(n,R), and SO(n,R) as\na subgroup of O(n,R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ∼ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all g1, g2, g\n\n′\n1, g\n′\n2 ∈ G,\n\n(1) If g1N = g′1N and g2N = g′2N , then g1g2N = g′1g\n′\n2N , and\n\n(2) If g1N = g2N , then g−1\n1 N = g−1\n\n2 N .\n\nAs a consequence, we can define a group structure on the set G/ ∼ of equivalence classes\nmodulo ∼, by setting\n\n(g1N)(g2N) = (g1g2)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(g1N)(g2N) = (g1g2)N, g1, g2 ∈ G\n\nis denoted G/N , and called the quotient of G by N . The equivalence class gN of an element\ng ∈ G is also denoted g (or [g]). The map π : G→ G/N given by\n\nπ(g) = g = gN\n\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups ϕ : G→ G′, the groups G/Ker ϕ and\nIm ϕ = ϕ(G) are isomorphic.\n\nProof. Since ϕ is surjective onto its image, we may assume that ϕ is surjective, so that\nG′ = Im ϕ. We define a map ϕ : G/Ker ϕ→ G′ as follows:\n\nϕ(g) = ϕ(g), g ∈ G.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = gKer ϕ, and that it is a homomorphism. If g′ is another element in\nthe coset gKer ϕ, which means that g′ = gh for some h ∈ Kerϕ, then\n\nϕ(g′) = ϕ(gh) = ϕ(g)ϕ(h) = ϕ(g)e′ = ϕ(g),\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\n1 fl i\\fa 0\\fa 71). fa -1\\ fa -1\\_ 1 -2\n4os*=(5 i)(0 S)(o a)=(0 a) (0 a )=(0 =):\nClearly AQA~' ¢ V, which shows that the Klein four group is not a normal subgroup of\n\nGL(2, R).\n\nThe reader should check that the subgroups nZ, GL*(n,R), SL(n, R), and SO(n, R) as\na subgroup of O(n, R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ~ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all gi, 92, 95,95 € G,\n\n(1) IfgN =g,N and gN = 95N, then gigoN = gig5N, and\n(2) If g.N = goN, then g7'N = go N.\n\nAs a consequence, we can define a group structure on the set G/ ~ of equivalence classes\nmodulo ~, by setting\n\n(mN)(g2N) = (giga)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(mN)(gN) =(ng)N, 1,92€G\n\nis denoted G/N, and called the quotient of G by N. The equivalence class gN of an element\ng € G is also denoted g (or [g]). The map 7: G > G/N given by\n\n™(9)=9=9N\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups yp: G > G\", the groups G/Ker vy and\nIm y = ¢(G) are isomorphic.\n\nProof. Since y is surjective onto its image, we may assume that y is surjective, so that\nG’ = Im y. We define a map G: G/Ker y > G’ as follows:\n\nAM=v(9), GEG.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = g Ker y, and that it is a homomorphism. If g’ is another element in\nthe coset g Ker y, which means that g’ = gh for some h € Ker y, then\n\n9(9') = v(gh) = v(g)y(h) = vlg)e’ = ¥(9),\n\n\n\n\n32 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nsince ϕ(h) = e′ as h ∈ Ker ϕ. This shows that\n\nϕ(g′) = ϕ(g′) = ϕ(g) = ϕ(g),\n\nso the map ϕ is well defined. It is a homomorphism because\n\nϕ(gg′) = ϕ(gg′)\n\n= ϕ(gg′)\n\n= ϕ(g)ϕ(g′)\n\n= ϕ(g)ϕ(g′).\n\nThe map ϕ is injective because ϕ(g) = e′ iff ϕ(g) = e′ iff g ∈ Ker ϕ, iff g = e. The map ϕ\nis surjective because ϕ is surjective. Therefore ϕ is a bijective homomorphism, and thus an\nisomorphism, as claimed.\n\nProposition 2.11 is called the first isomorphism theorem.\n\nA useful way to construct groups is the direct product construction.\n\nDefinition 2.12. Given two groups G an H, we let G×H be the Cartestian product of the\nsets G and H with the multiplication operation · given by\n\n(g1, h1) · (g2, h2) = (g1g2, h1h2).\n\nIt is immediately verified that G×H is a group called the direct product of G and H.\n\nSimilarly, given any n groups G1, . . . , Gn, we can define the direct product G1× · · ·×Gn\n\nis a similar way.\n\nIf G is an abelian group and H1, . . . , Hn are subgroups of G, the situation is simpler.\nConsider the map\n\na : H1 × · · · ×Hn → G\n\ngiven by\na(h1, . . . , hn) = h1 + · · ·+ hn,\n\nusing + for the operation of the group G. It is easy to verify that a is a group homomorphism,\nso its image is a subgroup of G denoted by H1 + · · ·+Hn, and called the sum of the groups\nHi. The following proposition will be needed.\n\nProposition 2.12. Given an abelian group G, if H1 and H2 are any subgroups of G such\nthat H1 ∩H2 = {0}, then the map a is an isomorphism\n\na : H1 ×H2 → H1 +H2.\n\nProof. The map is surjective by definition, so we just have to check that it is injective. For\nthis, we show that Ker a = {(0, 0)}. We have a(a1, a2) = 0 iff a1 + a2 = 0 iff a1 = −a2. Since\na1 ∈ H1 and a2 ∈ H2, we see that a1, a2 ∈ H1 ∩H2 = {0}, so a1 = a2 = 0, which proves that\nKer a = {(0, 0)}.\n\n\n\n2.2. CYCLIC GROUPS 33\n\nUnder the conditions of Proposition 2.12, namely H1 ∩H2 = {0}, the group H1 + H2 is\ncalled the direct sum of H1 and H2; it is denoted by H1 ⊕H2, and we have an isomorphism\nH1 ×H2\n\n∼= H1 ⊕H2.\n\n2.2 Cyclic Groups\n\nGiven a group G with unit element 1, for any element g ∈ G and for any natural number\nn ∈ N, define gn as follows:\n\ng0 = 1\n\ngn+1 = g · gn.\n\nFor any integer n ∈ Z, we define gn by\n\ngn =\n\n{\ngn if n ≥ 0\n\n(g−1)(−n) if n < 0.\n\nThe following properties are easily verified:\n\ngi · gj = gi+j\n\n(gi)−1 = g−i\n\ngi · gj = gj · gi,\n\nfor all i, j ∈ Z.\n\nDefine the subset 〈g〉 of G by\n\n〈g〉 = {gn | n ∈ Z}.\n\nThe following proposition is left as an exercise.\n\nProposition 2.13. Given a group G, for any element g ∈ G, the set 〈g〉 is the smallest\nabelian subgroup of G containing g.\n\nDefinition 2.13. A group G is cyclic iff there is some element g ∈ G such that G = 〈g〉.\nAn element g ∈ G with this property is called a generator of G.\n\nThe Klein four group V of Example 2.2 is abelian, but not cyclic. This is because V has\nfour elements, but all the elements different from the identity have order 2.\n\nCyclic groups are quotients of Z. For this, we use a basic property of Z. Recall that for\nany n ∈ Z, we let nZ denote the set of multiples of n,\n\nnZ = {nk | k ∈ Z}.\n\n\n\n34 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.14. Every subgroup H of Z is of the form H = nZ for some n ∈ N.\n\nProof. If H is the trivial group {0}, then let n = 0. If H is nontrivial, for any nonzero element\nm ∈ H, we also have −m ∈ H and either m or −m is positive, so let n be the smallest\npositive integer in H. By Proposition 2.13, nZ is the smallest subgroup of H containing n.\nFor any m ∈ H with m 6= 0, we can write\n\nm = nq + r, with 0 ≤ r < n.\n\nNow, since nZ ⊆ H, we have nq ∈ H, and since m ∈ H, we get r = m− nq ∈ H. However,\n0 ≤ r < n, contradicting the minimality of n, so r = 0, and H = nZ.\n\nGiven any cyclic group G, for any generator g of G, we can define a mapping ϕ : Z→ G\nby ϕ(m) = gm. Since g generates G, this mapping is surjective. The mapping ϕ is clearly a\ngroup homomorphism, so let H = Kerϕ be its kernel. By a previous observation, H = nZ\nfor some n ∈ Z, so by the first homomorphism theorem, we obtain an isomorphism\n\nϕ : Z/nZ −→ G\n\nfrom the quotient group Z/nZ onto G. Obviously, if G has finite order, then |G| = n. In\nsummary, we have the following result.\n\nProposition 2.15. Every cyclic group G is either isomorphic to Z, or to Z/nZ, for some\nnatural number n > 0. In the first case, we say that G is an infinite cyclic group, and in the\nsecond case, we say that G is a cyclic group of order n.\n\nThe quotient group Z/nZ consists of the cosets m+nZ = {m+nk | k ∈ Z}, with m ∈ Z,\nthat is, of the equivalence classes of Z under the equivalence relation ≡ defined such that\n\nx ≡ y iff x− y ∈ nZ iff x ≡ y (mod n).\n\nWe also denote the equivalence class x + nZ of x by x, or if we want to be more precise by\n[x]n. The group operation is given by\n\nx+ y = x+ y.\n\nFor every x ∈ Z, there is a unique representative, x mod n (the nonnegative remainder of\nthe division of x by n) in the class x of x, such that 0 ≤ x mod n ≤ n − 1. For this\nreason, we often identity Z/nZ with the set {0, . . . , n−1}. To be more rigorous, we can give\n{0, . . . , n− 1} a group structure by defining +n such that\n\nx+n y = (x+ y) mod n.\n\nThen, it is easy to see that {0, . . . , n − 1} with the operation +n is a group with identity\nelement 0 isomorphic to Z/nZ.\n\n\n\n2.2. CYCLIC GROUPS 35\n\nWe can also define a multiplication operation · on Z/nZ as follows:\n\na · b = ab = ab mod n.\n\nThen, it is easy to check that · is abelian, associative, that 1 is an identity element for ·, and\nthat · is distributive on the left and on the right with respect to addition. This makes Z/nZ\ninto a commutative ring . We usually suppress the dot and write a b instead of a · b.\n\nProposition 2.16. Given any integer n ≥ 1, for any a ∈ Z, the residue class a ∈ Z/nZ is\ninvertible with respect to multiplication iff gcd(a, n) = 1.\n\nProof. If a has inverse b in Z/nZ, then a b = 1, which means that\n\nab ≡ 1 (mod n),\n\nthat is ab = 1 + nk for some k ∈ Z, which is the Bezout identity\n\nab− nk = 1\n\nand implies that gcd(a, n) = 1. Conversely, if gcd(a, n) = 1, then by Bezout’s identity there\nexist u, v ∈ Z such that\n\nau+ nv = 1,\n\nso au = 1− nv, that is,\n\nau ≡ 1 (mod n),\n\nwhich means that a u = 1, so a is invertible in Z/nZ.\n\nDefinition 2.14. The group (under multiplication) of invertible elements of the ring Z/nZ\nis denoted by (Z/nZ)∗. Note that this group is abelian and only defined if n ≥ 2.\n\nThe Euler ϕ-function plays an important role in the theory of the groups (Z/nZ)∗.\n\nDefinition 2.15. Given any positive integer n ≥ 1, the Euler ϕ-function (or Euler totient\nfunction) is defined such that ϕ(n) is the number of integers a, with 1 ≤ a ≤ n, which are\nrelatively prime to n; that is, with gcd(a, n) = 1.1\n\nThen, by Proposition 2.16, we see that the group (Z/nZ)∗ has order ϕ(n).\n\nFor n = 2, (Z/2Z)∗ = {1}, the trivial group. For n = 3, (Z/3Z)∗ = {1, 2}, and for\nn = 4, we have (Z/4Z)∗ = {1, 3}. Both groups are isomorphic to the group {−1, 1}. Since\ngcd(a, n) = 1 for every a ∈ {1, . . . , n − 1} iff n is prime, by Proposition 2.16 we see that\n(Z/nZ)∗ = Z/nZ− {0} iff n is prime.\n\n1We allow a = n to accomodate the special case n = 1.\n\n\n\n36 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2.3 Rings and Fields\n\nThe groups Z,Q,R, C, Z/nZ, and Mn(R) are more than abelian groups, they are also\ncommutative rings. Furthermore, Q, R, and C are fields. We now introduce rings and fields.\n\nDefinition 2.16. A ring is a set A equipped with two operations +: A × A → A (called\naddition) and ∗ : A× A→ A (called multiplication) having the following properties:\n\n(R1) A is an abelian group w.r.t. +;\n\n(R2) ∗ is associative and has an identity element 1 ∈ A;\n\n(R3) ∗ is distributive w.r.t. +.\n\nThe identity element for addition is denoted 0, and the additive inverse of a ∈ A is\ndenoted by −a. More explicitly, the axioms of a ring are the following equations which hold\nfor all a, b, c ∈ A:\n\na+ (b+ c) = (a+ b) + c (associativity of +) (2.1)\n\na+ b = b+ a (commutativity of +) (2.2)\n\na+ 0 = 0 + a = a (zero) (2.3)\n\na+ (−a) = (−a) + a = 0 (additive inverse) (2.4)\n\na ∗ (b ∗ c) = (a ∗ b) ∗ c (associativity of ∗) (2.5)\n\na ∗ 1 = 1 ∗ a = a (identity for ∗) (2.6)\n\n(a+ b) ∗ c = (a ∗ c) + (b ∗ c) (distributivity) (2.7)\n\na ∗ (b+ c) = (a ∗ b) + (a ∗ c) (distributivity) (2.8)\n\nThe ring A is commutative if\n\na ∗ b = b ∗ a for all a, b ∈ A.\n\nFrom (2.7) and (2.8), we easily obtain\n\na ∗ 0 = 0 ∗ a = 0 (2.9)\n\na ∗ (−b) = (−a) ∗ b = −(a ∗ b). (2.10)\n\nNote that (2.9) implies that if 1 = 0, then a = 0 for all a ∈ A, and thus, A = {0}. The\nring A = {0} is called the trivial ring . A ring for which 1 6= 0 is called nontrivial . The\nmultiplication a ∗ b of two elements a, b ∈ A is often denoted by ab.\n\nExample 2.6.\n\n1. The additive groups Z,Q,R,C, are commutative rings.\n\n\n\n2.3. RINGS AND FIELDS 37\n\n2. For any positive integer n ∈ N, the group Z/nZ is a group under addition. We can\nalso define a multiplication operation by\n\na · b = ab = ab mod n,\n\nfor all a, b ∈ Z. The reader will easily check that the ring axioms are satisfied, with 0\nas zero and 1 as multiplicative unit. The resulting ring is denoted by Z/nZ.2\n\n3. The group R[X] of polynomials in one variable with real coefficients is a ring under\nmultiplication of polynomials. It is a commutative ring.\n\n4. Let d be any positive integer. If d is not divisible by any integer of the form m2, with\nm ∈ N and m ≥ 2, then we say that d is square-free. For example, d = 1, 2, 3, 5, 6, 7, 10\nare square-free, but 4, 8, 9, 12 are not square-free. If d is any square-free integer and if\nd ≥ 2, then the set of real numbers\n\nZ[\n√\nd] = {a+ b\n\n√\nd ∈ R | a, b ∈ Z}\n\nis a commutative a ring. If z = a + b\n√\nd ∈ Z[\n\n√\nd], we write z = a − b\n\n√\nd. Note that\n\nzz = a2 − db2.\n\n5. Similarly, if d ≥ 1 is a positive square-free integer, then the set of complex numbers\n\nZ[\n√\n−d] = {a+ ib\n\n√\nd ∈ C | a, b ∈ Z}\n\nis a commutative ring. If z = a + ib\n√\nd ∈ Z[\n\n√\n−d], we write z = a− ib\n\n√\nd. Note that\n\nzz = a2 + db2. The case where d = 1 is a famous example that was investigated by\nGauss, and Z[\n\n√\n−1], also denoted Z[i], is called the ring of Gaussian integers .\n\n6. The group of n× n matrices Mn(R) is a ring under matrix multiplication. However, it\nis not a commutative ring.\n\n7. The group C(a, b) of continuous functions f : (a, b) → R is a ring under the operation\nf · g defined such that\n\n(f · g)(x) = f(x)g(x)\n\nfor all x ∈ (a, b).\n\nDefinition 2.17. Given a ring A, for any element a ∈ A, if there is some element b ∈ A\nsuch that b 6= 0 and ab = 0, then we say that a is a zero divisor . A ring A is an integral\ndomain (or an entire ring) if 0 6= 1, A is commutative, and ab = 0 implies that a = 0 or\nb = 0, for all a, b ∈ A. In other words, an integral domain is a nontrivial commutative ring\nwith no zero divisors besides 0.\n\n2The notation Zn is sometimes used instead of Z/nZ but it clashes with the notation for the n-adic\nintegers so we prefer not to use it.\n\n\n\n38 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.7.\n\n1. The rings Z,Q,R,C, are integral domains.\n\n2. The ring R[X] of polynomials in one variable with real coefficients is an integral domain.\n\n3. For any positive integer, n ∈ N, we have the ring Z/nZ. Observe that if n is composite,\nthen this ring has zero-divisors. For example, if n = 4, then we have\n\n2 · 2 ≡ 0 (mod 4).\n\nThe reader should prove that Z/nZ is an integral domain iff n is prime (use Proposition\n2.16).\n\n4. If d is a square-free positive integer and if d ≥ 2, the ring Z[\n√\nd] is an integral domain.\n\nSimilarly, if d ≥ 1 is a square-free positive integer, the ring Z[\n√\n−d] is an integral\n\ndomain. Finding the invertible elements of these rings is a very interesting problem.\n\n5. The ring of n× n matrices Mn(R) has zero divisors.\n\nA homomorphism between rings is a mapping preserving addition and multiplication\n(and 0 and 1).\n\nDefinition 2.18. Given two rings A and B, a homomorphism between A and B is a function\nh : A→ B satisfying the following conditions for all x, y ∈ A:\n\nh(x+ y) = h(x) + h(y)\n\nh(xy) = h(x)h(y)\n\nh(0) = 0\n\nh(1) = 1.\n\nActually, because B is a group under addition, h(0) = 0 follows from\n\nh(x+ y) = h(x) + h(y).\n\nExample 2.8.\n\n1. If A is a ring, for any integer n ∈ Z, for any a ∈ A, we define n · a by\n\nn · a = a+ · · ·+ a︸ ︷︷ ︸\nn\n\nif n ≥ 0 (with 0 · a = 0) and\nn · a = −(−n) · a\n\nif n < 0. Then, the map h : Z→ A given by\n\nh(n) = n · 1A\nis a ring homomorphism (where 1A is the multiplicative identity of A).\n\n\n\n2.3. RINGS AND FIELDS 39\n\n2. Given any real λ ∈ R, the evaluation map ηλ : R[X]→ R defined by\n\nηλ(f(X)) = f(λ)\n\nfor every polynomial f(X) ∈ R[X] is a ring homomorphism.\n\nDefinition 2.19. A ring homomorphism h : A → B is an isomorphism iff there is a ring\nhomomorphism g : B → A such that g ◦ f = idA and f ◦ g = idB. An isomorphism from a\nring to itself is called an automorphism.\n\nAs in the case of a group isomorphism, the homomorphism g is unique and denoted by\nh−1, and it is easy to show that a bijective ring homomorphism h : A→ B is an isomorphism.\n\nDefinition 2.20. Given a ring A, a subset A′ of A is a subring of A if A′ is a subgroup of\nA (under addition), is closed under multiplication, and contains 1.\n\nFor example, we have the following sequence in which every ring on the left of an inlcusion\nsign is a subring of the ring on the right of the inclusion sign:\n\nZ ⊆ Q ⊆ R ⊆ C.\n\nThe ring Z is a subring of both Z[\n√\nd] and Z[\n\n√\n−d], the ring Z[\n\n√\nd] is a subring of R and the\n\nring Z[\n√\n−d] is a subring of C.\n\nIf h : A→ B is a homomorphism of rings, then it is easy to show for any subring A′, the\nimage h(A′) is a subring of B, and for any subring B′ of B, the inverse image h−1(B′) is a\nsubring of A.\n\nAs for groups, the kernel of a ring homomorphism h : A→ B is defined by\n\nKer h = {a ∈ A | h(a) = 0}.\n\nJust as in the case of groups, we have the following criterion for the injectivity of a ring\nhomomorphism. The proof is identical to the proof for groups.\n\nProposition 2.17. If h : A → B is a homomorphism of rings, then h : A → B is injective\niff Ker h = {0}. (We also write Ker h = (0).)\n\nThe kernel of a ring homomorphism is an abelian subgroup of the additive group A, but\nin general it is not a subring of A, because it may not contain the multiplicative identity\nelement 1. However, it satisfies the following closure property under multiplication:\n\nab ∈ Ker h and ba ∈ Ker h for all a ∈ Ker h and all b ∈ A.\n\nThis is because if h(a) = 0, then for all b ∈ A we have\n\nh(ab) = h(a)h(b) = 0h(b) = 0 and h(ba) = h(b)h(a) = h(b)0 = 0.\n\n\n\n40 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.21. Given a ring A, an additive subgroup I of A satisfying the property below\n\nab ∈ I and ba ∈ I for all a ∈ I and all b ∈ A (∗ideal)\n\nis called a two-sided ideal . If A is a commutative ring, we simply say an ideal .\n\nIt turns out that for any ring A and any two-sided ideal I, the set A/I of additive cosets\na + I (with a ∈ A) is a ring called a quotient ring . Then we have the following analog of\nProposition 2.11, also called the first isomorphism theorem.\n\nProposition 2.18. Given a homomorphism of rings h : A → B, the rings A/Ker h and\nIm h = h(A) are isomorphic.\n\nA field is a commutative ring K for which K − {0} is a group under multiplication.\n\nDefinition 2.22. A set K is a field if it is a ring and the following properties hold:\n\n(F1) 0 6= 1;\n\n(F2) K∗ = K − {0} is a group w.r.t. ∗ (i.e., every a 6= 0 has an inverse w.r.t. ∗);\n\n(F3) ∗ is commutative.\n\nIf ∗ is not commutative but (F1) and (F2) hold, we say that we have a skew field (or\nnoncommutative field).\n\nNote that we are assuming that the operation ∗ of a field is commutative. This convention\nis not universally adopted, but since ∗ will be commutative for most fields we will encounter,\nwe may as well include this condition in the definition.\n\nExample 2.9.\n\n1. The rings Q, R, and C are fields.\n\n2. The set of (formal) fractions f(X)/g(X) of polynomials f(X), g(X) ∈ R[X], where\ng(X) is not the null polynomial, is a field.\n\n3. The ring C(a, b) of continuous functions f : (a, b) → R such that f(x) 6= 0 for all\nx ∈ (a, b) is a field.\n\n4. Using Proposition 2.16, it is easy to see that the ring Z/pZ is a field iff p is prime.\n\n5. If d is a square-free positive integer and if d ≥ 2, the set\n\nQ(\n√\nd) = {a+ b\n\n√\nd ∈ R | a, b ∈ Q}\n\nis a field. If z = a + b\n√\nd ∈ Q(\n\n√\nd) and z = a − b\n\n√\nd, then it is easy to check that if\n\nz 6= 0, then z−1 = z/(zz).\n\n\n\n2.3. RINGS AND FIELDS 41\n\n6. Similarly, If d ≥ 1 is a square-free positive integer, the set of complex numbers\n\nQ(\n√\n−d) = {a+ ib\n\n√\nd ∈ C | a, b ∈ Q}\n\nis a field. If z = a + ib\n√\nd ∈ Q(\n\n√\n−d) and z = a− ib\n\n√\nd, then it is easy to check that\n\nif z 6= 0, then z−1 = z/(zz).\n\nDefinition 2.23. A homomorphism h : K1 → K2 between two fields K1 and K2 is just a\nhomomorphism between the rings K1 and K2.\n\nHowever, because K∗1 and K∗2 are groups under multiplication, a homomorphism of fields\nmust be injective.\n\nProof. First, observe that for any x 6= 0,\n\n1 = h(1) = h(xx−1) = h(x)h(x−1)\n\nand\n1 = h(1) = h(x−1x) = h(x−1)h(x),\n\nso h(x) 6= 0 and\nh(x−1) = h(x)−1.\n\nBut then, if h(x) = 0, we must have x = 0. Consequently, h is injective.\n\nDefinition 2.24. A field homomorphism h : K1 → K2 is an isomorphism iff there is a\nhomomorphism g : K2 → K1 such that g ◦ f = idK1 and f ◦ g = idK2 . An isomorphism from\na field to itself is called an automorphism.\n\nThen, just as in the case of rings, g is unique and denoted by h−1, and a bijective field\nhomomorphism h : K1 → K2 is an isomorphism.\n\nDefinition 2.25. Since every homomorphism h : K1 → K2 between two fields is injective,\nthe image f(K1) of K1 is a subfield of K2. We say that K2 is an extension of K1.\n\nFor example, R is an extension of Q and C is an extension of R. The fields Q(\n√\nd) and\n\nQ(\n√\n−d) are extensions of Q, the field R is an extension of Q(\n\n√\nd) and the field C is an\n\nextension of Q(\n√\n−d).\n\nDefinition 2.26. A field K is said to be algebraically closed if every polynomial p(X) with\ncoefficients in K has some root in K; that is, there is some a ∈ K such that p(a) = 0.\n\nIt can be shown that every field K has some minimal extension Ω which is algebraically\nclosed, called an algebraic closure of K. For example, C is the algebraic closure of R. The\nalgebraic closure of Q is called the field of algebraic numbers . This field consists of all\ncomplex numbers that are zeros of a polynomial with coefficients in Q.\n\n\n\n42 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.27. Given a field K and an automorphism h : K → K of K, it is easy to check\nthat the set\n\nFix(h) = {a ∈ K | h(a) = a}\nof elements of K fixed by h is a subfield of K called the field fixed by h.\n\nFor example, if d ≥ 2 is square-free, then the map c : Q(\n√\nd)→ Q(\n\n√\nd) given by\n\nc(a+ b\n√\nd) = a− b\n\n√\nd\n\nis an automorphism of Q(\n√\nd), and Fix(c) = Q.\n\nIf K is a field, we have the ring homomorphism h : Z → K given by h(n) = n · 1. If h\nis injective, then K contains a copy of Z, and since it is a field, it contains a copy of Q. In\nthis case, we say that K has characteristic 0. If h is not injective, then h(Z) is a subring of\nK, and thus an integral domain, the kernel of h is a subgroup of Z, which by Proposition\n2.14 must be of the form pZ for some p ≥ 1. By the first isomorphism theorem, h(Z) is\nisomorphic to Z/pZ for some p ≥ 1. But then, p must be prime since Z/pZ is an integral\ndomain iff it is a field iff p is prime. The prime p is called the characteristic of K, and we\nalso says that K is of finite characteristic.\n\nDefinition 2.28. If K is a field, then either\n\n(1) n · 1 6= 0 for all integer n ≥ 1, in which case we say that K has characteristic 0, or\n\n(2) There is some smallest prime number p such that p · 1 = 0 called the characteristic of\nK, and we say K is of finite characteristic.\n\nA field K of characteristic 0 contains a copy of Q, thus is infinite. As we will see in\nSection 8.10, a finite field has nonzero characteristic p. However, there are infinite fields of\nnonzero characteristic.\n\n\n\nPart I\n\nLinear Algebra\n\n43\n\n\n\n\n\n\n\nChapter 3\n\nVector Spaces, Bases, Linear Maps\n\n3.1 Motivations: Linear Combinations, Linear Inde-\n\npendence and Rank\n\nIn linear optimization problems, we often encounter systems of linear equations. For example,\nconsider the problem of solving the following system of three linear equations in the three\nvariables x1, x2, x3 ∈ R:\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3.\n\nOne way to approach this problem is introduce the “vectors” u, v, w, and b, given by\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−2\n\n w =\n\n−1\n1\n−2\n\n b =\n\n1\n2\n3\n\n\nand to write our linear system as\n\nx1u+ x2v + x3w = b.\n\nIn the above equation, we used implicitly the fact that a vector z can be multiplied by a\nscalar λ ∈ R, where\n\nλz = λ\n\nz1\n\nz2\n\nz3\n\n =\n\nλz1\n\nλz2\n\nλz3\n\n ,\n\nand two vectors y and and z can be added, where\n\ny + z =\n\ny1\n\ny2\n\ny3\n\n+\n\nz1\n\nz2\n\nz3\n\n =\n\ny1 + z1\n\ny2 + z2\n\ny3 + z3\n\n .\n\n45\n\n\n\n46 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAlso, given a vector\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\nwe define the additive inverse −x of x (pronounced minus x) as\n\n−x =\n\n−x1\n\n−x2\n\n−x3\n\n .\n\nObserve that −x = (−1)x, the scalar multiplication of x by −1.\n\nThe set of all vectors with three components is denoted by R3×1. The reason for using\nthe notation R3×1 rather than the more conventional notation R3 is that the elements of\nR3×1 are column vectors ; they consist of three rows and a single column, which explains the\nsuperscript 3 × 1. On the other hand, R3 = R × R × R consists of all triples of the form\n(x1, x2, x3), with x1, x2, x3 ∈ R, and these are row vectors . However, there is an obvious\nbijection between R3×1 and R3 and they are usually identified. For the sake of clarity, in\nthis introduction, we will denote the set of column vectors with n components by Rn×1.\n\nAn expression such as\nx1u+ x2v + x3w\n\nwhere u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using\nthis notion, the problem of solving our linear system\n\nx1u+ x2v + x3w = b.\n\nis equivalent to determining whether b can be expressed as a linear combination of u, v, w.\n\nNow if the vectors u, v, w are linearly independent , which means that there is no triple\n(x1, x2, x3) 6= (0, 0, 0) such that\n\nx1u+ x2v + x3w = 03,\n\nit can be shown that every vector in R3×1 can be written as a linear combination of u, v, w.\nHere, 03 is the zero vector\n\n03 =\n\n0\n0\n0\n\n .\n\nIt is customary to abuse notation and to write 0 instead of 03. This rarely causes a problem\nbecause in most cases, whether 0 denotes the scalar zero or the zero vector can be inferred\nfrom the context.\n\nIn fact, every vector z ∈ R3×1 can be written in a unique way as a linear combination\n\nz = x1u+ x2v + x3w.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nz = x1u+ x2v + x3w = y1u+ y2v + y3w,\n\nthen by using our (linear!) operations on vectors, we get\n\n(y1 − x1)u+ (y2 − x2)v + (y3 − x3)w = 0,\n\nwhich implies that\ny1 − x1 = y2 − x2 = y3 − x3 = 0,\n\nby linear independence. Thus,\n\ny1 = x1, y2 = x2, y3 = x3,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\n\nx1u+ x2v + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nx1 = 1.4\n\nx2 = −0.4\n\nx3 = −0.4\n\nis the solution.\n\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u, v, w), called the determinant of\n(u, v, w), and to check that it is nonzero. In our case, it turns out that\n\ndet(u, v, w) =\n\n∣∣∣∣∣∣\n1 2 −1\n2 1 1\n1 −2 −2\n\n∣∣∣∣∣∣ = 15,\n\nwhich confirms that u, v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\nA =\n(\nu v w\n\n)\n=\n\n1 2 −1\n2 1 1\n1 −2 −2\n\n .\n\nIf we form the vector of unknowns\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nZz XU LQV + U3W = YU + You + Y3U,\n\nthen by using our (linear!) operations on vectors, we get\n(yi — t1)u + (yo — t2)u + (y3 — Z3)w = 0,\n\nwhich implies that\nYi — Ly = Yo — Lo = y3 — ®3 = O,\n\nby linear independence. Thus,\n\nYF, Yor, Y3>= %,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\nrut rv + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nLy = 1.4\nt= —0.4\nL3 = —0.4\n\nis the solution.\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u,v,w), called the determinant of\n(u,v, w), and to check that it is nonzero. In our case, it turns out that\n\n1 2 -!1\ndet(u,v,w) =|2 1 1)/=15,\n1 -2 -2\n\nwhich confirms that u,v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\n1 2 -1\nA=(u v w) = 2 1 1\n1 —2 —2\n\nIf we form the vector of unknowns\n\n\n\n\n48 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nthen our linear combination x1u+ x2v + x3w can be written in matrix form as\n\nx1u+ x2v + x3w =\n\n1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n ,\n\nso our linear system is expressed by1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n =\n\n1\n2\n3\n\n ,\n\nor more concisely as\nAx = b.\n\nNow what if the vectors u, v, w are linearly dependent? For example, if we consider the\nvectors\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−1\n\n w =\n\n−1\n1\n2\n\n ,\n\nwe see that\nu− v = w,\n\na nontrivial linear dependence. It can be verified that u and v are still linearly independent.\nNow for our problem\n\nx1u+ x2v + x3w = b\n\nit must be the case that b can be expressed as linear combination of u and v. However,\nit turns out that u, v, b are linearly independent (one way to see this is to compute the\ndeterminant det(u, v, b) = −6), so b cannot be expressed as a linear combination of u and v\nand thus, our system has no solution.\n\nIf we change the vector b to\n\nb =\n\n3\n3\n0\n\n ,\n\nthen\nb = u+ v,\n\nand so the system\nx1u+ x2v + x3w = b\n\nhas the solution\nx1 = 1, x2 = 1, x3 = 0.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK49\n\nActually, since w = u− v, the above system is equivalent to\n\n(x1 + x3)u+ (x2 − x3)v = b,\n\nand because u and v are linearly independent, the unique solution in x1 + x3 and x2 − x3 is\n\nx1 + x3 = 1\n\nx2 − x3 = 1,\n\nwhich yields an infinite number of solutions parameterized by x3, namely\n\nx1 = 1− x3\n\nx2 = 1 + x3.\n\nIn summary, a 3× 3 linear system may have a unique solution, no solution, or an infinite\nnumber of solutions, depending on the linear independence (and dependence) or the vectors\nu, v, w, b. This situation can be generalized to any n × n system, and even to any n × m\nsystem (n equations in m variables), as we will see later.\n\nThe point of view where our linear system is expressed in matrix form as Ax = b stresses\nthe fact that the map x 7→ Ax is a linear transformation. This means that\n\nA(λx) = λ(Ax)\n\nfor all x ∈ R3×1 and all λ ∈ R and that\n\nA(u+ v) = Au+ Av,\n\nfor all u, v ∈ R3×1. We can view the matrix A as a way of expressing a linear map from R3×1\n\nto R3×1 and solving the system Ax = b amounts to determining whether b belongs to the\nimage of this linear map.\n\nGiven a 3× 3 matrix\n\nA =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n ,\n\nwhose columns are three vectors denoted A1, A2, A3, and given any vector x = (x1, x2, x3),\nwe defined the product Ax as the linear combination\n\nAx = x1A\n1 + x2A\n\n2 + x3A\n3 =\n\na11x1 + a12x2 + a13x3\n\na21x1 + a22x2 + a23x3\n\na31x1 + a32x2 + a33x3\n\n .\n\nThe common pattern is that the ith coordinate of Ax is given by a certain kind of product\ncalled an inner product , of a row vector , the ith row of A, times the column vector x:\n\n(\nai1 ai2 ai3\n\n)\n·\n\nx1\n\nx2\n\nx3\n\n = ai1x1 + ai2x2 + ai3x3.\n\n\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn) ∈ Rn, their\ninner product denoted x · y, or 〈x, y〉, is the number\n\nx · y =\n(\nx1 x2 · · · xn\n\n)\n·\n\n\ny1\n\ny2\n...\nyn\n\n =\nn∑\ni=1\n\nxiyi.\n\nInner products play a very important role. First, we quantity\n\n‖x‖2 =\n√\nx · x = (x2\n\n1 + · · ·+ x2\nn)1/2\n\nis a generalization of the length of a vector, called the Euclidean norm, or `2-norm. Second,\nit can be shown that we have the inequality\n\n|x · y| ≤ ‖x‖ ‖y‖ ,\n\nso if x, y 6= 0, the ratio (x · y)/(‖x‖ ‖y‖) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if x · y = 0 then the vectors x and y make the angle π/2,\nthat is, they are orthogonal . The (square) matrices Q that preserve the inner product, in\nthe sense that 〈Qx,Qy〉 = 〈x, y〉 for all x, y ∈ Rn, also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m × n matrix consisting of n columns A1, . . . , An (in\nRm), and B is a n× p matrix consisting of p columns B1, . . . , Bp (in Rn) we can form the p\nvectors (in Rm)\n\nAB1, . . . , ABp.\n\nThese p vectors constitute the m × p matrix denoted AB, whose jth column is ABj. But\nwe know that the ith coordinate of ABj is the inner product of the ith row of A by the jth\ncolumn of B,\n\n(\nai1 ai2 · · · ain\n\n)\n·\n\n\nb1j\n\nb2j\n...\nbnj\n\n =\nn∑\nk=1\n\naikbkj.\n\nThus we have defined a multiplication operation on matrices, namely if A = (aik) is a m×n\nmatrix and if B = (bjk) if n× p matrix, then their product AB is the m× n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\n(AB)ij =\nn∑\nk=1\n\naikbkj.\n\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are two n×n\nmatrices, in general, AB 6= BA.\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors 7 = (#1,...,%n,) and y = (y1,.--,Yn) € R”, their\ninner product denoted x - y, or (x,y), is the number\n\nY1\ny n\nvey = (a TQ 7° In) * ° = So xiyi.\n. i=l\nYn\n\nInner products play a very important role. First, we quantity\nvp = Vee = (ap +--+ 27)'?\n\nis a generalization of the length of a vector, called the Euclidean norm, or (?-norm. Second,\nit can be shown that we have the inequality\n\nIz-y| < fall ly,\n\nso if x,y £0, the ratio (x - y)/(||2|| ||y||) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if «-y = 0 then the vectors x and y make the angle 7/2,\nthat is, they are orthogonal. The (square) matrices Q that preserve the inner product, in\nthe sense that (Qz, Qy) = (x,y) for all x,y € R\", also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m X n matrix consisting of n columns A!,...,A” (in\nR™), and B is an Xx p matrix consisting of p columns B',...,B? (in R\") we can form the p\nvectors (in R”)\n\nAB',..., ABP.\n\nThese p vectors constitute the m x p matrix denoted AB, whose jth column is AB’. But\nwe know that the ith coordinate of AB is the inner product of the ith row of A by the jth\ncolumn of B,\n\nbi;\nn\nbo; _\n(ait aig \"7° Gin) * . =) i¢dp;-\n, k=1\nDnj\n\nThus we have defined a multiplication operation on matrices, namely if A = (a;,) isamxn\nmatrix and if B = (b;,) if n x p matrix, then their product AB is the m x n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\nn\n\n(AB);; = Ss\" indy; -\n\nk=1\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are twon xn\nmatrices, in general, AB 4 BA.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK51\n\nSuppose that A is an n× n matrix and that we are trying to solve the linear system\n\nAx = b,\n\nwith b ∈ Rn. Suppose we can find an n× n matrix B such that\n\nBAi = ei, i = 1, . . . , n,\n\nwith ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith slot. If we form\nthe n× n matrix\n\nIn =\n\n\n\n1 0 0 · · · 0 0\n0 1 0 · · · 0 0\n0 0 1 · · · 0 0\n...\n\n...\n...\n\n. . .\n...\n\n...\n0 0 0 · · · 1 0\n0 0 0 · · · 0 1\n\n\n,\n\ncalled the identity matrix , whose ith column is ei, then the above is equivalent to\n\nBA = In.\n\nIf Ax = b, then multiplying both sides on the left by B, we get\n\nB(Ax) = Bb.\n\nBut is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have\n\nx = Bb.\n\nWe can verify that x = Bb is indeed a solution, because it can be shown that\n\nA(Bb) = (AB)b = Inb = b.\n\nWhat is not obvious is that BA = In implies AB = In, but this is indeed provable. The\nmatrix B is usually denoted A−1 and called the inverse of A. It can be shown that it is the\nunique matrix such that\n\nAA−1 = A−1A = In.\n\nIf a square matrix A has an inverse, then we say that it is invertible or nonsingular , otherwise\nwe say that it is singular . We will show later that a square matrix is invertible iff its columns\nare linearly independent iff its determinant is nonzero.\n\nIn summary, if A is a square invertible matrix, then the linear system Ax = b has the\nunique solution x = A−1b. In practice, this is not a good way to solve a linear system because\ncomputing A−1 is too expensive. A practical method for solving a linear system is Gaussian\nelimination, discussed in Chapter 8. Other practical methods for solving a linear system\n\n\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m × n matrix A = (akl), the n × m matrix A> = (a>ij) whose ith row is the\nith column of A, which means that a>ij = aji for i = 1, . . . , n and j = 1, . . . ,m, is called the\ntranspose of A. An n× n matrix Q such that\n\nQQ> = Q>Q = In\n\nis called an orthogonal matrix . Equivalently, the inverse Q−1 of an orthogonal matrix Q is\nequal to its transpose Q>. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m× n matrix A can be written as\n\nA = V ΣU>,\n\nwhere V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and Σ is an m×n\nmatrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ · · · ≥ σp,\nwhere p = min(m,n), called the singular values of A. The factorization A = V ΣU> is called\na singular decomposition of A, or SVD .\n\nThe SVD can be used to “solve” a linear system Ax = b where A is an m × n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m > n) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax − b. Legendre and Gauss used ‖Ax− b‖2\n\n2, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\nx+ of minimum Euclidean norm that minimizes ‖Ax− b‖2\n\n2. Furthermore, x+ is given by the\nexpression x+ = A+b, where A+ is the pseudo-inverse of A, and A+ can be computed from\nan SVD A = V ΣU> of A. Indeed, A+ = UΣ+V >, where Σ+ is the matrix obtained from Σ\nby replacing every positive singular value σi by its inverse σ−1\n\ni , leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ‖Ax− b‖2\n2, we\n\ncan add the penalty term K ‖x‖2\n2 (for some positive K > 0) to ‖Ax− b‖2\n\n2 and minimize the\nquantity ‖Ax− b‖2\n\n2 + K ‖x‖2\n2. This approach is called ridge regression. It turns out that\n\nthere is a unique minimizer x+ given by x+ = (A>A + KIn)−1A>b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K ‖x‖2\n2 by K ‖x‖1, where ‖x‖1 = |x1|+\n\n· · · + |xn| (the `1-norm of x). The remarkable fact is that the minimizers x of ‖Ax− b‖2\n2 +\n\nK ‖x‖1 tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m x n matrix A = (aq), the n x m matrix A‘ = (a,;) whose ith row is the\nith column of A, which means that aj, =a; fori =1,...,n and j =1,...,m, is called the\ntranspose of A. An n X n matrix Q such that\n\nQQ'=Q'Q=In\n\nis called an orthogonal matrix. Equivalently, the inverse Q~! of an orthogonal matrix Q is\nequal to its transpose Q'. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m x n matrix A can be written as\n\nA=VSUT,\n\nwhere V is an m xX m orthogonal matrix, U is an n x n orthogonal matrix, and © is anm xn\nmatrix whose only nonzero entries are nonnegative diagonal entries 0) > 02 > ++: > Op,\nwhere p = min(m, n), called the singular values of A. The factorization A = VXU' is called\na singular decomposition of A, or SVD.\n\nThe SVD can be used to “solve” a linear system Ax = 6 where A is an m x n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m >) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax — b. Legendre and Gauss used ||Aa — ||}, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\na+ of minimum Euclidean norm that minimizes || Ax — ||}. Furthermore, «+ is given by the\nexpression xt = Atb, where At is the pseudo-inverse of A, and At can be computed from\nan SVD A=VU' of A. Indeed, At = UXNt+V', where ¥*+ is the matrix obtained from Y\nby replacing every positive singular value o; by its inverse a; ', leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ||Ax — 6||5, we\ncan add the penalty term K ||z||3 (for some positive K > 0) to || Ax — ||} and minimize the\nquantity || Ax — b||} + K ||a||3. This approach is called ridge regression. It turns out that\nthere is a unique minimizer 2+ given by + = (A'A+ KI,)~1A'b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K |||]. by K ||a||,, where |Ja'||, = |a1| +\n--»4+|x,| (the ¢'-norm of «). The remarkable fact is that the minimizers « of || Ax — b||3 +\nK |x|, tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK53\n\nAnother important application of the SVD is principal component analysis (or PCA), an\nimportant tool in data analysis.\n\nYet another fruitful way of interpreting the resolution of the system Ax = b is to view\nthis problem as an intersection problem. Indeed, each of the equations\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3\n\ndefines a subset of R3 which is actually a plane. The first equation\n\nx1 + 2x2 − x3 = 1\n\ndefines the plane H1 passing through the three points (1, 0, 0), (0, 1/2, 0), (0, 0,−1), on the\ncoordinate axes, the second equation\n\n2x1 + x2 + x3 = 2\n\ndefines the plane H2 passing through the three points (1, 0, 0), (0, 2, 0), (0, 0, 2), on the coor-\ndinate axes, and the third equation\n\nx1 − 2x2 − 2x3 = 3\n\ndefines the plane H3 passing through the three points (3, 0, 0), (0,−3/2, 0), (0, 0,−3/2), on\nthe coordinate axes. See Figure 3.1.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\nx -2x -2x = 31 2 3\n\nFigure 3.1: The planes defined by the preceding linear equations.\n\n\n\n54 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nx -2x -2x = 31 2 3\n\n2x + x + x = 21 2 3\n\n2x + 2x - x = 11 2 3\n\n(1.4, -0.4, -0.4)\n\nFigure 3.2: The solution of the system is the point in common with each of the three planes.\n\nThe intersection Hi∩Hj of any two distinct planes Hi and Hj is a line, and the intersection\nH1 ∩H2 ∩H3 of the three planes consists of the single point (1.4,−0.4,−0.4), as illustrated\nin Figure 3.2.\n\nThe planes corresponding to the system\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − x2 + 2x3 = 3,\n\nare illustrated in Figure 3.3.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\n1 2 3\n\nx - x +2x = 31 2 3\n\nFigure 3.3: The planes defined by the equations x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, and\nx1 − x2 + 2x3 = 3.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK55\n\nThis system has no solution since there is no point simultaneously contained in all three\nplanes; see Figure 3.4.\n\n2x + 2x - x = 11 2 3\n\nx - x +2x = 31 2 3\n\n2x + x + x = 21 2 32x + x + x = 21 2 3\n\nFigure 3.4: The linear system x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, x1 − x2 + 2x3 = 3 has\nno solution.\n\nFinally, the planes corresponding to the system\n\nx1 + 2x2 − x3 = 3\n\n2x1 + x2 + x3 = 3\n\nx1 − x2 + 2x3 = 0,\n\nare illustrated in Figure 3.5.\n\n2x + 2x -  x = 3\n1\n\n1\n\n1\n\n2\n\n2 3\n\n3\n\n2x + x + x = 32 3\n\nx - x + 2x = 01 2 3\n\n1\n\nFigure 3.5: The planes defined by the equations x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, and\nx1 − x2 + 2x3 = 0.\n\n\n\n56 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis system has infinitely many solutions, given parametrically by (1 − x3, 1 + x3, x3).\nGeometrically, this is a line common to all three planes; see Figure 3.6.\n\n2x + 2x -  x = 3\n1 2 3\n\nx - x + 2x = 01 2 3\n\n12x + x + x = 32 3\n\nFigure 3.6: The linear system x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, x1 − x2 + 2x3 = 0 has\nthe red line common to all three planes.\n\nUnder the above interpretation, observe that we are focusing on the rows of the matrix\nA, rather than on its columns , as in the previous interpretations.\n\nAnother great example of a real-world problem where linear algebra proves to be very\neffective is the problem of data compression, that is, of representing a very large data set\nusing a much smaller amount of storage.\n\nTypically the data set is represented as an m× n matrix A where each row corresponds\nto an n-dimensional data point and typically, m ≥ n. In most applications, the data are not\nindependent so the rank of A is a lot smaller than min{m,n}, and the the goal of low-rank\ndecomposition is to factor A as the product of two matrices B and C, where B is a m × k\nmatrix and C is a k×n matrix, with k � min{m,n} (here,� means “much smaller than”):\n\nA\nm× n\n\n\n=\n\n\nB\n\nm× k\n\n\n C\n\nk × n\n\n\n\nNow it is generally too costly to find an exact factorization as above, so we look for a\nlow-rank matrix A′ which is a “good” approximation of A. In order to make this statement\nprecise, we need to define a mechanism to determine how close two matrices are. This can\nbe done using matrix norms , a notion discussed in Chapter 9. The norm of a matrix A is a\n\n\n\n3.2. VECTOR SPACES 57\n\nnonnegative real number ‖A‖ which behaves a lot like the absolute value |x| of a real number\nx. Then our goal is to find some low-rank matrix A′ that minimizes the norm\n\n‖A− A′‖2\n,\n\nover all matrices A′ of rank at most k, for some given k � min{m,n}.\nSome advantages of a low-rank approximation are:\n\n1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus\nless storage and fewer operations are needed to reconstruct A.\n\n2. Often, the process for obtaining the decomposition exposes the underlying structure of\nthe data. Thus, it may turn out that “most” of the significant data are concentrated\nalong some directions called principal directions .\n\nLow-rank decompositions of a set of data have a multitude of applications in engineering,\nincluding computer science (especially computer vision), statistics, and machine learning.\nAs we will see later in Chapter 23, the singular value decomposition (SVD) provides a very\nsatisfactory solution to the low-rank approximation problem. Still, in many cases, the data\nsets are so large that another ingredient is needed: randomization. However, as a first step,\nlinear algebra often yields a good initial solution.\n\nWe will now be more precise as to what kinds of operations are allowed on vectors. In\nthe early 1900, the notion of a vector space emerged as a convenient and unifying framework\nfor working with “linear” objects and we will discuss this notion in the next few sections.\n\n3.2 Vector Spaces\n\nFor every n ≥ 1, let Rn be the set of n-tuples x = (x1, . . . , xn). Addition can be extended to\nRn as follows:\n\n(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn).\n\nWe can also define an operation · : R× Rn → Rn as follows:\n\nλ · (x1, . . . , xn) = (λx1, . . . , λxn).\n\nThe resulting algebraic structure has some interesting properties, those of a vector space.\n\nHowever, keep in mind that vector spaces are not just algebraic\nobjects; they are also geometric objects.\n\nVector spaces are defined as follows.\n\n\n\n58 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nDefinition 3.1. Given a field K (with addition + and multiplication ∗), a vector space over\nK (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E\n(called vector addition),1 and · : K × E → E (called scalar multiplication) satisfying the\nfollowing conditions for all α, β ∈ K and all u, v ∈ E;\n\n(V0) E is an abelian group w.r.t. +, with identity element 0;2\n\n(V1) α · (u+ v) = (α · u) + (α · v);\n\n(V2) (α + β) · u = (α · u) + (β · u);\n\n(V3) (α ∗ β) · u = α · (β · u);\n\n(V4) 1 · u = u.\n\nIn (V3), ∗ denotes multiplication in the field K.\n\nGiven α ∈ K and v ∈ E, the element α · v is also denoted by αv. The field K is often\ncalled the field of scalars.\n\nUnless specified otherwise or unless we are dealing with several different fields, in the rest\nof this chapter, we assume that all K-vector spaces are defined with respect to a fixed field\nK. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field\nK will be the field R of reals.\n\nFrom (V0), a vector space always contains the null vector 0, and thus is nonempty.\nFrom (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From (V2), we get 0 · v = 0, and\n(−α) · v = −(α · v).\n\nAnother important consequence of the axioms is the following fact:\n\nProposition 3.1. For any u ∈ E and any λ ∈ K, if λ 6= 0 and λ · u = 0, then u = 0.\n\nProof. Indeed, since λ 6= 0, it has a multiplicative inverse λ−1, so from λ · u = 0, we get\n\nλ−1 · (λ · u) = λ−1 · 0.\n\nHowever, we just observed that λ−1 · 0 = 0, and from (V3) and (V4), we have\n\nλ−1 · (λ · u) = (λ−1λ) · u = 1 · u = u,\n\nand we deduce that u = 0.\n\n1The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E.\nIt is usually clear from the context which + is intended.\n\n2The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element\nof E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.\n\n\n\n3.2. VECTOR SPACES 59\n\nRemark: One may wonder whether axiom (V4) is really needed. Could it be derived from\nthe other axioms? The answer is no. For example, one can take E = Rn and define\n· : R× Rn → Rn by\n\nλ · (x1, . . . , xn) = (0, . . . , 0)\n\nfor all (x1, . . . , xn) ∈ Rn and all λ ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails.\nLess trivial examples can be given using the notion of a basis, which has not been defined\nyet.\n\nThe field K itself can be viewed as a vector space over itself, addition of vectors being\naddition in the field, and multiplication by a scalar being multiplication in the field.\n\nExample 3.1.\n\n1. The fields R and C are vector spaces over R.\n\n2. The groups Rn and Cn are vector spaces over R, with scalar multiplication given by\n\nλ(x1, . . . , xn) = (λx1, . . . , λxn),\n\nfor any λ ∈ R and with (x1, . . . , xn) ∈ Rn or (x1, . . . , xn) ∈ Cn, and Cn is a vector\nspace over C with scalar multiplication as above, but with λ ∈ C.\n\n3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector\nspace over R, and the ring C[X]n of polynomials of degree at most n with complex\ncoefficients is a vector space over C, with scalar multiplication λ ·P (X) of a polynomial\n\nP (X) = amX\nm + am−1X\n\nm−1 + · · ·+ a1X + a0\n\n(with ai ∈ R or ai ∈ C) by the scalar λ (in R or C), with m ≤ n, given by\n\nλ · P (X) = λamX\nm + λam−1X\n\nm−1 + · · ·+ λa1X + λa0.\n\n4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the\nring C[X] of all polynomials with complex coefficients is a vector space over C, with\nthe same scalar multiplication as above.\n\n5. The ring of n× n matrices Mn(R) is a vector space over R.\n\n6. The ring of m× n matrices Mm,n(R) is a vector space over R.\n\n7. The ring C(a, b) of continuous functions f : (a, b) → R is a vector space over R, with\nthe scalar multiplication λf of a function f : (a, b)→ R by a scalar λ ∈ R given by\n\n(λf)(x) = λf(x), for all x ∈ (a, b).\n\n\n\n60 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n8. A very important example of vector space is the set of linear maps between two vector\nspaces to be defined in Section 11.1. Here is an example that will prepare us for the\nvector space of linear maps. Let X be any nonempty set and let E be a vector space.\nThe set of all functions f : X → E can be made into a vector space as follows: Given\nany two functions f : X → E and g : X → E, let (f + g) : X → E be defined such that\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that\n\n(λf)(x) = λf(x)\n\nfor all x ∈ X. The axioms of a vector space are easily verified.\n\nLet E be a vector space. We would like to define the important notions of linear combi-\nnation and linear independence.\n\nBefore defining these notions, we need to discuss a strategic choice which, depending\nhow it is settled, may reduce or increase headaches in dealing with notions such as linear\ncombinations and linear dependence (or independence). The issue has to do with using sets\nof vectors versus sequences of vectors.\n\n3.3 Indexed Families; the Sum Notation\n∑\n\ni∈I ai\n\nOur experience tells us that it is preferable to use sequences of vectors ; even better, indexed\nfamilies of vectors. (We are not alone in having opted for sequences over sets, and we are in\ngood company; for example, Artin [7], Axler [10], and Lang [108] use sequences. Nevertheless,\nsome prominent authors such as Lax [112] use sets. We leave it to the reader to conduct a\nsurvey on this issue.)\n\nGiven a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈ An of elements\nfrom A, for some natural number n. The elements of a sequence need not be distinct and\nthe order is important. For example, (a1, a2, a1) and (a2, a1, a1) are two distinct sequences\nin A3. Their underlying set is {a1, a2}.\n\nWhat we just defined are finite sequences, which can also be viewed as functions from\n{1, 2, . . . , n} to the set A; the ith element of the sequence (a1, . . . , an) is the image of i under\nthe function. This viewpoint is fruitful, because it allows us to define (countably) infinite\nsequences as functions s : N → A. But then, why limit ourselves to ordered sets such as\n{1, . . . , n} or N as index sets?\n\nThe main role of the index set is to tag each element uniquely, and the order of the tags\nis not crucial, although convenient. Thus, it is natural to define the notion of indexed family.\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 61\n\nDefinition 3.2. Given a set A, an I-indexed family of elements of A, for short a family ,\nis a function a : I → A where I is any set viewed as an index set. Since the function a is\ndetermined by its graph\n\n{(i, a(i)) | i ∈ I},\nthe family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈ I}. For notational simplicity,\nwe write ai instead of a(i), and denote the family a = {(i, a(i)) | i ∈ I} by (ai)i∈I .\n\nFor example, if I = {r, g, b, y} and A = N, the set of pairs\n\na = {(r, 2), (g, 3), (b, 2), (y, 11)}\n\nis an indexed family. The element 2 appears twice in the family with the two distinct tags\nr and b.\n\nWhen the indexed set I is totally ordered, a family (ai)i∈I is often called an I-sequence.\nInterestingly, sets can be viewed as special cases of families. Indeed, a set A can be viewed\nas the A-indexed family {(a, a) | a ∈ I} corresponding to the identity function.\n\nRemark: An indexed family should not be confused with a multiset. Given any set A, a\nmultiset is a similar to a set, except that elements of A may occur more than once. For\nexample, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d} is a multiset. Each element appears\nwith a certain multiplicity, but the order of the elements does not matter. For example, a\nhas multiplicity 3. Formally, a multiset is a function s : A→ N, or equivalently a set of pairs\n{(a, i) | a ∈ A}. Thus, a multiset is an A-indexed family of elements from N, but not a\nN-indexed family, since distinct elements may have the same multiplicity (such as c an d in\nthe example above). An indexed family is a generalization of a sequence, but a multiset is a\ngeneralization of a set.\n\nWe also need to take care of an annoying technicality, which is to define sums of the\nform\n\n∑\ni∈I ai, where I is any finite index set and (ai)i∈I is a family of elements in some set\n\nA equiped with a binary operation +: A × A → A which is associative (Axiom (G1)) and\ncommutative. This will come up when we define linear combinations.\n\nThe issue is that the binary operation + only tells us how to compute a1 + a2 for two\nelements of A, but it does not tell us what is the sum of three of more elements. For example,\nhow should a1 + a2 + a3 be defined?\n\nWhat we have to do is to define a1+a2+a3 by using a sequence of steps each involving two\nelements, and there are two possible ways to do this: a1 + (a2 +a3) and (a1 +a2) +a3. If our\noperation + is not associative, these are different values. If it associative, then a1+(a2+a3) =\n(a1 + a2) + a3, but then there are still six possible permutations of the indices 1, 2, 3, and if\n+ is not commutative, these values are generally different. If our operation is commutative,\nthen all six permutations have the same value. Thus, if + is associative and commutative,\nit seems intuitively clear that a sum of the form\n\n∑\ni∈I ai does not depend on the order of the\n\noperations used to compute it.\n\n\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form∑\n\ni∈I ai are indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums\n∑\n\ni∈I ai, where I is a finite sequence of distinct natural numbers,\nsay I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥ 2, we denote the sequence (i2, . . . , im) by\nI − {i1}. We proceed by induction on the size m of I. Let∑\n\ni∈I\nai = ai1 , if m = 1,\n\n∑\ni∈I\n\nai = ai1 +\n\n( ∑\ni∈I−{i1}\n\nai\n\n)\n, if m > 1.\n\nFor example, if I = (1, 2, 3, 4), we have∑\ni∈I\n\nai = a1 + (a2 + (a3 + a4)).\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\n\na1 + (a2 + (a3 + a4)) 6= (a1 + a2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum\n∑\n\ni∈I ai should not depend on the grouping\nof the elements in I, as long as their order is preserved. For example, if I = (1, 2, 3, 4, 5),\nJ1 = (1, 2), and J2 = (3, 4, 5), we expect that\n\n∑\ni∈I\n\nai =\n\n(∑\nj∈J1\n\naj\n\n)\n+\n\n(∑\nj∈J2\n\naj\n\n)\n.\n\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: A × A → A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Ik1 , . . . , Ikp, for some nonempty sequence K =\n(k1, . . . , kp) of distinct natural numbers such that ki < kj implies that α < β for all α ∈ Iki\nand all β ∈ Ikj , for every sequence (ai)i∈I of elements in A, we have\n\n∑\nα∈I\n\naα =\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n.\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds trivially.\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form\nvier 4% ave indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums }7;-,@;, where J is a finite sequence of distinct natural numbers,\nsay I = (i1,...,4m). If I = (i1,...,im) with m > 2, we denote the sequence (i2,...,%m) by\nI — {t,}. We proceed by induction on the size m of I. Let\n\n) a,=a,, Wwm=1,\n\nie]\nLa=a +( S- a). ifm > 1.\nie] i€I—{ir}\n\nFor example, if J = (1,2,3,4), we have\nSai =a, + (ag + (a3 + a4)).\niel\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\nay + (a2 + (a3 + a4)) A (a1 + G2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum }?,., a; should not depend on the grouping\nof the elements in J, as long as their order is preserved. For example, if J = (1,2,3,4,5),\nJ, = (1,2), and Jy = (3,4,5), we expect that\n\nYa=(La)+ (La).\n\niel jel je de\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: Ax A-— A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Iy,,...,Ik,, for some nonempty sequence K =\n(ky,...,kp) of distinct natural numbers such that k; < k; implies that a < 6 for alla € Ix,\nand all 8 € Iy,, for every sequence (a;)ier of elements in A, we have\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and IJ;, = J, so the proposition holds trivially.\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 63\n\nNext, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial, so assume that\np ≥ 2 and write J = (k2, . . . , kp). There are two cases.\n\nCase 1. The sequence Ik1 has a single element, say β, which is the first element of I.\nIn this case, write C for the sequence obtained from I by deleting its first element β. By\ndefinition, ∑\n\nα∈I\naα = aβ +\n\n(∑\nα∈C\n\naα\n\n)\n,\n\nand ∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n= aβ +\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nSince |C| = n− 1, by the induction hypothesis, we have(∑\nα∈C\n\naα\n\n)\n=\n∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)\n,\n\nwhich yields our identity.\n\nCase 2. The sequence Ik1 has at least two elements. In this case, let β be the first element\nof I (and thus of Ik1), let I ′ be the sequence obtained from I by deleting its first element β,\nlet I ′k1\n\nbe the sequence obtained from Ik1 by deleting its first element β, and let I ′ki = Iki for\ni = 2, . . . , p. Recall that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I ′ has n − 1\nelements, so by the induction hypothesis applied to I ′ and the I ′ki , we get∑\n\nα∈I′\naα =\n\n∑\nk∈K\n\n(∑\nα∈I′k\n\naα\n\n)\n=\n\n(∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nIf we add the lefthand side to aβ, by definition we get∑\nα∈I\n\naα.\n\nIf we add the righthand side to aβ, using associativity and the definition of an indexed sum,\nwe get\n\naβ +\n\n((∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)))\n=\n\n(\naβ +\n\n(∑\nα∈I′k1\n\naα\n\n))\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n\n(∑\nα∈Ik1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n,\n\nas claimed.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 63\n\nNext, assume n > 1. If p = 1, then J,, = J and the formula is trivial, so assume that\np > 2 and write J = (ko,...,k,). There are two cases.\n\nCase 1. The sequence J;, has a single element, say 3, which is the first element of I.\nIn this case, write C' for the sequence obtained from I by deleting its first element 6. By\n\ndefinition,\nSi aa =agt+ (Su),\n\nael acC\nand\n\nHE) (E(S\"))\n\nkeK ‘a&€ly, jEtJ Sael;\nSince |C| =n — 1, by the induction hypothesis, we have\n(Se)=X(Le)\naeC jet Sa€l;\n\nwhich yields our identity.\n\nCase 2. The sequence I;, has at least two elements. In this case, let 8 be the first element\nof I (and thus of J;,), let I’ be the sequence obtained from J by deleting its first element £,\nlet Ij, be the sequence obtained from J;,, by deleting its first element 6, and let J, = Ip, for\ni =2,...,p. Recall that J = (ko,...,k,) and kK = (ki,...,k,). The sequence I’ has n — 1\nelements, so by the induction hypothesis applied to J’ and the [;,, we get\n\nYa=O (Lae) = (LH a) + (HL):\nael! keK acl, al}, JET Sal;\nIf we add the lefthand side to ag, by definition we get\non\nael\n\nIf we add the righthand side to ag, using associativity and the definition of an indexed sum,\nwe get\n\nw+ ((X~)+(Z(Le)))= (+ (L4)) +S)\n\nacl, jEJ Sael; acl,\n\n-(L)*(E(E))\n-¥(E«).\n\nkeK \\a€ly\n\nas claimed. Oo\n\n\n\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf I = (1, . . . , n), we also write\n∑n\n\ni=1 ai instead of\n∑\n\ni∈I ai. Since + is associative, Propo-\nsition 3.2 shows that the sum\n\n∑n\ni=1 ai is independent of the grouping of its elements, which\n\njustifies the use the notation a1 + · · ·+ an (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\ncan show that the sum\n\n∑\ni∈I ai does not depend on the ordering of the index set I.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: A× A→ A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (ai)i∈I of elements in A, we have∑\n\nα∈I\naα =\n\n∑\nα∈J\n\naα.\n\nProof. We proceed by induction on the number p of elements in I. If p = 1, we have I = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that I = (1, . . . , p) and that J is a permutation\n(i1, . . . , ip) of I. First, assume that 2 ≤ i1 ≤ p−1, let J ′ be the sequence obtained from J by\ndeleting i1, I ′ be the sequence obtained from I by deleting i1, and let P = (1, 2, . . . , i1−1) and\nQ = (i1 + 1, . . . , p−1, p). Observe that the sequence I ′ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J ′ and I ′, and then by Proposition 3.2\napplied to I ′ and its partition (P,Q), we have\n\n∑\nα∈J ′\n\naα =\n∑\nα∈I′\n\naα =\n\n(i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n.\n\nIf we add the lefthand side to ai1 , by definition we get∑\nα∈J\n\naα.\n\nIf we add the righthand side to ai1 , we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n.\n\nUsing associativity, we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n=\n\n(\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n,\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf J = (1,...,n), we also write }°\"' | a; instead of )7,., a;. Since + is associative, Propo-\nsition 3.2 shows that the sum }>;\"_, a; is independent of the grouping of its elements, which\njustifies the use the notation a, +---+ a, (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\n\ncan show that the sum }°,., a; does not depend on the ordering of the index set J.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: Ax A— A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (a;)icr of elements in A, we have\n\nSota = Soa.\n\naéel aces\n\nProof. We proceed by induction on the number p of elements in J. If p = 1, we have J = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that J = (1,...,p) and that J is a permutation\n(i1,...,%)) of I. First, assume that 2 < 7, < p—1, let J’ be the sequence obtained from J by\ndeleting i, I’ be the sequence obtained from J by deleting 7,, and let P = (1,2,...,i;—1) and\nQ = (%44+1,...,p—1,p). Observe that the sequence I’ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J’ and I’, and then by Proposition 3.2\napplied to J’ and its partition (P,Q), we have\n\n44-1 Pp\n) da = u=() a) +() w).\naceJ’ ael’ i=l I=14+1\n\nIf we add the lefthand side to a;,, by definition we get\n\nSo.\n\naed\n\nIf we add the righthand side to a;,, we get\n\n(Eo) (29)\n\nUsing associativity, we get\n\no(E5)+(E9)-+E)-(E9)\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i1 − 1), we get(\n\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n=\n\n(i1−1∑\ni=1\n\nai\n\n)\n+ ai1 +\n\n( p∑\ni=i1+1\n\nai\n\n)\n\n=\n\np∑\ni=1\n\nai,\n\nas claimed.\n\nThe cases where i1 = 1 or i1 = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence).\n\nHaving done all this, we can now make sense of sums of the form\n∑\n\ni∈I ai, for any finite\nindexed set I and any family a = (ai)i∈I of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since I is finite, it is in bijection with the set {1, . . . , n} for some n ∈ N, and any\ntotal ordering � on I corresponds to a permutation I� of {1, . . . , n} (where we identify a\npermutation with its image). For any total ordering � on I, we define\n\n∑\ni∈I,� ai as∑\n\ni∈I,�\nai =\n\n∑\nj∈I�\n\naj.\n\nThen for any other total ordering �′ on I, we have∑\ni∈I,�′\n\nai =\n∑\nj∈I�′\n\naj,\n\nand since I� and I�′ are different permutations of {1, . . . , n}, by Proposition 3.3, we have∑\nj∈I�\n\naj =\n∑\nj∈I�′\n\naj.\n\nTherefore, the sum\n∑\n\ni∈I,� ai does not depend on the total ordering on I. We define the sum∑\ni∈I ai as the common value\n\n∑\ni∈I,� ai for all total orderings � of I.\n\nHere are some examples with A = R:\n\n1. If I = {1, 2, 3}, a = {(1, 2), (2,−3), (3,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n2. If I = {2, 5, 7}, a = {(2, 2), (5,−3), (7,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n3. If I = {r, g, b}, a = {(r, 2), (g,−3), (b, 1)}, then\n∑\n\ni∈I ai = 2− 3 + 1 = 0.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i; — 1), we get\n\n(+ (E9))(E9)- Ea) ome(E9)\n\nP\n\nS ai,\n\ni=1\n\nas claimed.\n\nThe cases where 7; = 1 or 7; = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence). O\n\nHaving done all this, we can now make sense of sums of the form }°,., a;, for any finite\nindexed set J and any family a = (a;);c7 of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since J is finite, it is in bijection with the set {1,...,n} for some n € N, and any\ntotal ordering < on J corresponds to a permutation J, of {1,...,n} (where we identify a\npermutation with its image). For any total ordering =< on J, we define }0,-; a as\n\nSo a= ray,\n\ni€l,x jel\n\nThen for any other total ordering =<’ on J, we have\n\na= Da\n\ni€1,<! j€ler\nand since J, and J are different permutations of {1,...,n}, by Proposition 3.3, we have\n) ay= ) aj.\nJET JEL:\n\nTherefore, the sum )7,-; . a; does not depend on the total ordering on J. We define the sum\nier % as the common value $7,-;~ a for all total orderings = of J.\n\nHere are some examples with A = R:\n1. If J = {1,2,3}, a = {(1, 2), (2, -3), (3, V2)}, then 0),-,a; = 2-34 V2 =-14 v2.\n2. If I = {2,5,7}, a = {(2, 2), (5, -3), (7, V2)}, then ),-, a; = 2-34 V2 =—-14 v2.\n\n3. If l= {r, 9, Bf, a= {(r, 2), (9, —3), (0, 1)}, then ier ay = 2—3+1=0.\n\n\n\n\n66 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n3.4 Linear Independence, Subspaces\n\nOne of the most useful properties of vector spaces is that they possess bases. What this\nmeans is that in every vector space E, there is some set of vectors, {e1, . . . , en}, such that\nevery vector v ∈ E can be written as a linear combination,\n\nv = λ1e1 + · · ·+ λnen,\n\nof the ei, for some scalars, λ1, . . . , λn ∈ R. Furthermore, the n-tuple, (λ1, . . . , λn), as above\nis unique.\n\nThis description is fine when E has a finite basis, {e1, . . . , en}, but this is not always the\ncase! For example, the vector space of real polynomials, R[X], does not have a finite basis\nbut instead it has an infinite basis, namely\n\n1, X, X2, . . . , Xn, . . .\n\nOne might wonder if it is possible for a vector space to have bases of different sizes, or even\nto have a finite basis as well as an infinite basis. We will see later on that this is not possible;\nall bases of a vector space have the same number of elements (cardinality), which is called\nthe dimension of the space. However, we have the following problem: If a vector space has\nan infinite basis, {e1, e2, . . . , }, how do we define linear combinations? Do we allow linear\ncombinations\n\nλ1e1 + λ2e2 + · · ·\nwith infinitely many nonzero coefficients?\n\nIf we allow linear combinations with infinitely many nonzero coefficients, then we have\nto make sense of these sums and this can only be done reasonably if we define such a sum\nas the limit of the sequence of vectors, s1, s2, . . . , sn, . . ., with s1 = λ1e1 and\n\nsn+1 = sn + λn+1en+1.\n\nBut then, how do we define such limits? Well, we have to define some topology on our space,\nby means of a norm, a metric or some other mechanism. This can indeed be done and this\nis what Banach spaces and Hilbert spaces are all about but this seems to require a lot of\nmachinery.\n\nA way to avoid limits is to restrict our attention to linear combinations involving only\nfinitely many vectors. We may have an infinite supply of vectors but we only form linear\ncombinations involving finitely many nonzero coefficients. Technically, this can be done by\nintroducing families of finite support . This gives us the ability to manipulate families of\nscalars indexed by some fixed infinite set and yet to be treat these families as if they were\nfinite.\n\nWith these motivations in mind, given a set A, recall that an I-indexed family (ai)i∈I\nof elements of A (for short, a family) is a function a : I → A, or equivalently a set of pairs\n{(i, ai) | i ∈ I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is finite if I is finite.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 67\n\nRemark: When considering a family (ai)i∈I , there is no reason to assume that I is ordered.\nThe crucial point is that every element of the family is uniquely indexed by an element of\nI. Thus, unless specified otherwise, we do not assume that the elements of an index set are\nordered.\n\nIf A is an abelian group with identity 0, we say that a family (ai)i∈I has finite support if\nai = 0 for all i ∈ I − J , where J is a finite subset of I (the support of the family).\n\nGiven two disjoint sets I and J , the union of two families (ui)i∈I and (vj)j∈J , denoted as\n(ui)i∈I ∪ (vj)j∈J , is the family (wk)k∈(I∪J) defined such that wk = uk if k ∈ I, and wk = vk\nif k ∈ J . Given a family (ui)i∈I and any element v, we denote by (ui)i∈I ∪k (v) the family\n(wi)i∈I∪{k} defined such that, wi = ui if i ∈ I, and wk = v, where k is any index such that\nk /∈ I. Given a family (ui)i∈I , a subfamily of (ui)i∈I is a family (uj)j∈J where J is any subset\nof I.\n\nIn this chapter, unless specified otherwise, is assumed that all families of scalars have\nfinite support .\n\nDefinition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family\n(ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nWhen I = ∅, we stipulate that v = 0. (By Proposition 3.3, sums of the form\n∑\n\ni∈I λiui are\nwell defined.) We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I\nof scalars in K, ∑\n\ni∈I\nλiui = 0 implies that λi = 0 for all i ∈ I.\n\nEquivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars\nin K such that ∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I.\n\nWe agree that when I = ∅, the family ∅ is linearly independent.\n\nObserve that defining linear combinations for families of vectors rather than for sets of\nvectors has the advantage that the vectors being combined need not be distinct. For example,\nfor I = {1, 2, 3} and the families (u, v, u) and (λ1, λ2, λ1), the linear combination∑\n\ni∈I\nλiui = λ1u+ λ2v + λ1u\n\nmakes sense. Using sets of vectors in the definition of a linear combination does not allow\nsuch linear combinations; this is too restrictive.\n\n\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (ui)i∈I is linearly dependent iff either I consists of a\nsingle element, say i, and ui = 0, or |I| ≥ 2 and some uj in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (λi)i∈I of scalars in K such that∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I,\n\nand since |I| ≥ 2, the set I − {j} is nonempty and we get\n\nuj =\n∑\n\ni∈(I−{j})\n−λ−1\n\nj λiui.\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (ui)i∈I is linearly independent iff either I = ∅, or I\nconsists of a single element i and ui 6= 0, or |I| ≥ 2 and no vector uj in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen I is nonempty, if the family (ui)i∈I is linearly independent, note that ui 6= 0 for\nall i ∈ I. Otherwise, if ui = 0 for some i ∈ I, then we get a nontrivial linear dependence∑\n\ni∈I λiui = 0 by picking any nonzero λi and letting λk = 0 for all k ∈ I with k 6= i, since\nλi0 = 0. If |I| ≥ 2, we must also have ui 6= uj for all i, j ∈ I with i 6= j, since otherwise we\nget a nontrivial linear dependence by picking λi = λ and λj = −λ for any nonzero λ, and\nletting λk = 0 for all k ∈ I with k 6= i, j.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, as we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n\n1. Any two distinct scalars λ, µ 6= 0 in K are linearly dependent.\n\n2. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent. See Figure\n3.7.\n\n3. In R4, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are linearly indepen-\ndent.\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (u;);<7 is linearly dependent iff either J consists of a\nsingle element, say 7, and u; = 0, or |I| > 2 and some u; in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (A;)ic7 of scalars in AK such that\n\nSo dit = 0 and A; #0 for some j € J,\n\ntel\n\nand since |I| > 2, the set J — {7} is nonempty and we get\n\nie(I—{3})\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (u;)jc7 is linearly independent iff either J = @, or I\nconsists of a single element i and u; ¥ 0, or |J| > 2 and no vector wu; in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen J is nonempty, if the family (u;)icr is linearly independent, note that u; 4 0 for\nalli € I. Otherwise, if u; = 0 for some 2 € J, then we get a nontrivial linear dependence\nier Mii = O by picking any nonzero 4; and letting A, = 0 for all k € J with k ¥ 7, since\nA;0 = 0. If |Z] > 2, we must also have u; 4 u; for all 7,7 € J with i 4 j, since otherwise we\nget a nontrivial linear dependence by picking A; = A and A; = —A for any nonzero A, and\nletting A, = 0 for all k € I with k £2, 7.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, aS we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n1. Any two distinct scalars A, #0 in K are linearly dependent.\n\n2. In R°, the vectors (1,0,0), (0,1,0), and (0,0,1) are linearly independent. See Figure\n3.7.\n\n3. In R*, the vectors (1,1,1,1), (0,1,1,1), (0,0,1,1), and (0,0,0,1) are linearly indepen-\ndent.\n\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 69\n\nFigure 3.7: A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),\nand the blue vector (0, 0, 1) in R3.\n\n4. In R2, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly dependent, since\n\nw = 2u+ v.\n\nSee Figure 3.8.\n\n(2,3)\n\n2u\n\nv\n\nw\n\nFigure 3.8: A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector\nv = (0, 1), and the vector sum w = 2u+ v.\n\nWhen I is finite, we often assume that it is the set I = {1, 2, . . . , n}. In this case, we\ndenote the family (ui)i∈I as (u1, . . . , un).\n\nThe notion of a subspace of a vector space is defined as follows.\n\nDefinition 3.4. Given a vector space E, a subset F of E is a linear subspace (or subspace)\nof E iff F is nonempty and λu+ µv ∈ F for all u, v ∈ F , and all λ, µ ∈ K.\n\n\n\n70 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is easy to see that a subspace F of E is indeed a vector space, since the restriction\nof +: E × E → E to F × F is indeed a function +: F × F → F , and the restriction of\n· : K × E → E to K × F is indeed a function · : K × F → F .\n\nSince a subspace F is nonempty, if we pick any vector u ∈ F and if we let λ = µ = 0,\nthen λu+ µu = 0u+ 0u = 0, so every subspace contains the vector 0.\n\nThe following facts also hold. The proof is left as an exercise.\n\nProposition 3.4.\n\n(1) The intersection of any family (even infinite) of subspaces of a vector space E is a\nsubspace.\n\n(2) Let F be any subspace of a vector space E. For any nonempty finite index set I,\nif (ui)i∈I is any family of vectors ui ∈ F and (λi)i∈I is any family of scalars, then∑\n\ni∈I λiui ∈ F .\n\nThe subspace {0} will be denoted by (0), or even 0 (with a mild abuse of notation).\n\nExample 3.3.\n\n1. In R2, the set of vectors u = (x, y) such that\n\nx+ y = 0\n\nis the subspace illustrated by Figure 3.9.\n\nFigure 3.9: The subspace x+ y = 0 is the line through the origin with slope −1. It consists\nof all vectors of the form λ(−1, 1).\n\n2. In R3, the set of vectors u = (x, y, z) such that\n\nx+ y + z = 0\n\nis the subspace illustrated by Figure 3.10.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+y+z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n ≥ 0, the set of polynomials f(X) ∈ R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n×n matrices is a subspace of the space of n×n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace 〈S〉 (or Span(S)) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\nof E, leaving as an exercise the verification that every subspace containing S also contains\nSpan(S).\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u =\n∑\n\ni∈I λiui\nand v =\n\n∑\nj∈J µjvj are any two linear combinations in Span(S), for any two scalars λ, µ ∈ K,\n\nλu+ µv = λ\n∑\ni∈I\n\nλiui + µ\n∑\nj∈J\n\nµjvj\n\n=\n∑\ni∈I\n\nλλiui +\n∑\nj∈J\n\nµµjvj\n\n=\n∑\ni∈I−J\n\nλλiui +\n∑\ni∈I∩J\n\n(λλi + µµi)ui +\n∑\nj∈J−I\n\nµµjvj,\n\nwhich is a linear combination with index set I ∪ J , and thus λu + µv ∈ Span(S), which\nproves that Span(S) is a subspace.\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+ y+ z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n > 0, the set of polynomials f(X) € R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n x n matrices is a subspace of the space of n x n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace (S) (or Span(S')) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\n\nof EF, leaving as an exercise the verification that every subspace containing S' also contains\nSpan().\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u = S),.; Aim\nandv=)> jeg Mj¥j are any two linear combinations in Span(.S), for any two scalars A, uw € K,\n\nAu + pv = NSO jue + HD p50;\n\ni€l jed\n= S- ANU; + S- LLjV5\niel jeJ\n= SO Mii + SS OA + mpedur + SS pyr,\nieI—J i€INI jeJ—1\n\nwhich is a linear combination with index set J U J, and thus Au + yu € Span(S), which\nproves that Span(S) is a subspace. O\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n\n\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations\n∑\n\ni∈I λiui for which∑\ni∈I\n\nλi = 1.\n\nThese are called affine combinations . One should realize that every linear combination∑\ni∈I λiui can be viewed as an affine combination. For example, if k is an index not\n\nin I, if we let J = I ∪ {k}, uk = 0, and λk = 1−∑i∈I λi, then\n∑\n\nj∈J λjuj is an affine\ncombination and ∑\n\ni∈I\nλiui =\n\n∑\nj∈J\n\nλjuj.\n\nHowever, we get new spaces. For example, in R3, the set of all affine combinations of\nthe three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0, 0, 0), it is not a linear\nsubspace.\n\n(2) Consider combinations\n∑\n\ni∈I λiui for which\n\nλi ≥ 0, for all i ∈ I.\n\nThese are called positive (or conic) combinations . It turns out that positive combina-\ntions of families of vectors are cones . They show up naturally in convex optimization.\n\n(3) Consider combinations\n∑\n\ni∈I λiui for which we require (1) and (2), that is∑\ni∈I\n\nλi = 1, and λi ≥ 0 for all i ∈ I.\n\nThese are called convex combinations . Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum\n\n∑\ni∈I λiui makes sense, we restrict our attention to families of finite\n\nsupport.\n\nDefinition 3.5. Given any field K, a family of scalars (λi)i∈I has finite support if λi = 0\nfor all i ∈ I − J , for some finite subset J of I.\n\nIf (λi)i∈I is a family of scalars of finite support, for any vector space E over K, for any\n(possibly infinite) family (ui)i∈I of vectors ui ∈ E, we define the linear combination\n\n∑\ni∈I λiui\n\nas the finite linear combination\n∑\n\nj∈J λjuj, where J is any finite subset of I such that λi = 0\nfor all i ∈ I − J . In general, results stated for finite families also hold for families of finite\nsupport.\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations }7,-, A;u; for which\n\nSov = 1.\n\nvel\n\nThese are called affine combinations. One should realize that every linear combination\nier Ais Can be viewed as an affine combination. For example, if k is an index not\nin I, if we let J= JU {k}, up = 0, and Ay =1—)97,-, i, then )7,-, Aju is an affine\n\ncombination and\n\nSs\" AjUj = S- Aj Uy.\n\niel jed\nHowever, we get new spaces. For example, in R°, the set of all affine combinations of\nthe three vectors e; = (1,0,0),e2 = (0,1,0), and e3 = (0,0,1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0,0,0), it is not a linear\nsubspace.\n\n(2) Consider combinations }7,-, A;u; for which\nA; > 0, for allie J.\n\nThese are called positive (or conic) combinations. It turns out that positive combina-\ntions of families of vectors are cones. They show up naturally in convex optimization.\n\n(3) Consider combinations )>,-, A;u; for which we require (1) and (2), that is\n\nwel\nSox Hh, and \\;>0 foralli eT.\n\nwel\n\nThese are called conver combinations. Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum 5°,_, A;u; makes sense, we restrict our attention to families of finite\nsupport.\n\niel\n\nDefinition 3.5. Given any field K, a family of scalars (\\;)ier has finite support if A; = 0\nfor all 2 € I — J, for some finite subset J of J.\n\nIf (A;)icr is a family of scalars of finite support, for any vector space F over K, for any\n(possibly infinite) family (u;);er of vectors u; € E, we define the linear combination 7) ,-, Ait\nas the finite linear combination )> jeg AjUj, Where J is any finite subset of J such that A; = 0\nfor all: € I — J. In general, results stated for finite families also hold for families of finite\nsupport.\n\n\n\n\n3.5. BASES OF A VECTOR SPACE 73\n\n3.5 Bases of a Vector Space\n\nGiven a vector space E, given a family (vi)i∈I , the subset V of E consisting of the null vector\n0 and of all linear combinations of (vi)i∈I is easily seen to be a subspace of E. The family\n(vi)i∈I is an economical way of representing the entire subspace V , but such a family would\nbe even nicer if it was not redundant. Subspaces having such an “efficient” generating family\n(called a basis) play an important role and motivate the following definition.\n\nDefinition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors\nvi ∈ V spans V or generates V iff for every v ∈ V , there is some family (λi)i∈I of scalars in\nK such that\n\nv =\n∑\ni∈I\n\nλivi.\n\nWe also say that the elements of (vi)i∈I are generators of V and that V is spanned by (vi)i∈I ,\nor generated by (vi)i∈I . If a subspace V of E is generated by a finite family (vi)i∈I , we say\nthat V is finitely generated . A family (ui)i∈I that spans V and is linearly independent is\ncalled a basis of V .\n\nExample 3.4.\n\n1. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Figure 3.9, form a basis.\n\n2. The vectors (1, 1, 1, 1), (1, 1,−1,−1), (1,−1, 0, 0), (0, 0, 1,−1) form a basis of R4 known\nas the Haar basis . This basis and its generalization to dimension 2n are crucial in\nwavelet theory.\n\n3. In the subspace of polynomials in R[X] of degree at most n, the polynomials 1, X,X2,\n. . . , Xn form a basis.\n\n4. The Bernstein polynomials\n\n(\nn\nk\n\n)\n(1 − X)n−kXk for k = 0, . . . , n, also form a basis of\n\nthat space. These polynomials play a major role in the theory of spline curves .\n\nThe first key result of linear algebra is that every vector space E has a basis. We begin\nwith a crucial lemma which formalizes the mechanism for building a basis incrementally.\n\nLemma 3.6. Given a linearly independent family (ui)i∈I of elements of a vector space E, if\nv ∈ E is not a linear combination of (ui)i∈I , then the family (ui)i∈I ∪k (v) obtained by adding\nv to the family (ui)i∈I is linearly independent (where k /∈ I).\n\nProof. Assume that µv+\n∑\n\ni∈I λiui = 0, for any family (λi)i∈I of scalars in K. If µ 6= 0, then\nµ has an inverse (because K is a field), and thus we have v = −∑i∈I(µ\n\n−1λi)ui, showing\nthat v is a linear combination of (ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But\nthen, we have\n\n∑\ni∈I λiui = 0, and since the family (ui)i∈I is linearly independent, we have\n\nλi = 0 for all i ∈ I.\n\n\n\n74 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThe next theorem holds in general, but the proof is more sophisticated for vector spaces\nthat do not have a finite set of generators. Thus, in this chapter, we only prove the theorem\nfor finitely generated vector spaces.\n\nTheorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any\nlinearly independent subfamily L = (uj)j∈J of S (where J ⊆ I), there is a basis B of E such\nthat L ⊆ B ⊆ S.\n\nProof. Consider the set of linearly independent families B such that L ⊆ B ⊆ S. Since this\nset is nonempty and finite, it has some maximal element (that is, a subfamily B = (uh)h∈H\nof S with H ⊆ I of maximum cardinality), say B = (uh)h∈H . We claim that B generates\nE. Indeed, if B does not generate E, then there is some up ∈ S that is not a linear\ncombination of vectors in B (since S generates E), with p /∈ H. Then by Lemma 3.6, the\nfamily B′ = (uh)h∈H∪{p} is linearly independent, and since L ⊆ B ⊂ B′ ⊆ S, this contradicts\nthe maximality of B. Thus, B is a basis of E such that L ⊆ B ⊆ S.\n\nRemark: Theorem 3.7 also holds for vector spaces that are not finitely generated. In this\ncase, the problem is to guarantee the existence of a maximal linearly independent family B\nsuch that L ⊆ B ⊆ S. The existence of such a maximal family can be shown using Zorn’s\nlemma, see Appendix B and the references given there.\n\nA situation where the full generality of Theorem 3.7 is needed is the case of the vector\nspace R over the field of coefficients Q. The numbers 1 and\n\n√\n2 are linearly independent\n\nover Q, so according to Theorem 3.7, the linearly independent family L = (1,\n√\n\n2) can be\nextended to a basis B of R. Since R is uncountable and Q is countable, such a basis must\nbe uncountable!\n\nThe notion of a basis can also be defined in terms of the notion of maximal linearly\nindependent family and minimal generating family.\n\nDefinition 3.7. Let (vi)i∈I be a family of vectors in a vector space E. We say that (vi)i∈I a\nmaximal linearly independent family of E if it is linearly independent, and if for any vector\nw ∈ E, the family (vi)i∈I ∪k {w} obtained by adding w to the family (vi)i∈I is linearly\ndependent. We say that (vi)i∈I a minimal generating family of E if it spans E, and if for\nany index p ∈ I, the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does\nnot span E.\n\nThe following proposition giving useful properties characterizing a basis is an immediate\nconsequence of Lemma 3.6.\n\nProposition 3.8. Given a vector space E, for any family B = (vi)i∈I of vectors of E, the\nfollowing properties are equivalent:\n\n(1) B is a basis of E.\n\n\n\n3.5. BASES OF A VECTOR SPACE 75\n\n(2) B is a maximal linearly independent family of E.\n\n(3) B is a minimal generating family of E.\n\nProof. We will first prove the equivalence of (1) and (2). Assume (1). Since B is a basis, it is\na linearly independent family. We claim that B is a maximal linearly independent family. If\nB is not a maximal linearly independent family, then there is some vector w ∈ E such that\nthe family B′ obtained by adding w to B is linearly independent. However, since B is a basis\nof E, the vector w can be expressed as a linear combination of vectors in B, contradicting\nthe fact that B′ is linearly independent.\n\nConversely, assume (2). We claim that B spans E. If B does not span E, then there is\nsome vector w ∈ E which is not a linear combination of vectors in B. By Lemma 3.6, the\nfamily B′ obtained by adding w to B is linearly independent. Since B is a proper subfamily\nof B′, this contradicts the assumption that B is a maximal linearly independent family.\nTherefore, B must span E, and since B is also linearly independent, it is a basis of E.\n\nNow we will prove the equivalence of (1) and (3). Again, assume (1). Since B is a basis,\nit is a generating family of E. We claim that B is a minimal generating family. If B is not\na minimal generating family, then there is a proper subfamily B′ of B that spans E. Then,\nevery w ∈ B−B′ can be expressed as a linear combination of vectors from B′, contradicting\nthe fact that B is linearly independent.\n\nConversely, assume (3). We claim that B is linearly independent. If B is not linearly\nindependent, then some vector w ∈ B can be expressed as a linear combination of vectors\nin B′ = B − {w}. Since B generates E, the family B′ also generates E, but B′ is a\nproper subfamily of B, contradicting the minimality of B. Since B spans E and is linearly\nindependent, it is a basis of E.\n\nThe second key result of linear algebra is that for any two bases (ui)i∈I and (vj)j∈J of a\nvector space E, the index sets I and J have the same cardinality. In particular, if E has a\nfinite basis of n elements, every basis of E has n elements, and the integer n is called the\ndimension of the vector space E.\n\nTo prove the second key result, we can use the following replacement lemma due to\nSteinitz. This result shows the relationship between finite linearly independent families and\nfinite families of generators of a vector space. We begin with a version of the lemma which is\na bit informal, but easier to understand than the precise and more formal formulation given\nin Proposition 3.10. The technical difficulty has to do with the fact that some of the indices\nneed to be renamed.\n\nProposition 3.9. (Replacement lemma, version 1) Given a vector space E, let (u1, . . . , um)\nbe any finite linearly independent family in E, and let (v1, . . . , vn) be any finite family such\nthat every ui is a linear combination of (v1, . . . , vn). Then we must have m ≤ n, and there\nis a replacement of m of the vectors vj by (u1, . . . , um), such that after renaming some of the\nindices of the vjs, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same\nsubspace of E.\n\n\n\n76 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProof. We proceed by induction on m. When m = 0, the family (u1, . . . , um) is empty, and\nthe proposition holds trivially. For the induction step, we have a linearly independent family\n(u1, . . . , um, um+1). Consider the linearly independent family (u1, . . . , um). By the induction\nhypothesis, m ≤ n, and there is a replacement of m of the vectors vj by (u1, . . . , um), such\nthat after renaming some of the indices of the vs, the families (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace of E. The vector um+1 can also be expressed as a lin-\near combination of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate\nthe same subspace, um+1 can be expressed as a linear combination of (u1, . . . , um, vm+1, . . .,\nvn), say\n\num+1 =\nm∑\ni=1\n\nλiui +\nn∑\n\nj=m+1\n\nλjvj.\n\nWe claim that λj 6= 0 for some j with m+ 1 ≤ j ≤ n, which implies that m+ 1 ≤ n.\n\nOtherwise, we would have\n\num+1 =\nm∑\ni=1\n\nλiui,\n\na nontrivial linear dependence of the ui, which is impossible since (u1, . . . , um+1) are linearly\nindependent.\n\nTherefore, m + 1 ≤ n, and after renaming indices if necessary, we may assume that\nλm+1 6= 0, so we get\n\nvm+1 = −\nm∑\ni=1\n\n(λ−1\nm+1λi)ui − λ−1\n\nm+1um+1 −\nn∑\n\nj=m+2\n\n(λ−1\nm+1λj)vj.\n\nObserve that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1, vm+2, . . . , vn) generate\nthe same subspace, since um+1 is a linear combination of (u1, . . . , um, vm+1, . . . , vn) and vm+1\n\nis a linear combination of (u1, . . . , um+1, vm+2, . . . , vn). Since (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1, vm+2, . . . , vn) and\nand (v1, . . . , vn) generate the same subspace, which concludes the induction hypothesis.\n\nHere is an example illustrating the replacement lemma. Consider sequences (u1, u2, u3)\nand (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly independent family and with the uis\nexpressed in terms of the vjs as follows:\n\nu1 = v4 + v5\n\nu2 = v3 + v4 − v5\n\nu3 = v1 + v2 + v3.\n\nFrom the first equation we get\n\nv4 = u1 − v5,\n\n\n\n3.5. BASES OF A VECTOR SPACE 77\n\nand by substituting in the second equation we have\n\nu2 = v3 + v4 − v5 = v3 + u1 − v5 − v5 = u1 + v3 − 2v5.\n\nFrom the above equation we get\n\nv3 = −u1 + u2 + 2v5,\n\nand so\nu3 = v1 + v2 + v3 = v1 + v2 − u1 + u2 + 2v5.\n\nFinally, we get\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nTherefore we have\n\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nv3 = −u1 + u2 + 2v5\n\nv4 = u1 − v5,\n\nwhich shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3, v4, v5). The vectors\n(v1, v3, v4) have been replaced by (u1, u2, u3), and the vectors left over are (v2, v5). We can\nrename them (v4, v5).\n\nFor the sake of completeness, here is a more formal statement of the replacement lemma\n(and its proof).\n\nProposition 3.10. (Replacement lemma, version 2) Given a vector space E, let (ui)i∈I be\nany finite linearly independent family in E, where |I| = m, and let (vj)j∈J be any finite family\nsuch that every ui is a linear combination of (vj)j∈J , where |J | = n. Then there exists a set\nL and an injection ρ : L→ J (a relabeling function) such that L ∩ I = ∅, |L| = n−m, and\nthe families (ui)i∈I ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E. In particular,\nm ≤ n.\n\nProof. We proceed by induction on |I| = m. When m = 0, the family (ui)i∈I is empty, and\nthe proposition holds trivially with L = J (ρ is the identity). Assume |I| = m+ 1. Consider\nthe linearly independent family (ui)i∈(I−{p}), where p is any member of I. By the induction\nhypothesis, there exists a set L and an injection ρ : L → J such that L ∩ (I − {p}) = ∅,\n|L| = n−m, and the families (ui)i∈(I−{p})∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace\nof E. If p ∈ L, we can replace L by (L− {p}) ∪ {p′} where p′ does not belong to I ∪ L, and\nreplace ρ by the injection ρ′ which agrees with ρ on L − {p} and such that ρ′(p′) = ρ(p).\nThus, we can always assume that L ∩ I = ∅. Since up is a linear combination of (vj)j∈J\nand the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E, up is\na linear combination of (ui)i∈(I−{p}) ∪ (vρ(l))l∈L. Let\n\nup =\n∑\n\ni∈(I−{p})\nλiui +\n\n∑\nl∈L\n\nλlvρ(l). (1)\n\n\n\n78 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf λl = 0 for all l ∈ L, we have ∑\ni∈(I−{p})\n\nλiui − up = 0,\n\ncontradicting the fact that (ui)i∈I is linearly independent. Thus, λl 6= 0 for some l ∈ L, say\nl = q. Since λq 6= 0, we have\n\nvρ(q) =\n∑\n\ni∈(I−{p})\n(−λ−1\n\nq λi)ui + λ−1\nq up +\n\n∑\nl∈(L−{q})\n\n(−λ−1\nq λl)vρ(l). (2)\n\nWe claim that the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (ui)i∈I ∪ (vρ(l))l∈(L−{q}) generate the\nsame subset of E. Indeed, the second family is obtained from the first by replacing vρ(q) by up,\nand vice-versa, and up is a linear combination of (ui)i∈(I−{p})∪ (vρ(l))l∈L, by (1), and vρ(q) is a\nlinear combination of (ui)i∈I∪(vρ(l))l∈(L−{q}), by (2). Thus, the families (ui)i∈I∪(vρ(l))l∈(L−{q})\nand (vj)j∈J generate the same subspace of E, and the proposition holds for L−{q} and the\nrestriction of the injection ρ : L→ J to L−{q}, since L∩ I = ∅ and |L| = n−m imply that\n(L− {q}) ∩ I = ∅ and |L− {q}| = n− (m+ 1).\n\nThe idea is that m of the vectors vj can be replaced by the linearly independent uis in\nsuch a way that the same subspace is still generated. The purpose of the function ρ : L→ J\nis to pick n −m elements j1, . . . , jn−m of J and to relabel them l1, . . . , ln−m in such a way\nthat these new indices do not clash with the indices in I; this way, the vectors vj1 , . . . , vjn−m\nwho “survive” (i.e. are not replaced) are relabeled vl1 , . . . , vln−m , and the other m vectors vj\nwith j ∈ J −{j1, . . . , jn−m} are replaced by the ui. The index set of this new family is I ∪L.\n\nActually, one can prove that Proposition 3.10 implies Theorem 3.7 when the vector space\nis finitely generated. Putting Theorem 3.7 and Proposition 3.10 together, we obtain the\nfollowing fundamental theorem.\n\nTheorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E\ncontains a subfamily (uj)j∈J which is a basis of E. Any linearly independent family (ui)i∈I\ncan be extended to a family (uj)j∈J which is a basis of E (with I ⊆ J). Furthermore, for\nevery two bases (ui)i∈I and (vj)j∈J of E, we have |I| = |J | = n for some fixed integer n ≥ 0.\n\nProof. The first part follows immediately by applying Theorem 3.7 with L = ∅ and S =\n(ui)i∈I . For the second part, consider the family S ′ = (ui)i∈I ∪ (vh)h∈H , where (vh)h∈H is any\nfinitely generated family generating E, and with I ∩ H = ∅. Then apply Theorem 3.7 to\nL = (ui)i∈I and to S ′. For the last statement, assume that (ui)i∈I and (vj)j∈J are bases of\nE. Since (ui)i∈I is linearly independent and (vj)j∈J spans E, Proposition 3.10 implies that\n|I| ≤ |J |. A symmetric argument yields |J | ≤ |I|.\n\nRemark: Theorem 3.11 also holds for vector spaces that are not finitely generated. This\ncan be shown as follows. Let (ui)i∈I be a basis of E, let (vj)j∈J be a generating family of E,\n\n\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that I is infinite. For every j ∈ J , let Lj ⊆ I be the finite set\n\nLj = {i ∈ I | vj =\n∑\ni∈I\n\nλiui, λi 6= 0}.\n\nLet L =\n⋃\nj∈J Lj. By definition L ⊆ I, and since (ui)i∈I is a basis of E, we must have I = L,\n\nsince otherwise (ui)i∈L would be another basis of E, and this would contradict the fact that\n(ui)i∈I is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe Lj are finite, I would be finite. But then, since I =\n\n⋃\nj∈J Lj with J infinite and the Lj\n\nfinite, by a standard result of set theory, |I| ≤ |J |. If (vj)j∈J is also a basis, by a symmetric\nargument, we obtain |J | ≤ |I|, and thus, |I| = |J | for any two bases (ui)i∈I and (vj)j∈J of E.\n\nDefinition 3.8. When a vector space E is not finitely generated, we say that E is of infinite\ndimension. The dimension of a finitely generated vector space E is the common dimension\nn of all of its bases and is denoted by dim(E).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K\nand a 6= 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n ≥ 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n−1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (ui)i∈I be a basis of a vector space E. For any vector v ∈ E, since the family (ui)i∈I\ngenerates E, there is a family (λi)i∈I of scalars in K, such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nA very important fact is that the family (λi)i∈I is unique.\n\nProposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E,\nand assume that v =\n\n∑\ni∈I λiui. Then the family (λi)i∈I of scalars such that v =\n\n∑\ni∈I λiui\n\nis unique iff (ui)i∈I is linearly independent.\n\nProof. First, assume that (ui)i∈I is linearly independent. If (µi)i∈I is another family of scalars\nin K such that v =\n\n∑\ni∈I µiui, then we have∑\n\ni∈I\n(λi − µi)ui = 0,\n\nand since (ui)i∈I is linearly independent, we must have λi−µi = 0 for all i ∈ I, that is, λi = µi\nfor all i ∈ I. The converse is shown by contradiction. If (ui)i∈I was linearly dependent, there\nwould be a family (µi)i∈I of scalars not all null such that∑\n\ni∈I\nµiui = 0\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that J is infinite. For every 7 € J, let L; C I be the finite set\n\nL; = {i el | vj = Sodus, Ai A OF-\ntel\nLet L= Uses L,;. By definition L C I, and since (u;);er is a basis of E, we must have J = L,\nsince otherwise (u;);ez would be another basis of FE’, and this would contradict the fact that\n(uwi)icr is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe L; are finite, J would be finite. But then, since J = U,-; £; with J infinite and the L;\nfinite, by a standard result of set theory, || < |J|. If (v;)j;e, is also a basis, by a symmetric\nargument, we obtain |J| < ||, and thus, |/| = |J| for any two bases (u;)ie7 and (v;)je7 of E.\n\nDefinition 3.8. When a vector space F is not finitely generated, we say that F is of infinite\ndimension. The dimension of a finitely generated vector space FE is the common dimension\nn of all of its bases and is denoted by dim(£).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a €\nand a # 0 is a basis. Thus dim(/‘) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n > 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n—1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (u;);er be a basis of a vector space EF. For any vector v € E, since the family (w,;)ie7\ngenerates EF’, there is a family (A;);c, of scalars in A’, such that\n\ni€l\nA very important fact is that the family (\\;)icr is unique.\n\nProposition 3.12. Given a vector space E, let (u;);er be a family of vectors in E. Letv € E,\nand assume that v = Yo,-, Ait. Then the family (Aj)ier of scalars such that v = Doi.) iti\nis unique iff (u;)ier ts linearly independent.\n\nwel\n\nProof. First, assume that (u;)jer is linearly independent. If (u;);er is another family of scalars\nin K such that v = }0,-; fii, then we have\n\nYOu = Hi)us = 0,\niel\n\nand since (w,;)ic7 is linearly independent, we must have \\;—/4; = 0 for alli € J, that is, A; = 14;\nfor alli € I. The converse is shown by contradiction. If (u;);¢7; was linearly dependent, there\nwould be a family (ju;);er of scalars not all null such that\n\nwel\n\n\n\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand µj 6= 0 for some j ∈ I. But then,\n\nv =\n∑\ni∈I\n\nλiui + 0 =\n∑\ni∈I\n\nλiui +\n∑\ni∈I\n\nµiui =\n∑\ni∈I\n\n(λi + µi)ui,\n\nwith λj 6= λj+µj since µj 6= 0, contradicting the assumption that (λi)i∈I is the unique family\nsuch that v =\n\n∑\ni∈I λiui.\n\nDefinition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is\nthe unique family of scalars in K such that\n\nv =\n∑\ni∈I\n\nxiui,\n\neach xi is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I .\n\nGiven a field K and any (nonempty) set I, we can form a vector space K(I) which, in\nsome sense, is the standard vector space of dimension |I|.\nDefinition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the\ncartesian product KI consisting of all families (λi)i∈I with finite support of scalars in K.3\n\nWe define addition and multiplication by a scalar as follows:\n\n(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,\n\nand\nλ · (µi)i∈I = (λµi)i∈I .\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K(I) is a vector space. Furthermore, because families with finite support are consid-\nered, the family (ei)i∈I of vectors ei, defined such that (ei)j = 0 if j 6= i and (ei)i = 1, is\nclearly a basis of the vector space K(I). When I = {1, . . . , n}, we denote K(I) by Kn. The\nfunction ι : I → K(I), such that ι(i) = ei for every i ∈ I, is clearly an injection.\n\n� When I is a finite set, K(I) = KI , but this is false when I is infinite. In fact, dim(K(I)) =\n|I|, but dim(KI) is strictly greater when I is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where KI denotes the set of all functions from I to K.\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand yu; #0 for some j € J. But then,\n\niel iel ie] ie]\nwith \\; A A; +; since wu; A 0, contradicting the assumption that (A;);er is the unique family\nsuch that v = So,e; Aiti- O\n\nDefinition 3.10. If (u;)jc7 is a basis of a vector space FE, for any vector v € EF, if (x;);e7 is\nthe unique family of scalars in K such that\n\nv= y Tis,\n\ntel\n\neach x; is called the component (or coordinate) of index i of v with respect to the basis (u;)ier-\n\nGiven a field K and any (nonempty) set J, we can form a vector space K“) which, in\nsome sense, is the standard vector space of dimension |J].\n\nDefinition 3.11. Given a field K and any (nonempty) set J, let K“™ be the subset of the\ncartesian product K?‘ consisting of all families (\\;);e¢7 with finite support of scalars in K.°\nWe define addition and multiplication by a scalar as follows:\n\n(Ai)ier + (Mi)ier = (Ai + Midier,\n\nand\nA+ (Mi)ier = (AMi)ier-\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K“ is a vector space. Furthermore, because families with finite support are consid-\nered, the family (e;)ie; of vectors e;, defined such that (e;); = 0 if 7 A i and (e;); = 1, is\nclearly a basis of the vector space K“). When J = {1,...,n}, we denote K“) by K”. The\nfunction 1: [+ K“, such that v(i) = e; for every i € I, is clearly an injection.\n\n© When J is a finite set, K“) = K', but this is false when J is infinite. In fact, dim(K”) =\n|Z|, but dim(K“) is strictly greater when J is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where K! denotes the set of all functions from I to K.\n\n\n\n\n3.6. MATRICES 81\n\nDefinition 3.12. If K = R or K = C, an m×n-matrix over K is a family (ai j)1≤i≤m, 1≤j≤n\nof scalars in K, represented by an array\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nIn the special case where m = 1, we have a row vector , represented by\n\n(a1 1 · · · a1n)\n\nand in the special case where n = 1, we have a column vector , represented bya1 1\n...\n\nam 1\n\n .\n\nIn these last two cases, we usually omit the constant index 1 (first index in case of a row,\nsecond index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K)\nor Mm,n. An n × n-matrix is called a square matrix of dimension n. The set of all square\nmatrices of dimension n is denoted by Mn(K), or Mn.\n\nRemark: As defined, a matrix A = (ai j)1≤i≤m, 1≤j≤n is a family , that is, a function from\n{1, 2, . . . ,m} × {1, 2, . . . , n} to K. As such, there is no reason to assume an ordering on\nthe indices. Thus, the matrix A can be represented in many different ways as an array, by\nadopting different orders for the rows or the columns. However, it is customary (and usually\nconvenient) to assume the natural ordering on the sets {1, 2, . . . ,m} and {1, 2, . . . , n}, and\nto represent A as an array according to this ordering of the rows and columns.\n\nWe define some operations on matrices as follows.\n\nDefinition 3.13. Given two m × n matrices A = (ai j) and B = (bi j), we define their sum\nA+B as the matrix C = (ci j) such that ci j = ai j + bi j; that is,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n+\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbm 1 bm 2 . . . bmn\n\n\n\n=\n\n\na1 1 + b1 1 a1 2 + b1 2 . . . a1n + b1n\n\na2 1 + b2 1 a2 2 + b2 2 . . . a2n + b2n\n...\n\n...\n. . .\n\n...\nam 1 + bm 1 am 2 + bm 2 . . . amn + bmn\n\n .\n\n\n\n82 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nFor any matrix A = (ai j), we let −A be the matrix (−ai j). Given a scalar λ ∈ K, we define\nthe matrix λA as the matrix C = (ci j) such that ci j = λai j; that is\n\nλ\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n =\n\n\nλa1 1 λa1 2 . . . λa1n\n\nλa2 1 λa2 2 . . . λa2n\n...\n\n...\n. . .\n\n...\nλam 1 λam 2 . . . λamn\n\n .\n\nGiven an m×n matrices A = (ai k) and an n× p matrices B = (bk j), we define their product\nAB as the m× p matrix C = (ci j) such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nb1 1 b1 2 . . . b1 p\n\nb2 1 b2 2 . . . b2 p\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bn p\n\n =\n\n\nc1 1 c1 2 . . . c1 p\n\nc2 1 c2 2 . . . c2 p\n...\n\n...\n. . .\n\n...\ncm 1 cm 2 . . . cmp\n\n ,\n\nnote that the entry of index i and j of the matrix AB obtained by multiplying the matrices\nA and B can be identified with the product of the row matrix corresponding to the i-th row\nof A with the column matrix corresponding to the j-column of B:\n\n(ai 1 · · · ai n)\n\nb1 j\n...\nbn j\n\n =\nn∑\nk=1\n\nai kbk j.\n\nDefinition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0\neverywhere else is called the identity matrix . It is denoted by\n\nIn =\n\n\n1 0 . . . 0\n0 1 . . . 0\n...\n\n...\n. . .\n\n...\n0 0 . . . 1\n\n\nDefinition 3.15. Given an m × n matrix A = (ai j), its transpose A> = (a>j i), is the\nn×m-matrix such that a>j i = ai j, for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n.\n\nThe transpose of a matrix A is sometimes denoted by At, or even by tA. Note that the\ntranspose A> of a matrix A has the property that the j-th row of A> is the j-th column of\n\n\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5× 6 matrix\n\nA =\n\n\n1 2 3 4 5 6\n7 1 2 3 4 5\n8 7 1 2 3 4\n9 8 7 1 2 3\n10 9 8 7 1 2\n\n ,\n\nthen A> is the 6× 5 matrix\n\nA> =\n\n\n1 7 8 9 10\n2 1 7 8 9\n3 2 1 7 8\n4 3 2 1 7\n5 4 3 2 1\n6 5 4 3 2\n\n .\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm× n matrix A and any n× p matrix B, if we denote the columns of A by A1, . . . , An and\nthe rows of B by B1, . . . , Bn, then we have\n\nAB = A1B1 + · · ·+ AnBn.\n\nFor every square matrix A of dimension n, it is immediately verified that AIn = InA = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A−1. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = I, or a right\ninverse, that is a matrix C such that AC = I, then A is actually invertible; so B = A−1 and\nC = A−1. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set Mm,n(K) of m×n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m × n-matrices Eij = (eh k), are defined such that ei j = 1, and\neh k = 0, if h 6= i or k 6= j; in other words, the (i, j)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the Eij matrices for m = 2 and n = 3:\n\nE11 =\n\n(\n1 0 0\n0 0 0\n\n)\n, E12 =\n\n(\n0 1 0\n0 0 0\n\n)\n, E13 =\n\n(\n0 0 1\n0 0 0\n\n)\nE21 =\n\n(\n0 0 0\n1 0 0\n\n)\n, E22 =\n\n(\n0 0 0\n0 1 0\n\n)\n, E23 =\n\n(\n0 0 0\n0 0 1\n\n)\n.\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5 x 6 matrix\n\n1 23 4 5 6\n\n7 12 3 4 5\nA=/]8 712 3 4],\n\n9 8 712 8\n\n1098 7 1 2\n\nthen A! is the 6 x 5 matrix\n\n1 7 8 9 10\n\n2178 9\n\nTt 3.217 8\n\nA l43217\n\n5 43 2 1\n\n65 4 3 2\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm Xn matrix A and any n Xx p matrix B, if we denote the columns of A by A!,..., A” and\nthe rows of B by B,,..., By, then we have\n\nAB = A'B, +---+A\"B,.\nFor every square matrix A of dimension n, it is immediately verified that Al, = [,A = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = I, exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A+. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = J, or a right\ninverse, that is a matrix C such that AC = J, then A is actually invertible; so B = A~' and\nC= A7!. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set My,,(J¢) of m x n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m x n-matrices E;; = (en,), are defined such that e;; = 1, and\neng = 0, if h A7 or k ¥ J; in other words, the (7, 7)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the £;; matrices for m = 2 and n = 3:\n\n1 0 0 0 1 0 0\nEy — (; ) ’ Ex, — (; 0 ) ’ E\\3 —_ (;\n0 0 0 0 0 0\nEo, — (; ) ’ Dey) — (j 1 ) ’ Eo —_ (\n\noOo oe\nor\nNW\n\noOo GO\n\n\n\n\n84 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is clear that every matrix A = (ai j) ∈ Mm,n(K) can be written in a unique way as\n\nA =\nm∑\ni=1\n\nn∑\nj=1\n\nai jEij.\n\nThus, the family (Eij)1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension\nmn.\n\nRemark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (com-\nmutative) ring rather than a field. In this more general setting, the framework of vector\nspaces is too narrow, but we can consider structures over a commutative ring A satisfying\nall the axioms of Definition 3.1. Such structures are called modules . The theory of modules\nis (much) more complicated than that of vector spaces. For example, modules do not always\nhave a basis, and other properties holding for vector spaces usually fail for modules. When\na module has a basis, it is called a free module. For example, when A is a commutative\nring, the structure An is a module such that the vectors ei, with (ei)i = 1 and (ei)j = 0 for\nj 6= i, form a basis of An. Many properties of vector spaces still hold for An. Thus, An is a\nfree module. As another example, when A is a commutative ring, Mm,n(A) is a free module\nwith basis (Ei,j)1≤i≤m,1≤j≤n. Polynomials over a commutative ring also form a free module\nof infinite dimension.\n\nThe properties listed in Proposition 3.13 are easily verified, although some of the com-\nputations are a bit tedious. A more conceptual proof is given in Proposition 4.1.\n\nProposition 3.13. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nThe properties of Proposition 3.13 together with the fact that AIn = InA = A for all\nsquare n×n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).\nThis is a noncommutative ring with zero divisors, as shown by the following example.\n\n\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A,B be the 2× 2-matrices\n\nA =\n\n(\n1 0\n0 0\n\n)\n, B =\n\n(\n0 0\n1 0\n\n)\n,\n\nthen\n\nAB =\n\n(\n1 0\n0 0\n\n)(\n0 0\n1 0\n\n)\n=\n\n(\n0 0\n0 0\n\n)\n,\n\nand\n\nBA =\n\n(\n0 0\n1 0\n\n)(\n1 0\n0 0\n\n)\n=\n\n(\n0 0\n1 0\n\n)\n.\n\nThus AB 6= BA, and AB = 0, even though both A,B 6= 0.\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F . A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\n\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces E and F , a linear map between E and F is a\nfunction f : E → F satisfying the following two conditions:\n\nf(x+ y) = f(x) + f(y) for all x, y ∈ E;\n\nf(λx) = λf(x) for all λ ∈ K, x ∈ E.\n\nSetting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui).\n\nThe above identity is shown by induction on |I| using the properties of Definition 3.18.\n\nExample 3.6.\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A, B be the 2 x 2-matrices\n1 0 0 0\n4=(0o)- (0):\n1 0\\ /0 O 0 0\nap=(5 (Co) = (0 0):\n0 0\\ /1l 0 0 0\npa=(1 a) (0.0) =( 0):\n\nThus AB 4 BA, and AB = 0, even though both A, B ¥ 0.\n\nthen\n\nand\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F’. A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces EF’ and F’, a linear map between FE and F is a\nfunction f: EF — F satisfying the following two conditions:\n\nf(at+y) = f(x) + f(y) for all x,y € E;\nf(Ar) = Af (x) forallAc Kk, xe LE.\n\nSetting x = y = 0 in the first identity, we get f(0) =0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(u;)icr of vectors in EL, given any family (A;)ier of scalars in K, we have\n\nThe above identity is shown by induction on |/| using the properties of Definition 3.18.\n\nExample 3.6.\n\n\n\n\n86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n1. The map f : R2 → R2 defined such that\n\nx′ = x− y\ny′ = x+ y\n\nis a linear map. The reader should check that it is the composition of a rotation by\nπ/4 with a magnification of ratio\n\n√\n2.\n\n2. For any vector space E, the identity map id : E → E given by\n\nid(u) = u for all u ∈ E\n\nis a linear map. When we want to be more precise, we write idE instead of id.\n\n3. The map D : R[X]→ R[X] defined such that\n\nD(f(X)) = f ′(X),\n\nwhere f ′(X) is the derivative of the polynomial f(X), is a linear map.\n\n4. The map Φ: C([a, b])→ R given by\n\nΦ(f) =\n\n∫ b\n\na\n\nf(t)dt,\n\nwhere C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear\nmap.\n\n5. The function 〈−,−〉 : C([a, b])× C([a, b])→ R given by\n\n〈f, g〉 =\n\n∫ b\n\na\n\nf(t)g(t)dt,\n\nis linear in each of the variable f , g. It also satisfies the properties 〈f, g〉 = 〈g, f〉 and\n〈f, f〉 = 0 iff f = 0. It is an example of an inner product .\n\nDefinition 3.19. Given a linear map f : E → F , we define its image (or range) Im f = f(E),\nas the set\n\nIm f = {y ∈ F | (∃x ∈ E)(y = f(x))},\n\nand its Kernel (or nullspace) Ker f = f−1(0), as the set\n\nKer f = {x ∈ E | f(x) = 0}.\n\n86\n\nCHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n. The map f: R? > R? defined such that\n\n/\n\nty\n\n/\n\nis a linear map. The reader should check that it is the composition of a rotation by\nx/4 with a magnification of ratio V2.\n\n. For any vector space E, the identity map id: E > E given by\n\nid(u) =u foralue E\n\nis a linear map. When we want to be more precise, we write idg instead of id.\n\n. The map D: R[X] — R[X] defined such that\n\nwhere f’(X) is the derivative of the polynomial f(X), is a linear map.\n\n. The map ®: C({a,b]) > R given by\n\na(f) = / f(t)at,\n\nwhere C(|a, b]) is the set of continuous functions defined on the interval |a, b], is a linear\nmap.\n\n. The function (—, —): C([a,b]) x C([a, b]) > R given by\n\n(f,9) = / f(t)g(t)dt,\n\nis linear in each of the variable f, g. It also satisfies the properties (f,g) = (g, f) and\n(f, f) = 0 iff f = 0. It is an example of an inner product.\n\nDefinition 3.19. Given a linear map f: EF — F’, we define its image (or range) Im f = f(F),\nas the set\n\nIm f= {ty € F'| (Se € E\\(y = f(2))},\n\nand its Kernel (or nullspace) Ker f = f~'(0), as the set\n\nKer f = {x € E | f(x) =O}.\n\n\n\n\n3.7. LINEAR MAPS 87\n\nThe derivative map D : R[X] → R[X] from Example 3.6(3) has kernel the constant\npolynomials, so KerD = R. If we consider the second derivative D ◦D : R[X]→ R[X], then\nthe kernel of D ◦D consists of all polynomials of degree ≤ 1. The image of D : R[X]→ R[X]\nis actually R[X] itself, because every polynomial P (X) = a0X\n\nn + · · ·+ an−1X + an of degree\nn is the derivative of the polynomial Q(X) of degree n+ 1 given by\n\nQ(X) = a0\nXn+1\n\nn+ 1\n+ · · ·+ an−1\n\nX2\n\n2\n+ anX.\n\nOn the other hand, if we consider the restriction of D to the vector space R[X]n of polyno-\nmials of degree ≤ n, then the kernel of D is still R, but the image of D is the R[X]n−1, the\nvector space of polynomials of degree ≤ n− 1.\n\nProposition 3.14. Given a linear map f : E → F , the set Im f is a subspace of F and the\nset Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0) (where\n(0) is the trivial subspace {0}).\n\nProof. Given any x, y ∈ Im f , there are some u, v ∈ E such that x = f(u) and y = f(v),\nand for all λ, µ ∈ K, we have\n\nf(λu+ µv) = λf(u) + µf(v) = λx+ µy,\n\nand thus, λx+ µy ∈ Im f , showing that Im f is a subspace of F .\n\nGiven any x, y ∈ Ker f , we have f(x) = 0 and f(y) = 0, and thus,\n\nf(λx+ µy) = λf(x) + µf(y) = 0,\n\nthat is, λx+ µy ∈ Ker f , showing that Ker f is a subspace of E.\n\nFirst, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.\nHowever, if f(x) = f(y), then f(x) − f(y) = 0, and by linearity of f we get f(x − y) = 0.\nBecause Ker f = (0), we must have x − y = 0, that is x = y, so f is injective. Conversely,\nassume that f is injective. If x ∈ Ker f , that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),\nand by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff\nKer f = (0).\n\nSince by Proposition 3.14, the image Im f of a linear map f is a subspace of F , we can\ndefine the rank rk(f) of f as the dimension of Im f .\n\nDefinition 3.20. Given a linear map f : E → F , the rank rk(f) of f is the dimension of\nthe image Im f of f .\n\nA fundamental property of bases in a vector space is that they allow the definition of\nlinear maps as unique homomorphic extensions, as shown in the following proposition.\n\n\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F , given any basis (ui)i∈I of E,\ngiven any other family of vectors (vi)i∈I in F , there is a unique linear map f : E → F such\nthat f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent,\nand f is surjective iff (vi)i∈I generates F .\n\nProof. If such a linear map f : E → F exists, since (ui)i∈I is a basis of E, every vector x ∈ E\ncan written uniquely as a linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nDefine the function f : E → F , by letting\n\nf(x) =\n∑\ni∈I\n\nxivi\n\nfor every x =\n∑\n\ni∈I xiui. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(ui) = vi.\n\nNow assume that f is injective. Let (λi)i∈I be any family of scalars, and assume that∑\ni∈I\n\nλivi = 0.\n\nSince vi = f(ui) for every i ∈ I, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui) =\n∑\ni∈I\n\nλivi = 0.\n\nSince f is injective iff Ker f = (0), we have∑\ni∈I\n\nλiui = 0,\n\nand since (ui)i∈I is a basis, we have λi = 0 for all i ∈ I, which shows that (vi)i∈I is linearly\nindependent. Conversely, assume that (vi)i∈I is linearly independent. Since (ui)i∈I is a basis\nof E, every vector x ∈ E is a linear combination x =\n\n∑\ni∈I λiui of (ui)i∈I . If\n\nf(x) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nthen ∑\ni∈I\n\nλivi =\n∑\ni∈I\n\nλif(ui) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nand λi = 0 for all i ∈ I because (vi)i∈I is linearly independent, which means that x = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise.\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F’, given any basis (u;)icr of E,\ngiven any other family of vectors (v;)icr in F, there is a unique linear map f: E > F such\nthat f(u;) = v; for alli € I. Furthermore, f is injective iff (vi)ier is linearly independent,\nand f is surjective iff (vi)icr generates F.\n\nProof. If such a linear map f: E — F exists, since (u;);c7 is a basis of E, every vector x € E\ncan written uniquely as a linear combination\n\nv= S- VyUj;,\nwel\nand by linearity, we must have\nf(z) = S- aif (ui) = S- LjUi-\niel i€l\nDefine the function f: E > F, by letting\nf(x) = Ss” LiVj\nwel\nfor every © = )0,-, iu. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(u;) = vu.\nNow assume that f is injective. Let (\\;)jcr be any family of scalars, and assume that\ni€l\nSince uv; = f(u;) for every i € I, we have\nfO, AjUi) = S- Af (ui) = S° Ai; = 0.\nie] i€l ier\nSince f is injective iff Ker f = (0), we have\nS- Aju = 0,\ni€l\nand since (wu;)ier is a basis, we have A; = 0 for all i € J, which shows that (v;)jer is linearly\n\nindependent. Conversely, assume that (v;);c7 is linearly independent. Since (u;);<r is a basis\nof E, every vector x € F is a linear combination « = )0,., Aiui of (u)ier. If\n\nf(z) = 0D Aju) = 9,\n\nthen\n\nSs\" Avi = S- Ai f (ui) = f>~ Aju) = 0,\n\niel iel ier\nand A; = 0 for all i € I because (v;)jer is linearly independent, which means that x7 = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise. im\n\n\n\n\n3.7. LINEAR MAPS 89\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nu = (0,0,1)\n3 v = (1,1)1v = (-1,1)\n\n2\n\nv = (1,0)\n3\n\nf(u )1\nf(u )\n\n2\n-\n\n2f(u  )3\n\nE = \n\nf\n\nF =\nR\n\nR\n2\n\n3\n\nf is not injective\n\ndefining f\n\nFigure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),\nv3 = (1, 0), define the unique linear map f : R3 → R2 by f(u1) = v1, f(u2) = v2, and\nf(u3) = v3. This map is surjective but not injective since f(u1 − u2) = f(u1) − f(u2) =\n(1, 1)− (−1, 1) = (2, 0) = 2f(u3) = f(2u3).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R3 and V = R2\n\nBy the second part of Proposition 3.15, an injective linear map f : E → F sends a basis\n(ui)i∈I to a linearly independent family (f(ui))i∈I of F , which is also a basis when f is\nbijective. Also, when E and F have the same finite dimension n, (ui)i∈I is a basis of E, and\nf : E → F is injective, then (f(ui))i∈I is a basis of F (by Proposition 3.8).\n\nWe can now show that the vector space K(I) of Definition 3.11 has a universal property\nthat amounts to saying that K(I) is the vector space freely generated by I. Recall that\nι : I → K(I), such that ι(i) = ei for every i ∈ I, is an injection from I to K(I).\n\nProposition 3.16. Given any set I, for any vector space F , and for any function f : I → F ,\nthere is a unique linear map f : K(I) → F , such that\n\nf = f ◦ ι,\n\nas in the following diagram:\n\nI ι //\n\nf !!CCCCCCCCC K(I)\n\nf\n��\nF\n\nProof. If such a linear map f : K(I) → F exists, since f = f ◦ ι, we must have\n\nf(i) = f(ι(i)) = f(ei),\n\n3.7. LINEAR MAPS 89\n\ndefining f\n\n2f(u3)\n\nfis not injective\n\nFigure 3.11: Given wu; = (1,0,0), w2 = (0,1,0), us = (0,0,1) and vy; = (1,1), ve = (-1,1),\nv3 = (1,0), define the unique linear map f: R? — R? by f(u1) = v1, flue) = vo, and\nf(u3) = v3. This map is surjective but not injective since f(u; — u2) = f(u1) — f(ue) =\n\n(1, 1) _ (-1, 1) = (2, 0) = 2 f (us) = f(2us).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R? and V = R?\n\nBy the second part of Proposition 3.15, an injective linear map f: E — F sends a basis\n(u;)icr to a linearly independent family (f(wu;))icr of F', which is also a basis when f is\nbijective. Also, when F and F have the same finite dimension n, (u;);ey is a basis of E', and\nf: E > F is injective, then (f(ui))ier is a basis of F' (by Proposition 3.8).\n\nWe can now show that the vector space K™) of Definition 3.11 has a universal property\nthat amounts to saying that AK“) is the vector space freely generated by J. Recall that\nu: I + K“, such that «(i) = e; for every i € I, is an injection from I to KY),\n\nProposition 3.16. Given any set I, for any vector space F’, and for any function f: I > F,\nthere is a unique linear map f: K\\“) — F, such that\n\nf=fou,\n\nas in the following diagram:\n\nProof. If such a linear map f: K“ > F exists, since f = f ov, we must have\n\nF(t) = Fu) = Fei),\n\n\n\n\n90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nfor every i ∈ I. However, the family (ei)i∈I is a basis of K(I), and (f(i))i∈I is a family of\nvectors in F , and by Proposition 3.15, there is a unique linear map f : K(I) → F such that\nf(ei) = f(i) for every i ∈ I, which proves the existence and uniqueness of a linear map f\nsuch that f = f ◦ ι.\n\nThe following simple proposition is also useful.\n\nProposition 3.17. Given any two vector spaces E and F , with F nontrivial, given any\nfamily (ui)i∈I of vectors in E, the following properties hold:\n\n(1) The family (ui)i∈I generates E iff for every family of vectors (vi)i∈I in F , there is at\nmost one linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\n(2) The family (ui)i∈I is linearly independent iff for every family of vectors (vi)i∈I in F ,\nthere is some linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\nProof. (1) If there is any linear map f : E → F such that f(ui) = vi for all i ∈ I, since\n(ui)i∈I generates E, every vector x ∈ E can be written as some linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nThis shows that f is unique if it exists. Conversely, assume that (ui)i∈I does not generate E.\nSince F is nontrivial, there is some some vector y ∈ F such that y 6= 0. Since (ui)i∈I does\nnot generate E, there is some vector w ∈ E that is not in the subspace generated by (ui)i∈I .\nBy Theorem 3.11, there is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the\nsame subspace. Since by hypothesis, w ∈ E is not in the subspace generated by (ui)i∈I0 , by\nLemma 3.6 and by Theorem 3.11 again, there is a basis (ej)j∈I0∪J of E, such that ei = ui\nfor all i ∈ I0, and w = ej0 for some j0 ∈ J . Letting (vi)i∈I be the family in F such that\nvi = 0 for all i ∈ I, defining f : E → F to be the constant linear map with value 0, we have\na linear map such that f(ui) = 0 for all i ∈ I. By Proposition 3.15, there is a unique linear\nmap g : E → F such that g(w) = y, and g(ej) = 0 for all j ∈ (I0 ∪ J)− {j0}. By definition\nof the basis (ej)j∈I0∪J of E, we have g(ui) = 0 for all i ∈ I, and since f 6= g, this contradicts\nthe fact that there is at most one such map. See Figure 3.12.\n\n(2) If the family (ui)i∈I is linearly independent, then by Theorem 3.11, (ui)i∈I can be\nextended to a basis of E, and the conclusion follows by Proposition 3.15. Conversely, assume\nthat (ui)i∈I is linearly dependent. Then there is some family (λi)i∈I of scalars (not all zero)\nsuch that ∑\n\ni∈I\nλiui = 0.\n\n\n\n3.7. LINEAR MAPS 91\nf\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nw = (0,0,1)\n\nw = (0,0,1)\n\ndefining f as the zero\n\ndefining g\ny = (1,0)\n\ng(w) = y\n\nFigure 3.12: Let E = R3 and F = R2. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not\ngenerate R3 since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach\nxy-plane to the origin.\n\nBy the assumption, for any nonzero vector y ∈ F , for every i ∈ I, there is some linear map\nfi : E → F , such that fi(ui) = y, and fi(uj) = 0, for j ∈ I − {i}. Then we would get\n\n0 = fi(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλifi(ui) = λiy,\n\nand since y 6= 0, this implies λi = 0 for every i ∈ I. Thus, (ui)i∈I is linearly independent.\n\nGiven vector spaces E, F , and G, and linear maps f : E → F and g : F → G, it is easily\nverified that the composition g ◦ f : E → G of f and g is a linear map.\n\nDefinition 3.21. A linear map f : E → F is an isomorphism iff there is a linear map\ng : F → E, such that\n\ng ◦ f = idE and f ◦ g = idF . (∗)\n\nThe map g in Definition 3.21 is unique. This is because if g and h both satisfy g◦f = idE,\nf ◦ g = idF , h ◦ f = idE, and f ◦ h = idF , then\n\ng = g ◦ idF = g ◦ (f ◦ h) = (g ◦ f) ◦ h = idE ◦ h = h.\n\nThe map g satisfying (∗) above is called the inverse of f and it is also denoted by f−1.\n\n\n\n92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nObserve that Proposition 3.15 shows that if F = Rn, then we get an isomorphism between\nany vector space E of dimension |J | = n and Rn. Proposition 3.15 also implies that if E\nand F are two vector spaces, (ui)i∈I is a basis of E, and f : E → F is a linear map which is\nan isomorphism, then the family (f(ui))i∈I is a basis of F .\n\nOne can verify that if f : E → F is a bijective linear map, then its inverse f−1 : F → E,\nas a function, is also a linear map, and thus f is an isomorphism.\n\nAnother useful corollary of Proposition 3.15 is this:\n\nProposition 3.18. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be\nany linear map. The following properties hold:\n\n(1) If f has a left inverse g, that is, if g is a linear map such that g ◦ f = id, then f is an\nisomorphism and f−1 = g.\n\n(2) If f has a right inverse h, that is, if h is a linear map such that f ◦ h = id, then f is\nan isomorphism and f−1 = h.\n\nProof. (1) The equation g ◦ f = id implies that f is injective; this is a standard result\nabout functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since\ng ◦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.15, since f is injective,\n(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of\nE (if (f(u1), . . . , f(un)) doesn’t span E, then it can be extended to a basis of dimension\nstrictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous\nobservation its inverse is a linear map. We also have\n\ng = g ◦ id = g ◦ (f ◦ f−1) = (g ◦ f) ◦ f−1 = id ◦ f−1 = f−1.\n\n(2) The equation f ◦ h = id implies that f is surjective; this is a standard result about\nfunctions (for any y ∈ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By\nProposition 3.15, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension\nn, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans\nE, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).\nThen f is bijective, and by a previous observation its inverse is a linear map. We also have\n\nh = id ◦ h = (f−1 ◦ f) ◦ h = f−1 ◦ (f ◦ h) = f−1 ◦ id = f−1.\n\nThis completes the proof.\n\nDefinition 3.22. The set of all linear maps between two vector spaces E and F is denoted by\nHom(E,F ) or by L(E;F ) (the notation L(E;F ) is usually reserved to the set of continuous\nlinear maps, where E and F are normed vector spaces). When we wish to be more precise and\nspecify the field K over which the vector spaces E and F are defined we write HomK(E,F ).\n\n\n\n3.8. QUOTIENT SPACES 93\n\nThe set Hom(E,F ) is a vector space under the operations defined in Example 3.1, namely\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ E, and\n(λf)(x) = λf(x)\n\nfor all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which\nuses the commutativity of ∗ in the field K (typically, K = R or K = C). Indeed, we have\n\n(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).\n\nWhen E and F have finite dimensions, the vector space Hom(E,F ) also has finite di-\nmension, as we shall see shortly.\n\nDefinition 3.23. When E = F , a linear map f : E → E is also called an endomorphism.\nThe space Hom(E,E) is also denoted by End(E).\n\nIt is also important to note that composition confers to Hom(E,E) a ring structure.\nIndeed, composition is an operation ◦ : Hom(E,E) × Hom(E,E) → Hom(E,E), which is\nassociative and has an identity idE, and the distributivity properties hold:\n\n(g1 + g2) ◦ f = g1 ◦ f + g2 ◦ f ;\n\ng ◦ (f1 + f2) = g ◦ f1 + g ◦ f2.\n\nThe ring Hom(E,E) is an example of a noncommutative ring.\n\nIt is easily seen that the set of bijective linear maps f : E → E is a group under compo-\nsition.\n\nDefinition 3.24. Bijective linear maps f : E → E are also called automorphisms . The\ngroup of automorphisms of E is called the general linear group (of E), and it is denoted by\nGL(E), or by Aut(E), or when E = Rn, by GL(n,R), or even by GL(n).\n\nAlthough in this book, we will not have many occasions to use quotient spaces, they are\nfundamental in algebra. The next section may be omitted until needed.\n\n3.8 Quotient Spaces\n\nLet E be a vector space, and let M be any subspace of E. The subspace M induces a relation\n≡M on E, defined as follows: For all u, v ∈ E,\n\nu ≡M v iff u− v ∈M .\n\nWe have the following simple proposition.\n\n\n\n94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.19. Given any vector space E and any subspace M of E, the relation ≡M\nis an equivalence relation with the following two congruential properties:\n\n1. If u1 ≡M v1 and u2 ≡M v2, then u1 + u2 ≡M v1 + v2, and\n\n2. if u ≡M v, then λu ≡M λv.\n\nProof. It is obvious that ≡M is an equivalence relation. Note that u1 ≡M v1 and u2 ≡M v2\n\nare equivalent to u1 − v1 = w1 and u2 − v2 = w2, with w1, w2 ∈M , and thus,\n\n(u1 + u2)− (v1 + v2) = w1 + w2,\n\nand w1 + w2 ∈ M , since M is a subspace of E. Thus, we have u1 + u2 ≡M v1 + v2. If\nu− v = w, with w ∈M , then\n\nλu− λv = λw,\n\nand λw ∈M , since M is a subspace of E, and thus λu ≡M λv.\n\nProposition 3.19 shows that we can define addition and multiplication by a scalar on the\nset E/M of equivalence classes of the equivalence relation ≡M .\n\nDefinition 3.25. Given any vector space E and any subspaceM of E, we define the following\noperations of addition and multiplication by a scalar on the set E/M of equivalence classes\nof the equivalence relation ≡M as follows: for any two equivalence classes [u], [v] ∈ E/M , we\nhave\n\n[u] + [v] = [u+ v],\n\nλ[u] = [λu].\n\nBy Proposition 3.19, the above operations do not depend on the specific choice of represen-\ntatives in the equivalence classes [u], [v] ∈ E/M . It is also immediate to verify that E/M is\na vector space. The function π : E → E/F , defined such that π(u) = [u] for every u ∈ E, is\na surjective linear map called the natural projection of E onto E/F . The vector space E/M\nis called the quotient space of E by the subspace M .\n\nGiven any linear map f : E → F , we know that Ker f is a subspace of E, and it is\nimmediately verified that Im f is isomorphic to the quotient space E/Ker f .\n\n3.9 Linear Forms and the Dual Space\n\nWe already observed that the field K itself (K = R or K = C) is a vector space (over itself).\nThe vector space Hom(E,K) of linear maps from E to the field K, the linear forms, plays\na particular role. In this section, we only define linear forms and show that every finite-\ndimensional vector space has a dual basis. A more advanced presentation of dual spaces and\nduality is given in Chapter 11.\n\n\n\n3.9. LINEAR FORMS AND THE DUAL SPACE 95\n\nDefinition 3.26. Given a vector space E, the vector space Hom(E,K) of linear maps from\nE to the field K is called the dual space (or dual) of E. The space Hom(E,K) is also denoted\nby E∗, and the linear maps in E∗ are called the linear forms , or covectors . The dual space\nE∗∗ of the space E∗ is called the bidual of E.\n\nAs a matter of notation, linear forms f : E → K will also be denoted by starred symbol,\nsuch as u∗, x∗, etc.\n\nIf E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear\nform f ∗ ∈ E∗, for every x = x1u1 + · · ·+ xnun ∈ E, by linearity we have\n\nf ∗(x) = f ∗(u1)x1 + · · ·+ f ∗(un)xn\n\n= λ1x1 + · · ·+ λnxn,\n\nwith λi = f ∗(ui) ∈ K for every i, 1 ≤ i ≤ n. Thus, with respect to the basis (u1, . . . , un),\nthe linear form f ∗ is represented by the row vector\n\n(λ1 · · · λn),\n\nwe have\n\nf ∗(x) =\n(\nλ1 · · · λn\n\n)x1\n...\nxn\n\n ,\n\na linear combination of the coordinates of x, and we can view the linear form f ∗ as a linear\nequation. If we decide to use a column vector of coefficients\n\nc =\n\nc1\n...\ncn\n\n\ninstead of a row vector, then the linear form f ∗ is defined by\n\nf ∗(x) = c>x.\n\nThe above notation is often used in machine learning.\n\nExample 3.7. Given any differentiable function f : Rn → R, by definition, for any x ∈ Rn,\nthe total derivative dfx of f at x is the linear form dfx : Rn → R defined so that for all\nu = (u1, . . . , un) ∈ Rn,\n\ndfx(u) =\n\n(\n∂f\n\n∂x1\n\n(x) · · · ∂f\n\n∂xn\n(x)\n\n)u1\n...\nun\n\n =\nn∑\ni=1\n\n∂f\n\n∂xi\n(x)ui.\n\n\n\n96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nExample 3.8. Let C([0, 1]) be the vector space of continuous functions f : [0, 1]→ R. The\nmap I : C([0, 1])→ R given by\n\nI(f) =\n\n∫ 1\n\n0\n\nf(x)dx for any f ∈ C([0, 1])\n\nis a linear form (integration).\n\nExample 3.9. Consider the vector space Mn(R) of real n×n matrices. Let tr : Mn(R)→ R\nbe the function given by\n\ntr(A) = a11 + a22 + · · ·+ ann,\n\ncalled the trace of A. It is a linear form. Let s : Mn(R)→ R be the function given by\n\ns(A) =\nn∑\n\ni,j=1\n\naij,\n\nwhere A = (aij). It is immediately verified that s is a linear form.\n\nGiven a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear\nform u∗i ∈ E∗, and the u∗i have some remarkable properties.\n\nDefinition 3.27. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.15,\nfor every i ∈ I, there is a unique linear form u∗i such that\n\nu∗i (uj) =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor every j ∈ I. The linear form u∗i is called the coordinate form of index i w.r.t. the basis\n(ui)i∈I .\n\nRemark: Given an index set I, authors often define the so called “Kronecker symbol” δi j\nsuch that\n\nδi j =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor all i, j ∈ I. Then, u∗i (uj) = δi j.\n\nThe reason for the terminology coordinate form is as follows: If E has finite dimension\nand if (u1, . . . , un) is a basis of E, for any vector\n\nv = λ1u1 + · · ·+ λnun,\n\nwe have\n\nu∗i (v) = u∗i (λ1u1 + · · ·+ λnun)\n\n= λ1u\n∗\ni (u1) + · · ·+ λiu\n\n∗\ni (ui) + · · ·+ λnu\n\n∗\ni (un)\n\n= λi,\n\n\n\n3.10. SUMMARY 97\n\nsince u∗i (uj) = δi j. Therefore, u∗i is the linear function that returns the ith coordinate of a\nvector expressed over the basis (u1, . . . , un).\n\nThe following theorem shows that in finite-dimension, every basis (u1, . . . , un) of a vector\nspace E yields a basis (u∗1, . . . , u\n\n∗\nn) of the dual space E∗, called a dual basis .\n\nTheorem 3.20. (Existence of dual bases) Let E be a vector space of dimension n. The\nfollowing properties hold: For every basis (u1, . . . , un) of E, the family of coordinate forms\n(u∗1, . . . , u\n\n∗\nn) is a basis of E∗ (called the dual basis of (u1, . . . , un)).\n\nProof. (a) If v∗ ∈ E∗ is any linear form, consider the linear form\n\nf ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nObserve that because u∗i (uj) = δi j,\n\nf ∗(ui) = (v∗(u1)u∗1 + · · ·+ v∗(un)u∗n)(ui)\n\n= v∗(u1)u∗1(ui) + · · ·+ v∗(ui)u\n∗\ni (ui) + · · ·+ v∗(un)u∗n(ui)\n\n= v∗(ui),\n\nand so f ∗ and v∗ agree on the basis (u1, . . . , un), which implies that\n\nv∗ = f ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nTherefore, (u∗1, . . . , u\n∗\nn) spans E∗. We claim that the covectors u∗1, . . . , u\n\n∗\nn are linearly inde-\n\npendent. If not, we have a nontrivial linear dependence\n\nλ1u\n∗\n1 + · · ·+ λnu\n\n∗\nn = 0,\n\nand if we apply the above linear form to each ui, using a familar computation, we get\n\n0 = λiu\n∗\ni (ui) = λi,\n\nproving that u∗1, . . . , u\n∗\nn are indeed linearly independent. Therefore, (u∗1, . . . , u\n\n∗\nn) is a basis of\n\nE∗.\n\nIn particular, Theorem 3.20 shows a finite-dimensional vector space and its dual E∗ have\nthe same dimension.\n\n3.10 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The notion of a vector space.\n\n• Families of vectors.\n\n\n\n98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n• Linear combinations of vectors; linear dependence and linear independence of a family\nof vectors.\n\n• Linear subspaces .\n\n• Spanning (or generating) family; generators , finitely generated subspace; basis of a\nsubspace.\n\n• Every linearly independent family can be extended to a basis (Theorem 3.7).\n\n• A family B of vectors is a basis iff it is a maximal linearly independent family iff it is\na minimal generating family (Proposition 3.8).\n\n• The replacement lemma (Proposition 3.10).\n\n• Any two bases in a finitely generated vector space E have the same number of elements ;\nthis is the dimension of E (Theorem 3.11).\n\n• Hyperplanes .\n\n• Every vector has a unique representation over a basis (in terms of its coordinates).\n\n• Matrices\n\n• Column vectors , row vectors .\n\n• Matrix operations : addition, scalar multiplication, multiplication.\n\n• The vector space Mm,n(K) of m × n matrices over the field K; The ring Mn(K) of\nn× n matrices over the field K.\n\n• The notion of a linear map.\n\n• The image Im f (or range) of a linear map f .\n\n• The kernel Ker f (or nullspace) of a linear map f .\n\n• The rank rk(f) of a linear map f .\n\n• The image and the kernel of a linear map are subspaces. A linear map is injective iff\nits kernel is the trivial space (0) (Proposition 3.14).\n\n• The unique homomorphic extension property of linear maps with respect to bases\n(Proposition 3.15 ).\n\n• Quotient spaces .\n\n• The vector space of linear maps HomK(E,F ).\n\n\n\n3.11. PROBLEMS 99\n\n• Linear forms (covectors) and the dual space E∗.\n\n• Coordinate forms.\n\n• The existence of dual bases (in finite dimension).\n\n3.11 Problems\n\nProblem 3.1. Let H be the set of 3× 3 upper triangular matrices given by\n\nH =\n\n\n1 a b\n\n0 1 c\n0 0 1\n\n | a, b, c ∈ R\n\n .\n\n(1) Prove that H with the binary operation of matrix multiplication is a group; find\nexplicitly the inverse of every matrix in H. Is H abelian (commutative)?\n\n(2) Given two groups G1 and G2, recall that a homomorphism if a function ϕ : G1 → G2\n\nsuch that\nϕ(ab) = ϕ(a)ϕ(b), a, b ∈ G1.\n\nProve that ϕ(e1) = e2 (where ei is the identity element of Gi) and that\n\nϕ(a−1) = (ϕ(a))−1, a ∈ G1.\n\n(3) Let S1 be the unit circle, that is\n\nS1 = {eiθ = cos θ + i sin θ | 0 ≤ θ < 2π},\n\nand let ϕ be the function given by\n\nϕ\n\n1 a b\n0 1 c\n0 0 1\n\n = (a, c, eib).\n\nProve that ϕ is a surjective function onto G = R × R × S1, and that if we define\nmultiplication on this set by\n\n(x1, y1, u1) · (x2, y2, u2) = (x1 + x2, y1 + y2, e\nix1y2u1u2),\n\nthen G is a group and ϕ is a group homomorphism from H onto G.\n\n(4) The kernel of a homomorphism ϕ : G1 → G2 is defined as\n\nKer (ϕ) = {a ∈ G1 | ϕ(a) = e2}.\n\nFind explicitly the kernel of ϕ and show that it is a subgroup of H.\n\n\n\n100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.2. For any m ∈ Z with m > 0, the subset mZ = {mk | k ∈ Z} is an abelian\nsubgroup of Z. Check this.\n\n(1) Give a group isomorphism (an invertible homomorphism) from mZ to Z.\n\n(2) Check that the inclusion map i : mZ→ Z given by i(mk) = mk is a group homomor-\nphism. Prove that if m ≥ 2 then there is no group homomorphism p : Z → mZ such that\np ◦ i = id.\n\nRemark: The above shows that abelian groups fail to have some of the properties of vector\nspaces. We will show later that a linear map satisfying the condition p◦ i = id always exists.\n\nProblem 3.3. Let E = R× R, and define the addition operation\n\n(x1, y1) + (x2, y2) = (x1 + x2, y1,+y2), x1, x2, y1, y2 ∈ R,\n\nand the multiplication operation · : R× E → E by\n\nλ · (x, y) = (λx, y), λ, x, y ∈ R.\n\nShow that E with the above operations + and · is not a vector space. Which of the\naxioms is violated?\n\nProblem 3.4. (1) Prove that the axioms of vector spaces imply that\n\nα · 0 = 0\n\n0 · v = 0\n\nα · (−v) = −(α · v)\n\n(−α) · v = −(α · v),\n\nfor all v ∈ E and all α ∈ K, where E is a vector space over K.\n\n(2) For every λ ∈ R and every x = (x1, . . . , xn) ∈ Rn, define λx by\n\nλx = λ(x1, . . . , xn) = (λx1, . . . , λxn).\n\nRecall that every vector x = (x1, . . . , xn) ∈ Rn can be written uniquely as\n\nx = x1e1 + · · ·+ xnen,\n\nwhere ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i. For any operation · : R×Rn →\nRn, if · satisfies the Axiom (V1) of a vector space, then prove that for any α ∈ R, we have\n\nα · x = α · (x1e1 + · · ·+ xnen) = α · (x1e1) + · · ·+ α · (xnen).\n\nConclude that · is completely determined by its action on the one-dimensional subspaces of\nRn spanned by e1, . . . , en.\n\n\n\n3.11. PROBLEMS 101\n\n(3) Use (2) to define operations · : R × Rn → Rn that satisfy the Axioms (V1–V3), but\nfor which Axiom V4 fails.\n\n(4) For any operation · : R×Rn → Rn, prove that if · satisfies the Axioms (V2–V3), then\nfor every rational number r ∈ Q and every vector x ∈ Rn, we have\n\nr · x = r(1 · x).\n\nIn the above equation, 1 · x is some vector (y1, . . . , yn) ∈ Rn not necessarily equal to x =\n(x1, . . . , xn), and\n\nr(1 · x) = (ry1, . . . , ryn),\n\nas in Part (2).\n\nUse (4) to conclude that any operation · : Q×Rn → Rn that satisfies the Axioms (V1–V3)\nis completely determined by the action of 1 on the one-dimensional subspaces of Rn spanned\nby e1, . . . , en.\n\nProblem 3.5. Let A1 be the following matrix:\n\nA1 =\n\n 2 3 1\n1 2 −1\n−3 −5 1\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 2,−7) over the basis consisting of the column vectors of A1.\n\nProblem 3.6. Let A2 be the following matrix:\n\nA2 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 3 0\n\n .\n\nExpress the fourth column of A2 as a linear combination of the first three columns of A2. Is\nthe vector x = (7, 14,−1, 2) a linear combination of the columns of A2?\n\nProblem 3.7. Let A3 be the following matrix:\n\nA3 =\n\n1 1 1\n1 1 2\n1 2 3\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 9, 14) over the basis consisting of the column vectors of A3.\n\n\n\n102 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.8. Let A4 be the following matrix:\n\nA4 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 4 0\n\n .\n\nProve that the columns of A4 are linearly independent. Find the coordinates of the vector\nx = (7, 14,−1, 2) over the basis consisting of the column vectors of A4.\n\nProblem 3.9. Consider the following Haar matrix\n\nH =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\nProve that the columns of H are linearly independent.\n\nHint . Compute the product H>H.\n\nProblem 3.10. Consider the following Hadamard matrix\n\nH4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n .\n\nProve that the columns of H4 are linearly independent.\n\nHint . Compute the product H>4 H4.\n\nProblem 3.11. In solving this problem, do not use determinants.\n\n(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some vector space E.\nAssume that each vi is a linear combination of the ujs, so that\n\nvi = ai 1u1 + · · ·+ aimum, 1 ≤ i ≤ m,\n\nand that the matrix A = (ai j) is an upper-triangular matrix, which means that if 1 ≤ j <\ni ≤ m, then ai j = 0. Prove that if (u1, . . . , um) are linearly independent and if all the\ndiagonal entries of A are nonzero, then (v1, . . . , vm) are also linearly independent.\n\nHint . Use induction on m.\n\n(2) Let A = (ai j) be an upper-triangular matrix. Prove that if all the diagonal entries of\nA are nonzero, then A is invertible and the inverse A−1 of A is also upper-triangular.\n\nHint . Use induction on m.\n\nProve that if A is invertible, then all the diagonal entries of A are nonzero.\n\n(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related as in (1), then\n(u1, . . . , um) are linearly independent iff (v1, . . . , vm) are linearly independent.\n\n\n\n3.11. PROBLEMS 103\n\nProblem 3.12. In solving this problem, do not use determinants. Consider the n × n\nmatrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n0 1 2 0 . . . 0 0\n0 0 1 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 0 1 2 0\n0 0 . . . 0 0 1 2\n0 0 . . . 0 0 0 1\n\n\n.\n\n(1) Find the solution x = (x1, . . . , xn) of the linear system\n\nAx = b,\n\nfor\n\nb =\n\n\nb1\n\nb2\n...\nbn\n\n .\n\n(2) Prove that the matrix A is invertible and find its inverse A−1. Given that the number\nof atoms in the universe is estimated to be ≤ 1082, compare the size of the coefficients the\ninverse of A to 1082, if n ≥ 300.\n\n(3) Assume b is perturbed by a small amount δb (note that δb is a vector). Find the new\nsolution of the system\n\nA(x+ δx) = b+ δb,\n\nwhere δx is also a vector. In the case where b = (0, . . . , 0, 1), and δb = (0, . . . , 0, ε), show\nthat\n\n|(δx)1| = 2n−1|ε|.\n(where (δx)1 is the first component of δx).\n\n(4) Prove that (A− I)n = 0.\n\nProblem 3.13. An n × n matrix N is nilpotent if there is some integer r ≥ 1 such that\nN r = 0.\n\n(1) Prove that if N is a nilpotent matrix, then the matrix I −N is invertible and\n\n(I −N)−1 = I +N +N2 + · · ·+N r−1.\n\n(2) Compute the inverse of the following matrix A using (1):\n\nA =\n\n\n1 2 3 4 5\n0 1 2 3 4\n0 0 1 2 3\n0 0 0 1 2\n0 0 0 0 1\n\n .\n\n\n\n104 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.14. (1) Let A be an n×n matrix. If A is invertible, prove that for any x ∈ Rn,\nif Ax = 0, then x = 0.\n\n(2) Let A be an m × n matrix and let B be an n ×m matrix. Prove that Im − AB is\ninvertible iff In −BA is invertible.\n\nHint . If for all x ∈ Rn, Mx = 0 implies that x = 0, then M is invertible.\n\nProblem 3.15. Consider the following n× n matrix, for n ≥ 3:\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n(1) If we denote the columns of B by b1, . . . , bn, prove that\n\n(n− 3)b1 − (b2 + · · ·+ bn) = 2(n− 2)e1\n\nb1 − b2 = 2(e1 + e2)\n\nb1 − b3 = 2(e1 + e3)\n\n...\n...\n\nb1 − bn = 2(e1 + en),\n\nwhere e1, . . . , en are the canonical basis vectors of Rn.\n\n(2) Prove that B is invertible and that its inverse A = (aij) is given by\n\na11 =\n(n− 3)\n\n2(n− 2)\n, ai1 = − 1\n\n2(n− 2)\n2 ≤ i ≤ n\n\nand\n\naii = − (n− 3)\n\n2(n− 2)\n, 2 ≤ i ≤ n\n\naji =\n1\n\n2(n− 2)\n, 2 ≤ i ≤ n, j 6= i.\n\n(3) Show that the n diagonal n× n matrices Di defined such that the diagonal entries of\nDi are equal the entries (from top down) of the ith column of B form a basis of the space of\n\n\n\n3.11. PROBLEMS 105\n\nn × n diagonal matrices (matrices with zeros everywhere except possibly on the diagonal).\nFor example, when n = 4, we have\n\nD1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n D2 =\n\n\n−1 0 0 0\n0 −1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nD3 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 −1 0\n0 0 0 1\n\n , D4 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 −1\n\n .\n\nProblem 3.16. Given any m×n matrix A and any n×p matrix B, if we denote the columns\nof A by A1, . . . , An and the rows of B by B1, . . . , Bn, prove that\n\nAB = A1B1 + · · ·+ AnBn.\n\nProblem 3.17. Let f : E → F be a linear map which is also a bijection (it is injective and\nsurjective). Prove that the inverse function f−1 : F → E is linear.\n\nProblem 3.18. Given two vectors spaces E and F , let (ui)i∈I be any basis of E and let\n(vi)i∈I be any family of vectors in F . Prove that the unique linear map f : E → F such that\nf(ui) = vi for all i ∈ I is surjective iff (vi)i∈I spans F .\n\nProblem 3.19. Let f : E → F be a linear map with dim(E) = n and dim(F ) = m. Prove\nthat f has rank 1 iff f is represented by an m× n matrix of the form\n\nA = uv>\n\nwith u a nonzero column vector of dimension m and v a nonzero column vector of dimension\nn.\n\nProblem 3.20. Find a nontrivial linear dependence among the linear forms\n\nϕ1(x, y, z) = 2x− y + 3z, ϕ2(x, y, z) = 3x− 5y + z, ϕ3(x, y, z) = 4x− 7y + z.\n\nProblem 3.21. Prove that the linear forms\n\nϕ1(x, y, z) = x+ 2y + z, ϕ2(x, y, z) = 2x+ 3y + 3z, ϕ3(x, y, z) = 3x+ 7y + z\n\nare linearly independent. Express the linear form ϕ(x, y, z) = x+y+z as a linear combination\nof ϕ1, ϕ2, ϕ3.\n\n\n\n106 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n\n\nChapter 4\n\nMatrices and Linear Maps\n\nIn this chapter, all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n4.1 Representation of Linear Maps by Matrices\n\nProposition 3.15 shows that given two vector spaces E and F and a basis (uj)j∈J of E, every\nlinear map f : E → F is uniquely determined by the family (f(uj))j∈J of the images under\nf of the vectors in the basis (uj)j∈J .\n\nIf we also have a basis (vi)i∈I of F , then every vector f(uj) can be written in a unique\nway as\n\nf(uj) =\n∑\ni∈I\n\nai jvi,\n\nwhere j ∈ J , for a family of scalars (ai j)i∈I . Thus, with respect to the two bases (uj)j∈J\nof E and (vi)i∈I of F , the linear map f is completely determined by a “I × J-matrix”\nM(f) = (ai j)i∈I, j∈J .\n\nRemark: Note that we intentionally assigned the index set J to the basis (uj)j∈J of E, and\nthe index set I to the basis (vi)i∈I of F , so that the rows of the matrix M(f) associated\nwith f : E → F are indexed by I, and the columns of the matrix M(f) are indexed by J .\nObviously, this causes a mildly unpleasant reversal. If we had considered the bases (ui)i∈I of\nE and (vj)j∈J of F , we would obtain a J × I-matrix M(f) = (aj i)j∈J, i∈I . No matter what\nwe do, there will be a reversal! We decided to stick to the bases (uj)j∈J of E and (vi)i∈I of\nF , so that we get an I × J-matrix M(f), knowing that we may occasionally suffer from this\ndecision!\n\nWhen I and J are finite, and say, when |I| = m and |J | = n, the linear map f is\ndetermined by the matrix M(f) whose entries in the j-th column are the components of the\n\n107\n\n\n\n108 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nvector f(uj) over the basis (v1, . . . , vm), that is, the matrix\n\nM(f) =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nwhose entry on Row i and Column j is ai j (1 ≤ i ≤ m, 1 ≤ j ≤ n).\n\nWe will now show that when E and F have finite dimension, linear maps can be very\nconveniently represented by matrices, and that composition of linear maps corresponds to\nmatrix multiplication. We will follow rather closely an elegant presentation method due to\nEmil Artin.\n\nLet E and F be two vector spaces, and assume that E has a finite basis (u1, . . . , un) and\nthat F has a finite basis (v1, . . . , vm). Recall that we have shown that every vector x ∈ E\ncan be written in a unique way as\n\nx = x1u1 + · · ·+ xnun,\n\nand similarly every vector y ∈ F can be written in a unique way as\n\ny = y1v1 + · · ·+ ymvm.\n\nLet f : E → F be a linear map between E and F . Then for every x = x1u1 + · · ·+ xnun in\nE, by linearity, we have\n\nf(x) = x1f(u1) + · · ·+ xnf(un).\n\nLet\nf(uj) = a1 jv1 + · · ·+ amjvm,\n\nor more concisely,\n\nf(uj) =\nm∑\ni=1\n\nai jvi,\n\nfor every j, 1 ≤ j ≤ n. This can be expressed by writing the coefficients a1j, a2j, . . . , amj of\nf(uj) over the basis (v1, . . . , vm), as the jth column of a matrix, as shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThen substituting the right-hand side of each f(uj) into the expression for f(x), we get\n\nf(x) = x1(\nm∑\ni=1\n\nai 1vi) + · · ·+ xn(\nm∑\ni=1\n\nai nvi),\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 109\n\nwhich, by regrouping terms to obtain a linear combination of the vi, yields\n\nf(x) = (\nn∑\nj=1\n\na1 jxj)v1 + · · ·+ (\nn∑\nj=1\n\namjxj)vm.\n\nThus, letting f(x) = y = y1v1 + · · ·+ ymvm, we have\n\nyi =\nn∑\nj=1\n\nai jxj (1)\n\nfor all i, 1 ≤ i ≤ m.\n\nTo make things more concrete, let us treat the case where n = 3 and m = 2. In this case,\n\nf(u1) = a11v1 + a21v2\n\nf(u2) = a12v1 + a22v2\n\nf(u3) = a13v1 + a23v2,\n\nwhich in matrix form is expressed by\n\nf(u1) f(u2) f(u3)\n\nv1\n\nv2\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)\n,\n\nand for any x = x1u1 + x2u2 + x3u3, we have\n\nf(x) = f(x1u1 + x2u2 + x3u3)\n\n= x1f(u1) + x2f(u2) + x3f(u3)\n\n= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)\n\n= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.\n\nConsequently, since\ny = y1v1 + y2v2,\n\nwe have\n\ny1 = a11x1 + a12x2 + a13x3\n\ny2 = a21x1 + a22x2 + a23x3.\n\nThis agrees with the matrix equation(\ny1\n\ny2\n\n)\n=\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)x1\n\nx2\n\nx3\n\n .\n\nWe now formalize the representation of linear maps by matrices.\n\n\n\n110 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nDefinition 4.1. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E,\nand (v1, . . . , vm) be a basis for F . Each vector x ∈ E expressed in the basis (u1, . . . , un) as\nx = x1u1 + · · ·+ xnun is represented by the column matrix\n\nM(x) =\n\nx1\n...\nxn\n\n\nand similarly for each vector y ∈ F expressed in the basis (v1, . . . , vm).\n\nEvery linear map f : E → F is represented by the matrix M(f) = (ai j), where ai j is the\ni-th component of the vector f(uj) over the basis (v1, . . . , vm), i.e., where\n\nf(uj) =\nm∑\ni=1\n\nai jvi, for every j, 1 ≤ j ≤ n.\n\nThe coefficients a1j, a2j, . . . , amj of f(uj) over the basis (v1, . . . , vm) form the jth column of\nthe matrix M(f) shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThe matrix M(f) associated with the linear map f : E → F is called the matrix of f with\nrespect to the bases (u1, . . . , un) and (v1, . . . , vm). When E = F and the basis (v1, . . . , vm)\nis identical to the basis (u1, . . . , un) of E, the matrix M(f) associated with f : E → E (as\nabove) is called the matrix of f with respect to the basis (u1, . . . , un).\n\nRemark: As in the remark after Definition 3.12, there is no reason to assume that the\nvectors in the bases (u1, . . . , un) and (v1, . . . , vm) are ordered in any particular way. However,\nit is often convenient to assume the natural ordering. When this is so, authors sometimes\nrefer to the matrix M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un)\nand (v1, . . . , vm).\n\nLet us illustrate the representation of a linear map by a matrix in a concrete situation.\nLet E be the vector space R[X]4 of polynomials of degree at most 4, let F be the vector\nspace R[X]3 of polynomials of degree at most 3, and let the linear map be the derivative\nmap d: that is,\n\nd(P +Q) = dP + dQ\n\nd(λP ) = λdP,\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 111\n\nwith λ ∈ R. We choose (1, x, x2, x3, x4) as a basis of E and (1, x, x2, x3) as a basis of F .\nThen the 4 × 5 matrix D associated with d is obtained by expressing the derivative dxi of\neach basis vector xi for i = 0, 1, 2, 3, 4 over the basis (1, x, x2, x3). We find\n\nD =\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n .\n\nIf P denotes the polynomial\n\nP = 3x4 − 5x3 + x2 − 7x+ 5,\n\nwe have\ndP = 12x3 − 15x2 + 2x− 7.\n\nThe polynomial P is represented by the vector (5,−7, 1,−5, 3), the polynomial dP is repre-\nsented by the vector (−7, 2,−15, 12), and we have\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n5\n−7\n1\n−5\n3\n\n =\n\n\n−7\n2\n−15\n12\n\n ,\n\nas expected! The kernel (nullspace) of d consists of the polynomials of degree 0, that is, the\nconstant polynomials. Therefore dim(Ker d) = 1, and from\n\ndim(E) = dim(Ker d) + dim(Im d)\n\n(see Theorem 6.13), we get dim(Im d) = 4 (since dim(E) = 5).\n\nFor fun, let us figure out the linear map from the vector space R[X]3 to the vector space\nR[X]4 given by integration (finding the primitive, or anti-derivative) of xi, for i = 0, 1, 2, 3).\nThe 5× 4 matrix S representing\n\n∫\nwith respect to the same bases as before is\n\nS =\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n .\n\nWe verify that DS = I4,\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n\n\n112 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThis is to be expected by the fundamental theorem of calculus since the derivative of an\nintegral returns the function. As we will shortly see, the above matrix product corresponds\nto this functional composition. The equation DS = I4 shows that S is injective and has D\nas a left inverse. However, SD 6= I5, and instead\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n =\n\n\n0 0 0 0 0\n0 1 0 0 0\n0 0 1 0 0\n0 0 0 1 0\n0 0 0 0 1\n\n ,\n\nbecause constant polynomials (polynomials of degree 0) belong to the kernel of D.\n\n4.2 Composition of Linear Maps and Matrix\n\nMultiplication\n\nLet us now consider how the composition of linear maps is expressed in terms of bases.\n\nLet E, F , and G, be three vectors spaces with respective bases (u1, . . . , up) for E,\n(v1, . . . , vn) for F , and (w1, . . . , wm) for G. Let g : E → F and f : F → G be linear maps.\nAs explained earlier, g : E → F is determined by the images of the basis vectors uj, and\nf : F → G is determined by the images of the basis vectors vk. We would like to understand\nhow f ◦ g : E → G is determined by the images of the basis vectors uj.\n\nRemark: Note that we are considering linear maps g : E → F and f : F → G, instead\nof f : E → F and g : F → G, which yields the composition f ◦ g : E → G instead of\ng ◦ f : E → G. Our perhaps unusual choice is motivated by the fact that if f is represented\nby a matrix M(f) = (ai k) and g is represented by a matrix M(g) = (bk j), then f ◦g : E → G\nis represented by the product AB of the matrices A and B. If we had adopted the other\nchoice where f : E → F and g : F → G, then g ◦ f : E → G would be represented by the\nproduct BA. Personally, we find it easier to remember the formula for the entry in Row i and\nColumn j of the product of two matrices when this product is written by AB, rather than\nBA. Obviously, this is a matter of taste! We will have to live with our perhaps unorthodox\nchoice.\n\nThus, let\n\nf(vk) =\nm∑\ni=1\n\nai kwi,\n\nfor every k, 1 ≤ k ≤ n, and let\n\ng(uj) =\nn∑\nk=1\n\nbk jvk,\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 113\n\nfor every j, 1 ≤ j ≤ p; in matrix form, we have\n\nf(v1) f(v2) . . . f(vn)\n\nw1\n\nw2\n...\nwm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n\nand\n\ng(u1) g(u2) . . . g(up)\n\nv1\n\nv2\n...\nvn\n\n\nb11 b12 . . . b1p\n\nb21 b22 . . . b2p\n...\n\n...\n. . .\n\n...\nbn1 bn2 . . . bnp\n\n .\n\nBy previous considerations, for every\n\nx = x1u1 + · · ·+ xpup,\n\nletting g(x) = y = y1v1 + · · ·+ ynvn, we have\n\nyk =\n\np∑\nj=1\n\nbk jxj (2)\n\nfor all k, 1 ≤ k ≤ n, and for every\n\ny = y1v1 + · · ·+ ynvn,\n\nletting f(y) = z = z1w1 + · · ·+ zmwm, we have\n\nzi =\nn∑\nk=1\n\nai kyk (3)\n\nfor all i, 1 ≤ i ≤ m. Then if y = g(x) and z = f(y), we have z = f(g(x)), and in view of (2)\nand (3), we have\n\nzi =\nn∑\nk=1\n\nai k(\n\np∑\nj=1\n\nbk jxj)\n\n=\nn∑\nk=1\n\np∑\nj=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\nn∑\nk=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\n(\nn∑\nk=1\n\nai kbk j)xj.\n\n\n\n114 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThus, defining ci j such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p, we have\n\nzi =\n\np∑\nj=1\n\nci jxj (4)\n\nIdentity (4) shows that the composition of linear maps corresponds to the product of\nmatrices.\n\nThen given a linear map f : E → F represented by the matrix M(f) = (ai j) w.r.t. the\nbases (u1, . . . , un) and (v1, . . . , vm), by Equation (1), namely\n\nyi =\nn∑\nj=1\n\nai jxj 1 ≤ i ≤ m,\n\nand the definition of matrix multiplication, the equation y = f(x) corresponds to the matrix\nequation M(y) = M(f)M(x), that is,y1\n\n...\nym\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nam 1 . . . amn\n\n\nx1\n\n...\nxn\n\n .\n\nRecall that\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nx1\n\nx2\n...\nxn\n\n = x1\n\n\na1 1\n\na2 1\n...\n\nam 1\n\n+ x2\n\n\na1 2\n\na2 2\n...\n\nam 2\n\n+ · · ·+ xn\n\n\na1n\n\na2n\n...\n\namn\n\n .\n\nSometimes, it is necessary to incorporate the bases (u1, . . . , un) and (v1, . . . , vm) in the\nnotation for the matrix M(f) expressing f with respect to these bases. This turns out to be\na messy enterprise!\n\nWe propose the following course of action:\n\nDefinition 4.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases of E and F , and\ndenote by MU ,V(f) the matrix of f with respect to the bases U and V . Furthermore, write\nxU for the coordinates M(x) = (x1, . . . , xn) of x ∈ E w.r.t. the basis U and write yV for the\ncoordinates M(y) = (y1, . . . , ym) of y ∈ F w.r.t. the basis V . Then\n\ny = f(x)\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 115\n\nis expressed in matrix form by\nyV = MU ,V(f)xU .\n\nWhen U = V , we abbreviate MU ,V(f) as MU(f).\n\nThe above notation seems reasonable, but it has the slight disadvantage that in the\nexpression MU ,V(f)xU , the input argument xU which is fed to the matrix MU ,V(f) does not\nappear next to the subscript U in MU ,V(f). We could have used the notation MV,U(f), and\nsome people do that. But then, we find a bit confusing that V comes before U when f maps\nfrom the space E with the basis U to the space F with the basis V . So, we prefer to use the\nnotation MU ,V(f).\n\nBe aware that other authors such as Meyer [124] use the notation [f ]U ,V , and others such\nas Dummit and Foote [55] use the notation MV\n\nU (f), instead of MU ,V(f). This gets worse!\nYou may find the notation MU\n\nV (f) (as in Lang [108]), or U [f ]V , or other strange notations.\n\nDefinition 4.2 shows that the function which associates to a linear map f : E → F the\nmatrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm) has the property that matrix mul-\ntiplication corresponds to composition of linear maps. This allows us to transfer properties\nof linear maps to matrices. Here is an illustration of this technique:\n\nProposition 4.1. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nProof. (1) Every m× n matrix A = (ai j) defines the function fA : Kn → Km given by\n\nfA(x) = Ax,\n\nfor all x ∈ Kn. It is immediately verified that fA is linear and that the matrix M(fA)\nrepresenting fA over the canonical bases in Kn and Km is equal to A. Then Formula (4)\nproves that\n\nM(fA ◦ fB) = M(fA)M(fB) = AB,\n\nso we get\nM((fA ◦ fB) ◦ fC) = M(fA ◦ fB)M(fC) = (AB)C\n\n\n\n116 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nand\n\nM(fA ◦ (fB ◦ fC)) = M(fA)M(fB ◦ fC) = A(BC),\n\nand since composition of functions is associative, we have (fA ◦ fB) ◦ fC = fA ◦ (fB ◦ fC),\nwhich implies that\n\n(AB)C = A(BC).\n\n(2) It is immediately verified that if f1, f2 ∈ HomK(E,F ), A,B ∈ Mm,n(K), (u1, . . . , un) is\nany basis of E, and (v1, . . . , vm) is any basis of F , then\n\nM(f1 + f2) = M(f1) +M(f2)\n\nfA+B = fA + fB.\n\nThen we have\n\n(A+B)C = M(fA+B)M(fC)\n\n= M(fA+B ◦ fC)\n\n= M((fA + fB) ◦ fC))\n\n= M((fA ◦ fC) + (fB ◦ fC))\n\n= M(fA ◦ fC) +M(fB ◦ fC)\n\n= M(fA)M(fC) +M(fB)M(fC)\n\n= AC +BC.\n\nThe equation A(C + D) = AC + AD is proven in a similar fashion, and the last two\nequations are easily verified. We could also have verified all the identities by making matrix\ncomputations.\n\nNote that Proposition 4.1 implies that the vector space Mn(K) of square matrices is a\n(noncommutative) ring with unit In. (It even shows that Mn(K) is an associative algebra.)\n\nThe following proposition states the main properties of the mapping f 7→M(f) between\nHom(E,F ) and Mm,n. In short, it is an isomorphism of vector spaces.\n\nProposition 4.2. Given three vector spaces E, F , G, with respective bases (u1, . . . , up),\n(v1, . . . , vn), and (w1, . . . , wm), the mapping M : Hom(E,F )→ Mn,p that associates the ma-\ntrix M(g) to a linear map g : E → F satisfies the following properties for all x ∈ E, all\ng, h : E → F , and all f : F → G:\n\nM(g(x)) = M(g)M(x)\n\nM(g + h) = M(g) +M(h)\n\nM(λg) = λM(g)\n\nM(f ◦ g) = M(f)M(g),\n\n\n\n4.3. CHANGE OF BASIS MATRIX 117\n\nwhere M(x) is the column vector associated with the vector x and M(g(x)) is the column\nvector associated with g(x), as explained in Definition 4.1.\n\nThus, M : Hom(E,F ) → Mn,p is an isomorphism of vector spaces, and when p = n\nand the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up), M : Hom(E,E) → Mn is an\nisomorphism of rings.\n\nProof. That M(g(x)) = M(g)M(x) was shown by Definition 4.2 or equivalently by Formula\n(1). The identities M(g+ h) = M(g) +M(h) and M(λg) = λM(g) are straightforward, and\nM(f ◦ g) = M(f)M(g) follows from Identity (4) and the definition of matrix multiplication.\nThe mapping M : Hom(E,F ) → Mn,p is clearly injective, and since every matrix defines a\nlinear map (see Proposition 4.1), it is also surjective, and thus bijective. In view of the above\nidentities, it is an isomorphism (and similarly for M : Hom(E,E)→ Mn, where Proposition\n4.1 is used to show that Mn is a ring).\n\nIn view of Proposition 4.2, it seems preferable to represent vectors from a vector space\nof finite dimension as column vectors rather than row vectors. Thus, from now on, we will\ndenote vectors of Rn (or more generally, of Kn) as column vectors.\n\n4.3 Change of Basis Matrix\n\nIt is important to observe that the isomorphism M : Hom(E,F )→ Mn,p given by Proposition\n4.2 depends on the choice of the bases (u1, . . . , up) and (v1, . . . , vn), and similarly for the\nisomorphism M : Hom(E,E) → Mn, which depends on the choice of the basis (u1, . . . , un).\nThus, it would be useful to know how a change of basis affects the representation of a linear\nmap f : E → F as a matrix. The following simple proposition is needed.\n\nProposition 4.3. Let E be a vector space, and let (u1, . . . , un) be a basis of E. For every\nfamily (v1, . . . , vn), let P = (ai j) be the matrix defined such that vj =\n\n∑n\ni=1 ai jui. The matrix\n\nP is invertible iff (v1, . . . , vn) is a basis of E.\n\nProof. Note that we have P = M(f), the matrix associated with the unique linear map\nf : E → E such that f(ui) = vi. By Proposition 3.15, f is bijective iff (v1, . . . , vn) is a basis\nof E. Furthermore, it is obvious that the identity matrix In is the matrix associated with the\nidentity id : E → E w.r.t. any basis. If f is an isomorphism, then f ◦f−1 = f−1◦f = id, and\nby Proposition 4.2, we get M(f)M(f−1) = M(f−1)M(f) = In, showing that P is invertible\nand that M(f−1) = P−1.\n\nProposition 4.3 suggests the following definition.\n\nDefinition 4.3. Given a vector space E of dimension n, for any two bases (u1, . . . , un) and\n(v1, . . . , vn) of E, let P = (ai j) be the invertible matrix defined such that\n\nvj =\nn∑\ni=1\n\nai jui,\n\n\n\n118 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nwhich is also the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), in that order . Indeed, we express each id(vj) = vj over the basis (u1, . . . , un).\nThe coefficients a1j, a2j, . . . , anj of vj over the basis (u1, . . . , un) form the jth column of the\nmatrix P shown below:\n\nv1 v2 . . . vn\n\nu1\n\nu2\n...\nun\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nan1 an2 . . . ann\n\n .\n\nThe matrix P is called the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn).\n\nClearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un) is P−1. Since P =\n(ai j) is the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), given any vector x ∈ E, if x = x1u1 + · · ·+xnun over the basis (u1, . . . , un) and\nx = x′1v1 + · · ·+ x′nvn over the basis (v1, . . . , vn), from Proposition 4.2, we havex1\n\n...\nxn\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nan 1 . . . ann\n\n\nx\n\n′\n1\n...\nx′n\n\n ,\n\nshowing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed in terms of the\nnew coordinates (x′i) of x (over (v1, . . . , vn)).\n\nNow we face the painful task of assigning a “good” notation incorporating the bases\nU = (u1, . . . , un) and V = (v1, . . . , vn) into the notation for the change of basis matrix from\nU to V . Because the change of basis matrix from U to V is the matrix of the identity map\nidE with respect to the bases V and U in that order , we could denote it by MV,U(id) (Meyer\n[124] uses the notation [I]V,U). We prefer to use an abbreviation for MV,U(id).\n\nDefinition 4.4. The change of basis matrix from U to V is denoted\n\nPV,U .\n\nNote that\nPU ,V = P−1\n\nV,U .\n\nThen, if we write xU = (x1, . . . , xn) for the old coordinates of x with respect to the basis U\nand xV = (x′1, . . . , x\n\n′\nn) for the new coordinates of x with respect to the basis V , we have\n\nxU = PV,U xV , xV = P−1\nV,U xU .\n\nThe above may look backward, but remember that the matrix MU ,V(f) takes input\nexpressed over the basis U to output expressed over the basis V . Consequently, PV,U takes\ninput expressed over the basis V to output expressed over the basis U , and xU = PV,U xV\nmatches this point of view!\n\n\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n� Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto V as PU ,V = P−1\n\nV,U . Under this point of view, the old basis U is expressed in terms of\nthe new basis V . We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms of the old basis\n(u1, . . ., un), we observe that the coordinates (xi) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant .\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R2, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1) and v2 = (−1, 1).\nThe change of basis matrix P from the basis U = (u1, u2) to the basis V = (v1, v2) is\n\nP =\n\n(\n1 −1\n1 1\n\n)\nand its inverse is\n\nP−1 =\n\n(\n1/2 1/2\n−1/2 1/2\n\n)\n.\n\nThe old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms of the new\ncoordinates (x′1, x\n\n′\n2) with respect to (v1, v2) by(\n\nx1\n\nx2\n\n)\n=\n\n(\n1 −1\n1 1\n\n)(\nx′1\nx′2\n\n)\n,\n\nand the new coordinates (x′1, x\n′\n2) with respect to (v1, v2) are expressed in terms of the old\n\ncoordinates (x1, x2) with respect to (u1, u2) by(\nx′1\nx′2\n\n)\n=\n\n(\n1/2 1/2\n−1/2 1/2\n\n)(\nx1\n\nx2\n\n)\n.\n\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\nand consider the bases U = (1, x, x2, x3) and V = (B3\n\n0(x), B3\n1(x), B3\n\n2(x), B3\n3(x)), where\n\nB3\n0(x), B3\n\n1(x), B3\n2(x), B3\n\n3(x) are the Bernstein polynomials of degree 3, given by\n\nB3\n0(x) = (1− x)3 B3\n\n1(x) = 3(1− x)2x B3\n2(x) = 3(1− x)x2 B3\n\n3(x) = x3.\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix PV,U is\ngiven by\n\nPV,U =\n\n\n1 0 0 0\n−3 3 0 0\n3 −6 3 0\n−1 3 −3 1\n\n .\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n© Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto Vas Puy = Py 1. Under this point of view, the old basis U/ is expressed in terms of\nthe new basis V. We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = P)y expresses the new basis (v1,...,Un) in terms of the old basis\n(U1,.--, Un), we observe that the coordinates (;) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant.\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R’, with wu, = (1,0), wa = (0,1), vy = (1,1) and ve = (-1,1).\nThe change of basis matrix P from the basis U = (uy, ug) to the basis V = (vj, v2) is\n\np=(, 7)\n\npte (1 ta):\n\nThe old coordinates (21,22) with respect to (ui,u2) are expressed in terms of the new\ncoordinates (x, 24) with respect to (v1, v2) by\n\n(2) = 7) @)\n\nand the new coordinates (2,25) with respect to (v1, v2) are expressed in terms of the old\ncoordinates (21,22) with respect to (ui, U2) by\n\nvy) _ ( 1/2 1/2) (x\nX5 —1/2 1/2) \\a)\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\n\nand consider the bases UY = (1,2,x?,73) and V = (B3(x), B3(x), B3(x), B3(x)), where\nBe (x), Bi(x), B3(x), B3(x) are the Bernstein polynomials of degree 3, given by\n\nB3(x) = (1-2)? B3(x) = 3(1—2)?x B3 (x) = 3(1 — x)2? B3 (x) = 2°.\n\nand its inverse is\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix Pyy is\ngiven by\n\n1 0 0 0\n3 3 0 0\nPou = 3-6 3 0\n-1 3 -3 1\n\n\n\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of PV,U is\n\nP−1\nV,U =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n .\n\nTherefore, the coordinates of the polynomial 2x3 − x+ 1 over the basis V are\n1\n\n2/3\n1/3\n2\n\n =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n\n\n\n1\n−1\n0\n2\n\n ,\n\nand so\n\n2x3 − x+ 1 = B3\n0(x) +\n\n2\n\n3\nB3\n\n1(x) +\n1\n\n3\nB3\n\n2(x) + 2B3\n3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn)\n\nbe two bases of E, and let V = (v1, . . . , vm) and V ′ = (v′1, . . . , v\n′\nm) be two bases of F . Let\n\nP = PU ′,U be the change of basis matrix from U to U ′, and let Q = PV ′,V be the change of\nbasis matrix from V to V ′. For any linear map f : E → F , let M(f) = MU ,V(f) be the matrix\nassociated to f w.r.t. the bases U and V, and let M ′(f) = MU ′,V ′(f) be the matrix associated\nto f w.r.t. the bases U ′ and V ′. We have\n\nM ′(f) = Q−1M(f)P,\n\nor more explicitly\n\nMU ′,V ′(f) = P−1\nV ′,VMU ,V(f)PU ′,U = PV,V ′MU ,V(f)PU ′,U .\n\nProof. Since f : E → F can be written as f = idF ◦ f ◦ idE, since P is the matrix of idE\nw.r.t. the bases (u′1, . . . , u\n\n′\nn) and (u1, . . . , un), and Q−1 is the matrix of idF w.r.t. the bases\n\n(v1, . . . , vm) and (v′1, . . . , v\n′\nm), by Proposition 4.2, we have M ′(f) = Q−1M(f)P .\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn) be\n\ntwo bases of E. Let P = PU ′,U be the change of basis matrix from U to U ′. For any linear\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of Pyy is\n\n10 0 0\npi_|{i 1/3 0 0\nU~11 9/3 1/3 0\n\n11 11\n\nTherefore, the coordinates of the polynomial 27° — x + 1 over the basis V are\n\n1 10 0 0\\/1\n\n2/3) [1 1/3 0 oO] f-1\n\n1/3 1 2/3 1/3 of | 0 J?\n2 11 1 1) \\2\n\nand so\n\n2 1\n2x? —x2+1= Be(x) + 3 Pile) + 3 B2(@) + 2B3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\n/\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1,..., tn) andU' = (u},...,ui,)\n\nan\n\nbe two bases of E, and let V = (u,...,Um) and V! = (v},...,v/,) be two bases of F'. Let\nP= Pyy be the change of basis matriz from U to U', and let Q = Pyy be the change of\nbasis matriz from V to V'. For any linear map f: E > F, let M(f) = Mu y(f) be the matriz\nassociated to f w.r.t. the basesU and V, and let M'(f) = My y(f) be the matrix associated\n\nto f w.r.t. the bases UW’ and VY’. We have\n\nM'(f) =Q°M(f)P,\n\nor more explicitly\nMuy (f) = Poy Muy (Pf) Pau = Poy Muy (f)Puru-\n\nProof. Since f: E — F can be written as f = idg o f oidg, since P is the matrix of idg\n\nw.r.t. the bases (uj,...,u/,) and (w1,...,Un), and Q7! is the matrix of idp w.r.t. the bases\n\nan\n\n(v1,---,Um) and (v},...,v/,), by Proposition 4.2, we have M’(f) = Q-'M(f)P. O\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (uj,...,Un) andU’ = (u},...,ui,) be\ntwo bases of E. Let P = Pyy be the change of basis matrix from U to U'. For any linear\n\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f : E → E, let M(f) = MU(f) be the matrix associated to f w.r.t. the basis U , and let\nM ′(f) = MU ′(f) be the matrix associated to f w.r.t. the basis U ′. We have\n\nM ′(f) = P−1M(f)P,\n\nor more explicitly,\n\nMU ′(f) = P−1\nU ′,UMU(f)PU ′,U = PU ,U ′MU(f)PU ′,U .\n\nExample 4.3. Let E = R2, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1) are the canonical\nbasis vectors, let V = (v1, v2) = (e1, e1 − e2), and let\n\nA =\n\n(\n2 1\n0 1\n\n)\n.\n\nThe change of basis matrix P = PV,U from U to V is\n\nP =\n\n(\n1 1\n0 −1\n\n)\n,\n\nand we check that\nP−1 = P.\n\nTherefore, in the basis V , the matrix representing the linear map f defined by A is\n\nA′ = P−1AP = PAP =\n\n(\n1 1\n0 −1\n\n)(\n2 1\n0 1\n\n)(\n1 1\n0 −1\n\n)\n=\n\n(\n2 0\n0 1\n\n)\n= D,\n\na diagonal matrix. In the basis V , it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v1 direction and it is the identity in the v2 direction. Observe that v1 and\nv2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v1 and v2 are corresponding eigenvectors . We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two n×n matrices A and B are said to be similar iff there is some invertible\nmatrix P such that\n\nB = P−1AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n × n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f: E + E, let M(f) = Mu (f) be the matrix associated to f w.r.t. the basis U, and let\nM'(f) = Mw(f) be the matrix associated to f w.r.t. the basis U'. We have\n\nM'(f) =PUM(f)P.\nor more explicitly,\nMu(f) = Pry Mul f) Pau = Puw Mu(f) Pew.\n\nExample 4.3. Let E = R?, U = (e1, e2) where e; = (1,0) and e2 = (0,1) are the canonical\nbasis vectors, let V = (v1, v2) = (€1, €1 — €2), and let\n\n=)\n\nThe change of basis matrix P = Pyy from U to Y is\n\n11\np=(p 4):\n\nP'=P.\n\nand we check that\n\nTherefore, in the basis Y, the matrix representing the linear map f defined by A is\n\nb plyap.. —f1 1\\(2 I) fa 1\\_ (2 0) ~\nserarcrare( EDGE )-2\n\na diagonal matrix. In the basis VY, it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v, direction and it is the identity in the v2 direction. Observe that v, and\nV2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v; and vy are corresponding eigenvectors. We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two nxn matrices A and B are said to be similar iff there is some invertible\n\nmatrix P such that\nB=P'AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n x n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n\n\n\n122 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nmatrix A is similar to its transpose A>. The proof requires advanced concepts (the Jordan\nform or similarity invariants).\n\nIf U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change of basis matrix\n\nP = PV,U =\n\n\na11 a12 · · · a1n\n\na21 a22 · · · a2n\n...\n\n...\n. . .\n\n...\nan1 an2 · · · ann\n\n\nfrom (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists of the coordinates\nof vj over the basis (u1, . . . , un), which means that\n\nvj =\nn∑\ni=1\n\naijui.\n\nIt is natural to extend the matrix notation and to express the vector\n\nv1\n...\nvn\n\n in En as the\n\nproduct of a matrix times the vector\n\nu1\n...\nun\n\n in En, namely as\n\n\nv1\n\nv2\n...\nvn\n\n =\n\n\na11 a21 · · · an1\n\na12 a22 · · · an2\n...\n\n...\n. . .\n\n...\na1n a2n · · · ann\n\n\n\nu1\n\nu2\n...\nun\n\n ,\n\nbut notice that the matrix involved is not P , but its transpose P>.\n\nThis observation has the following consequence: if U = (u1, . . . , un) and V = (v1, . . . , vn)\nare two bases of E and if v1\n\n...\nvn\n\n = A\n\nu1\n...\nun\n\n ,\n\nthat is,\n\nvi =\nn∑\nj=1\n\naijuj,\n\nfor any vector w ∈ E, if\n\nw =\nn∑\ni=1\n\nxiui =\nn∑\nk=1\n\nykvk,\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 123\n\nthen x1\n...\nxn\n\n = A>\n\ny1\n...\nyn\n\n ,\n\nand so y1\n...\nyn\n\n = (A>)−1\n\nx1\n...\nxn\n\n .\n\nIt is easy to see that (A>)−1 = (A−1)>. Also, if U = (u1, . . . , un), V = (v1, . . . , vn), and\nW = (w1, . . . , wn) are three bases of E, and if the change of basis matrix from U to V is\nP = PV,U and the change of basis matrix from V to W is Q = PW,V , thenv1\n\n...\nvn\n\n = P>\n\nu1\n...\nun\n\n ,\n\nw1\n...\nwn\n\n = Q>\n\nv1\n...\nvn\n\n ,\n\nso w1\n...\nwn\n\n = Q>P>\n\nu1\n...\nun\n\n = (PQ)>\n\nu1\n...\nun\n\n ,\n\nwhich means that the change of basis matrix PW,U from U to W is PQ. This proves that\n\nPW,U = PV,UPW,V .\n\nEven though matrices are indispensable since they are the major tool in applications of\nlinear algebra, one should not lose track of the fact that\n\nlinear maps are more fundamental because they are intrinsic\nobjects that do not depend on the choice of bases.\n\nConsequently, we advise the reader to try to think in terms of\nlinear maps rather than reduce everything to matrices.\n\nIn our experience, this is particularly effective when it comes to proving results about\nlinear maps and matrices, where proofs involving linear maps are often more “conceptual.”\nThese proofs are usually more general because they do not depend on the fact that the\ndimension is finite. Also, instead of thinking of a matrix decomposition as a purely algebraic\noperation, it is often illuminating to view it as a geometric decomposition. This is the case of\nthe SVD, which in geometric terms says that every linear map can be factored as a rotation,\nfollowed by a rescaling along orthogonal axes and then another rotation.\n\nAfter all,\n\n\n\n124 CHAPTER 4. MATRICES AND LINEAR MAPS\n\na matrix is a representation of a linear map,\n\nand most decompositions of a matrix reflect the fact that with a suitable choice of a basis\n(or bases), the linear map is a represented by a matrix having a special shape. The problem\nis then to find such bases.\n\nStill, for the beginner, matrices have a certain irresistible appeal, and we confess that\nit takes a certain amount of practice to reach the point where it becomes more natural to\ndeal with linear maps. We still recommend it! For example, try to translate a result stated\nin terms of matrices into a result stated in terms of linear maps. Whenever we tried this\nexercise, we learned something.\n\nAlso, always try to keep in mind that\n\nlinear maps are geometric in nature; they act on space.\n\n4.5 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The representation of linear maps by matrices .\n\n• The matrix representation mapping M : Hom(E,F ) → Mn,p and the representation\nisomorphism (Proposition 4.2).\n\n• Change of basis matrix and Proposition 4.4.\n\n4.6 Problems\n\nProblem 4.1. Prove that the column vectors of the matrix A1 given by\n\nA1 =\n\n1 2 3\n2 3 7\n1 3 1\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B1 over the basis consisting\nof the column vectors of A1 given by\n\nB1 =\n\n3 5 1\n1 2 1\n4 3 −6\n\n\n\n\n\n4.6. PROBLEMS 125\n\nare the columns of the matrix P1 given by\n\nP1 =\n\n−27 −61 −41\n9 18 9\n4 10 8\n\n .\n\nGive a nontrivial linear dependence of the columns of P1. Check that B1 = A1P1. Is the\nmatrix B1 invertible?\n\nProblem 4.2. Prove that the column vectors of the matrix A2 given by\n\nA2 =\n\n\n1 1 1 1\n1 2 1 3\n1 1 2 2\n1 1 1 3\n\n\nare linearly independent.\n\nProve that the column vectors of the matrix B2 given by\n\nB2 =\n\n\n1 −2 2 −2\n0 −3 2 −3\n3 −5 5 −4\n3 −4 4 −4\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B2 over the basis consisting\nof the column vectors of A2 are the columns of the matrix P2 given by\n\nP2 =\n\n\n2 0 1 −1\n−3 1 −2 1\n1 −2 2 −1\n1 −1 1 −1\n\n .\n\nCheck that A2P2 = B2. Prove that\n\nP−1\n2 =\n\n\n−1 −1 −1 1\n2 1 1 −2\n2 1 2 −3\n−1 −1 0 −1\n\n .\n\nWhat are the coordinates over the basis consisting of the column vectors of B2 of the vector\nwhose coordinates over the basis consisting of the column vectors of A1 are (2,−3, 0, 0)?\n\n\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nB2\n0(t) = (1− t)2 B2\n\n1(t) = 2(1− t)t B2\n2(t) = t2\n\nB3\n0(t) = (1− t)3 B3\n\n1(t) = 3(1− t)2t B3\n2(t) = 3(1− t)t2 B3\n\n3(t) = t3,\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B2\n0(t), B2\n\n1(t), B2\n2(t) are expressed as linear com-\n\nbinations of the basis (1, t, t2) of the vector space of polynomials of degree at most 2 as\nfollows: B2\n\n0(t)\nB2\n\n1(t)\nB2\n\n2(t)\n\n =\n\n1 −2 1\n0 2 −2\n0 0 1\n\n1\nt\nt2\n\n .\n\nProve that\nB2\n\n0(t) +B2\n1(t) +B2\n\n2(t) = 1.\n\n(2) Show that the Bernstein polynomials B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t) are expressed as linear\ncombinations of the basis (1, t, t2, t3) of the vector space of polynomials of degree at most 3\nas follows: \n\nB3\n0(t)\n\nB3\n1(t)\n\nB3\n2(t)\n\nB3\n3(t)\n\n =\n\n\n1 −3 3 −1\n0 3 −6 3\n0 0 3 −3\n0 0 0 1\n\n\n\n\n1\nt\nt2\n\nt3\n\n .\n\nProve that\nB3\n\n0(t) +B3\n1(t) +B3\n\n2(t) +B3\n3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient\n(\nm\nk\n\n)\nis given by(\n\nm\n\nk\n\n)\n=\n\nm!\n\nk!(m− k)!\n,\n\nwith 0 ≤ k ≤ m.\n\nFor any m ≥ 1, we have the m+ 1 Bernstein polynomials of degree m given by\n\nBm\nk (t) =\n\n(\nm\n\nk\n\n)\n(1− t)m−ktk, 0 ≤ k ≤ m.\n\n(1) Prove that\n\nBm\nk (t) =\n\nm∑\nj=k\n\n(−1)j−k\n(\nm\n\nj\n\n)(\nj\n\nk\n\n)\ntj. (∗)\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nBU) =(-0) BA) =BU—N* BA =30—HP BR) =e\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B3(t), B?(t), B3(t) are expressed as linear com-\nbinations of the basis (1,¢,¢?) of the vector space of polynomials of degree at most 2 as\nfollows:\n\nBe(t) 1-2 1\nBet)}=|0 2 -2 t\nB3(t) 0 0 1 ?\n\nProve that\nB3(t) + Be(t) + BS(t) = 1.\n\n(2) Show that the Bernstein polynomials B3(t), B}(t), B3(t), B3(t) are expressed as linear\ncombinations of the basis (1, t, ¢?, #2) of the vector space of polynomials of degree at most 3\nas follows:\n\nBe(t) 1-3 3 -1 1\nBeth} {0 3 -6 3 t\nBt)} {0 0 3 -3) |\nB3(t) 00 0 1 t?\n\nProve that\nBe (t) + Be(t) + B3(t) + B3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient (\"\") is given by\nm\\ m!\nkk} kM(m—k)V\n\nFor any m > 1, we have the m-+1 Bernstein polynomials of degree m given by\n\nwithO<k<m.\n\nBrit) = (‘\") (1—t)\"*t*®, O<k<m.\n\n(1) Prove that\n\n\n\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bm\n0 (t), . . . , Bm\n\nm(t) are linearly independent.\n\n(2) Prove that\n\nBm\n0 (t) + · · ·+Bm\n\nm(t) = 1.\n\n(3) What can you say about the symmetries of the (m+ 1)× (m+ 1) matrix expressing\nBm\n\n0 , . . . , B\nm\nm in terms of the basis 1, t, . . . , tm?\n\nProve your claim (beware that in equation (∗) the coefficient of tj in Bm\nk is the entry on\n\nthe (k+1)th row of the (j+1)th column, since 0 ≤ k, j ≤ m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the ti in terms of the Bernstein polynomials\nBm\n\n0 (t), . . . , Bm\nm(t), with 0 ≤ i ≤ m.\n\nFirst, prove that\n\nti =\nm−i∑\nj=0\n\ntiBm−i\nj (t), 0 ≤ i ≤ m.\n\nThen prove that (\nm\n\ni\n\n)(\nm− i\nj\n\n)\n=\n\n(\nm\n\ni+ j\n\n)(\ni+ j\n\ni\n\n)\n.\n\nUse the above facts to prove that\n\nti =\nm−i∑\nj=0\n\n(\ni+j\ni\n\n)(\nm\ni\n\n) Bm\ni+j(t).\n\nConclude that the Bernstein polynomials Bm\n0 (t), . . . , Bm\n\nm(t) form a basis of the vector\nspace of polynomials of degree ≤ m.\n\nCompute the matrix expressing 1, t, t2 in terms of B2\n0(t), B2\n\n1(t), B2\n2(t), and the matrix\n\nexpressing 1, t, t2, t3 in terms of B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t).\n\nYou should find 1 1 1\n0 1/2 1\n0 0 1\n\n\nand \n\n1 1 1 1\n0 1/3 2/3 1\n0 0 1/3 1\n0 0 0 1\n\n .\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bj’(t),..., Bi (t) are linearly independent.\n(2) Prove that\nBo (t) +--+ Bt) =1.\n(3) What can you say about the symmetries of the (m+ 1) x (m+ 1) matrix expressing\nBo',..., BM in terms of the basis 1,t,...,t?\n\nProve your claim (beware that in equation («) the coefficient of t? in Bi” is the entry on\nthe (k+1)th row of the (j+1)th column, since 0 < k, 7 < m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the t’ in terms of the Bernstein polynomials\nBy (t),..., B(t), with 0<i<m.\n\nFirst, prove that\n\n= S UB (t), O<i<m.\nj=0\n\nOlen monies)\n\nUse the above facts to prove that\n\nThen prove that\n\ni’ Bm (t).\n(\"\") ig (t)\n\nConclude that the Bernstein polynomials Bj’(t),...,B/\"(t) form a basis of the vector\nspace of polynomials of degree < m.\n\nCompute the matrix expressing 1,t,¢? in terms of B3(t), B?(t), B3(t), and the matrix\nexpressing 1,t,t*,¢° in terms of B3(t), BR(t), B3(t), B3(t).\n\nYou should find\n\nand\n\noO CF\n\n_\noOo ™.\n\nw\nre bo\n—~=\nwm w\nRee\n\n\n\n\n128 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) A polynomial curve C(t) of degree m in the plane is the set of points\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\ngiven by two polynomials of degree ≤ m,\n\nx(t) = α0t\nm1 + α1t\n\nm1−1 + · · ·+ αm1\n\ny(t) = β0t\nm2 + β1t\n\nm2−1 + · · ·+ βm2 ,\n\nwith 1 ≤ m1,m2 ≤ m and α0, β0 6= 0.\n\nProve that there exist m+ 1 points b0, . . . , bm ∈ R2 so that\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\n= Bm\n\n0 (t)b0 +Bm\n1 (t)b1 + · · ·+Bm\n\nm(t)bm\n\nfor all t ∈ R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1 generally on the\ncurve?\n\nWe say that the curve C is a Bézier curve and (b0, . . . , bm) is the list of control points of\nthe curve (control points need not be distinct).\n\nRemark: Because Bm\n0 (t) + · · · + Bm\n\nm(t) = 1 and Bm\ni (t) ≥ 0 when t ∈ [0, 1], the curve\n\nsegment C[0, 1] corresponding to t ∈ [0, 1] belongs to the convex hull of the control points.\nThis is an important property of Bézier curves which is used in geometric modeling to\nfind the intersection of curve segments. Bézier curves play an important role in computer\ngraphics and geometric modeling, but also in robotics because they can be used to model\nthe trajectories of moving objects.\n\nProblem 4.5. Consider the n× n matrix\n\nA =\n\n\n\n0 0 0 · · · 0 −an\n1 0 0 · · · 0 −an−1\n\n0 1 0 · · · 0 −an−2\n...\n\n. . . . . . . . .\n...\n\n...\n\n0 0 0\n. . . 0 −a2\n\n0 0 0 · · · 1 −a1\n\n\n,\n\nwith an 6= 0.\n\n(1) Find a matrix P such that\nA> = P−1AP.\n\nWhat happens when an = 0?\n\nHint . First, try n = 3, 4, 5. Such a matrix must have zeros above the “antidiagonal,” and\nidentical entries pij for all i, j ≥ 0 such that i+ j = n+ k, where k = 1, . . . , n.\n\n(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be chosen so that\nthe entries in P−1 are also integers.\n\n\n\n4.6. PROBLEMS 129\n\nProblem 4.6. For any matrix A ∈ Mn(C), let RA and LA be the maps from Mn(C) to itself\ndefined so that\n\nLA(B) = AB, RA(B) = BA, for all B ∈ Mn(C).\n\n(1) Check that LA and RA are linear, and that LA and RB commute for all A,B.\n\nLet adA : Mn(C)→ Mn(C) be the linear map given by\n\nadA(B) = LA(B)−RA(B) = AB −BA = [A,B], for all B ∈ Mn(C).\n\nNote that [A,B] is the Lie bracket.\n\n(2) Prove that if A is invertible, then LA and RA are invertible; in fact, (LA)−1 = LA−1\n\nand (RA)−1 = RA−1 . Prove that if A = PBP−1 for some invertible matrix P , then\n\nLA = LP ◦ LB ◦ L−1\nP , RA = R−1\n\nP ◦RB ◦RP .\n\n(3) Recall that the n2 matrices Eij defined such that all entries in Eij are zero except\nthe (i, j)th entry, which is equal to 1, form a basis of the vector space Mn(C). Consider the\npartial ordering of the Eij defined such that for i = 1, . . . , n, if n ≥ j > k ≥ 1, then then Eij\nprecedes Eik, and for j = 1, . . . , n, if 1 ≤ i < h ≤ n, then Eij precedes Ehj.\n\nDraw the Hasse diagram of the partial order defined above when n = 3.\n\nThere are total orderings extending this partial ordering. How would you find them\nalgorithmically? Check that the following is such a total order:\n\n(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).\n\n(4) Let the total order of the basis (Eij) extending the partial ordering defined in (2) be\ngiven by\n\n(i, j) < (h, k) iff\n\n{\ni = h and j > k\nor i < h.\n\nLet R be the n× n permutation matrix given by\n\nR =\n\n\n0 0 . . . 0 1\n0 0 . . . 1 0\n...\n\n...\n. . .\n\n...\n...\n\n0 1 . . . 0 0\n1 0 . . . 0 0\n\n .\n\nObserve that R−1 = R. Prove that for any n ≥ 1, the matrix of LA is given by A⊗In, and the\nmatrix of RA is given by In⊗RA>R (over the basis (Eij) ordered as specified above), where\n⊗ is the Kronecker product (also called tensor product) of matrices defined in Definition 5.4.\n\nHint . Figure out what are RB(Eij) = EijB and LB(Eij) = BEij.\n\n\n\n130 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) Prove that if A is upper triangular, then the matrices representing LA and RA are\nalso upper triangular.\n\nNote that if instead of the ordering\n\nE1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,\n\nthat I proposed you use the standard lexicographic ordering\n\nE11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,\n\nthen the matrix representing LA is still A⊗ In, but the matrix representing RA is In ⊗ A>.\nIn this case, if A is upper-triangular, then the matrix of RA is lower triangular . This is the\nmotivation for using the first basis (avoid upper becoming lower).\n\n\n\nChapter 5\n\nHaar Bases, Haar Wavelets,\nHadamard Matrices\n\nIn this chapter, we discuss two types of matrices that have applications in computer science\nand engineering:\n\n(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool in signal pro-\ncessing and computer graphics.\n\n2) Hadamard matrices which have applications in error correcting codes, signal processing,\nand low rank approximation.\n\n5.1 Introduction to Signal Compression Using Haar\n\nWavelets\n\nWe begin by considering Haar wavelets in R4. Wavelets play an important role in audio\nand video signal processing, especially for compressing long signals into much smaller ones\nthat still retain enough information so that when they are played, we can’t see or hear any\ndifference.\n\nConsider the four vectors w1, w2, w3, w4 given by\n\nw1 =\n\n\n1\n1\n1\n1\n\n w2 =\n\n\n1\n1\n−1\n−1\n\n w3 =\n\n\n1\n−1\n0\n0\n\n w4 =\n\n\n0\n0\n1\n−1\n\n .\n\nNote that these vectors are pairwise orthogonal, so they are indeed linearly independent\n(we will see this in a later chapter). Let W = {w1, w2, w3, w4} be the Haar basis , and let\nU = {e1, e2, e3, e4} be the canonical basis of R4. The change of basis matrix W = PW,U from\n\n131\n\n\n\n132 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nU to W is given by\n\nW =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nand we easily find that the inverse of W is given by\n\nW−1 =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n .\n\nSo the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4) over the Haar basis\nW , with \n\nc1\n\nc2\n\nc3\n\nc4\n\n =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n\n\n\n6\n4\n5\n1\n\n =\n\n\n4\n1\n1\n2\n\n .\n\nGiven a signal v = (v1, v2, v3, v4), we first transform v into its coefficients c = (c1, c2, c3, c4)\nover the Haar basis by computing c = W−1v. Observe that\n\nc1 =\nv1 + v2 + v3 + v4\n\n4\n\nis the overall average value of the signal v. The coefficient c1 corresponds to the background\nof the image (or of the sound). Then, c2 gives the coarse details of v, whereas, c3 gives the\ndetails in the first part of v, and c4 gives the details in the second half of v.\n\nReconstruction of the signal consists in computing v = Wc. The trick for good compres-\nsion is to throw away some of the coefficients of c (set them to zero), obtaining a compressed\nsignal ĉ, and still retain enough crucial information so that the reconstructed signal v̂ = Wĉ\nlooks almost as good as the original signal v. Thus, the steps are:\n\ninput v −→ coefficients c = W−1v −→ compressed ĉ −→ compressed v̂ = Wĉ.\n\nThis kind of compression scheme makes modern video conferencing possible.\n\nIt turns out that there is a faster way to find c = W−1v, without actually using W−1.\nThis has to do with the multiscale nature of Haar wavelets.\n\nGiven the original signal v = (6, 4, 5, 1) shown in Figure 5.1, we compute averages and\nhalf differences obtaining Figure 5.2. We get the coefficients c3 = 1 and c4 = 2. Then\nagain we compute averages and half differences obtaining Figure 5.3. We get the coefficients\nc1 = 4 and c2 = 1. Note that the original signal v can be reconstructed from the two signals\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 133\n\n6 4 5 1\n\nFigure 5.1: The original signal v.\n\n5 5 3 3\n\n1\n\n−1\n\n2\n\n−2\n\nFigure 5.2: First averages and first half differences.\n\nin Figure 5.2, and the signal on the left of Figure 5.2 can be reconstructed from the two\nsignals in Figure 5.3. In particular, the data from Figure 5.2 gives us\n\n5 + 1 =\nv1 + v2\n\n2\n+\nv1 − v2\n\n2\n= v1\n\n5− 1 =\nv1 + v2\n\n2\n− v1 − v2\n\n2\n= v2\n\n3 + 2 =\nv3 + v4\n\n2\n+\nv3 − v4\n\n2\n= v3\n\n3− 2 =\nv3 + v4\n\n2\n− v3 − v4\n\n2\n= v4.\n\n5.2 Haar Bases and Haar Matrices, Scaling Properties\n\nof Haar Wavelets\n\nThe method discussed in Section 5.2 can be generalized to signals of any length 2n. The\nprevious case corresponds to n = 2. Let us consider the case n = 3. The Haar basis\n\n\n\n134 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n4 4 4 4\n1 1\n\n−1 −1\n\nFigure 5.3: Second averages and second half differences.\n\n(w1, w2, w3, w4, w5, w6, w7, w8) is given by the matrix\n\nW =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nThe columns of this matrix are orthogonal, and it is easy to see that\n\nW−1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W>.\n\nA pattern is beginning to emerge. It looks like the second Haar basis vector w2 is the\n“mother” of all the other basis vectors, except the first, whose purpose is to perform aver-\naging. Indeed, in general, given\n\nw2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n2n\n\n,\n\nthe other Haar basis vectors are obtained by a “scaling and shifting process.” Starting from\nw2, the scaling process generates the vectors\n\nw3, w5, w9, . . . , w2j+1, . . . , w2n−1+1,\n\nsuch that w2j+1+1 is obtained from w2j+1 by forming two consecutive blocks of 1 and −1\nof half the size of the blocks in w2j+1, and setting all other entries to zero. Observe that\nw2j+1 has 2j blocks of 2n−j elements. The shifting process consists in shifting the blocks of\n1 and −1 in w2j+1 to the right by inserting a block of (k − 1)2n−j zeros from the left, with\n0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Note that our convention is to use j as the scaling index and\nk as the shifting index. Thus, we obtain the following formula for w2j+k:\n\nw2j+k(i) =\n\n\n0 1 ≤ i ≤ (k − 1)2n−j\n\n1 (k − 1)2n−j + 1 ≤ i ≤ (k − 1)2n−j + 2n−j−1\n\n−1 (k − 1)2n−j + 2n−j−1 + 1 ≤ i ≤ k2n−j\n\n0 k2n−j + 1 ≤ i ≤ 2n,\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 135\n\nwith 0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Of course\n\nw1 = (1, . . . , 1)︸ ︷︷ ︸\n2n\n\n.\n\nThe above formulae look a little better if we change our indexing slightly by letting k vary\nfrom 0 to 2j − 1, and using the index j instead of 2j.\n\nDefinition 5.1. The vectors of the Haar basis of dimension 2n are denoted by\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\nwhere\n\nhjk(i) =\n\n\n0 1 ≤ i ≤ k2n−j\n\n1 k2n−j + 1 ≤ i ≤ k2n−j + 2n−j−1\n\n−1 k2n−j + 2n−j−1 + 1 ≤ i ≤ (k + 1)2n−j\n\n0 (k + 1)2n−j + 1 ≤ i ≤ 2n,\n\nwith 0 ≤ j ≤ n− 1 and 0 ≤ k ≤ 2j − 1. The 2n × 2n matrix whose columns are the vectors\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\n(in that order), is called the Haar matrix of dimension 2n, and is denoted by Wn.\n\nIt turns out that there is a way to understand these formulae better if we interpret a\nvector u = (u1, . . . , um) as a piecewise linear function over the interval [0, 1).\n\nDefinition 5.2. Given a vector u = (u1, . . . , um), the piecewise linear function plf(u) is\ndefined such that\n\nplf(u)(x) = ui,\ni− 1\n\nm\n≤ x <\n\ni\n\nm\n, 1 ≤ i ≤ m.\n\nIn words, the function plf(u) has the value u1 on the interval [0, 1/m), the value u2 on\n[1/m, 2/m), etc., and the value um on the interval [(m− 1)/m, 1).\n\nFor example, the piecewise linear function associated with the vector\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3)\n\nis shown in Figure 5.4.\nThen each basis vector hjk corresponds to the function\n\nψjk = plf(hjk).\n\nIn particular, for all n, the Haar basis vectors\n\nh0\n0 = w2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n\n2n\n\n\n\n136 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n−2\n\n−1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nFigure 5.4: The piecewise linear function plf(u).\n\nyield the same piecewise linear function ψ given by\n\nψ(x) =\n\n\n1 if 0 ≤ x < 1/2\n\n−1 if 1/2 ≤ x < 1\n\n0 otherwise,\n\nwhose graph is shown in Figure 5.5. It is easy to see that ψjk is given by the simple expression\n\n1\n\n1\n\n−1\n\n0\n\nFigure 5.5: The Haar wavelet ψ.\n\nψjk(x) = ψ(2jx− k), 0 ≤ j ≤ n− 1, 0 ≤ k ≤ 2j − 1.\n\nThe above formula makes it clear that ψjk is obtained from ψ by scaling and shifting.\n\nDefinition 5.3. The function φ0\n0 = plf(w1) is the piecewise linear function with the constant\n\nvalue 1 on [0, 1), and the functions ψjk = plf(hjk) together with φ0\n0 are known as the Haar\n\nwavelets .\n\nRather than using W−1 to convert a vector u to a vector c of coefficients over the Haar\nbasis, and the matrix W to reconstruct the vector u from its Haar coefficients c, we can use\nfaster algorithms that use averaging and differencing.\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 137\n\nIf c is a vector of Haar coefficients of dimension 2n, we compute the sequence of vectors\nu0, u1, . . ., un as follows:\n\nu0 = c\n\nuj+1 = uj\n\nuj+1(2i− 1) = uj(i) + uj(2j + i)\n\nuj+1(2i) = uj(i)− uj(2j + i),\n\nfor j = 0, . . . , n− 1 and i = 1, . . . , 2j. The reconstructed vector (signal) is u = un.\n\nIf u is a vector of dimension 2n, we compute the sequence of vectors cn, cn−1, . . . , c0 as\nfollows:\n\ncn = u\n\ncj = cj+1\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/2,\n\nfor j = n− 1, . . . , 0 and i = 1, . . . , 2j. The vector over the Haar basis is c = c0.\n\nWe leave it as an exercise to implement the above programs in Matlab using two variables\nu and c, and by building iteratively 2j. Here is an example of the conversion of a vector to\nits Haar coefficients for n = 3.\n\nGiven the sequence u = (31, 29, 23, 17,−6,−8,−2,−4), we get the sequence\n\nc3 = (31, 29, 23, 17,−6,−8, 2,−4)\n\nc2 =\n\n(\n31 + 29\n\n2\n,\n23 + 17\n\n2\n,\n−6− 8\n\n2\n,\n−2− 4\n\n2\n,\n31− 29\n\n2\n,\n23− 17\n\n2\n,\n−6− (−8)\n\n2\n,\n−2− (−4)\n\n2\n\n)\n= (30, 20,−7,−3, 1, 3, 1, 1)\n\nc1 =\n\n(\n30 + 20\n\n2\n,\n−7− 3\n\n2\n,\n30− 20\n\n2\n,\n−7− (−3)\n\n2\n, 1, 3, 1, 1\n\n)\n= (25,−5, 5,−2, 1, 3, 1, 1)\n\nc0 =\n\n(\n25− 5\n\n2\n,\n25− (−5)\n\n2\n, 5,−2, 1, 3, 1, 1\n\n)\n= (10, 15, 5,−2, 1, 3, 1, 1)\n\nso c = (10, 15, 5,−2, 1, 3, 1, 1). Conversely, given c = (10, 15, 5,−2, 1, 3, 1, 1), we get the\nsequence\n\nu0 = (10, 15, 5,−2, 1, 3, 1, 1)\n\nu1 = (10 + 15, 10− 15, 5,−2, 1, 3, 1, 1) = (25,−5, 5,−2, 1, 3, 1, 1)\n\nu2 = (25 + 5, 25− 5,−5 + (−2),−5− (−2), 1, 3, 1, 1) = (30, 20,−7,−3, 1, 3, 1, 1)\n\nu3 = (30 + 1, 30− 1, 20 + 3, 20− 3,−7 + 1,−7− 1,−3 + 1,−3− 1)\n\n= (31, 29, 23, 17,−6,−8,−2,−4),\n\nwhich gives back u = (31, 29, 23, 17,−6,−8,−2,−4).\n\n\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix Wn of dimension 2n\n\nthat makes it clearer why the columns of Wn are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split Wn into two\n2n × 2n−1 matrices, then the second matrix containing the last 2n−1 columns of Wn has a\nvery simple structure: it consists of the vector\n\n(1,−1, 0, . . . , 0)︸ ︷︷ ︸\n2n\n\nand 2n−1 − 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 0 0\n−1 0 0 0\n0 1 0 0\n0 −1 0 0\n0 0 1 0\n0 0 −1 0\n0 0 0 1\n0 0 0 −1\n\n\n.\n\nObserve that this matrix can be obtained from the identity matrix I2n−1 , in our example\n\nI4 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nby forming the 2n × 2n−1 matrix obtained by replacing each 1 by the column vector(\n1\n−1\n\n)\nand each zero by the column vector (\n\n0\n0\n\n)\n.\n\nNow the first half of Wn, that is the matrix consisting of the first 2n−1 columns of Wn, can\nbe obtained from Wn−1 by forming the 2n× 2n−1 matrix obtained by replacing each 1 by the\ncolumn vector (\n\n1\n1\n\n)\n,\n\neach −1 by the column vector (\n−1\n−1\n\n)\n,\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix W,, of dimension 2”\nthat makes it clearer why the columns of W,, are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split W,, into two\n2” x 2\"-! matrices, then the second matrix containing the last 2\"~' columns of W,, has a\nvery simple structure: it consists of the vector\n\n(1, -1,0,...,0)\n\nXX w)\n\\-\n\nQn\n\nand 2”~! — 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 OO\n-l1 0 O 0\n0 1 0 0\n0 -l 0 0O\n0 0 1 0\n0 O -l 0O\n0 O O 1\n0 O QO -Il\n\nObserve that this matrix can be obtained from the identity matrix [gn-1, in our example\n\n0 0\n\n0\nI, 0 ’\n1\n\noOo OrF\noor ©\nor c&\n\nby forming the 2” x 2”~! matrix obtained by replacing each 1 by the column vector\n\n(4)\n()\n\nNow the first half of W,,, that is the matrix consisting of the first 2”~! columns of W,,, can\nbe obtained from W,,_; by forming the 2” x 2”! matrix obtained by replacing each 1 by the\ncolumn vector\n\nand each zero by the column vector\n\neach —1 by the column vector\n\n\n\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector (\n0\n0\n\n)\n.\n\nFor n = 3, the first half of W3 is the matrix\n\n1 1 1 0\n1 1 1 0\n1 1 −1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 1\n1 −1 0 −1\n1 −1 0 −1\n\n\nwhich is indeed obtained from\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given a m×n matrix A = (aij) and a p×q matrix B = (bij), the Kronecker\nproduct (or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined. Then it is immediately verified that Wn is given by\nthe following neat recursive equations:\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector\n\nFor n = 3, the first half of W3 is the matrix\n\nje ee ee ee ee oe oe\n\nwhich is indeed obtained from\n\nW2 =\n\nfo oe ee ee\n\n-1 0 -l\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given am xn matrix A = (a;;) anda pxq matrix B = (b;;), the Kronecker\nproduct (or tensor product) A® B of A and B is the mp x nq matrix\n\nay,B ai2B te QinB\nA 2 B= an B an 7 : Amn B\nAmB Am2PB vee AmnP\n\nIt can be shown that © is associative and that\n\n(A@ B)\\(C®D) =AC@BD\n(A@B)'=A' @B',\n\nwhenever AC and BD are well defined. Then it is immediately verified that W,, is given by\nthe following neat recursive equations:\n\nHe-(000(0) 2(2))\n\n\n\n\n140 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nwith W0 = (1). If we let\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n,\n\nthen it is not hard to use the Kronecker product formulation of Wn to obtain a rigorous\nproof of the equation\n\nW>\nn Wn = Bn, for all n ≥ 1.\n\nThe above equation offers a clean justification of the fact that the columns of Wn are pairwise\northogonal.\n\nObserve that the right block (of size 2n × 2n−1) shows clearly how the detail coefficients\nin the second half of the vector c are added and subtracted to the entries in the first half of\nthe partially reconstructed vector after n− 1 steps.\n\n5.4 Multiresolution Signal Analysis with Haar Bases\n\nAn important and attractive feature of the Haar basis is that it provides a multiresolution\nanalysis of a signal. Indeed, given a signal u, if c = (c1, . . . , c2n) is the vector of its Haar coef-\nficients, the coefficients with low index give coarse information about u, and the coefficients\nwith high index represent fine information. For example, if u is an audio signal corresponding\nto a Mozart concerto played by an orchestra, c1 corresponds to the “background noise,” c2\n\nto the bass, c3 to the first cello, c4 to the second cello, c5, c6, c7, c7 to the violas, then the\nviolins, etc. This multiresolution feature of wavelets can be exploited to compress a signal,\nthat is, to use fewer coefficients to represent it. Here is an example.\n\nConsider the signal\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3),\n\nwhose Haar transform is\nc = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).\n\nThe piecewise-linear curves corresponding to u and c are shown in Figure 5.6. Since some of\nthe coefficients in c are small (smaller than or equal to 0.2) we can compress c by replacing\nthem by 0. We get\n\nc2 = (2, 0, 0, 3, 0, 0, 2, 0),\n\nand the reconstructed signal is\n\nu2 = (2, 2, 2, 2, 7, 3,−1,−1).\n\nThe piecewise-linear curves corresponding to u2 and c2 are shown in Figure 5.7.\n\n\n\n5.4. MULTIRESOLUTION SIGNAL ANALYSIS WITH HAAR BASES 141\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n2\n\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.6: A signal and its Haar transform.\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.7: A compressed signal and its compressed Haar transform.\n\nAn interesting (and amusing) application of the Haar wavelets is to the compression of\naudio signals. It turns out that if your type load handel in Matlab an audio file will be\nloaded in a vector denoted by y, and if you type sound(y), the computer will play this piece\nof music. You can convert y to its vector of Haar coefficients c. The length of y is 73113,\nso first tuncate the tail of y to get a vector of length 65536 = 216. A plot of the signals\ncorresponding to y and c is shown in Figure 5.8. Then run a program that sets all coefficients\nof c whose absolute value is less that 0.05 to zero. This sets 37272 coefficients to 0. The\nresulting vector c2 is converted to a signal y2. A plot of the signals corresponding to y2 and\nc2 is shown in Figure 5.9. When you type sound(y2), you find that the music doesn’t differ\nmuch from the original, although it sounds less crisp. You should play with other numbers\ngreater than or less than 0.05. You should hear what happens when you type sound(c). It\nplays the music corresponding to the Haar transform c of y, and it is quite funny.\n\n\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n0 1 2 3 4 5 6 7\nx 104\n\n−1\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2n × 2n matrix Wn of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W>\n\nn is not the inverse of Wn,\nbut rather the matrix\n\nW−1\nn = DnW\n\n>\nn\n\nwith Dn = diag\n(\n\n2−n, 2−n︸︷︷︸\n20\n\n, 2−(n−1), 2−(n−1)︸ ︷︷ ︸\n21\n\n, 2−(n−2), . . . , 2−(n−2)︸ ︷︷ ︸\n22\n\n, . . . , 2−1, . . . , 2−1︸ ︷︷ ︸\n2n−1\n\n)\n.\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n-0.6-\n\n-0.8\n0\n\nx10\" x10!\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n1 7 7 7 7 7 7 0.6\n\n-0.6+\n\nA L L L L L L 08\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2” x 2” matrix W,, of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W,! is not the inverse of Wp,\nbut rather the matrix\n\nW,' = D,,W,!\n\nwith D,, = diag (2-\", gam Q-(M-V) Q-(M=1) g-(n-2)g(n-2)— gd ).\nae 2 .\n2° 21 22 gn—1\n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n\nHn = WnD\n1\n2\nn\n\nwhose columns are the normalized Haar basis vectors, with\n\nD\n1\n2\nn = diag\n\n(\n2−\n\nn\n2 , 2−\n\nn\n2︸︷︷︸\n\n20\n\n, 2−\nn−1\n\n2 , 2−\nn−1\n\n2︸ ︷︷ ︸\n21\n\n, 2−\nn−2\n\n2 , . . . , 2−\nn−2\n\n2︸ ︷︷ ︸\n22\n\n, . . . , 2−\n1\n2 , . . . , 2−\n\n1\n2︸ ︷︷ ︸\n\n2n−1\n\n)\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c = H>n u\nthe normalized Haar coefficients of u.\n\nBecause Hn is orthogonal, H−1\nn = H>n .\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H>n u and Hnc as follows: When computing the sequence of ujs, use\n\nuj+1(2i− 1) = (uj(i) + uj(2j + i))/\n√\n\n2\n\nuj+1(2i) = (uj(i)− uj(2j + i))/\n√\n\n2,\n\nand when computing the sequence of cjs, use\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/\n√\n\n2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/\n√\n\n2.\n\nNote that things are now more symmetric, at the expense of a division by\n√\n\n2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/\n√\n\n2n\n\nand u by\n√\n\n2n. This is because the norm of the basis functions ψjk is not equal to 1 (under\n\nthe inner product 〈f, g〉 =\n∫ 1\n\n0\nf(t)g(t)dt). The normalized basis functions are the functions√\n\n2jψjk.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix Wn, the method using Hn being identical (except that H−1\n\nn = H>n , but this does\nnot hold for W−1\n\nn ). Given a 2m × 2n matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W−1\n\nn , obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W−1\n\nm . Because columns and rows\nare exchanged in the first step,\n\nB = A(W−1\nn )>,\n\nand in the second step C = W−1\nm B, thus, we have\n\nC = W−1\nm A(W−1\n\nn )> = DmW\n>\nmAWnDn.\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n1\nHH, = W,D;i\nwhose columns are the normalized Haar basis vectors, with\n\n4 . -R 9 m jg mel Qj m=1 2-2 _n=2 = _1\nDy = diag(272,2°2,2°> 2,27 2 272 ,...,2707 ,...,272,...,2°2\n— A A\n“ a “- -~_—\"’\n20 Q1 92 gn-1\n\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c= Hu\nthe normalized Haar coefficients of wu.\n\nBecause H,, is orthogonal, Hy! = H,.\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H,'u and H,,c as follows: When computing the sequence of u/s, use\n\nw+1(2i — 1) = (w(t) + wi (2’ +: 1))/V2\nw*\"(2i) = (wi (i) — w (2? + i))/v2,\n\nand when computing the sequence of c’s, use\n\n(i) = (At (21 — 1) + At (21))/V2\n(2) +i) = (F*1(2i — 1) — F121) /Vv2.\n\nNote that things are now more symmetric, at the expense of a division by 2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/2”\nand u by V2”. This is because el norm t the basis functions wi is not equal to 1 (under\n\nthe inner product (f,g) = fr fl . The normalized basis functions are the functions\nV2.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix W,,, the method using H,, being identical (except that H7' = H,’, but this does\nnot hold for W,-'). Given a 2” x 2” matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W,-', obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W,,'. Because columns and rows\nare exchanged in the first step,\n\nB= A(W,,')\",\nand in the second step C = W,;'B, thus, we have\n\nC=W,'A(W,')' = D,W,) AW, Dn-\n\n\n\n\n144 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIn the other direction, given a 2m × 2n matrix C of Haar coefficients, we reconstruct the\nmatrix A (the image) by first applying Wm to the columns of C, obtaining B, and then W>\n\nn\n\nto the rows of B. Therefore\n\nA = WmCW\n>\nn .\n\nOf course, we don’t actually have to invert Wm and Wn and perform matrix multiplications.\nWe just have to use our algorithms using averaging and differencing. Here is an example.\n\nIf the data matrix (the image) is the 8× 8 matrix\n\nA =\n\n\n\n64 2 3 61 60 6 7 57\n9 55 54 12 13 51 50 16\n17 47 46 20 21 43 42 24\n40 26 27 37 36 30 31 33\n32 34 35 29 28 38 39 25\n41 23 22 44 45 19 18 48\n49 15 14 52 53 11 10 56\n8 58 59 5 4 62 63 1\n\n\n,\n\nthen applying our algorithms, we find that\n\nC =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0.5 0.5 27 −25 23 −21\n0 0 −0.5 −0.5 −11 9 −7 5\n0 0 0.5 0.5 −5 7 −9 11\n0 0 −0.5 −0.5 21 −23 25 −27\n\n\n.\n\nAs we can see, C has more zero entries than A; it is a compressed version of A. We can\nfurther compress C by setting to 0 all entries of absolute value at most 0.5. Then we get\n\nC2 =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0 0 27 −25 23 −21\n0 0 0 0 −11 9 −7 5\n0 0 0 0 −5 7 −9 11\n0 0 0 0 21 −23 25 −27\n\n\n.\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 145\n\nWe find that the reconstructed image is\n\nA2 =\n\n\n\n63.5 1.5 3.5 61.5 59.5 5.5 7.5 57.5\n9.5 55.5 53.5 11.5 13.5 51.5 49.5 15.5\n17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5\n39.5 25.5 27.5 37.5 35.5 29.5 31.5 33.5\n31.5 33.5 35.5 29.5 27.5 37.5 39.5 25.5\n41.5 23.5 21.5 43.5 45.5 19.5 17.5 47.5\n49.5 15.5 13.5 51.5 53.5 11.5 9.5 55.5\n7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5\n\n\n,\n\nwhich is pretty close to the original image matrix A.\n\nIt turns out that Matlab has a wonderful command, image(X) (also imagesc(X), which\noften does a better job), which displays the matrix X has an image in which each entry\nis shown as a little square whose gray level is proportional to the numerical value of that\nentry (lighter if the value is higher, darker if the value is closer to zero; negative values are\ntreated as zero). The images corresponding to A and C are shown in Figure 5.10. The\n\nFigure 5.10: An image and its Haar transform.\n\ncompressed images corresponding to A2 and C2 are shown in Figure 5.11. The compressed\nversions appear to be indistinguishable from the originals!\n\nIf we use the normalized matrices Hm and Hn, then the equations relating the image\nmatrix A and its normalized Haar transform C are\n\nC = H>mAHn\n\nA = HmCH\n>\nn .\n\nThe Haar transform can also be used to send large images progressively over the internet.\nIndeed, we can start sending the Haar coefficients of the matrix C starting from the coarsest\n\n\n\n146 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.11: Compressed image and its Haar transform.\n\ncoefficients (the first column from top down, then the second column, etc.), and at the\nreceiving end we can start reconstructing the image as soon as we have received enough\ndata.\n\nObserve that instead of performing all rounds of averaging and differencing on each row\nand each column, we can perform partial encoding (and decoding). For example, we can\nperform a single round of averaging and differencing for each row and each column. The\nresult is an image consisting of four subimages, where the top left quarter is a coarser version\nof the original, and the rest (consisting of three pieces) contain the finest detail coefficients.\nWe can also perform two rounds of averaging and differencing, or three rounds, etc. The\nsecond round of averaging and differencing is applied to the top left quarter of the image.\nGenerally, the kth round is applied to the 2m+1−k × 2n+1−k submatrix consisting of the first\n2m+1−k rows and the first 2n+1−k columns (1 ≤ k ≤ n) of the matrix obtained at the end of\nthe previous round. This process is illustrated on the image shown in Figure 5.12. The result\nof performing one round, two rounds, three rounds, and nine rounds of averaging is shown in\nFigure 5.13. Since our images have size 512× 512, nine rounds of averaging yields the Haar\ntransform, displayed as the image on the bottom right. The original image has completely\ndisappeared! We leave it as a fun exercise to modify the algorithms involving averaging and\ndifferencing to perform k rounds of averaging/differencing. The reconstruction algorithm is\na little tricky.\n\nA nice and easily accessible account of wavelets and their uses in image processing and\ncomputer graphics can be found in Stollnitz, Derose and Salesin [166]. A very detailed\naccount is given in Strang and and Nguyen [170], but this book assumes a fair amount of\nbackground in signal processing.\n\nWe can find easily a basis of 2n × 2n = 22n vectors wij (2n × 2n matrices) for the linear\nmap that reconstructs an image from its Haar coefficients, in the sense that for any 2n × 2n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 147\n\nFigure 5.12: Original drawing by Durer.\n\nmatrix C of Haar coefficients, the image matrix A is given by\n\nA =\n2n∑\ni=1\n\n2n∑\nj=1\n\ncijwij.\n\nIndeed, the matrix wij is given by the so-called outer product\n\nwij = wi(wj)\n>.\n\nSimilarly, there is a basis of 2n × 2n = 22n vectors hij (2n × 2n matrices) for the 2D Haar\ntransform, in the sense that for any 2n × 2n matrix A, its matrix C of Haar coefficients is\ngiven by\n\nC =\n2n∑\ni=1\n\n2n∑\nj=1\n\naijhij.\n\nIf the columns of W−1 are w′1, . . . , w\n′\n2n , then\n\nhij = w′i(w\n′\nj)\n>.\n\nWe leave it as exercise to compute the bases (wij) and (hij) for n = 2, and to display the\ncorresponding images using the command imagesc.\n\n\n\n148 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.13: Haar tranforms after one, two, three, and nine rounds of averaging.\n\n\n\n5.6. HADAMARD MATRICES 149\n\n5.6 Hadamard Matrices\n\nThere is another famous family of matrices somewhat similar to Haar matrices, but these\nmatrices have entries +1 and −1 (no zero entries).\n\nDefinition 5.6. A real n × n matrix H is a Hadamard matrix if hij = ±1 for all i, j such\nthat 1 ≤ i, j ≤ n and if\n\nH>H = nIn.\n\nThus the columns of a Hadamard matrix are pairwise orthogonal. Because H is a square\nmatrix, the equation H>H = nIn shows that H is invertible, so we also have HH> = nIn.\nThe following matrices are example of Hadamard matrices:\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n, H4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n ,\n\nand\n\nH8 =\n\n\n\n1 1 1 1 1 1 1 1\n1 −1 1 −1 1 −1 1 −1\n1 1 −1 −1 1 1 −1 −1\n1 −1 −1 1 1 −1 −1 1\n1 1 1 1 −1 −1 −1 −1\n1 −1 1 −1 −1 1 −1 1\n1 1 −1 −1 −1 −1 1 1\n1 −1 −1 1 −1 1 1 −1\n\n\n.\n\nA natural question is to determine the positive integers n for which a Hadamard matrix\nof dimension n exists, but surprisingly this is an open problem. The Hadamard conjecture is\nthat for every positive integer of the form n = 4k, there is a Hadamard matrix of dimension\nn.\n\nWhat is known is a necessary condition and various sufficient conditions.\n\nTheorem 5.1. If H is an n×n Hadamard matrix, then either n = 1, 2, or n = 4k for some\npositive integer k.\n\nSylvester introduced a family of Hadamard matrices and proved that there are Hadamard\nmatrices of dimension n = 2m for all m ≥ 1 using the following construction.\n\nProposition 5.2. (Sylvester, 1867) If H is a Hadamard matrix of dimension n, then the\nblock matrix of dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\n\n\n150 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIf we start with\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n,\n\nwe obtain an infinite family of symmetric Hadamard matrices usually called Sylvester–\nHadamard matrices and denoted by H2m . The Sylvester–Hadamard matrices H2, H4 and\nH8 are shown on the previous page.\n\nIn 1893, Hadamard gave examples of Hadamard matrices for n = 12 and n = 20. At the\npresent, Hadamard matrices are known for all n = 4k ≤ 1000, except for n = 668, 716, and\n892.\n\nHadamard matrices have various applications to error correcting codes, signal processing,\nand numerical linear algebra; see Seberry, Wysocki and Wysocki [152] and Tropp [175]. For\nexample, there is a code based on H32 that can correct 7 errors in any 32-bit encoded block,\nand can detect an eighth. This code was used on a Mariner spacecraft in 1969 to transmit\npictures back to the earth.\n\nFor every m ≥ 0, the piecewise affine functions plf((H2m)i) associated with the 2m rows\nof the Sylvester–Hadamard matrix H2m are functions on [0, 1] known as the Walsh functions .\nIt is customary to index these 2m functions by the integers 0, 1, . . . , 2m−1 in such a way that\nthe Walsh function Wal(k, t) is equal to the function plf((H2m)i) associated with the Row i\nof H2m that contains k changes of signs between consecutive groups of +1 and consecutive\ngroups of −1. For example, the fifth row of H8, namely(\n\n1 −1 −1 1 1 −1 −1 1\n)\n,\n\nhas five consecutive blocks of +1s and −1s, four sign changes between these blocks, and thus\nis associated with Wal(4, t). In particular, Walsh functions corresponding to the rows of H8\n\n(from top down) are:\n\nWal(0, t), Wal(7, t), Wal(3, t), Wal(4, t), Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).\n\nBecause of the connection between Sylvester–Hadamard matrices and Walsh functions,\nSylvester–Hadamard matrices are called Walsh–Hadamard matrices by some authors. For\nevery m, the 2m Walsh functions are pairwise orthogonal. The countable set of Walsh\nfunctions Wal(k, t) for all m ≥ 0 and all k such that 0 ≤ k ≤ 2m − 1 can be ordered in\nsuch a way that it is an orthogonal Hilbert basis of the Hilbert space L2([0, 1)]; see Seberry,\nWysocki and Wysocki [152].\n\nThe Sylvester–Hadamard matrix H2m plays a role in various algorithms for dimension\nreduction and low-rank matrix approximation. There is a type of structured dimension-\nreduction map known as the subsampled randomized Hadamard transform, for short SRHT;\nsee Tropp [175] and Halko, Martinsson and Tropp [86]. For ` � n = 2m, an SRHT matrix\nis an `× n matrix of the form\n\nΦ =\n\n√\nn\n\n`\nRHD,\n\nwhere\n\n\n\n5.7. SUMMARY 151\n\n1. D is a random n× n diagonal matrix whose entries are independent random signs.\n\n2. H = n−1/2Hn, a normalized Sylvester–Hadamard matrix of dimension n.\n\n3. R is a random ` × n matrix that restricts an n-dimensional vector to ` coordinates,\nchosen uniformly at random.\n\nIt is explained in Tropp [175] that for any input x such that ‖x‖2 = 1, the probability\n\nthat |(HDx)i| ≥\n√\nn−1 log(n) for any i is quite small. Thus HD has the effect of “flattening”\n\nthe input x. The main result about the SRHT is that it preserves the geometry of an entire\nsubspace of vectors; see Tropp [175] (Theorem 1.3).\n\n5.7 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Haar basis vectors and a glimpse at Haar wavelets .\n\n• Kronecker product (or tensor product) of matrices.\n\n• Hadamard and Sylvester–Hadamard matrices.\n\n• Walsh functions.\n\n5.8 Problems\n\nProblem 5.1. (Haar extravaganza) Consider the matrix\n\nW3,3 =\n\n\n\n1 0 0 0 1 0 0 0\n1 0 0 0 −1 0 0 0\n0 1 0 0 0 1 0 0\n0 1 0 0 0 −1 0 0\n0 0 1 0 0 0 1 0\n0 0 1 0 0 0 −1 0\n0 0 0 1 0 0 0 1\n0 0 0 1 0 0 0 −1\n\n\n.\n\n(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,3c of applying\nW3,3 to c is\n\nW3,3c = (c1 + c5, c1 − c5, c2 + c6, c2 − c6, c3 + c7, c3 − c7, c4 + c8, c4 − c8),\n\nthe last step in reconstructing a vector from its Haar coefficients.\n\n\n\n152 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n(2) Prove that the inverse of W3,3 is (1/2)W>\n3,3. Prove that the columns and the rows of\n\nW3,3 are orthogonal.\n\n(3) Let W3,2 and W3,1 be the following matrices:\n\nW3,2 =\n\n\n\n1 0 1 0 0 0 0 0\n1 0 −1 0 0 0 0 0\n0 1 0 1 0 0 0 0\n0 1 0 −1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n, W3,1 =\n\n\n\n1 1 0 0 0 0 0 0\n1 −1 0 0 0 0 0 0\n0 0 1 0 0 0 0 0\n0 0 0 1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n.\n\nShow that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c of applying W3,2\n\nto c is\nW3,2c = (c1 + c3, c1 − c3, c2 + c4, c2 − c4, c5, c6, c7, c8),\n\nthe second step in reconstructing a vector from its Haar coefficients, and the result W3,1c of\napplying W3,1 to c is\n\nW3,1c = (c1 + c2, c1 − c2, c3, c4, c5, c6, c7, c8),\n\nthe first step in reconstructing a vector from its Haar coefficients.\n\nConclude that\nW3,3W3,2W3,1 = W3,\n\nthe Haar matrix\n\nW3 =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nHint . First check that\n\nW3,2W3,1 =\n\n(\nW2 04,4\n\n04,4 I4\n\n)\n,\n\nwhere\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\n\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W−1\n\n3 are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W−1\n\n3 orthogonal? Find the inverse of W3,2 and\nthe inverse of W3,1.\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n ≥ 2, the 2n × 2n matrix Wn,n is obtained form the two rows\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n, 1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n,−1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\nby shifting them 2n−1 − 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (c1, c2, . . . , c2n), show that Wn,nc is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W−1\n\nn,n = (1/2)W>\nn,n,\n\nand that the columns and the rows of Wn,n are orthogonal.\n\n(2) Given a m× n matrix A = (aij) and a p× q matrix B = (bij), the Kronecker product\n(or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown (and you may use these facts without proof) that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined.\n\nCheck that\n\nWn,n =\n\n(\nI2n−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\nand that\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n.\n\nUse the above to reprove that\n\nWn,nW\n>\nn,n = 2I2n .\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W35 and W3, are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W; ' are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W;' orthogonal? Find the inverse of W3,9 and\nthe inverse of Ws).\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n > 2, the 2” x 2” matrix W,,,, is obtained form the two rows\n\nwe we\n\ngn-1 gn-1\n1,0,.. ,0, —1, 0, ,0\n\na ww\n\ngn-1 gn—-1\n\nby shifting them 2”~! — 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (C1, C2,...,C2n), show that W,,,c is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W,,, = (1/2)W,),,\nand that the columns and the rows of W,,,, are orthogonal.\n\n(2) Given am x n matrix A = (a;;) and a p x q matrix B = (b;;), the Kronecker product\n(or tensor product) A ® B of A and B is the mp x nq matrix\n\nayiB aygB -:+ ayy,B\nAg pa | OW CR G08\nAmB Am2B vee AmnP\n\nIt can be shown (and you may use these facts without proof) that @ is associative and that\n\n(A@ B)(C®D) =AC @ BD\n(A@B)'=A' @B',\n\nwhenever AC’ and BD are well defined.\n\nCheck that\n1 1\nHan=(t09(2) tove(4)),\n1 1\nHis (tise(?) tes0(3)),\n\nUse the above to reprove that\n\nand that\n\nWrnW. n= 2lon.\n\n\n\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n.\n\nProve that\nW>\nn Wn = Bn, for all n ≥ 1.\n\n(3) The matrix Wn,i is obtained from the matrix Wi,i (1 ≤ i ≤ n− 1) as follows:\n\nWn,i =\n\n(\nWi,i 02i,2n−2i\n\n02n−2i,2i I2n−2i\n\n)\n.\n\nIt consists of four blocks, where 02i,2n−2i and 02n−2i,2i are matrices of zeros and I2n−2i is the\nidentity matrix of dimension 2n − 2i.\n\nExplain what Wn,i does to c and prove that\n\nWn,nWn,n−1 · · ·Wn,1 = Wn,\n\nwhere Wn is the Haar matrix of dimension 2n.\n\nHint . Use induction on k, with the induction hypothesis\n\nWn,kWn,k−1 · · ·Wn,1 =\n\n(\nWk 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProve that the columns and rows of Wn,k are orthogonal, and use this to prove that the\ncolumns of Wn and the rows of W−1\n\nn are orthogonal. Are the rows of Wn orthogonal? Are\nthe columns of W−1\n\nn orthogonal? Prove that\n\nW−1\nn,k =\n\n(\n1\n2\nW>\nk,k 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\nof dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k, t) for k = 0, 1, . . . , 7.\n\nProblem 5.5. Describe a recursive algorithm to compute the productH2m x of the Sylvester–\nHadamard matrix H2m by a vector x ∈ R2m that uses m recursive calls.\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n1 0 2 0\nm=2(5 t)= (0 3)\nand for n > 1,\nB, 0\nBri = 2 ( 0 .)\nProve that\n\nW/W, =Bn, for alln>1.\n\n(3) The matrix W,,; is obtained from the matrix W;; (1 <7 <n -—1) as follows:\n\nWri = ( Wii Man)\n\nQon_9i, 24 Ton _9i\n\nIt consists of four blocks, where 09: 9n_9i and Ogn_9i 9: are matrices of zeros and Jn_»: is the\nidentity matrix of dimension 2” — 2°.\n\nExplain what W,,; does to c and prove that\nWrinWrn—1 ute Writ = Wr,\n\nwhere W,, is the Haar matrix of dimension 2”.\n\nHint. Use induction on k, with the induction hypothesis\n\nWrrWnr pis Wra = ( We 2)\n\nOon_9k 9k Ton _9k\n\nProve that the columns and rows of W,,, are orthogonal, and use this to prove that the\ncolumns of W,, and the rows of W,! are orthogonal. Are the rows of W,, orthogonal? Are\nthe columns of W,-! orthogonal? Prove that\n\n1 T\nw-! _ 5W ik Ook on_9k\nnk .\n\nOon_9k 9k Ton _9k\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\n\nof dimension 2n,\nH H\nH —-H]’\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k,t) for k = 0,1,..., 7.\n\nis a Hadamard matrix.\n\nProblem 5.5. Describe a recursive algorithm to compute the product Hom x of the Sylvester—\nHadamard matrix Hym by a vector x € R?” that uses m recursive calls.\n\n\n\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E\n\n∐\nF of two vector spaces using the cartesian\n\nproduct E × F , we don’t quite get the right notion because elements of E × F are ordered\npairs, but we want E\n\n∐\nF = F\n\n∐\nE. Thus, we want to think of the elements of E\n\n∐\nF as\n\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(Ei)i∈{1,2}, and more generally of a family (Ei)i∈I . For simplicity, we begin by considering\nthe case where I = {1, 2}.\nDefinition 6.1. Given a family (Ei)i∈{1,2} of two vector spaces, we define the (external)\ndirect sum E1\n\n∐\nE2 (or coproduct) of the family (Ei)i∈{1,2} as the set\n\nE1\n\n∐\nE2 = {{〈1, u〉, 〈2, v〉} | u ∈ E1, v ∈ E2},\n\nwith addition\n\n{〈1, u1〉, 〈2, v1〉}+ {〈1, u2〉, 〈2, v2〉} = {〈1, u1 + u2〉, 〈2, v1 + v2〉},\nand scalar multiplication\n\nλ{〈1, u〉, 〈2, v〉} = {〈1, λu〉, 〈2, λv〉}.\nWe define the injections in1 : E1 → E1\n\n∐\nE2 and in2 : E2 → E1\n\n∐\nE2 as the linear maps\n\ndefined such that,\nin1(u) = {〈1, u〉, 〈2, 0〉},\n\nand\nin2(v) = {〈1, 0〉, 〈2, v〉}.\n\n155\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field AK. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E' || F of two vector spaces using the cartesian\nproduct E x F’, we don’t quite get the right notion because elements of EF x F' are ordered\npairs, but we want E || F = F [| £. Thus, we want to think of the elements of E'[] F as\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(E;)iefi2}, and more generally of a family (£;)ier. For simplicity, we begin by considering\nthe case where I = {1,2}.\n\nDefinition 6.1. Given a family (F;)ic{1,2} of two vector spaces, we define the (external)\ndirect sum E\\ [| E, (or coproduct) of the family (E;):e(1,2} as the set\n\nE, [| & = {{(1,u), (2,0)} we Ey, v € By},\nwith addition\n{(1, u1), (2, v1) } + {(1, U2), (2, v2) } = {(1, U1 + U2), (2, Ut + V2) },\n\nand scalar multiplication\n\nA{(1,u), (2,v)} = {(1, Nu), (2, rv) }-\n\nWe define the injections inj: Ey > E, [|] EF, and ing: Ey > E, [|] FE as the linear maps\n\ndefined such that,\niny(u) = {(1, u), (2, 0),\n\nand\n\ning(v) = {(1, 0), (2, v)}.\n\n155\n\n\n\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\n\nE2\n\n∐\nE1 = {{〈2, v〉, 〈1, u〉} | v ∈ E2, u ∈ E1} = E1\n\n∐\nE2.\n\nThus, every member {〈1, u〉, 〈2, v〉} of E1\n\n∐\nE2 can be viewed as an unordered pair consisting\n\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, E1\n\n∐\nE2 is just the product\n\n∏\ni∈{1,2}Ei of the family (Ei)i∈{1,2}.\n\n� This is not to be confused with the cartesian product E1×E2. The vector space E1×E2\n\nis the set of all ordered pairs 〈u, v〉, where u ∈ E1, and v ∈ E2, with addition and\nmultiplication by a scalar defined such that\n\n〈u1, v1〉+ 〈u2, v2〉 = 〈u1 + u2, v1 + v2〉,\nλ〈u, v〉 = 〈λu, λv〉.\n\nThere is a bijection between\n∏\n\ni∈{1,2}Ei and E1 × E2, but as we just saw, elements of∏\ni∈{1,2}Ei are certain sets. The product E1 × · · · × En of any number of vector spaces\n\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E1 and E2, the set E1\n\n∐\nE2 is a vector\n\nspace. For every pair of linear maps, f : E1 → G and g : E2 → G, there is a unique linear\nmap, f + g : E1\n\n∐\nE2 → G, such that (f + g) ◦ in1 = f and (f + g) ◦ in2 = g, as in the\n\nfollowing diagram:\nE1\n\nin1\n\n��\n\nf\n\n''PPPPPPPPPPPPPPPP\n\nE1\n\n∐\nE2\n\nf+g // G\n\nE2\n\nin2\n\nOO\n\ng\n\n77nnnnnnnnnnnnnnnn\n\nProof. Define\n(f + g)({〈1, u〉, 〈2, v〉}) = f(u) + g(v),\n\nfor every u ∈ E1 and v ∈ E2. It is immediately verified that f + g is the unique linear map\nwith the required properties.\n\nWe already noted that E1\n\n∐\nE2 is in bijection with E1 ×E2. If we define the projections\n\nπ1 : E1\n\n∐\nE2 → E1 and π2 : E1\n\n∐\nE2 → E2, such that\n\nπ1({〈1, u〉, 〈2, v〉}) = u,\n\nand\nπ2({〈1, u〉, 〈2, v〉}) = v,\n\nwe have the following proposition.\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\nFy] ] FE. = {{(2,v), (l.u)} |v € Be, we BY} = E, [] F.\n\nThus, every member {(1, wu), (2,v)} of E [] £2 can be viewed as an unordered pair consisting\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, FE, [[ E> is just the product [Hien.23 EF; of the family (2; )ieg2}-\n\n© This is not to be confused with the cartesian product EF, x E,. The vector space EF; x Es\nis the set of all ordered pairs (u,v), where u € Fy, and v € £5, with addition and\nmultiplication by a scalar defined such that\n\n(U1, U1) + (Ug, V2) = (uy + Ug, U1 + V2),\nA(u,v) = (Au, Av).\n\nThere is a bijection between [];. 12} E, and E, x E,, but as we just saw, elements of\n\nIL- 4.2} Ej, are certain sets. The product FE, x --- x E, of any number of vector spaces\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E, and E>, the set E, [| E> is a vector\nspace. For every pair of linear maps, f: FE, >~ G and g: Ep > G, there is a unique linear\nmap, f +g: E, [|] Ek, - G, such that (f +g) coin, = f and (f + 9) cing = g, as in the\nfollowing diagram:\n\nEy\namy\nEi Il Ey f+g G\nin| a\nEy\n\nProof. Define\n(f + 9) ACL, u), (2,0)}) = Flu) + g(r),\n\nfor every u € FE; and v € Eg. It is immediately verified that f + g is the unique linear map\nwith the required properties. im\n\nWe already noted that FE, [| £2 is in bijection with FE, x E>. If we define the projections\nTy: Fy [| £2 —> EF, and T9: EF, [| £2 > Es, such that\n\nm({(1,u), (2, v)}) =U,\nand\nmo({(1, u), (2, v)}) =v,\n\nwe have the following proposition.\n\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E1 and E2, for every pair of linear maps,\nf : D → E1 and g : D → E2, there is a unique linear map, f × g : D → E1\n\n∐\nE2, such that\n\nπ1 ◦ (f × g) = f and π2 ◦ (f × g) = g, as in the following diagram:\n\nE1\n\nD\nf×g //\n\nf\n\n77nnnnnnnnnnnnnnnn\n\ng\n((PPPPPPPPPPPPPPPP E1\n\n∐\nE2\n\nπ1\n\nOO\n\nπ2\n\n��\nE2\n\nProof. Define\n(f × g)(w) = {〈1, f(w)〉, 〈2, g(w)〉},\n\nfor every w ∈ D. It is immediately verified that f × g is the unique linear map with the\nrequired properties.\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U, V are subspaces of a vector space E, letting i1 : U → E and i2 : V → E be the\ninclusion maps, if U\n\n∐\nV is isomomorphic to E under the map i1 + i2 given by Proposition\n\n6.1, we say that E is a direct sum of U and V , and we write E = U\n∐\nV (with a slight abuse\n\nof notation, since E and U\n∐\nV are only isomorphic). It is also convenient to define the sum\n\nU1 + · · ·+ Up and the internal direct sum U1 ⊕ · · · ⊕ Up of any number of subspaces of E.\n\nDefinition 6.2. Given p ≥ 2 vector spaces E1, . . . , Ep, the product F = E1 × · · · × Ep can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)\n\nλ(u1, . . . , up) = (λu1, . . . , λup),\n\nfor all ui, vi ∈ Ei and all λ ∈ R. The zero vector of E1 × · · · × Ep is the p-tuple\n\n( 0, . . . , 0︸ ︷︷ ︸\np\n\n),\n\nwhere the ith zero is the zero vector of Ei.\n\nWith the above addition and multiplication, the vector space F = E1× · · ·×Ep is called\nthe direct product of the vector spaces E1, . . . , Ep.\n\nAs a special case, when E1 = · · · = Ep = R, we find again the vector space F = Rp. The\nprojection maps pri : E1 × · · · × Ep → Ei given by\n\npri(u1, . . . , up) = ui\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E, and E>, for every pair of linear maps,\nf: D> E, and g: D> En, there is a unique linear map, f x g: D > E, [| Es, such that\n™0(f xg) =f andm0(f x g) = 4, as in the following diagram:\n\nProof. Define\n(f x g)(w) = {C1 F(w)), (2; g(w)) fs\n\nfor every w € D. It is immediately verified that f x g is the unique linear map with the\nrequired properties. C]\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U,V are subspaces of a vector space E, letting i;: U > E and 12: V > E be the\ninclusion maps, if U]]V is isomomorphic to F under the map 7; + 22 given by Proposition\n6.1, we say that E is a direct sum of U and V, and we write E = U|[V (with a slight abuse\nof notation, since F and U [| V are only isomorphic). It is also convenient to define the sum\nU, +---+U, and the internal direct sum U; © --- @ U, of any number of subspaces of F.\n\nDefinition 6.2. Given p > 2 vector spaces E},...,E,, the product F' = EF, x --- x E, can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(U1,.--,Up) + (U1,---, Up) = (Ur + U1, ---, Up + Up)\n\nA(u1,-+-,Up) = (Au, --, AUp),\n\nfor all u;,v; € E; and all A € R. The zero vector of EF, x --- x E, is the p-tuple\n\nwhere the ith zero is the zero vector of E;.\n\nWith the above addition and multiplication, the vector space fF’ = LE) x --- x E, is called\nthe direct product of the vector spaces F),..., Ep.\n\nAs a special case, when EF, = --- = E, = R, we find again the vector space F = R?. The\nprojection maps pr;: Ey x --- x E, + E; given by\n\npri(uy,...,Up) = Uj\n\n\n\n\n158 CHAPTER 6. DIRECT SUMS\n\nare clearly linear. Similarly, the maps ini : Ei → E1 × · · · × Ep given by\n\nini(ui) = (0, . . . , 0, ui, 0, . . . , 0)\n\nare injective and linear. If dim(Ei) = ni and if (ei1, . . . , e\ni\nni\n\n) is a basis of Ei for i = 1, . . . , p,\nthen it is easy to see that the n1 + · · ·+ np vectors\n\n(e1\n1, 0, . . . , 0), . . . , (e1\n\nn1\n, 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ei1, 0, . . . , 0), . . . , (0, . . . , 0, eini , 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ep1), . . . , (0, . . . , 0, epnp)\n\nform a basis of E1 × · · · × Ep, and so\n\ndim(E1 × · · · × Ep) = dim(E1) + · · ·+ dim(Ep).\n\nLet us now consider a vector space E and p subspaces U1, . . . , Up of E. We have a map\n\na : U1 × · · · × Up → E\n\ngiven by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p. It is clear that this map is linear, and so its image is a subspace\nof E denoted by\n\nU1 + · · ·+ Up\n\nand called the sum of the subspaces U1, . . . , Up. By definition,\n\nU1 + · · ·+ Up = {u1 + · · ·+ up | ui ∈ Ui, 1 ≤ i ≤ p},\n\nand it is immediately verified that U1 + · · · + Up is the smallest subspace of E containing\nU1, . . . , Up. This also implies that U1 + · · ·+ Up does not depend on the order of the factors\nUi; in particular,\n\nU1 + U2 = U2 + U1.\n\nDefinition 6.3. For any vector space E and any p ≥ 2 subspaces U1, . . . , Up of E, if the\nmap a defined above is injective, then the sum U1 + · · ·+ Up is called a direct sum and it is\ndenoted by\n\nU1 ⊕ · · · ⊕ Up.\nThe space E is the direct sum of the subspaces Ui if\n\nE = U1 ⊕ · · · ⊕ Up.\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 159\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1.\n\nIf the map a is injective, then by Proposition 3.14 we have Ker a = {( 0, . . . , 0︸ ︷︷ ︸\np\n\n)} where\n\neach 0 is the zero vector of E, which means that if ui ∈ Ui for i = 1, . . . , p and if\n\nu1 + · · ·+ up = 0,\n\nthen (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.\n\nProposition 6.3. If the map a : U1×· · ·×Up → E is injective, then every u ∈ U1 + · · ·+Up\nhas a unique expression as a sum\n\nu = u1 + · · ·+ up,\n\nwith ui ∈ Ui, for i = 1, . . . , p.\n\nProof. If\nu = v1 + · · ·+ vp = w1 + · · ·+ wp,\n\nwith vi, wi ∈ Ui, for i = 1, . . . , p, then we have\n\nw1 − v1 + · · ·+ wp − vp = 0,\n\nand since vi, wi ∈ Ui and each Ui is a subspace, wi−vi ∈ Ui. The injectivity of a implies that\nwi−vi = 0, that is, wi = vi for i = 1, . . . , p, which shows the uniqueness of the decomposition\nof u.\n\nProposition 6.4. If the map a : U1 × · · · ×Up → E is injective, then any p nonzero vectors\nu1, . . . , up with ui ∈ Ui are linearly independent.\n\nProof. To see this, assume that\n\nλ1u1 + · · ·+ λpup = 0\n\nfor some λi ∈ R. Since ui ∈ Ui and Ui is a subspace, λiui ∈ Ui, and the injectivity of a\nimplies that λiui = 0, for i = 1, . . . , p. Since ui 6= 0, we must have λi = 0 for i = 1, . . . , p;\nthat is, u1, . . . , up with ui ∈ Ui and ui 6= 0 are linearly independent.\n\nObserve that if a is injective, then we must have Ui ∩Uj = (0) whenever i 6= j. However,\nthis condition is generally not sufficient if p ≥ 3. For example, if E = R2 and U1 the line\nspanned by e1 = (1, 0), U2 is the line spanned by d = (1, 1), and U3 is the line spanned by\ne2 = (0, 1), then U1∩U2 = U1∩U3 = U2∩U3 = {(0, 0)}, but U1+U2 = U1+U3 = U2+U3 = R2,\nso U1 + U2 + U3 is not a direct sum. For example, d is expressed in two different ways as\n\nd = (1, 1) = (1, 0) + (0, 1) = e1 + e2.\n\n\n\n160 CHAPTER 6. DIRECT SUMS\n\nSee Figure 6.1.\n\ne1\nU1\n\ne\n\nU3\n\n2 (1,1)\n\nU2\n\nFigure 6.1: The linear subspaces U1, U2, and U3 illustrated as lines in R2.\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1. Observe that when the map a is injective,\nthen it is a linear isomorphism between U1 × · · · × Up and U1 ⊕ · · · ⊕ Up. The difference is\nthat U1 × · · · × Up is defined even if the spaces Ui are not assumed to be subspaces of some\ncommon space.\n\nIf E is a direct sum E = U1⊕· · ·⊕Up, since any p nonzero vectors u1, . . . , up with ui ∈ Ui\nare linearly independent, if we pick a basis (uk)k∈Ij in Uj for j = 1, . . . , p, then (ui)i∈I with\nI = I1 ∪ · · · ∪ Ip is a basis of E. Intuitively, E is split into p independent subspaces.\n\nConversely, given a basis (ui)i∈I of E, if we partition the index set I as I = I1 ∪ · · · ∪ Ip,\nthen each subfamily (uk)k∈Ij spans some subspace Uj of E, and it is immediately verified\nthat we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\nDefinition 6.4. Let f : E → E be a linear map. For any subspace U of E, if f(U) ⊆ U we\nsay that U is invariant under f .\n\nAssume that E is finite-dimensional, a direct sum E = U1 ⊕ · · · ⊕ Up, and that each Uj\nis invariant under f . If we pick a basis (ui)i∈I as above with I = I1 ∪ · · · ∪ Ip and with\neach (uk)k∈Ij a basis of Uj, since each Uj is invariant under f , the image f(uk) of every basis\nvector uk with k ∈ Ij belongs to Uj, so the matrix A representing f over the basis (ui)i∈I is\na block diagonal matrix of the form\n\nA =\n\n\nA1\n\nA2\n\n. . .\n\nAp\n\n ,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 161\n\nwith each block Aj a dj × dj-matrix with dj = dim(Uj) and all other entries equal to 0. If\ndj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.\n\nThere are natural injections from each Ui to E denoted by ini : Ui → E.\n\nNow, if p = 2, it is easy to determine the kernel of the map a : U1 × U2 → E. We have\n\na(u1, u2) = u1 + u2 = 0 iff u1 = −u2, u1 ∈ U1, u2 ∈ U2,\n\nwhich implies that\n\nKer a = {(u,−u) | u ∈ U1 ∩ U2}.\nNow, U1 ∩ U2 is a subspace of E and the linear map u 7→ (u,−u) is clearly an isomorphism\nbetween U1 ∩U2 and Ker a, so Ker a is isomorphic to U1 ∩U2. As a consequence, we get the\nfollowing result:\n\nProposition 6.5. Given any vector space E and any two subspaces U1 and U2, the sum\nU1 + U2 is a direct sum iff U1 ∩ U2 = (0).\n\nAn interesting illustration of the notion of direct sum is the decomposition of a square\nmatrix into its symmetric part and its skew-symmetric part. Recall that an n × n matrix\nA ∈ Mn is symmetric if A> = A, skew -symmetric if A> = −A. It is clear that s\n\nS(n) = {A ∈ Mn | A> = A} and Skew(n) = {A ∈ Mn | A> = −A}\n\nare subspaces of Mn, and that S(n)∩Skew(n) = (0). Observe that for any matrix A ∈ Mn,\nthe matrix H(A) = (A + A>)/2 is symmetric and the matrix S(A) = (A − A>)/2 is skew-\nsymmetric. Since\n\nA = H(A) + S(A) =\nA+ A>\n\n2\n+\nA− A>\n\n2\n,\n\nwe see that Mn = S(n) + Skew(n), and since S(n)∩Skew(n) = (0), we have the direct sum\n\nMn = S(n)⊕ Skew(n).\n\nRemark: The vector space Skew(n) of skew-symmetric matrices is also denoted by so(n).\nIt is the Lie algebra of the group SO(n).\n\nProposition 6.5 can be generalized to any p ≥ 2 subspaces at the expense of notation.\nThe proof of the following proposition is left as an exercise.\n\nProposition 6.6. Given any vector space E and any p ≥ 2 subspaces U1, . . . , Up, the fol-\nlowing properties are equivalent:\n\n(1) The sum U1 + · · ·+ Up is a direct sum.\n\n\n\n162 CHAPTER 6. DIRECT SUMS\n\n(2) We have\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0), i = 1, . . . , p.\n\n(3) We have\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nBecause of the isomorphism\n\nU1 × · · · × Up ≈ U1 ⊕ · · · ⊕ Up,\n\nwe have\n\nProposition 6.7. If E is any vector space, for any (finite-dimensional) subspaces U1, . . .,\nUp of E, we have\n\ndim(U1 ⊕ · · · ⊕ Up) = dim(U1) + · · ·+ dim(Up).\n\nIf E is a direct sum\nE = U1 ⊕ · · · ⊕ Up,\n\nsince every u ∈ E can be written in a unique way as\n\nu = u1 + · · ·+ up\n\nwith ui ∈ Ui for i = 1 . . . , p, we can define the maps πi : E → Ui, called projections , by\n\nπi(u) = πi(u1 + · · ·+ up) = ui.\n\nIt is easy to check that these maps are linear and satisfy the following properties:\n\nπj ◦ πi =\n\n{\nπi if i = j\n\n0 if i 6= j,\n\nπ1 + · · ·+ πp = idE.\n\nFor example, in the case of the direct sum\n\nMn = S(n)⊕ Skew(n),\n\nthe projection onto S(n) is given by\n\nπ1(A) = H(A) =\nA+ A>\n\n2\n,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nπ2(A) = S(A) =\nA− A>\n\n2\n.\n\nClearly, H(A)+S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that f ◦ f = f is said to be idempotent . Thus, the projections πi are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p ≥ 2 linear maps fi : E → E, if\n\nfj ◦ fi =\n\n{\nfi if i = j\n\n0 if i 6= j,\n\nf1 + · · ·+ fp = idE,\n\nthen if we let Ui = fi(E), we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f : E → E is an idempotent linear map, i.e.,\nf ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set I. First,\nlet us recall the notion of the product of a family (Ei)i∈I . Given a family of sets (Ei)i∈I , its\nproduct\n\n∏\ni∈I Ei, is the set of all functions f : I → ⋃\n\ni∈I Ei, such that, f(i) ∈ Ei, for every\ni ∈ I. It is one of the many versions of the axiom of choice, that, if Ei 6= ∅ for every i ∈ I,\nthen\n\n∏\ni∈I Ei 6= ∅. A member f ∈ ∏i∈I Ei, is often denoted as (fi)i∈I . For every i ∈ I, we\n\nhave the projection πi :\n∏\n\ni∈I Ei → Ei, defined such that, πi((fi)i∈I) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let I be any nonempty set, and let (Ei)i∈I be a family of vector spaces.\nThe (external) direct sum\n\n∐\ni∈I Ei (or coproduct) of the family (Ei)i∈I is defined as follows:∐\n\ni∈I Ei consists of all f ∈ ∏i∈I Ei, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(fi)i∈I + (gi)i∈I = (fi + gi)i∈I ,\n\nλ(fi)i∈I = (λfi)i∈I .\n\nWe also have injection maps ini : Ei →\n∐\n\ni∈I Ei, defined such that, ini(x) = (fi)i∈I , where\nfi = x, and fj = 0, for all j ∈ (I − {i}).\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nm™(A) = S(A) =\n\nClearly, H(A) +$(A) = A, H(H(A)) = H(A), $(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that fo f = f is said to be idempotent. Thus, the projections 7; are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p > 2 linear maps f;: E > E, if\n\nfi ifisj\nfo fi= “pe ye\n0 ift AF),\nfitet fp = ide,\nthen if we let U; = f,(E), we have a direct sum\n\nE=U,®-:®@U,.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f: E > E is an idempotent linear map, 1.e.,\nfof=f, then we have a direct sum\n\nE=Kerf @lImf,\n\nso that f is the projection onto its image Im f.\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set J. First,\nlet us recall the notion of the product of a family (£;);e7. Given a family of sets (F;)jer, its\nproduct [],-; i, is the set of all functions f: J > U,_, Ei, such that, f(i) € Ej, for every\ni € I. It is one of the many versions of the axiom of choice, that, if E; 4 @ for every 7 € J,\nthen [],<; £; 4 0. A member f € [J,-; Fi, is often denoted as (f;)ier. For every i € I, we\nhave the projection 7: [[,<; Ei + Ej, defined such that, m((fi)ier) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let J be any nonempty set, and let (F;)ic7 be a family of vector spaces.\nThe (external) direct sum [],-; E; (or coproduct) of the family (£;)ier is defined as follows:\n\nL<, £i consists of all f € [],<; i, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(flier + (Qi)ier = (fi + M)ier,\nA fidier = (Afidier-\n\nWe also have injection maps in;: E; > |],-; Fi, defined such that, in;(x) = (fi)ier, where\n\nfi =x, and f; =0, for all 7 € (J — {i}).\n\ntel\n\n\n\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (Ei)i∈I be a family of vector spaces, and\nlet G be any vector space. The direct sum\n\n∐\ni∈I Ei is a vector space, and for every family\n\n(hi)i∈I of linear maps hi : Ei → G, there is a unique linear map(∑\ni∈I\n\nhi\n\n)\n:\n∐\ni∈I\n\nEi → G,\n\nsuch that, (\n∑\n\ni∈I hi) ◦ ini = hi, for every i ∈ I.\n\nRemarks:\n\n(1) One might wonder why the direct sum\n∐\n\ni∈I Ei consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (Ei)i∈I as\n\n∏\ni∈I Ei? The product space\n\n∏\ni∈I Ei with addition and scalar\n\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map ĥ :\n\n∏\ni∈I Ei → G such that ĥ ◦ ini = hi for all ∈ I must be given by\n\nh̃((ui)∈I) =\n∑\ni∈I\n\nhi(ui),\n\nand if I is infinite, the sum on the right-hand side is infinite, and thus undefined! If I\nis finite then\n\n∏\ni∈I Ei and\n\n∐\ni∈I Ei are isomorphic.\n\n(2) When Ei = E, for all i ∈ I, we denote\n∐\n\ni∈I Ei by E(I). In particular, when Ei = K,\nfor all i ∈ I, we find the vector space K(I) of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f : E → F be a linear map. If\nf : E → F is injective, then there is a surjective linear map r : F → E called a retraction,\nsuch that r ◦ f = idE. See Figure 6.2. If f : E → F is surjective, then there is an injective\nlinear map s : F → E called a section, such that f ◦ s = idF . See Figure 6.3.\n\nProof. Let (ui)i∈I be a basis of E. Since f : E → F is an injective linear map, by Proposition\n3.15, (f(ui))i∈I is linearly independent in F . By Theorem 3.7, there is a basis (vj)j∈J of F ,\nwhere I ⊆ J , and where vi = f(ui), for all i ∈ I. By Proposition 3.15, a linear map r : F → E\ncan be defined such that r(vi) = ui, for all i ∈ I, and r(vj) = w for all j ∈ (J − I), where w\nis any given vector in E, say w = 0. Since r(f(ui)) = ui for all i ∈ I, by Proposition 3.15,\nwe have r ◦ f = idE.\n\nNow, assume that f : E → F is surjective. Let (vj)j∈J be a basis of F . Since f : E → F\nis surjective, for every vj ∈ F , there is some uj ∈ E such that f(uj) = vj. Since (vj)j∈J is a\nbasis of F , by Proposition 3.15, there is a unique linear map s : F → E such that s(vj) = uj.\nAlso, since f(s(vj)) = vj, by Proposition 3.15 (again), we must have f ◦ s = idF .\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (E;)icr be a family of vector spaces, and\nlet G be any vector space. The direct sum [],-,; Ei; 1s a vector space, and for every family\n(hi)icr of linear maps h;: E; + G, there is a unique linear map\n\n(Soni): []2-¢\n\n1EL tel\n\nsuch that, (Yoje, hi) 9 ini = hy, for every i € I.\n\nRemarks:\n\n(1) One might wonder why the direct sum [],_,; £; consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (E;)ier as [[,<; H;? The product space [],., £; with addition and scalar\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map h: [Ler E; > G such that ho in; = h; for all € J must be given by\n\nh((ui)er) = >, hi(ws);\niel\nand if J is infinite, the sum on the right-hand side is infinite, and thus undefined! If /\n\nis finite then [[,-, £; and [],_,; £; are isomorphic.\n\n(2) When E; = £, for all i € I, we denote [],-, E; by E. In particular, when E; = K,\nfor all i € I, we find the vector space K“ of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f: E + F be a linear map. If\nf: E - F is injective, then there is a surjective linear map r: F — E called a retraction,\nsuch that ro f =idg. See Figure 6.2. If f: E > F 1s surjective, then there is an injective\nlinear map s: F — E called a section, such that fos =idp. See Figure 6.3.\n\nProof. Let (u;)ier be a basis of E. Since f: E — F is an injective linear map, by Proposition\n3.15, (f(us))ier is linearly independent in F. By Theorem 3.7, there is a basis (v;)jey of F,\nwhere J C J, and where v; = f(u;), for alli € J. By Proposition 3.15, a linear map r: F > EF\ncan be defined such that r(v;) = u;, for all i € I, and r(v;) = w for all j € (J — J), where w\nis any given vector in FE, say w = 0. Since r(f(u;)) = u; for all i € I, by Proposition 3.15,\nwe have ro f = idg.\n\nNow, assume that f: E — F is surjective. Let (v;)je7 be a basis of F. Since f: E> F\nis surjective, for every v; € F, there is some u; € E such that f(u;) = v;. Since (v;)jey is a\nbasis of F’, by Proposition 3.15, there is a unique linear map s: F' > E such that s(v;) = uj;.\nAlso, since f(s(v;)) = v;, by Proposition 3.15 (again), we must have f os = idp. O\n\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 165\n\nu = (1,0)\n1\n\nu = (1,1)\n2\n\nf(x,y) = (x,y,0)\n\nv = f(u ) = (1,0,0)1 1 v  = f(u ) = (1,1,0)2 2\n\nv = (0,0,1)\n3\n\nr(x,y,z) = (x,y)\nE = R\n\n2\n\nF = R3\n\nFigure 6.2: Let f : E → F be the injective linear map from R2 to R3 given by f(x, y) =\n(x, y, 0). Then a surjective retraction is given by r : R3 → R2 is given by r(x, y, z) = (x, y).\nObserve that r(v1) = u1, r(v2) = u2, and r(v3) = 0 .\n\nThe converse of Proposition 6.11 is obvious.\n\nWe are now ready to prove a very crucial result relating the rank and the dimension of\nthe kernel of a linear map.\n\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation\n\nWe begin with the following fundamental proposition.\n\nProposition 6.12. Let E, F and G, be three vector spaces, f : E → F an injective linear\nmap, g : F → G a surjective linear map, and assume that Im f = Ker g. Then, the following\nproperties hold. (a) For any section s : G → F of g, we have F = Ker g ⊕ Im s, and the\nlinear map f + s : E ⊕G→ F is an isomorphism.1\n\n(b) For any retraction r : F → E of f , we have F = Im f ⊕Ker r.2\n\nE\nf //\n\nF\nr\noo\n\ng //\nG\n\ns\noo\n\nProof. (a) Since s : G→ F is a section of g, we have g ◦ s = idG, and for every u ∈ F ,\n\ng(u− s(g(u))) = g(u)− g(s(g(u))) = g(u)− g(u) = 0.\n\n1The existence of a section s : G→ F of g follows from Proposition 6.11.\n2The existence of a retraction r : F → E of f follows from Proposition 6.11.\n\n\n\n166 CHAPTER 6. DIRECT SUMS\n\nThus, u − s(g(u)) ∈ Ker g, and we have F = Ker g + Im s. On the other hand, if u ∈\nKer g ∩ Im s, then u = s(v) for some v ∈ G because u ∈ Im s, g(u) = 0 because u ∈ Ker g,\nand so,\n\ng(u) = g(s(v)) = v = 0,\n\nbecause g ◦ s = idG, which shows that u = s(v) = 0. Thus, F = Ker g ⊕ Im s, and since by\nassumption, Im f = Ker g, we have F = Im f ⊕ Im s. But then, since f and s are injective,\nf + s : E ⊕G→ F is an isomorphism. The proof of (b) is very similar.\n\nNote that we can choose a retraction r : F → E so that Ker r = Im s, since\nF = Ker g ⊕ Im s = Im f ⊕ Im s and f is injective so we can set r ≡ 0 on Im s.\n\nGiven a sequence of linear maps E\nf−→ F\n\ng−→ G, when Im f = Ker g, we say that the\n\nsequence E\nf−→ F\n\ng−→ G is exact at F . If in addition to being exact at F , f is injective\nand g is surjective, we say that we have a short exact sequence, and this is denoted as\n\n0 −→ E\nf−→ F\n\ng−→ G −→ 0.\n\nThe property of a short exact sequence given by Proposition 6.12 is often described by saying\n\nthat 0 −→ E\nf−→ F\n\ng−→ G −→ 0 is a (short) split exact sequence.\n\nAs a corollary of Proposition 6.12, we have the following result which shows that given\na linear map f : E → F , its domain E is the direct sum of its kernel Ker f with some\nisomorphic copy of its image Im f .\n\nTheorem 6.13. (Rank-nullity theorem) Let E and F be vector spaces, and let f : E → F\nbe a linear map. Then, E is isomorphic to Ker f ⊕ Im f , and thus,\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).\n\nSee Figure 6.3.\n\nProof. Consider\n\nKer f\ni−→ E\n\nf ′−→ Im f,\n\nwhere Ker f\ni−→ E is the inclusion map, and E\n\nf ′−→ Im f is the surjection associated\n\nwith E\nf−→ F . Then, we apply Proposition 6.12 to any section Im f\n\ns−→ E of f ′ to\nget an isomorphism between E and Ker f ⊕ Im f , and Proposition 6.7, to get dim(E) =\ndim(Ker f) + dim(Im f).\n\nDefinition 6.6. The dimension dim(Ker f) of the kernel of a linear map f is called the\nnullity of f .\n\nWe now derive some important results using Theorem 6.13.\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 167\n\nu = (0,1,1)\n\n2\n\nu = (1,0,1)\n1\n\nKer f\n\nf  = f(u  ) = (1,0)11\n\nf  =  f(u  ) = (0, 1)2 2 f(u) = (1,1)\n\nf(x,y,z) = (x,y)\n\ns(x,y) = (x,y,x+y)\n\nu = (1,1,1)\n\ns (f(u)) = (1,1,2)\n\nh = (0,0,-1)\n\nFigure 6.3: Let f : E → F be the linear map from R3 to R2 given by f(x, y, z) = (x, y).\nThen s : R2 → R3 is given by s(x, y) = (x, y, x + y) and maps the pink R2 isomorphically\nonto the slanted pink plane of R3 whose equation is −x − y + z = 0. Theorem 6.13 shows\nthat R3 is the direct sum of the plane −x− y + z = 0 and the kernel of f which the orange\nz-axis.\n\nProposition 6.14. Given a vector space E, if U and V are any two subspaces of E, then\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nan equation known as Grassmann’s relation.\n\nProof. Recall that U + V is the image of the linear map\n\na : U × V → E\n\ngiven by\n\na(u, v) = u+ v,\n\nand that we proved earlier that the kernel Ker a of a is isomorphic to U ∩ V . By Theorem\n6.13,\n\ndim(U × V ) = dim(Ker a) + dim(Im a),\n\nbut dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩ V ), and Im a = U + V , so the\nGrassmann relation holds.\n\nThe Grassmann relation can be very useful to figure out whether two subspace have a\nnontrivial intersection in spaces of dimension > 3. For example, it is easy to see that in R5,\nthere are subspaces U and V with dim(U) = 3 and dim(V ) = 2 such that U ∩ V = (0); for\nexample, let U be generated by the vectors (1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be\n\n\n\n168 CHAPTER 6. DIRECT SUMS\n\ngenerated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim that if dim(U) = 3\nand dim(V ) = 3, then dim(U ∩ V ) ≥ 1. Indeed, by the Grassmann relation, we have\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nnamely\n\n3 + 3 = 6 = dim(U + V ) + dim(U ∩ V ),\n\nand since U + V is a subspace of R5, dim(U + V ) ≤ 5, which implies\n\n6 ≤ 5 + dim(U ∩ V ),\n\nthat is 1 ≤ dim(U ∩ V ).\n\nAs another consequence of Proposition 6.14, if U and V are two hyperplanes in a vector\nspace of dimension n, so that dim(U) = n− 1 and dim(V ) = n− 1, the reader should show\nthat\n\ndim(U ∩ V ) ≥ n− 2,\n\nand so, if U 6= V , then\n\ndim(U ∩ V ) = n− 2.\n\nHere is a characterization of direct sums that follows directly from Theorem 6.13.\n\nProposition 6.15. If U1, . . . , Up are any subspaces of a finite dimensional vector space E,\nthen\n\ndim(U1 + · · ·+ Up) ≤ dim(U1) + · · ·+ dim(Up),\n\nand\n\ndim(U1 + · · ·+ Up) = dim(U1) + · · ·+ dim(Up)\n\niff the Uis form a direct sum U1 ⊕ · · · ⊕ Up.\n\nProof. If we apply Theorem 6.13 to the linear map\n\na : U1 × · · · × Up → U1 + · · ·+ Up\n\ngiven by a(u1, . . . , up) = u1 + · · ·+ up, we get\n\ndim(U1 + · · ·+ Up) = dim(U1 × · · · × Up)− dim(Ker a)\n\n= dim(U1) + · · ·+ dim(Up)− dim(Ker a),\n\nso the inequality follows. Since a is injective iff Ker a = (0), the Uis form a direct sum iff\nthe second equation holds.\n\nAnother important corollary of Theorem 6.13 is the following result:\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 169\n\nProposition 6.16. Let E and F be two vector spaces with the same finite dimension\ndim(E) = dim(F ) = n. For every linear map f : E → F , the following properties are\nequivalent:\n\n(a) f is bijective.\n\n(b) f is surjective.\n\n(c) f is injective.\n\n(d) Ker f = (0).\n\nProof. Obviously, (a) implies (b).\n\nIf f is surjective, then Im f = F , and so dim(Im f) = n. By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which means that\nKer f = (0), and so f is injective (see Proposition 3.14). This proves that (b) implies (c).\n\nIf f is injective, then by Proposition 3.14, Ker f = (0), so (c) implies (d).\n\nFinally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is injective (by Proposi-\ntion 3.14). By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(Ker f) = 0, we get\n\ndim(Im f) = dim(E) = dim(F ),\n\nwhich proves that f is also surjective, and thus bijective. This proves that (d) implies (a)\nand concludes the proof.\n\nOne should be warned that Proposition 6.16 fails in infinite dimension.\n\nThe following Proposition will also be useful.\n\nProposition 6.17. Let E be a vector space. If E = U ⊕ V and E = U ⊕W , then there is\nan isomorphism f : V → W between V and W .\n\nProof. Let R be the relation between V and W , defined such that\n\n〈v, w〉 ∈ R iff w − v ∈ U.\n\nWe claim that R is a functional relation that defines a linear isomorphism f : V → W\nbetween V and W , where f(v) = w iff 〈v, w〉 ∈ R (R is the graph of f). If w − v ∈ U and\nw′ − v ∈ U , then w′ − w ∈ U , and since U ⊕W is a direct sum, U ∩W = (0), and thus\n\n\n\n170 CHAPTER 6. DIRECT SUMS\n\nw′ − w = 0, that is w′ = w. Thus, R is functional. Similarly, if w − v ∈ U and w − v′ ∈ U ,\nthen v′ − v ∈ U , and since U ⊕ V is a direct sum, U ∩ V = (0), and v′ = v. Thus, f is\ninjective. Since E = U ⊕ V , for every w ∈ W , there exists a unique pair 〈u, v〉 ∈ U × V ,\nsuch that w = u+ v. Then, w− v ∈ U , and f is surjective. We also need to verify that f is\nlinear. If\n\nw − v = u\n\nand\nw′ − v′ = u′,\n\nwhere u, u′ ∈ U , then, we have\n\n(w + w′)− (v + v′) = (u+ u′),\n\nwhere u+ u′ ∈ U . Similarly, if\nw − v = u\n\nwhere u ∈ U , then we have\nλw − λv = λu,\n\nwhere λu ∈ U . Thus, f is linear.\n\nGiven a vector space E and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U ⊕ V depends only on U . We call dim(V ) the\ncodimension of U , and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f : E → F is the dimension dim(Im f) of the image\nsubspace Im f of F .\n\nWe have the following simple proposition.\n\nProposition 6.18. Given a linear map f : E → F , the following properties hold:\n\n(i) rk(f) = codim(Ker f).\n\n(ii) rk(f) + dim(Ker f) = dim(E).\n\n(iii) rk(f) ≤ min(dim(E), dim(F )).\n\nProof. Since by Proposition 6.13, dim(E) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = codim(Ker f). Since rk(f) = dim(Im f), (ii) follows\nfrom dim(E) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F , we have\nrk(f) ≤ dim(F ), and since rk(f) + dim(Ker f) = dim(E), we have rk(f) ≤ dim(E).\n\nThe rank of a matrix is defined as follows.\n\n170 CHAPTER 6. DIRECT SUMS\n\nw' —w = 0, that is w’ = w. Thus, R is functional. Similarly, if w—-v ¢€U andw—v' €U,\nthen v' — v € U, and since U @ V is a direct sum, UNV = (0), and v’ = v. Thus, f is\ninjective. Since EF = U @V, for every w € W, there exists a unique pair (u,v) € U x V,\nsuch that w = u+v. Then, w—v € U, and f is surjective. We also need to verify that f is\nlinear. If\n\nw-v=U\n\nand\n/ / /\nw—-v =u,\n\nwhere u, u’ € U, then, we have\n(wtw')—(v+v') =(utv),\n\nwhere u+u' € U. Similarly, if\n\nwhere u € U, then we have\nAw — Av = Au,\n\nwhere Au € U. Thus, f is linear. O\n\nGiven a vector space F and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U @V depends only on U. We call dim(V) the\ncodimension of U, and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f: E — F is the dimension dim(Im f) of the image\nsubspace Im f of F’.\n\nWe have the following simple proposition.\nProposition 6.18. Given a linear map f: E — F, the following properties hold:\n(i) rk(f) = codim(Ker f).\n(ii) rk(f) + dim(Ker f) = dim(E).\n(iit) rk(f) < min(dim(£), dim(F)).\n\nProof. Since by Proposition 6.13, dim(#) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = = codim(Ker f). Since rk(f) = dim/(Im f), (ii) follows\nfrom dim(F) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F’, we have\nrk(f) < dim(F), and since rk(f) + dim(Ker f) = dim(£), we have rk(f) < dim(E). O\n\nThe rank of a matrix is defined as follows.\n\n\n\n\n6.3. SUMMARY 171\n\nDefinition 6.7. Given a m × n-matrix A = (ai j) over the field K, the rank rk(A) of the\nmatrix A is the maximum number of linearly independent columns of A (viewed as vectors\nin Km).\n\nIn view of Proposition 3.8, the rank of a matrix A is the dimension of the subspace of\nKm generated by the columns of A. Let E and F be two vector spaces, and let (u1, . . . , un)\nbe a basis of E, and (v1, . . . , vm) a basis of F . Let f : E → F be a linear map, and let M(f)\nbe its matrix w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the\ndimension of Im f , which is generated by (f(u1), . . . , f(un)), the rank of f is the maximum\nnumber of linearly independent vectors in (f(u1), . . . , f(un)), which is equal to the number\nof linearly independent columns of M(f), since F and Km are isomorphic. Thus, we have\nrk(f) = rk(M(f)), for every matrix representing f .\n\nWe will see later, using duality, that the rank of a matrix A is also equal to the maximal\nnumber of linearly independent rows of A.\n\nIf U is a hyperplane, then E = U ⊕ V for some subspace V of dimension 1. However, a\nsubspace V of dimension 1 is generated by any nonzero vector v ∈ V , and thus we denote\nV by Kv, and we write E = U ⊕ Kv. Clearly, v /∈ U . Conversely, let x ∈ E be a vector\nsuch that x /∈ U (and thus, x 6= 0). We claim that E = U ⊕ Kx. Indeed, since U is a\nhyperplane, we have E = U ⊕Kv for some v /∈ U (with v 6= 0). Then, x ∈ E can be written\nin a unique way as x = u + λv, where u ∈ U , and since x /∈ U , we must have λ 6= 0, and\nthus, v = −λ−1u + λ−1x. Since E = U ⊕Kv, this shows that E = U + Kx. Since x /∈ U ,\nwe have U ∩Kx = 0, and thus E = U ⊕Kx. This argument shows that a hyperplane is a\nmaximal proper subspace H of E.\n\nIn Chapter 11, we shall see that hyperplanes are precisely the Kernels of nonnull linear\nmaps f : E → K, called linear forms.\n\n6.3 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Direct products, sums, direct sums .\n\n• Projections .\n\n• The fundamental equation\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)\n\n(Proposition 6.13).\n\n• Grassmann’s relation\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ).\n\n\n\n172 CHAPTER 6. DIRECT SUMS\n\n• Characterizations of a bijective linear map f : E → F .\n\n• Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E. Prove that if V ∪W is\na subspace of E, then either V ⊆ W or W ⊆ V .\n\nProblem 6.2. Prove that for every vector space E, if f : E → E is an idempotent linear\nmap, i.e., f ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nProblem 6.3. Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E and recall\nthat the linear map\n\na : U1 × · · · × Up → E\n\nis given by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p.\n\n(1) If we let Zi ⊆ U1 × · · · × Up be given by\n\nZi =\n\n{(\nu1, . . . , ui−1,−\n\np∑\nj=1,j 6=i\n\nuj, ui+1, . . . , up\n\n) ∣∣∣∣∣\np∑\n\nj=1,j 6=i\nuj ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)}\n,\n\nfor i = 1, . . . , p, then prove that\n\nKer a = Z1 = · · · = Zp.\n\nIn general, for any given i, the condition Ui ∩\n(∑p\n\nj=1,j 6=i Uj\n\n)\n= (0) does not necessarily\n\nimply that Zi = (0). Thus, let\n\nZ =\n\n{(\nu1, . . . , ui−1, ui, ui+1, . . . , up\n\n) ∣∣∣∣ ui = −\np∑\n\nj=1,j 6=i\nuj, ui ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n, 1 ≤ i ≤ p\n\n}\n.\n\nSince Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p,\n\n172 CHAPTER 6. DIRECT SUMS\n\ne Characterizations of a bijective linear map f: E > F.\n\ne Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E’. Prove that if V UW is\na subspace of £, then either V CW or W CV.\n\nProblem 6.2. Prove that for every vector space E, if f: E — E is an idempotent linear\nmap, i.e., fo f = f, then we have a direct sum\n\nE=Ker f @Imf,\nso that f is the projection onto its image Im f.\n\nProblem 6.3. Let U;,...,U, be any p > 2 subspaces of some vector space F and recall\nthat the linear map\na: U,xX+:+xU,> EF\n\nis given by\nA(U1,-.-,Up) = Uy +--+ + Up,\n\nwith u; € U; fori =1,...,p.\n(1) If we let Z; C U; x --- x U, be given by\n\nP\nZi, = { (tus tian ) Uj trys +5 Up)\n\nj=l ji\n\nS wevin( Su}.\n\njHljxi j=l jAt\n\nfori =1,...,p, then prove that\n\nKera = 24, =-:- = Z,.\n\nIn general, for any given i, the condition U;M ( eer v;) = (0) does not necessarily\nimply that Z; = (0). Thus, let\n\nUz=— 3 ww EU 3 uj), 1sisoh,\n\nj=l ix j=l j#i\n\nZ= { (tases tira thstssss sty)\n\nSince Kera = Z; =--: = Z,, we have Z = Kera. Prove that if\n\nun ( 3 5) =) l<ic<p,\n\nj=l Hi\n\n\n\n\n6.4. PROBLEMS 173\n\nthen Z = Ker a = (0).\n\n(2) Prove that U1 + · · ·+ Up is a direct sum iff\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p.\n\nProblem 6.4. Assume that E is finite-dimensional, and let fi : E → E be any p ≥ 2 linear\nmaps such that\n\nf1 + · · ·+ fp = idE.\n\nProve that the following properties are equivalent:\n\n(1) f 2\ni = fi, 1 ≤ i ≤ p.\n\n(2) fj ◦ fi = 0, for all i 6= j, 1 ≤ i, j ≤ p.\n\nHint . Use Problem 6.2.\n\nLet U1, . . . , Up be any p ≥ 2 subspaces of some vector space E. Prove that U1 + · · ·+ Up\nis a direct sum iff\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nProblem 6.5. Given any vector space E, a linear map f : E → E is an involution if\nf ◦ f = id.\n\n(1) Prove that an involution f is invertible. What is its inverse?\n\n(2) Let E1 and E−1 be the subspaces of E defined as follows:\n\nE1 = {u ∈ E | f(u) = u}\nE−1 = {u ∈ E | f(u) = −u}.\n\nProve that we have a direct sum\nE = E1 ⊕ E−1.\n\nHint . For every u ∈ E, write\n\nu =\nu+ f(u)\n\n2\n+\nu− f(u)\n\n2\n.\n\n(3) If E is finite-dimensional and f is an involution, prove that there is some basis of E\nwith respect to which the matrix of f is of the form\n\nIk,n−k =\n\n(\nIk 0\n0 −In−k\n\n)\n,\n\nwhere Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1). Can you give a\ngeometric interpretation of the action of f (especially when k = n− 1)?\n\n\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all (j, k) such that\nj − k ≥ 0. An upper Hessenberg matrix is unreduced if hi+1i 6= 0 for i = 1, . . . , n− 1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n× k matrix.\n\n(1) Prove that the k × k matrix A>A and the matrix A have the same nullspace. Use\nthis to prove that rank(A>A) = rank(A). Similarly, prove that the n × n matrix AA> and\nthe matrix A> have the same nullspace, and conclude that rank(AA>) = rank(A>).\n\nWe will prove later that rank(A>) = rank(A).\n\n(2) Let a1, . . . , ak be k linearly independent vectors in Rn (1 ≤ k ≤ n), and let A be the\nn× k matrix whose ith column is ai. Prove that A>A has rank k, and that it is invertible.\nLet P = A(A>A)−1A> (an n× n matrix). Prove that\n\nP 2 = P\n\nP> = P.\n\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak, or equivalently\nthe set of all vectors in Rn of the form Ax, with x ∈ Rk. Prove that the nullspace U of P is\nthe set of vectors u ∈ Rn such that A>u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of Rn onto the subspace V spanned by a1, . . . , ak, and\nthat\n\nRn = U ⊕ V.\n\nProblem 6.8. A rotation Rθ in the plane R2 is given by the matrix\n\nRθ =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n.\n\n(1) Use Matlab to show the action of a rotation Rθ on a simple figure such as a triangle\nor a rectangle, for various values of θ, including θ = π/6, π/4, π/3, π/2.\n\n(2) Prove that Rθ is invertible and that its inverse is R−θ.\n\n(3) For any two rotations Rα and Rβ, prove that\n\nRβ ◦Rα = Rα ◦Rβ = Rα+β.\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\nSO(2).\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n x n matrix H is upper Hessenberg if hj, = 0 for all (j,k) such that\nj —k > 0. An upper Hessenberg matrix is unreduced if hii4; 4 0 for? =1,...,n—1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n x k matrix.\n\n(1) Prove that the k x k matrix A'A and the matrix A have the same nullspace. Use\nthis to prove that rank(A' A) = rank(A). Similarly, prove that the n x n matrix AA’ and\nthe matrix A’ have the same nullspace, and conclude that rank(AA') = rank(A').\n\nWe will prove later that rank(A') = rank(A).\n\n(2) Let a1,...,a, be & linearly independent vectors in R\" (1 < k <n), and let A be the\nn x k matrix whose ith column is a;. Prove that A'A has rank k, and that it is invertible.\nLet P = A(A'A)~'A! (an n x n matrix). Prove that\n\nP? =P\nP'=P.\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a,...,a,, or equivalently\nthe set of all vectors in R” of the form Az, with x € R*. Prove that the nullspace U of P is\nthe set of vectors u € R” such that A'u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of R” onto the subspace V spanned by ay,,...,a,, and\nthat\n\nR\"=U®6V.\n\nProblem 6.8. A rotation Ro in the plane R? is given by the matrix\ncos@ —sin@\nRo = er cos 6 )\n\n(1) Use Matlab to show the action of a rotation Ry on a simple figure such as a triangle\nor a rectangle, for various values of 0, including 0 = 1/6, 7/4, 7/3, 7/2.\n\n(2) Prove that Rg is invertible and that its inverse is R_g.\n\n(3) For any two rotations Ry and Rg, prove that\nRg ° Ra = Ra ie) Rg = Ra+p-\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\n\nSO(2).\n\n\n\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Rθ,(a1,a2) in R2 given by(\ny1\n\ny2\n\n)\n=\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)(\nx1\n\nx2\n\n)\n+\n\n(\na1\n\na2\n\n)\n.\n\n(1) Prove that if θ 6= k2π, with k ∈ Z, then Rθ,(a1,a2) has a unique fixed point (c1, c2),\nthat is, there is a unique point (c1, c2) such that(\n\nc1\n\nc2\n\n)\n= Rθ,(a1,a2)\n\n(\nc1\n\nc2\n\n)\n,\n\nand this fixed point is given by(\nc1\n\nc2\n\n)\n=\n\n1\n\n2 sin(θ/2)\n\n(\ncos(π/2− θ/2) − sin(π/2− θ/2)\nsin(π/2− θ/2) cos(π/2− θ/2)\n\n)(\na1\n\na2\n\n)\n.\n\n(2) In this question we still assume that θ 6= k2π, with k ∈ Z. By translating the\ncoordinate system with origin (0, 0) to the new coordinate system with origin (c1, c2), which\nmeans that if (x1, x2) are the coordinates with respect to the standard origin (0, 0) and if\n(x′1, x\n\n′\n2) are the coordinates with respect to the new origin (c1, c2), we have\n\nx1 = x′1 + c1\n\nx2 = x′2 + c2\n\nand similarly for (y1, y2) and (y′1, y\n′\n2), then show that(\ny1\n\ny2\n\n)\n= Rθ,(a1,a2)\n\n(\nx1\n\nx2\n\n)\nbecomes (\n\ny′1\ny′2\n\n)\n= Rθ\n\n(\nx′1\nx′2\n\n)\n.\n\nConclude that with respect to the new origin (c1, c2), the affine map Rθ,(a1,a2) becomes\nthe rotation Rθ. We say that Rθ,(a1,a2) is a rotation of center (c1, c2).\n\n(3) Use Matlab to show the action of the affine map Rθ,(a1,a2) on a simple figure such as a\ntriangle or a rectangle, for θ = π/3 and various values of (a1, a2). Display the center (c1, c2)\nof the rotation.\n\nWhat kind of transformations correspond to θ = k2π, with k ∈ Z?\n\n(4) Prove that the inverse of Rθ,(a1,a2) is of the form R−θ,(b1,b2), and find (b1, b2) in terms\nof θ and (a1, a2).\n\n(5) Given two affine maps Rα,(a1,a2) and Rβ,(b1,b2), prove that\n\nRβ,(b1,b2) ◦Rα,(a1,a2) = Rα+β,(t1,t2)\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Ro (a,,a2) in R? given by\n\nV1 cos@ —sind L1 a1\n=(_ + .\nYo sind cos v9 a9\n(1) Prove that if 0 A k27, with k € Z, then Rova,,a,) has a unique fixed point (cy, ce),\nthat is, there is a unique point (c),c2) such that\n\nCy \\ Cy\n(‘:) = Ro (01,02) (“) )\nand this fixed point is given by\n\nc\\ 1 cos(7/2 — 0/2) —sin(a/2—6/2)\\ (a,\nC2)  2sin(@/2) \\sin(/2— 6/2) — cos(m/2 — 6/2) a2)”\n(2) In this question we still assume that 0 # k27, with k € Z. By translating the\ncoordinate system with origin (0,0) to the new coordinate system with origin (c1,c2), which\n\nmeans that if (7,22) are the coordinates with respect to the standard origin (0,0) and if\n(x, 75) are the coordinates with respect to the new origin (c;,c2), we have\n\n!\nT= XU,+\n\n!\nUq = Lo + CQ\n\nand similarly for (y1, y2) and (y{, 45), then show that\n\nY\\ Ly\n(32) = Boon (22)\nYi\\ ry\n(,) = Re (\").\n\nConclude that with respect to the new origin (ci, c2), the affine map Ro (a,,a,) becomes\nthe rotation Rg. We say that Rg (a,,a.) is a rotation of center (c1, C2).\n\nbecomes\n\n(3) Use Matlab to show the action of the affine map Rg (a,,a,) on a simple figure such as a\ntriangle or a rectangle, for 6 = 7/3 and various values of (a1, a2). Display the center (c1, ce)\nof the rotation.\n\nWhat kind of transformations correspond to 6 = k27, with k € Z?\n\n(4) Prove that the inverse of Rg (a,,a.) is of the form R_g (5,,.), and find (b), b2) in terms\nof @ and (a1, a2).\n\n(5) Given two affine maps Ro (a;,a.) and Rgo,,b), prove that\n\nR3.(b1,b2) ° Ra,(ar,a2) = Rots, (t1,t2)\n\n\n\n\n176 CHAPTER 6. DIRECT SUMS\n\nfor some (t1, t2), and find (t1, t2) in terms of β, (a1, a2) and (b1, b2).\n\nEven in the case where (a1, a2) = (0, 0), prove that in general\n\nRβ,(b1,b2) ◦Rα 6= Rα ◦Rβ,(b1,b2).\n\nUse (4)-(5) to show that the affine maps of the plane defined in this problem form a\nnonabelian group denoted SE(2).\n\nProve that Rβ,(b1,b2) ◦Rα,(a1,a2) is not a translation (possibly the identity) iff α+β 6= k2π,\nfor all k ∈ Z. Find its center of rotation when (a1, a2) = (0, 0).\n\nIf α+β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2) is a pure translation. Find the translation vector\nof Rβ,(b1,b2) ◦Rα,(a1,a2).\n\nProblem 6.10. (Affine subspaces) A subset A of Rn is called an affine subspace if either\nA = ∅, or there is some vector a ∈ Rn and some subspace U of Rn such that\n\nA = a+ U = {a+ u | u ∈ U}.\nWe define the dimension dim(A) of A as the dimension dim(U) of U .\n\n(1) If A = a+ U , why is a ∈ A?\n\nWhat are affine subspaces of dimension 0? What are affine subspaces of dimension 1\n(begin with R2)? What are affine subspaces of dimension 2 (begin with R3)?\n\nProve that any nonempty affine subspace is closed under affine combinations.\n\n(2) Prove that if A = a + U is any nonempty affine subspace, then A = b + U for any\nb ∈ A.\n\n(3) Let A be any nonempty subset of Rn closed under affine combinations. For any\na ∈ A, prove that\n\nUa = {x− a ∈ Rn | x ∈ A}\nis a (linear) subspace of Rn such that\n\nA = a+ Ua.\n\nProve that Ua does not depend on the choice of a ∈ A; that is, Ua = Ub for all a, b ∈ A. In\nfact, prove that\n\nUa = U = {y − x ∈ Rn | x, y ∈ A}, for all a ∈ A,\nand so\n\nA = a+ U, for any a ∈ A.\n\nRemark: The subspace U is called the direction of A.\n\n(4) Two nonempty affine subspaces A and B are said to be parallel iff they have the same\ndirection. Prove that that if A 6= B and A and B are parallel, then A ∩ B = ∅.\n\nRemark: The above shows that affine subspaces behave quite differently from linear sub-\nspaces.\n\n\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v1, . . . , vn) ∈ Rn, let\nv̂ ∈ Rn+1 be the vector v̂ = (v1, . . . , vn, 1). Equivalently, v̂ = (v̂1, . . . , v̂n+1) ∈ Rn+1 is the\nvector defined by\n\nv̂i =\n\n{\nvi if 1 ≤ i ≤ n,\n\n1 if i = n+ 1.\n\n(1) For any m+ 1 vectors (u0, u1, . . . , um) with ui ∈ Rn and m ≤ n, prove that if the m\nvectors (u1 − u0, . . . , um − u0) are linearly independent, then the m+ 1 vectors (û0, . . . , ûm)\nare linearly independent.\n\n(2) Prove that if the m + 1 vectors (û0, . . . , ûm) are linearly independent, then for any\nchoice of i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are\nlinearly independent.\n\nAny m+ 1 vectors (u0, u1, . . . , um) such that the m+ 1 vectors (û0, . . . , ûm) are linearly\nindependent are said to be affinely independent .\n\nFrom (1) and (2), the vector (u0, u1, . . . , um) are affinely independent iff for any any choice\nof i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are linearly\nindependent. If m = n, we say that n+ 1 affinely independent vectors (u0, u1, . . . , un) form\nan affine frame of Rn.\n\n(3) if (u0, u1, . . . , un) is an affine frame of Rn, then prove that for every vector v ∈ Rn,\nthere is a unique (n+ 1)-tuple (λ0, λ1, . . . , λn) ∈ Rn+1, with λ0 +λ1 + · · ·+λn = 1, such that\n\nv = λ0u0 + λ1u1 + · · ·+ λnun.\n\nThe scalars (λ0, λ1, . . . , λn) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (u0, u1, . . . , un).\n\nIf we write ei = ui − u0, for i = 1, . . . , n, then prove that we have\n\nv = u0 + λ1e1 + · · ·+ λnen,\n\nand since (e1, . . . , en) is a basis of Rn (by (1) & (2)), the n-tuple (λ1, . . . , λn) consists of the\nstandard coordinates of v − u0 over the basis (e1, . . . , en).\n\nConversely, for any vector u0 ∈ Rn and for any basis (e1, . . . , en) of Rn, let ui = u0 + ei\nfor i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an affine frame of Rn, and for any v ∈ Rn, if\n\nv = u0 + x1e1 + · · ·+ xnen,\n\nwith (x1, . . . , xn) ∈ Rn (unique), then\n\nv = (1− (x1 + · · ·+ xx))u0 + x1u1 + · · ·+ xnun,\n\nso that (1− (x1 + · · ·+xx)), x1, · · · , xn), are the barycentric coordinates of v w.r.t. the affine\nframe (u0, u1, . . . , un).\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v),...,Un) € R”, let\nv0 € R\"! be the vector 0 = (v,...,Un, 1). Equivalently, 0 = (01,...,0n41) € R°! is the\nvector defined by\n\n. {! ifl<i<n,\n\nUi= Lp\n1 ift=n-+1.\n(1) For any m+ 1 vectors (uo, U1,...,Um) with u; € R” and m <n, prove that if the m\nvectors (U1; — Uo,---,;Um — Uo) are linearly independent, then the m+ 1 vectors (to,...,Um)\n\nare linearly independent.\n\n(2) Prove that if the m+ 1 vectors (Uo,...,Um) are linearly independent, then for any\nchoice of 7, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with 7 —i # 0 are\nlinearly independent.\n\nAny m+ 1 vectors (uo, U1,---,Um) such that the m+ 1 vectors (Uo,..., Um) are linearly\nindependent are said to be affinely independent.\n\nFrom (1) and (2), the vector (uo, w1,..., Um) are affinely independent iff for any any choice\nof i, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with j —i ¥ 0 are linearly\nindependent. If m = n, we say that n +1 affinely independent vectors (uo, u1,...,Un) form\nan affine frame of R”.\n\n(3) if (uo, U1,...,Un) is an affine frame of R”, then prove that for every vector v € R”,\nthere is a unique (n+ 1)-tuple (Ag, \\1,---; An) € R°*!, with Ag+ Ai +--+: +An = 1, such that\n\nv= Agtto + Aut + +++ + Antn-\n\nThe scalars (Ao, A1,---,; An) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (uo, U1,.--, Un).\nIf we write e; = uj; — uo, for = 1,...,n, then prove that we have\n\nVv = Uo + Azer +++ + Anen,\n\nand since (€1,...,€n) is a basis of R” (by (1) & (2)), the n-tuple (\\1,..., An) consists of the\n\nstandard coordinates of v — uo over the basis (e1,...,€n).\nConversely, for any vector uo € R” and for any basis (€1,...,€n) of R”, let wu; = uo + e;\nfori =1,...,n. Prove that (uo, u1,..., Un) is an affine frame of R”, and for any v € R”, if\n\nU = Up + LE, Ft + Len,\nwith (a,...,%,) € R” (unique), then\nv= (1— (ay +--+ + 2z))uo + oyu H+ F 2p Un,\n\nso that (1— (a1 +--:+2z)),%1,-+++ ,2n), are the barycentric coordinates of v w.r.t. the affine\nframe (uo, U41,---, Un).\n\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (u0, . . .,\nun) and pairs (u0, (e1, . . . , en)), with (e1, . . . , en) a basis. Given an affine frame (u0, . . . , un),\nwe obtain the basis (e1, . . . , en) with ei = ui−u0, for i = 1, . . . , n; given the pair (u0, (e1, . . .,\nen)) where (e1, . . . , en) is a basis, we obtain the affine frame (u0, . . . , un), with ui = u0 + ei,\nfor i = 1, . . . , n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (u0, . . . , un) and standard coordinates w.r.t. the basis (e1, . . . , en).\nThe barycentric cordinates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the\nstandard coordinates (λ1, . . . , λn) of v − u0; the standard coordinates (x1, . . . , xn) of v − u0\n\nyield the barycentric coordinates (1− (x1 + · · ·+ xn), x1, . . . , xn) of v.\n\n(4) Recall that an affine map is a map f : E → F between vector spaces that preserves\naffine combinations ; that is,\n\nf\n\n(\nm∑\ni=1\n\nλiui\n\n)\n=\n\nm∑\ni=1\n\nλif(ui),\n\nfor all u1 . . . , um ∈ E and all λi ∈ K such that\n∑m\n\ni=1 λi = 1.\n\nLet (u0, . . . , un) be any affine frame in Rn and let (v0, . . . , vn) be any vectors in Rm. Prove\nthat there is a unique affine map f : Rn → Rm such that\n\nf(ui) = vi, i = 0, . . . , n.\n\n(5) Let (a0, . . . , an) be any affine frame in Rn and let (b0, . . . , bn) be any n+ 1 points in\nRn. Prove that there is a unique (n+ 1)× (n+ 1) matrix\n\nA =\n\n(\nB w\n0 1\n\n)\ncorresponding to the unique affine map f such that\n\nf(ai) = bi, i = 0, . . . , n,\n\nin the sense that\nAâi = b̂i, i = 0, . . . , n,\n\nand that A is given by\n\nA =\n(\nb̂0 b̂1 · · · b̂n\n\n) (\nâ0 â1 · · · ân\n\n)−1\n.\n\nMake sure to prove that the bottom row of A is (0, . . . , 0, 1).\n\nIn the special case where (a0, . . . , an) is the canonical affine frame with ai = ei+1 for\ni = 0, . . . , n− 1 and an = (0, . . . , 0) (where ei is the ith canonical basis vector), show that\n\n(\nâ0 â1 · · · ân\n\n)\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n1 1 · · · 1 1\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (up, ...,\nUn) and pairs (uo, (€1,.--,@n)), with (e1,...,e,) a basis. Given an affine frame (uo,..., Un),\nwe obtain the basis (€1,...,€,) with e; = u; — uo, for i = 1,...,; given the pair (uo, (e1,...,\n€,,)) where (€1,...,€n) is a basis, we obtain the affine frame (uo,..., Un), with wu; = uo + &;,\nfor 2 =1,...,n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (uo,...,Un) and standard coordinates w.r.t. the basis (e1,...,€n).\nThe barycentric cordinates (Xo, A1,---,;An) of v (with Ap + Ay +--+: + An = 1) yield the\nstandard coordinates (Ai,...,An) of vu — uo; the standard coordinates (1%1,...,2n) of v — uo\nyield the barycentric coordinates (1 — (a1 +-+++%n),%1,.--,2n) of v.\n\n(4) Recall that an affine map is a map f: EF — F' between vector spaces that preserves\naffine combinations; that is,\n\nf (> se) = » Af (ui);\n\nfor all uy ...,Um € E and all A; € K such that $0\", A; = 1.\n\nLet (uo,..., Un) be any affine frame in R” and let (vo,..., Un) be any vectors in R™. Prove\nthat there is a unique affine map f: R” > R” such that\n\nf(ui) =v;, t=0,...,n.\n\n(5) Let (ao,...,@n) be any affine frame in R” and let (bo,...,b,) be any n + 1 points in\nR”. Prove that there is a unique (n + 1) x (n + 1) matrix\n\nBow\na=(0 4)\ncorresponding to the unique affine map f such that\n\nf(a;) = b;, 1=0,...,N,\n\nin the sense that .\nAa; = b;, 1=0,...,N,\n\nand that A is given by\nA- (i Boe in) (@ @ +++ Gi).\n\nMake sure to prove that the bottom row of A is (0,...,0,1).\n\nIn the special case where (ao,...,@n) is the canonical affine frame with a; = e;4, for\ni=0,...,n—1 and a, = (0,...,0) (where e; is the 7th canonical basis vector), show that\n1 0.:--. 0 0\n\n0 1\n\na\n\nQ)\n\nOo\n\nQ)\n\n=\n\nQ\n\n=\n\n—\"\n\nI|\n\nOeee\nrer © oS\nFe Oa ©\n\ne\ne\n\n\n\n\n6.4. PROBLEMS 179\n\nand\n\n(\nâ0 â1 · · · ân\n\n)−1\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n−1 −1 · · · −1 1\n\n .\n\nFor example, when n = 2, if we write bi = (xi, yi), then we have\n\nA =\n\nx1 x2 x3\n\ny1 y2 y3\n\n1 1 1\n\n 1 0 0\n0 1 0\n−1 −1 1\n\n =\n\nx1 − x3 x2 − x3 x3\n\ny1 − y3 y2 − y3 y3\n\n0 0 1\n\n .\n\n(6) Recall that a nonempty affine subspace A of Rn is any nonempty subset of Rn closed\nunder affine combinations. For any affine map f : Rn → Rm, for any affine subspace A of\nRn, and any affine subspace B of Rm, prove that f(A) is an affine subspace of Rm, and that\nf−1(B) is an affine subspace of Rn.\n\n\n\n180 CHAPTER 6. DIRECT SUMS\n\n\n\nChapter 7\n\nDeterminants\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n7.1 Permutations, Signature of a Permutation\n\nThis chapter contains a review of determinants and their use in linear algebra. We begin\nwith permutations and the signature of a permutation. Next, we define multilinear maps\nand alternating multilinear maps. Determinants are introduced as alternating multilinear\nmaps taking the value 1 on the unit matrix (following Emil Artin). It is then shown how\nto compute a determinant using the Laplace expansion formula, and the connection with\nthe usual definition is made. It is shown how determinants can be used to invert matrices\nand to solve (at least in theory!) systems of linear equations (the Cramer formulae). The\ndeterminant of a linear map is defined. We conclude by defining the characteristic polynomial\nof a matrix (and of a linear map) and by proving the celebrated Cayley-Hamilton theorem\nwhich states that every matrix is a “zero” of its characteristic polynomial (we give two proofs;\none computational, the other one more conceptual).\n\nDeterminants can be defined in several ways. For example, determinants can be defined\nin a fancy way in terms of the exterior algebra (or alternating algebra) of a vector space.\nWe will follow a more algorithmic approach due to Emil Artin. No matter which approach\nis followed, we need a few preliminaries about permutations on a finite set. We need to\nshow that every permutation on n elements is a product of transpositions, and that the\nparity of the number of transpositions involved is an invariant of the permutation. Let\n[n] = {1, 2 . . . , n}, where n ∈ N, and n > 0.\n\nDefinition 7.1. A permutation on n elements is a bijection π : [n]→ [n]. When n = 1, the\nonly function from [1] to [1] is the constant map: 1 7→ 1. Thus, we will assume that n ≥ 2.\nA transposition is a permutation τ : [n]→ [n] such that, for some i < j (with 1 ≤ i < j ≤ n),\nτ(i) = j, τ(j) = i, and τ(k) = k, for all k ∈ [n] − {i, j}. In other words, a transposition\nexchanges two distinct elements i, j ∈ [n]. A cyclic permutation of order k (or k-cycle) is a\n\n181\n\n\n\n182 CHAPTER 7. DETERMINANTS\n\npermutation σ : [n]→ [n] such that, for some sequence (i1, i2, . . . , ik) of distinct elements of\n[n] with 2 ≤ k ≤ n,\n\nσ(i1) = i2, σ(i2) = i3, . . . , σ(ik−1) = ik, σ(ik) = i1,\n\nand σ(j) = j, for j ∈ [n]−{i1, . . . , ik}. The set {i1, . . . , ik} is called the domain of the cyclic\npermutation, and the cyclic permutation is usually denoted by (i1 i2 . . . ik).\n\nIf τ is a transposition, clearly, τ ◦ τ = id. Also, a cyclic permutation of order 2 is a\ntransposition, and for a cyclic permutation σ of order k, we have σk = id. Clearly, the\ncomposition of two permutations is a permutation and every permutation has an inverse\nwhich is also a permutation. Therefore, the set of permutations on [n] is a group often\ndenoted Sn. It is easy to show by induction that the group Sn has n! elements. We will\nalso use the terminology product of permutations (or transpositions), as a synonym for\ncomposition of permutations.\n\nA permutation σ on n elements, say σ(i) = ki for i = 1, . . . , n, can be represented in\nfunctional notation by the 2× n array(\n\n1 · · · i · · · n\nk1 · · · ki · · · kn\n\n)\nknown as Cauchy two-line notation. For example, we have the permutation σ denoted by(\n\n1 2 3 4 5 6\n2 4 3 6 5 1\n\n)\n.\n\nA more concise notation often used in computer science and in combinatorics is to rep-\nresent a permutation by its image, namely by the sequence\n\nσ(1) σ(2) · · · σ(n)\n\nwritten as a row vector without commas separating the entries. The above is known as\nthe one-line notation. For example, in the one-line notation, our previous permutation σ is\nrepresented by\n\n2 4 3 6 5 1.\n\nThe reason for not enclosing the above sequence within parentheses is avoid confusion with\nthe notation for cycles, for which is it customary to include parentheses.\n\nThe following proposition shows the importance of cyclic permutations and transposi-\ntions.\n\nProposition 7.1. For every n ≥ 2, for every permutation π : [n]→ [n], there is a partition\nof [n] into r subsets called the orbits of π, with 1 ≤ r ≤ n, where each set J in this partition\nis either a singleton {i}, or it is of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\n\n\n\n7.1. PERMUTATIONS, SIGNATURE OF A PERMUTATION 183\n\nwhere ri is the smallest integer, such that, πri(i) = i and 2 ≤ ri ≤ n. If π is not the identity,\nthen it can be written in a unique way (up to the order) as a composition π = σ1 ◦ . . . ◦ σs\nof cyclic permutations with disjoint domains, where s is the number of orbits with at least\ntwo elements. Every permutation π : [n]→ [n] can be written as a nonempty composition of\ntranspositions.\n\nProof. Consider the relation Rπ defined on [n] as follows: iRπj iff there is some k ≥ 1 such\nthat j = πk(i). We claim that Rπ is an equivalence relation. Transitivity is obvious. We\nclaim that for every i ∈ [n], there is some least r (1 ≤ r ≤ n) such that πr(i) = i.\n\nIndeed, consider the following sequence of n+ 1 elements:\n\n〈i, π(i), π2(i), . . . , πn(i)〉.\n\nSince [n] only has n distinct elements, there are some h, k with 0 ≤ h < k ≤ n such that\n\nπh(i) = πk(i),\n\nand since π is a bijection, this implies πk−h(i) = i, where 0 ≤ k − h ≤ n. Thus, we proved\nthat there is some integer m ≥ 1 such that πm(i) = i, so there is such a smallest integer r.\n\nConsequently, Rπ is reflexive. It is symmetric, since if j = πk(i), letting r be the least\nr ≥ 1 such that πr(i) = i, then\n\ni = πkr(i) = πk(r−1)(πk(i)) = πk(r−1)(j).\n\nNow, for every i ∈ [n], the equivalence class (orbit) of i is a subset of [n], either the singleton\n{i} or a set of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\nwhere ri is the smallest integer such that πri(i) = i and 2 ≤ ri ≤ n, and in the second case,\nthe restriction of π to J induces a cyclic permutation σi, and π = σ1 ◦ . . . ◦σs, where s is the\nnumber of equivalence classes having at least two elements.\n\nFor the second part of the proposition, we proceed by induction on n. If n = 2, there are\nexactly two permutations on [2], the transposition τ exchanging 1 and 2, and the identity.\nHowever, id2 = τ 2. Now, let n ≥ 3. If π(n) = n, since by the induction hypothesis, the\nrestriction of π to [n − 1] can be written as a product of transpositions, π itself can be\nwritten as a product of transpositions. If π(n) = k 6= n, letting τ be the transposition such\nthat τ(n) = k and τ(k) = n, it is clear that τ ◦ π leaves n invariant, and by the induction\nhypothesis, we have τ ◦ π = τm ◦ . . . ◦ τ1 for some transpositions, and thus\n\nπ = τ ◦ τm ◦ . . . ◦ τ1,\n\na product of transpositions (since τ ◦ τ = idn).\n\n\n\n184 CHAPTER 7. DETERMINANTS\n\nRemark: When π = idn is the identity permutation, we can agree that the composition of\n0 transpositions is the identity. The second part of Proposition 7.1 shows that the transpo-\nsitions generate the group of permutations Sn.\n\nIn writing a permutation π as a composition π = σ1 ◦ . . . ◦ σs of cyclic permutations, it\nis clear that the order of the σi does not matter, since their domains are disjoint. Given\na permutation written as a product of transpositions, we now show that the parity of the\nnumber of transpositions is an invariant.\n\nDefinition 7.2. For every n ≥ 2, since every permutation π : [n] → [n] defines a partition\nof r subsets over which π acts either as the identity or as a cyclic permutation, let ε(π),\ncalled the signature of π, be defined by ε(π) = (−1)n−r, where r is the number of sets in the\npartition.\n\nIf τ is a transposition exchanging i and j, it is clear that the partition associated with\nτ consists of n − 1 equivalence classes, the set {i, j}, and the n − 2 singleton sets {k}, for\nk ∈ [n]− {i, j}, and thus, ε(τ) = (−1)n−(n−1) = (−1)1 = −1.\n\nProposition 7.2. For every n ≥ 2, for every permutation π : [n] → [n], for every transpo-\nsition τ , we have\n\nε(τ ◦ π) = −ε(π).\n\nConsequently, for every product of transpositions such that π = τm ◦ . . . ◦ τ1, we have\n\nε(π) = (−1)m,\n\nwhich shows that the parity of the number of transpositions is an invariant.\n\nProof. Assume that τ(i) = j and τ(j) = i, where i < j. There are two cases, depending\nwhether i and j are in the same equivalence class Jl of Rπ, or if they are in distinct equivalence\nclasses. If i and j are in the same class Jl, then if\n\nJl = {i1, . . . , ip, . . . iq, . . . ik},\n\nwhere ip = i and iq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = iq\n\nand\nτ(π(iq−1)) = τ(iq) = τ(j) = i = ip,\n\nit is clear that Jl splits into two subsets, one of which is {ip, . . . , iq−1}, and thus, the number\nof classes associated with τ ◦ π is r + 1, and ε(τ ◦ π) = (−1)n−r−1 = −(−1)n−r = −ε(π). If i\nand j are in distinct equivalence classes Jl and Jm, say\n\n{i1, . . . , ip, . . . ih}\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{j1, . . . , jq, . . . jk},\n\nwhere ip = i and jq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = jq\n\nand\nτ(π(π−1(jq))) = τ(jq) = τ(j) = i = ip,\n\nwe see that the classes Jl and Jm merge into a single class, and thus, the number of classes\nassociated with τ ◦ π is r − 1, and ε(τ ◦ π) = (−1)n−r+1 = −(−1)n−r = −ε(π).\n\nNow, let π = τm ◦ . . . ◦ τ1 be any product of transpositions. By the first part of the\nproposition, we have\n\nε(π) = (−1)m−1ε(τ1) = (−1)m−1(−1) = (−1)m,\n\nsince ε(τ1) = −1 for a transposition.\n\nRemark: When π = idn is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (−1)0 = ε(id) = +1. From the\nproposition, it is immediate that ε(π′ ◦ π) = ε(π′)ε(π). In particular, since π−1 ◦ π = idn, we\nget ε(π−1) = ε(π).\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over K (free modules, when bases are\nneeded).\n\nLet E1, . . . , En, and F , be vector spaces over a field K, where n ≥ 1.\n\nDefinition 7.3. A function f : E1 × . . . × En → F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every i,\n1 ≤ i ≤ n, for all x1 ∈ E1, . . ., xi−1 ∈ Ei−1, xi+1 ∈ Ei+1, . . ., xn ∈ En, for all x, y ∈ Ei, for all\nλ ∈ K,\n\nf(x1, . . . , xi−1, x+ y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)\n\n+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),\n\nf(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{i1, see Jar : Jkt,\n\nwhere 7, = 7% and j, = j, since\n\nand\n\nT(m(m~*(jq))) =T(Jq) =TU) = t= ty,\nwe see that the classes J; and J;, merge into a single class, and thus, the number of classes\nassociated with 7 om is r—1, and e(7 om) = (—1)\"\"*1 = —(-1)\"\" = —c(z).\n\nNow, let 7 = Tm 0...07, be any product of transpositions. By the first part of the\nproposition, we have\n\nsince €(7,) = —1 for a transposition. O\n\nRemark: When z = id, is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (—1)° = e(id) = +1. From the\nproposition, it is immediate that e(a’ om) = e(n’)e(z). In particular, since 77!\nget e(7!) = e(z).\n\nom =id,, we\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over A (free modules, when bases are\n\nneeded).\nLet E,,...,E,, and F, be vector spaces over a field K, where n > 1.\n\nDefinition 7.3. A function f: FE, x... x E, > F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every 1,\n1<i<n, for all x, € Fy,..., m1 © Fj-4, tin. © Figs, --, Un © En, for all x,y € K, for all\nAE K,\n\nf(@1,---, Ui, + Y, Vig, ---5 En) = f(41,-- +, Vi-1, V, Lig,---, Ln)\n+ f(a, wee Vi-1,Y, Vit1,--- En),\n\nf(“1, te ,Tj—1, AL, Vin, te Ln) = Af (£1, vee Di-1, VU, Vi41,--- En).\n\n\n\n\n186 CHAPTER 7. DETERMINANTS\n\nWhen F = K, we call f an n-linear form (or multilinear form). If n ≥ 2 and E1 =\nE2 = . . . = En, an n-linear map f : E × . . .×E → F is called symmetric, if f(x1, . . . , xn) =\nf(xπ(1), . . . , xπ(n)) for every permutation π on {1, . . . , n}. An n-linear map f : E×. . .×E → F\nis called alternating , if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤ i ≤ n − 1 (in\nother words, when two adjacent arguments are equal). It does no harm to agree that when\nn = 1, a linear map is considered to be both symmetric and alternating, and we will do so.\n\nWhen n = 2, a 2-linear map f : E1 × E2 → F is called a bilinear map. We have already\nseen several examples of bilinear maps. Multiplication · : K × K → K is a bilinear map,\ntreating K as a vector space over itself.\n\nThe operation 〈−,−〉 : E∗×E → K applying a linear form to a vector is a bilinear map.\n\nSymmetric bilinear maps (and multilinear maps) play an important role in geometry\n(inner products, quadratic forms) and in differential calculus (partial derivatives).\n\nA bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈ E.\n\nAlternating multilinear maps satisfy the following simple but crucial properties.\n\nProposition 7.3. Let f : E× . . .×E → F be an n-linear alternating map, with n ≥ 2. The\nfollowing properties hold:\n\n(1)\nf(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .)\n\n(2)\nf(. . . , xi, . . . , xj, . . .) = 0,\n\nwhere xi = xj, and 1 ≤ i < j ≤ n.\n\n(3)\nf(. . . , xi, . . . , xj, . . .) = −f(. . . , xj, . . . , xi, . . .),\n\nwhere 1 ≤ i < j ≤ n.\n\n(4)\nf(. . . , xi, . . .) = f(. . . , xi + λxj, . . .),\n\nfor any λ ∈ K, and where i 6= j.\n\nProof. (1) By multilinearity applied twice, we have\n\nf(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi, xi, . . .) + f(. . . , xi, xi+1, . . .)\n\n+ f(. . . , xi+1, xi, . . .) + f(. . . , xi+1, xi+1, . . .),\n\nand since f is alternating, this yields\n\n0 = f(. . . , xi, xi+1, . . .) + f(. . . , xi+1, xi, . . .),\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 187\n\nthat is, f(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .).\n\n(2) If xi = xj and i and j are not adjacent, we can interchange xi and xi+1, and then xi\nand xi+2, etc, until xi and xj become adjacent. By (1),\n\nf(. . . , xi, . . . , xj, . . .) = εf(. . . , xi, xj, . . .),\n\nwhere ε = +1 or −1, but f(. . . , xi, xj, . . .) = 0, since xi = xj, and (2) holds.\n\n(3) follows from (2) as in (1). (4) is an immediate consequence of (2).\n\nProposition 7.3 will now be used to show a fundamental property of alternating multilin-\near maps. First we need to extend the matrix notation a little bit. Let E be a vector space\nover K. Given an n× n matrix A = (ai j) over K, we can define a map L(A) : En → En as\nfollows:\n\nL(A)1(u) = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nL(A)n(u) = an 1u1 + · · ·+ annun,\n\nfor all u1, . . . , un ∈ E and with u = (u1, . . . , un). It is immediately verified that L(A) is\nlinear. Then given two n×n matrices A = (ai j) and B = (bi j), by repeating the calculations\nestablishing the product of matrices (just before Definition 3.12), we can show that\n\nL(AB) = L(A) ◦ L(B).\n\nIt is then convenient to use the matrix notation to describe the effect of the linear map L(A),\nas \n\nL(A)1(u)\nL(A)2(u)\n\n...\nL(A)n(u)\n\n =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nu1\n\nu2\n...\nun\n\n .\n\nLemma 7.4. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un) and\n(v1, . . . , vn) be two families of n vectors, such that,\n\nv1 = a1 1u1 + · · ·+ an 1un,\n\n. . .\n\nvn = a1nu1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A>\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nf(u1, . . . , un),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nProof. Expanding f(v1, . . . , vn) by multilinearity, we get a sum of terms of the form\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)),\n\nfor all possible functions π : {1, . . . , n} → {1, . . . , n}. However, because f is alternating, only\nthe terms for which π is a permutation are nonzero. By Proposition 7.1, every permutation\nπ is a product of transpositions, and by Proposition 7.2, the parity ε(π) of the number of\ntranspositions only depends on π. Then applying Proposition 7.3 (3) to each transposition\nin π, we get\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)) = ε(π)aπ(1) 1 · · · aπ(n)nf(u1, . . . , un).\n\nThus, we get the expression of the lemma.\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, v2) = f(a11u1 + a21u2, a12u1 + a22u2)\n\n= f(a11u1 + a21u2, a12u1) + f(a11u1 + a21u2, a22u2)\n\n= f(a11u1, a12u1) + f(a21u2, a12u1) + f(a11ua, a22u2) + f(a21u2, a22u2)\n\n= a11a12f(u1, u1) + a21a12f(u2, u1) + a11a22f(u1, u2) + a21a22f(u2, u2)\n\n= a21a12f(u2, u1)a11a22f(u1, u2)\n\n= (a11a22 − a12a22) f(u1, u2).\n\nHopefully the reader will recognize the quantity a11a22− a12a22. It is the determinant of the\n2× 2 matrix\n\nA =\n\n(\na11 a12\n\na21 a22\n\n)\n.\n\nThis is no accident. The quantity\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have\n\nUy U1\nU2 _ At U2\nUn Un\nThen,\nf(ur, te ,Un) = ( S- €(T)an(1)1 oe Arin)n) f(t, te , Un),\nTEGn\nwhere the sum ranges over all permutations 7 on {1,...,n}.\nProof. Expanding f(v1,...,Un) by multilinearity, we get a sum of terms of the form\nAn(1)1*** On(n) nd (Un(1)s+++5Um(n))s\nfor all possible functions 7: {1,...,n}— {1,...,n}. However, because f is alternating, only\n\nthe terms for which 7 is a permutation are nonzero. By Proposition 7.1, every permutation\nm is a product of transpositions, and by Proposition 7.2, the parity e(7) of the number of\ntranspositions only depends on 7. Then applying Proposition 7.3 (3) to each transposition\nin 77, we get\n\nAn(1)1°** An(n) nd (Un(1)>+ «+5 Um(n)) = E(T)An(1) 1° ** Ar(n) nd (Ua, -.. 5 Un):\nThus, we get the expression of the lemma. im\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, 02) = f\n=f\nf\n\nay U + Gg1U2, A121 + A22QU2)\nQ41Uy + Ag U2, A12U1) + f (Qiu + Goi U2, do2U2)\n\nQy1U1, A121) + f(aeit2, di2t1) + f(a11ta, d22U2) + f(ao1U2, d22U2)\n\n—_~~ ~~\n\n= dz G12 f (U2, U1) A114 f (ur, U2)\n\n= (a4 122 _— 12422) f (ui, Ug).\n\nHopefully the reader will recognize the quantity a11d@92 — @12d22. It is the determinant of the\n2 xX 2 matrix\nA- ai1 G12\na21 422\nThis is no accident. The quantity\n\ndet(A) = S° €(T)Ax(1)1°** Ax(n)n\n\nTEGH\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 189\n\nis in fact the value of the determinant of A (which, as we shall see shortly, is also equal to the\ndeterminant of A>). However, working directly with the above definition is quite awkward,\nand we will proceed via a slightly indirect route\n\nRemark: The reader might have been puzzled by the fact that it is the transpose matrix\nA> rather than A itself that appears in Lemma 7.4. The reason is that if we want the generic\nterm in the determinant to be\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the permutation applies to the first index, then we have to express the vjs in terms\nof the uis in terms of A> as we did. Furthermore, since\n\nvj = a1 ju1 + · · ·+ ai jui + · · ·+ an jun,\n\nwe see that vj corresponds to the jth column of the matrix A, and so the determinant is\nviewed as a function of the columns of A.\n\nThe literature is split on this point. Some authors prefer to define a determinant as we\ndid. Others use A itself, which amounts to viewing det as a function of the rows, in which\ncase we get the expression ∑\n\nσ∈Sn\nε(σ)a1σ(1) · · · anσ(n).\n\nCorollary 7.7 show that these two expressions are equal, so it doesn’t matter which is chosen.\nThis is a matter of taste.\n\n7.3 Definition of a Determinant\n\nRecall that the set of all square n × n-matrices with coefficients in a field K is denoted by\nMn(K).\n\nDefinition 7.4. A determinant is defined as any map\n\nD : Mn(K)→ K,\n\nwhich, when viewed as a map on (Kn)n, i.e., a map of the n columns of a matrix, is n-linear\nalternating and such that D(In) = 1 for the identity matrix In. Equivalently, we can consider\na vector space E of dimension n, some fixed basis (e1, . . . , en), and define\n\nD : En → K\n\nas an n-linear alternating map such that D(e1, . . . , en) = 1.\n\n\n\n190 CHAPTER 7. DETERMINANTS\n\nFirst we will show that such maps D exist, using an inductive definition that also gives\na recursive method for computing determinants. Actually, we will define a family (Dn)n≥1\n\nof (finite) sets of maps D : Mn(K)→ K. Second we will show that determinants are in fact\nuniquely defined, that is, we will show that each Dn consists of a single map. This will show\nthe equivalence of the direct definition det(A) of Lemma 7.4 with the inductive definition\nD(A). Finally, we will prove some basic properties of determinants, using the uniqueness\ntheorem.\n\nGiven a matrix A ∈ Mn(K), we denote its n columns by A1, . . . , An. In order to describe\nthe recursive process to define a determinant we need the notion of a minor.\n\nDefinition 7.5. Given any n×n matrix with n ≥ 2, for any two indices i, j with 1 ≤ i, j ≤ n,\nlet Aij be the (n − 1) × (n − 1) matrix obtained by deleting Row i and Column j from A\nand called a minor :\n\nAij =\n\n\n\n×\n×\n\n× × × × × × ×\n×\n×\n×\n×\n\n\n.\n\nFor example, if\n\nA =\n\n\n2 −1 0 0 0\n−1 2 −1 0 0\n0 −1 2 −1 0\n0 0 −1 2 −1\n0 0 0 −1 2\n\n\nthen\n\nA2 3 =\n\n\n2 −1 0 0\n0 −1 −1 0\n0 0 2 −1\n0 0 −1 2\n\n .\n\nDefinition 7.6. For every n ≥ 1, we define a finite set Dn of maps D : Mn(K) → K\ninductively as follows:\n\nWhen n = 1, D1 consists of the single map D such that, D(A) = a, where A = (a), with\na ∈ K.\n\nAssume that Dn−1 has been defined, where n ≥ 2. Then Dn consists of all the maps D\nsuch that, for some i, 1 ≤ i ≤ n,\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+nai nD(Ai n),\n\nwhere for every j, 1 ≤ j ≤ n, D(Ai j) is the result of applying any D in Dn−1 to the minor\nAi j.\n\n\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n� We confess that the use of the same letter D for the member of Dn being defined, and\nfor members of Dn−1, may be slightly confusing. We considered using subscripts to\n\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each Dn contains just one map.\n\nEach (−1)i+jD(Ai j) is called the cofactor of ai j, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row . Given a matrix A ∈ Mn(K),\neach D(A) is called a determinant of A.\n\nWe can think of each member of Dn as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nDn consists of a single map). Assuming this fact, given a n× n-matrix A = (ai j),\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\ndet(A) =\n\n∣∣∣∣∣∣∣∣∣\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n∣∣∣∣∣∣∣∣∣ .\n\nLet us first consider some examples.\n\nExample 7.1.\n\n1. When n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen by expanding according to any row, we have\n\nD(A) = ad− bc.\n\n2. When n = 3, if\n\nA =\n\na1 1 a1 2 a1 3\n\na2 1 a2 2 a2 3\n\na3 1 a3 2 a3 3\n\n ,\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n© We confess that the use of the same letter D for the member of D,, being defined, and\nfor members of D,_1, may be slightly confusing. We considered using subscripts to\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each D, contains just one map.\n\nEach (—1)'*? D(A;;) is called the cofactor of a;;, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row. Given a matrix A € M,(KkK),\neach D(A) is called a determinant of A.\n\nWe can think of each member of D,, as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nD,, consists of a single map). Assuming this fact, given a n x n-matrix A = (a;,;),\n\nQi, Qay2q ..-- Atn\n\na2, G22 ... Aan\nA= . . ;\n\nQn1 GQn2 ---» Ann\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\na1, %aaq .-- Ain\n\na2, G22 ... QAan\ndet(A) =] . j\n\nQn1 Qn2 +--+. Ann\n\nLet us first consider some examples.\nExample 7.1.\n1. When n = 2, if\nA= (: ‘) .\n\nthen by expanding according to any row, we have\nD(A) = ad — be.\n\n2. When n = 3, if\nQ11 G12 13\nA= |do1 G2 93\n\n431 432 433\n\n\n\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\nD(A) = a1 1\n\n∣∣∣∣a2 2 a2 3\n\na3 2 a3 3\n\n∣∣∣∣− a1 2\n\n∣∣∣∣a2 1 a2 3\n\na3 1 a3 3\n\n∣∣∣∣+ a1 3\n\n∣∣∣∣a2 1 a2 2\n\na3 1 a3 2\n\n∣∣∣∣ ,\nthat is,\n\nD(A) = a1 1(a2 2a3 3 − a3 2a2 3)− a1 2(a2 1a3 3 − a3 1a2 3) + a1 3(a2 1a3 2 − a3 1a2 2),\n\nwhich gives the explicit formula\n\nD(A) = a1 1a2 2a3 3 + a2 1a3 2a1 3 + a3 1a1 2a2 3 − a1 1a3 2a2 3 − a2 1a1 2a3 3 − a3 1a2 2a1 3.\n\nWe now show that each D ∈ Dn is a determinant (map).\n\nLemma 7.5. For every n ≥ 1, for every D ∈ Dn as defined in Definition 7.6, D is an\nalternating multilinear map such that D(In) = 1.\n\nProof. By induction on n, it is obvious that D(In) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+jai jD(Ai j) + · · ·+ (−1)i+nai nD(Ai n),\n\nif j 6= k, then by induction, D(Ai j) is linear in Column k, and ai j does not belong to Column\nk, so (−1)i+jai jD(Ai j) is linear in Column k. If j = k, then D(Ai j) does not depend on\nColumn k = j, since Ai j is obtained from A by deleting Row i and Column j = k, and ai j\nbelongs to Column j = k. Thus, (−1)i+jai jD(Ai j) is linear in Column k. Consequently, in\nall cases, (−1)i+jai jD(Ai j) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say Ak = Ak+1. Assume that j 6= k and j 6= k + 1. Then the matrix Ai j has two\nidentical adjacent columns, and by the induction hypothesis, D(Ai j) = 0. The remaining\nterms of D(A) are\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1).\n\nHowever, the two matrices Ai k and Ai k+1 are equal, since we are assuming that Columns k\nand k + 1 of A are identical and Ai k is obtained from A by deleting Row i and Column k\nwhile Ai k+1 is obtained from A by deleting Row i and Column k+ 1. Similarly, ai k = ai k+1,\nsince Columns k and k + 1 of A are equal. But then,\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1) = (−1)i+kai kD(Ai k)− (−1)i+kai kD(Ai k) = 0.\n\nThis shows that D is alternating and completes the proof.\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\na21 422\na31 432\n\na21 423\n\n2 + a13\n431 433\n\nv]\n\nthat is,\nD(A) = 11 (22033 _ 32023) _ 1 2(a21433 _ 31423) + 1 3(G21432 _ 31422),\nwhich gives the explicit formula\n\nD(A) = 011422433 + 421432013 + 431412423 — 411032423 — 421012433 — 4314220)3.\n\nWe now show that each D € D,, is a determinant (map).\n\nLemma 7.5. For every n > 1, for every D € Dy, as defined in Definition 7.6, D is an\nalternating multilinear map such that D(I,) = 1.\n\nProof. By induction on n, it is obvious that D(/,) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (-1)'*\"a;1D(Ajit) +--+ + (-1)' aj D(Aij) + + (-1)GinD(Ain),\n\nif 7 # k, then by induction, D(A;,) is linear in Column k, and a;; does not belong to Column\nk, so (—1)'a,;;D(A;,;) is linear in Column k. If 7 = k, then D(A;;) does not depend on\nColumn k = j, since A;; is obtained from A by deleting Row 7 and Column j = k, and a; ;\nbelongs to Column j = k. Thus, (—1)'’a;;D(A;;) is linear in Column k. Consequently, in\nall cases, (—1)'*7a;;D(A;,;) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say A* = A**t!, Assume that 7 # k and j 4 k+1. Then the matrix A;; has two\nidentical adjacent columns, and by the induction hypothesis, D(A;;) = 0. The remaining\nterms of D(A) are\n\n(—1)'*ajxD(Ain) + (1) ain D(Ainyt)-\n\nHowever, the two matrices A;, and A;,4; are equal, since we are assuming that Columns k\nand k +1 of A are identical and A;; is obtained from A by deleting Row i and Column k\nwhile A;;41 is obtained from A by deleting Row i and Column k +1. Similarly, aj, = ajx4i,\nsince Columns k and k + 1 of A are equal. But then,\n\n(—1)*8a;4D(Aig) + (—1) ai ng (Aina) = (-1)' aie D(Ain) — (-1) \"ain D(Aix) = 0.\nThis shows that D is alternating and completes the proof. im\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 193\n\nTheorem 7.6. For every n ≥ 1, for every D ∈ Dn, for every matrix A ∈ Mn(K), we have\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. As a consequence, Dn consists\nof a single map for every n ≥ 1, and this map is given by the above explicit formula.\n\nProof. Consider the standard basis (e1, . . . , en) of Kn, where (ei)i = 1 and (ei)j = 0, for\nj 6= i. Then each column Aj of A corresponds to a vector vj whose coordinates over the\nbasis (e1, . . . , en) are the components of Aj, that is, we can write\n\nv1 = a1 1e1 + · · ·+ an 1en,\n\n. . .\n\nvn = a1ne1 + · · ·+ annen.\n\nSince by Lemma 7.5, each D is a multilinear alternating map, by applying Lemma 7.4, we\nget\n\nD(A) = D(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nD(e1, . . . , en),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. But D(e1, . . . , en) = D(In),\nand by Lemma 7.5, we have D(In) = 1. Thus,\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nFrom now on we will favor the notation det(A) over D(A) for the determinant of a square\nmatrix.\n\nRemark: There is a geometric interpretation of determinants which we find quite illumi-\nnating. Given n linearly independent vectors (u1, . . . , un) in Rn, the set\n\nPn = {λ1u1 + · · ·+ λnun | 0 ≤ λi ≤ 1, 1 ≤ i ≤ n}\n\nis called a parallelotope. If n = 2, then P2 is a parallelogram and if n = 3, then P3 is a\nparallelepiped , a skew box having u1, u2, u3 as three of its corner sides. See Figures 7.1 and\n7.2.\n\nThen it turns out that det(u1, . . . , un) is the signed volume of the parallelotope Pn (where\nvolume means n-dimensional volume). The sign of this volume accounts for the orientation\nof Pn in Rn.\n\nWe can now prove some properties of determinants.\n\n\n\n194 CHAPTER 7. DETERMINANTS\n\nu = (1,0)1\n\nu = (1,1)\n2\n\nFigure 7.1: The parallelogram in Rw spanned by the vectors u1 = (1, 0) and u2 = (1, 1).\n\nCorollary 7.7. For every matrix A ∈ Mn(K), we have det(A) = det(A>).\n\nProof. By Theorem 7.6, we have\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. Since a permutation is invertible,\nevery product\n\naπ(1) 1 · · · aπ(n)n\n\ncan be rewritten as\na1π−1(1) · · · anπ−1(n),\n\nand since ε(π−1) = ε(π) and the sum is taken over all permutations on {1, . . . , n}, we have∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n),\n\nwhere π and σ range over all permutations. But it is immediately verified that\n\ndet(A>) =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n).\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n194 CHAPTER 7. DETERMINANTS\n\n0.8 4\n0.64\n0.44\n\n0.24\n\n1\nu= (1,0)\n\nFigure 7.1: The parallelogram in R” spanned by the vectors u; = (1,0) and uz = (1,1).\n\nCorollary 7.7. For every matriz A € M,,(K), we have det(A) = det(A‘).\n\nProof. By Theorem 7.6, we have\n\ndet (A) = S- €(17)ax(1) 1° °° An(n)ns\n\nTEGn\n\nwhere the sum ranges over all permutations 7 on {1,...,}. Since a permutation is invertible,\nevery product\n\nAn(1)1°°* Ax(n)n\ncan be rewritten as\n\nA1q-1(1) °° Anal (n)s\n\nand since e(7~!) = e(7) and the sum is taken over all permutations on {1,...,n}, we have\nS- €(1)Ax(1) 1° °° Ag(n)n = S- €(o)ay o(1) °°\" Gno(n)>\nTEGn aEGn\n\nwhere 7 and o range over all permutations. But it is immediately verified that\n\ndet(A') = S- €(7)Q1 (1) *** An o(n): d\n\naEGn\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 195\n\nu = (1,1,0)\n1\n\nu = (0,1,0)\n2\n\nu = (1,1,1)\n3\n\nFigure 7.2: The parallelepiped in R3 spanned by the vectors u1 = (1, 1, 0), u2 = (0, 1, 0), and\nu3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\nx1 x2 . . . xn\nx2\n\n1 x2\n2 . . . x2\n\nn\n...\n\n...\n. . .\n\n...\nxn−1\n\n1 xn−1\n2 . . . xn−1\n\nn\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nWe claim that\n\nV (x1, . . . , xn) =\n∏\n\n1≤i<j≤n\n(xj − xi),\n\nwith V (x1, . . . , xn) = 1, when n = 1. We prove it by induction on n ≥ 1. The case n = 1 is\nobvious. Assume n ≥ 2. We proceed as follows: multiply Row n − 1 by x1 and subtract it\nfrom Row n (the last row), then multiply Row n− 2 by x1 and subtract it from Row n− 1,\netc, multiply Row i− 1 by x1 and subtract it from row i, until we reach Row 1. We obtain\n\n7.3. DEFINITION OF A DETERMINANT\n\n14\n084\nv4\n044\n\n0.25\n\n195\n\nFigure 7.2: The parallelepiped in R® spanned by the vectors u, = (1, 1,0), we = (0,1,0), and\n\nU3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\n1 1 1\n\nLy 2 In\n\n2 2 2\n\nV(a1,...,%n) =] %1  % Ly,\ncp! at grt\n\nWe claim that\n\nV(a1,.--,2%n) = II (x; — 7),\n\n1<i<j<n\n\nwith V(a1,...,%) = 1, when n = 1. We prove it by induction on n > 1. The case n = 1 is\nobvious. Assume n > 2. We proceed as follows: multiply Row n — 1 by x, and subtract it\nfrom Row n (the last row), then multiply Row n — 2 by x; and subtract it from Row n— 1,\netc, multiply Row 7 — 1 by x, and subtract it from row 7, until we reach Row 1. We obtain\n\n\n\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\n0 x2 − x1 . . . xn − x1\n\n0 x2(x2 − x1) . . . xn(xn − x1)\n...\n\n...\n. . .\n\n...\n0 xn−2\n\n2 (x2 − x1) . . . xn−2\nn (xn − x1)\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (xi − x1) from the column of index i − 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV (x1, . . . , xn) = (x2 − x1)(x3 − x1) · · · (xn − x1)V (x2, . . . , xn),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un)\nand (v1, . . . , vn) be two families of n vectors, such that\n\nv1 = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nvn = an 1u1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) = det(A)f(u1, . . . , un).\n\nProof. The only difference with Lemma 7.4 is that here we are using A> instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result.\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\n1 1 a 1\n\n0 L— Ly a In — Ly\nV(a1,---,2n) = 0 %2(%2—- 21)... En (4% — 11)\n\n0 af 7(ag-91) ... 2\"? (a, — 21)\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (7; — x1) from the column of index i — 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV(a1,.--,%n) = (\"2 — ©1)(43 — 1) +++ (pn — 21)V (H0,...,Ln),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f: Ex...x E— F be an n-linear alternating map. Let (u1,..., Un)\nand (U1,...,Un) be two families of n vectors, such that\n\nVy = 441Uy +--+ + A nUn,\n\nUn = An{Uy + +++ + AnnUn-\n\nEquivalently, letting\n\na11 G12 «+. An\nA= a21 (122 -.. Qan .\nQn1 Gn2 Ann\nassume that we have\nU1 U1\nV2 _A U2\nUn, Un\n\nThen,\nf(v1,-.-,Un) = det(A)f (ur, ...,Un)-\n\nProof. The only difference with Lemma 7.4 is that here we are using A' instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result. im\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 197\n\nAs a consequence, we get the very useful property that the determinant of a product of\nmatrices is the product of the determinants of these matrices.\n\nProposition 7.9. For any two n×n-matrices A and B, we have det(AB) = det(A) det(B).\n\nProof. We use Proposition 7.8 as follows: let (e1, . . . , en) be the standard basis of Kn, and\nlet \n\nw1\n\nw2\n...\nwn\n\n = AB\n\n\ne1\n\ne2\n...\nen\n\n .\n\nThen we get\n\ndet(w1, . . . , wn) = det(AB) det(e1, . . . , en) = det(AB),\n\nsince det(e1, . . . , en) = 1. Now letting\nv1\n\nv2\n...\nvn\n\n = B\n\n\ne1\n\ne2\n...\nen\n\n ,\n\nwe get\n\ndet(v1, . . . , vn) = det(B),\n\nand since \nw1\n\nw2\n...\nwn\n\n = A\n\n\nv1\n\nv2\n...\nvn\n\n ,\n\nwe get\n\ndet(w1, . . . , wn) = det(A) det(v1, . . . , vn) = det(A) det(B).\n\nIt should be noted that all the results of this section, up to now, also hold when K is a\ncommutative ring and not necessarily a field. We can now characterize when an n×n-matrix\nA is invertible in terms of its determinant det(A).\n\n7.4 Inverse Matrices and Determinants\n\nIn the next two sections, K is a commutative ring and when needed a field.\n\n\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A ∈ Mn(K), let Ã = (bi j)\nbe the matrix defined such that\n\nbi j = (−1)i+j det(Aj i),\n\nthe cofactor of aj i. The matrix Ã is called the adjugate of A, and each matrix Aj i is called\na minor of the matrix A.\n\nFor example, if\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n ,\n\nwe have\n\nb11 = det(A11) =\n\n∣∣∣∣ −2 −2\n3 −3\n\n∣∣∣∣ = 12 b12 = − det(A21) = −\n∣∣∣∣ 1 1\n\n3 −3\n\n∣∣∣∣ = 6\n\nb13 = det(A31) =\n\n∣∣∣∣ 1 1\n−2 −2\n\n∣∣∣∣ = 0 b21 = − det(A12) = −\n∣∣∣∣ 2 −2\n\n3 −3\n\n∣∣∣∣ = 0\n\nb22 = det(A22) =\n\n∣∣∣∣ 1 1\n3 −3\n\n∣∣∣∣ = −6 b23 = − det(A32) = −\n∣∣∣∣ 1 1\n\n2 −2\n\n∣∣∣∣ = 4\n\nb31 = det(A13) =\n\n∣∣∣∣ 2 −2\n3 3\n\n∣∣∣∣ = 12 b32 = − det(A23) = −\n∣∣∣∣ 1 1\n\n3 3\n\n∣∣∣∣ = 0\n\nb33 = det(A33) =\n\n∣∣∣∣ 1 1\n2 −2\n\n∣∣∣∣ = −4,\n\nwe find that\n\nÃ =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n .\n\n� Note the reversal of the indices in\n\nbi j = (−1)i+j det(Aj i).\n\nThus, Ã is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A ∈ Mn(K), we have\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, A−1 = (det(A))−1Ã.\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A € M,(K), let A = (0; ;)\nbe the matrix defined such that\n\nbij = (-1)'™ det(A;,),\n\nthe cofactor of a;;. The matrix A is called the adjugate of A, and each matrix A,; is called\na minor of the matrix A.\n\nFor example, if\n\n1 1 1\nA= |{2 -—2 -2],\n3 3 -3\nwe have\n2 ~9 1 1\nby, = det(Ay1) = 3 3 | = 12 byg = — det (Agi) = — | 3 3 | = 6\n1 1 2 —2\nbi3 = det(A31) = 2 _2 | =0 by = —det(Aiz) = — | 3-3 | =O\n1 1 1 1\nboo = det (A22) = 3 3 =—6 bos = — det (Az) =~ | 2 —2 | = 4\n2 —2 1 1\n531 = det (Aj3) = 3 3 = 12 b32 = — det (Ags) =~ | 3 3 | =0\n1 1\n633 = det (A33) = a) = —4,\nwe find that\n7 12 6 O\nA=|0 -6 4\n12 0 —4\n\n© Note the reversal of the indices in\nbi = (-1)'\" det (Aj;;).\nThus, A is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A € M,,(K), we have\nAA = AA = det(A)Iy.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, AT! = (det(A))~1A.\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If Ã = (bi j) and AÃ = (ci j), we know that the entry ci j in row i and column j of AÃ\nis\n\nci j = ai 1b1 j + · · ·+ ai kbk j + · · ·+ ai nbn j,\n\nwhich is equal to\n\nai 1(−1)j+1 det(Aj 1) + · · ·+ ai n(−1)j+n det(Aj n).\n\nIf j = i, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\n\nci i = det(A) = ai 1(−1)i+1 det(Ai 1) + · · ·+ ai n(−1)i+n det(Ai n).\n\nIf j 6= i, we can form the matrix A′ by replacing the j-th row of A by the i-th row of A.\nNow the matrix Aj k obtained by deleting row j and column k from A is equal to the matrix\nA′j k obtained by deleting row j and column k from A′, since A and A′ only differ by the j-th\nrow. Thus,\n\ndet(Aj k) = det(A′j k),\n\nand we have\n\nci j = ai 1(−1)j+1 det(A′j 1) + · · ·+ ai n(−1)j+n det(A′j n).\n\nHowever, this is the expansion of det(A′) according to the j-th row, since the j-th row of A′\n\nis equal to the i-th row of A. Furthermore, since A′ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A′) = 0. Thus,\nwe have shown that ci i = det(A), and ci j = 0, when j 6= i, and so\n\nAÃ = det(A)In.\n\nIt is also obvious from the definition of Ã, that\n\nÃ> = Ã>.\n\nThen applying the first part of the argument to A>, we have\n\nA>Ã> = det(A>)In,\n\nand since det(A>) = det(A), Ã> = Ã>, and (ÃA)> = A>Ã>, we get\n\ndet(A)In = A>Ã> = A>Ã> = (ÃA)>,\n\nthat is,\n\n(ÃA)> = det(A)In,\n\nwhich yields\n\nÃA = det(A)In,\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If A= (b;;) and AA= (c;;), we know that the entry c¢;; in row 7 and column j of AA\nis\nCig = Aii1d1g + +++ + Gindeg + +++ + Ginbns,\n\nwhich is equal to\naj1(—1)7*\" det(A; 1) feee Hb in(—1)7*\" det(A;,,).\n\nIf 7 = 7, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\nCi = det (A) = aj1(—1)\"*! det (A;1) fteeet din(—1)'*” det (A; »).\n\nIf 7 4 7%, we can form the matrix A’ by replacing the j-th row of A by the i-th row of A.\nNow the matrix A;;, obtained by deleting row 7 and column k from A is equal to the matrix\nA’,,, obtained by deleting row j and column k from A’, since A and A’ only differ by the j-th\nrow. Thus,\n\ndet(Aj,) = det(Aj,),\n\nand we have\nCijy = ay1(—1)'** det( Aj) +--+ + ain(-1)7*™ det(4j,,,).\n\nHowever, this is the expansion of det(A’) according to the j-th row, since the j-th row of A’\nis equal to the i-th row of A. Furthermore, since A’ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A’) = 0. Thus,\nwe have shown that c;; = det(A), and c;; = 0, when j 47, and so\nAA = det(A) In.\nIt is also obvious from the definition of A, that\nwaa\nThen applying the first part of the argument to A', we have\nA'AT = det(A')In,\nand since det(A‘) = det(A), AY = AT, and (AA)\" = A™AT, we get\ndet(A)I, = AA’ = ATA! = (AA)’,\n\nthat is, 7\n(AA)! = det(A)In,\n\nwhich yields 7\nAA = det(A)In,\n\n\n\n\n200 CHAPTER 7. DETERMINANTS\n\nsince I>n = In. This proves that\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, if det(A) is invertible, we have A−1 = (det(A))−1Ã. Conversely, if A is\ninvertible, from AA−1 = In, by Proposition 7.9, we have det(A) det(A−1) = 1, and det(A) is\ninvertible.\n\nFor example, we saw earlier that\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n and Ã =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n ,\n\nand we have 1 1 1\n2 −2 −2\n3 3 −3\n\n12 6 0\n0 −6 4\n12 0 −4\n\n = 24\n\n1 0 0\n0 1 0\n0 0 1\n\n\nwith det(A) = 24.\n\nWhen K is a field, an element a ∈ K is invertible iff a 6= 0. In this case, the second part\nof the proposition can be stated as A is invertible iff det(A) 6= 0. Note in passing that this\nmethod of computing the inverse of a matrix is usually not practical.\n\n7.5 Systems of Linear Equations and Determinants\n\nWe now consider some applications of determinants to linear independence and to solving\nsystems of linear equations. Although these results hold for matrices over certain rings, their\nproofs require more sophisticated methods. Therefore, we assume again that K is a field\n(usually, K = R or K = C).\n\nLet A be an n×n-matrix, x a column vectors of variables, and b another column vector,\nand let A1, . . . , An denote the columns of A. Observe that the system of equations Ax = b,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nx1\n\nx2\n...\nxn\n\n =\n\n\nb1\n\nb2\n...\nbn\n\n\nis equivalent to\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = b,\n\nsince the equation corresponding to the i-th row is in both cases\n\nai 1x1 + · · ·+ ai jxj + · · ·+ ai nxn = bi.\n\nFirst we characterize linear independence of the column vectors of a matrix A in terms\nof its determinant.\n\n\n\n7.5. SYSTEMS OF LINEAR EQUATIONS AND DETERMINANTS 201\n\nProposition 7.11. Given an n × n-matrix A over a field K, the columns A1, . . . , An of\nA are linearly dependent iff det(A) = det(A1, . . . , An) = 0. Equivalently, A has rank n iff\ndet(A) 6= 0.\n\nProof. First assume that the columns A1, . . . , An of A are linearly dependent. Then there\nare x1, . . . , xn ∈ K, such that\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = 0,\n\nwhere xj 6= 0 for some j. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , 0, . . . , An) = 0,\n\nwhere 0 occurs in the j-th position. By multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = xj det(A1, . . . , An) = 0.\n\nSince xj 6= 0 and K is a field, we must have det(A1, . . . , An) = 0.\n\nConversely, we show that if the columns A1, . . . , An of A are linearly independent, then\ndet(A1, . . . , An) 6= 0. If the columns A1, . . . , An of A are linearly independent, then they\nform a basis of Kn, and we can express the standard basis (e1, . . . , en) of Kn in terms of\nA1, . . . , An. Thus, we have\n\ne1\n\ne2\n...\nen\n\n =\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bnn\n\n\n\nA1\n\nA2\n\n...\nAn\n\n ,\n\nfor some matrix B = (bi j), and by Proposition 7.8, we get\n\ndet(e1, . . . , en) = det(B) det(A1, . . . , An),\n\nand since det(e1, . . . , en) = 1, this implies that det(A1, . . . , An) 6= 0 (and det(B) 6= 0). For\nthe second assertion, recall that the rank of a matrix is equal to the maximum number of\nlinearly independent columns, and the conclusion is clear.\n\nWe now characterize when a system of linear equations of the form Ax = b has a unique\nsolution.\n\nProposition 7.12. Given an n× n-matrix A over a field K, the following properties hold:\n\n(1) For every column vector b, there is a unique column vector x such that Ax = b iff the\nonly solution to Ax = 0 is the trivial vector x = 0, iff det(A) 6= 0.\n\n\n\n202 CHAPTER 7. DETERMINANTS\n\n(2) If det(A) 6= 0, the unique solution of Ax = b is given by the expressions\n\nxj =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n,\n\nknown as Cramer’s rules.\n\n(3) The system of linear equations Ax = 0 has a nonzero solution iff det(A) = 0.\n\nProof. (1) Assume that Ax = b has a single solution x0, and assume that Ay = 0 with y 6= 0.\nThen,\n\nA(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,\n\nand x0 + y 6= x0 is another solution of Ax = b, contradicting the hypothesis that Ax = b has\na single solution x0. Thus, Ax = 0 only has the trivial solution. Now assume that Ax = 0\nonly has the trivial solution. This means that the columns A1, . . . , An of A are linearly\nindependent, and by Proposition 7.11, we have det(A) 6= 0. Finally, if det(A) 6= 0, by\nProposition 7.10, this means that A is invertible, and then for every b, Ax = b is equivalent\nto x = A−1b, which shows that Ax = b has a single solution.\n\n(2) Assume that Ax = b. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , b, . . . , An),\n\nwhere b occurs in the j-th position, by multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\nxj det(A1, . . . , An) = det(A1, . . . , Aj−1, b, Aj+1, . . . , An),\n\nfor every j, 1 ≤ j ≤ n. Since we assumed that det(A) = det(A1, . . . , An) 6= 0, we get the\ndesired expression.\n\n(3) Note that Ax = 0 has a nonzero solution iff A1, . . . , An are linearly dependent (as\nobserved in the proof of Proposition 7.11), which, by Proposition 7.11, is equivalent to\ndet(A) = 0.\n\nAs pleasing as Cramer’s rules are, it is usually impractical to solve systems of linear\nequations using the above expressions. However, these formula imply an interesting fact,\nwhich is that the solution of the system Ax = b are continuous in A and b. If we assume that\nthe entries in A are continuous functions aij(t) and the entries in b are are also continuous\nfunctions bj(t) of a real parameter t, since determinants are polynomial functions of their\nentries, the expressions\n\nxj(t) =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n\nare ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero.\nSimilarly, if the functions aij(t) and bj(t) are differentiable, so are the xj(t).\n\n\n\n7.6. DETERMINANT OF A LINEAR MAP 203\n\n7.6 Determinant of a Linear Map\n\nGiven a vector space E of finite dimension n, given a basis (u1, . . . , un) of E, for every linear\nmap f : E → E, if M(f) is the matrix of f w.r.t. the basis (u1, . . . , un), we can define\ndet(f) = det(M(f)). If (v1, . . . , vn) is any other basis of E, and if P is the change of basis\nmatrix, by Corollary 4.5, the matrix of f with respect to the basis (v1, . . . , vn) is P−1M(f)P .\nBy Proposition 7.9, we have\n\ndet(P−1M(f)P ) = det(P−1) det(M(f)) det(P ) = det(P−1) det(P ) det(M(f)) = det(M(f)).\n\nThus, det(f) is indeed independent of the basis of E.\n\nDefinition 7.8. Given a vector space E of finite dimension, for any linear map f : E → E,\nwe define the determinant det(f) of f as the determinant det(M(f)) of the matrix of f in\nany basis (since, from the discussion just before this definition, this determinant does not\ndepend on the basis).\n\nThen we have the following proposition.\n\nProposition 7.13. Given any vector space E of finite dimension n, a linear map f : E → E\nis invertible iff det(f) 6= 0.\n\nProof. The linear map f : E → E is invertible iff its matrix M(f) in any basis is invertible\n(by Proposition 4.2), iff det(M(f)) 6= 0, by Proposition 7.10.\n\nGiven a vector space of finite dimension n, it is easily seen that the set of bijective linear\nmaps f : E → E such that det(f) = 1 is a group under composition. This group is a\nsubgroup of the general linear group GL(E). It is called the special linear group (of E), and\nit is denoted by SL(E), or when E = Kn, by SL(n,K), or even by SL(n).\n\n7.7 The Cayley–Hamilton Theorem\n\nWe next discuss an interesting and important application of Proposition 7.10, the Cayley–\nHamilton theorem. The results of this section apply to matrices over any commutative ring\nK. First we need the concept of the characteristic polynomial of a matrix.\n\nDefinition 7.9. If K is any commutative ring, for every n × n matrix A ∈ Mn(K), the\ncharacteristic polynomial PA(X) of A is the determinant\n\nPA(X) = det(XI − A).\n\n\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial PA(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen\n\nPA(X) =\n\n∣∣∣∣X − a −b\n−c X − d\n\n∣∣∣∣ = X2 − (a+ d)X + ad− bc.\n\nWe can substitute the matrix A for the variable X in the polynomial PA(X), obtaining a\nmatrix PA. If we write\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn,\n\nthen\nPA = An + c1A\n\nn−1 + · · ·+ cnI.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley–Hamilton) If K is any commutative ring, for every n× n matrix\nA ∈ Mn(K), if we let\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nbe the characteristic polynomial of A, then\n\nPA = An + c1A\nn−1 + · · ·+ cnI = 0.\n\nProof. We can view the matrix B = XI −A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B̃ which is the transpose of the matrix of\n\ncofactors of elements of B. Each entry in B̃ is an (n− 1)× (n− 1) determinant, and thus a\n\npolynomial of degree a most n− 1, so we can write B̃ as\n\nB̃ = Xn−1B0 +Xn−2B1 + · · ·+Bn−1,\n\nfor some n× n matrices B0, . . . , Bn−1 with coefficients in K. For example, when n = 2, we\nhave\n\nB =\n\n(\nX − a −b\n−c X − d\n\n)\n, B̃ =\n\n(\nX − d b\nc X − a\n\n)\n= X\n\n(\n1 0\n0 1\n\n)\n+\n\n(\n−d b\nc −a\n\n)\n.\n\nBy Proposition 7.10, we have\n\nBB̃ = det(B)I = PA(X)I.\n\nOn the other hand, we have\n\nBB̃ = (XI − A)(Xn−1B0 +Xn−2B1 + · · ·+Xn−j−1Bj + · · ·+Bn−1),\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial P4(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\na b\na=(\" i)\n\nX—a —b\n—c X-d\n\nthen\nPa(X) =\n\n=X? (wa) + ad be\n\nWe can substitute the matrix A for the variable X in the polynomial P4(X), obtaining a\nmatrix P,. If we write\nP4(X) =X\" +E, X\" 1 +++ +en,\n\nthen\nPy =A\" +c A\" '+---+e, 1.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley-Hamilton) If K is any commutative ring, for every n x n matriz\nAEM, (kK), if we let\nPy(X) =X\" +X\" 1 +---+e\n\nbe the characteristic polynomial of A, then\nPy =A\" +c, A\" 14+---+e,1 =0.\n\nProof. We can view the matrix B = XJ — A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B which is the transpose of the matrix of\ncofactors of elements of B. Each entry in B is an (n — 1) x (n — 1) determinant, and thus a\npolynomial of degree a most n — 1, so we can write Bas\n\nB= X\"'Bo +X\"? By, +--+ Baa,\n\nfor some n x n matrices Bo,..., By, with coefficients in K. For example, when n = 2, we\n\nhave\nX—-a@a —b ~ X—d b 1 0 —d 0b\nB= (“7 yea) B= ( C xa) =* (i N+ 1).\n\nBy Proposition 7.10, we have\nBB = det(B)I = Pa(X)I.\nOn the other hand, we have\n\nBB=(XI-A)(X\"'Bo +X\" 7B, + +--+ X77 'B; +---+ Br),\n\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 205\n\nand by multiplying out the right-hand side, we get\n\nBB̃ = XnD0 +Xn−1D1 + · · ·+Xn−jDj + · · ·+Dn,\n\nwith\n\nD0 = B0\n\nD1 = B1 − AB0\n\n...\n\nDj = Bj − ABj−1\n\n...\n\nDn−1 = Bn−1 − ABn−2\n\nDn = −ABn−1.\n\nSince\nPA(X)I = (Xn + c1X\n\nn−1 + · · ·+ cn)I,\n\nthe equality\nXnD0 +Xn−1D1 + · · ·+Dn = (Xn + c1X\n\nn−1 + · · ·+ cn)I\n\nis an equality between two matrices, so it requires that all corresponding entries are equal,\nand since these are polynomials, the coefficients of these polynomials must be identical,\nwhich is equivalent to the set of equations\n\nI = B0\n\nc1I = B1 − AB0\n\n...\n\ncjI = Bj − ABj−1\n\n...\n\ncn−1I = Bn−1 − ABn−2\n\ncnI = −ABn−1,\n\nfor all j, with 1 ≤ j ≤ n− 1. If, as in the table below,\n\nAn = AnB0\n\nc1A\nn−1 = An−1(B1 − AB0)\n\n...\n\ncjA\nn−j = An−j(Bj − ABj−1)\n\n...\n\ncn−1A = A(Bn−1 − ABn−2)\n\ncnI = −ABn−1,\n\n\n\n206 CHAPTER 7. DETERMINANTS\n\nwe multiply the first equation by An, the last by I, and generally the (j + 1)th by An−j,\nwhen we add up all these new equations, we see that the right-hand side adds up to 0, and\nwe get our desired equation\n\nAn + c1A\nn−1 + · · ·+ cnI = 0,\n\nas claimed.\n\nAs a concrete example, when n = 2, the matrix\n\nA =\n\n(\na b\nc d\n\n)\nsatisfies the equation\n\nA2 − (a+ d)A+ (ad− bc)I = 0.\n\nMost readers will probably find the proof of Theorem 7.14 rather clever but very myste-\nrious and unmotivated. The conceptual difficulty is that we really need to understand how\npolynomials in one variable “act” on vectors in terms of the matrix A. This can be done and\nyields a more “natural” proof. Actually, the reasoning is simpler and more general if we free\nourselves from matrices and instead consider a finite-dimensional vector space E and some\ngiven linear map f : E → E. Given any polynomial p(X) = a0X\n\nn + a1X\nn−1 + · · ·+ an with\n\ncoefficients in the field K, we define the linear map p(f) : E → E by\n\np(f) = a0f\nn + a1f\n\nn−1 + · · ·+ anid,\n\nwhere fk = f ◦ · · · ◦ f , the k-fold composition of f with itself. Note that\n\np(f)(u) = a0f\nn(u) + a1f\n\nn−1(u) + · · ·+ anu,\n\nfor every vector u ∈ E. Then we define a new kind of scalar multiplication · : K[X]×E → E\nby polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,\n\np(X) · u = p(f)(u).\n\nIt is easy to verify that this is a “good action,” which means that\n\np · (u+ v) = p · u+ p · v\n(p+ q) · u = p · u+ q · u\n\n(pq) · u = p · (q · u)\n\n1 · u = u,\n\nfor all p, q ∈ K[X] and all u, v ∈ E. With this new scalar multiplication, E is a K[X]-module.\n\nIf p = λ is just a scalar in K (a polynomial of degree 0), then\n\nλ · u = (λid)(u) = λu,\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 207\n\nwhich means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial\nX), then\n\nX · u = f(u).\n\nNow if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈ K[X] has the property\nthat\n\np(X) · ei = 0, i = 1, . . . , n,\n\nthen this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the linear map p(f)\nvanishes on E. We can also check, as we did in Section 7.2, that if A and B are two n× n\nmatrices and if (u1, . . . , un) are any n vectors, then\n\nA ·\n\nB ·\nu1\n\n...\nun\n\n\n = (AB) ·\n\nu1\n...\nun\n\n .\n\nThis suggests the plan of attack for our second proof of the Cayley–Hamilton theorem.\nFor simplicity, we prove the theorem for vector spaces over a field. The proof goes through\nfor a free module over a commutative ring.\n\nTheorem 7.15. (Cayley–Hamilton) For every finite-dimensional vector space over a field\nK, for every linear map f : E → E, for every basis (e1, . . . , en), if A is the matrix over f\nover the basis (e1, . . . , en) and if\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nis the characteristic polynomial of A, then\n\nPA(f) = fn + c1f\nn−1 + · · ·+ cnid = 0.\n\nProof. Since the columns of A consist of the vector f(ej) expressed over the basis (e1, . . . , en),\nwe have\n\nf(ej) =\nn∑\ni=1\n\nai jei, 1 ≤ j ≤ n.\n\nUsing our action of K[X] on E, the above equations can be expressed as\n\nX · ej =\nn∑\ni=1\n\nai j · ei, 1 ≤ j ≤ n,\n\nwhich yields\n\nj−1∑\ni=1\n\n−ai j · ei + (X − aj j) · ej +\nn∑\n\ni=j+1\n\n−ai j · ei = 0, 1 ≤ j ≤ n.\n\n\n\n208 CHAPTER 7. DETERMINANTS\n\nObserve that the transpose of the characteristic polynomial shows up, so the above system\ncan be written as\n\nX − a1 1 −a2 1 · · · −an 1\n\n−a1 2 X − a2 2 · · · −an 2\n...\n\n...\n...\n\n...\n−a1n −a2n · · · X − ann\n\n ·\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n .\n\nIf we let B = XI −A>, then as in the previous proof, if B̃ is the transpose of the matrix of\ncofactors of B, we have\n\nB̃B = det(B)I = det(XI − A>)I = det(XI − A)I = PAI.\n\nBut since\n\nB ·\n\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n ,\n\nand since B̃ is matrix whose entries are polynomials in K[X], it makes sense to multiply on\n\nthe left by B̃ and we get\n\nB̃ ·B ·\n\n\ne1\n\ne2\n...\nen\n\n = (B̃B) ·\n\n\ne1\n\ne2\n...\nen\n\n = PAI ·\n\n\ne1\n\ne2\n...\nen\n\n = B̃ ·\n\n\n0\n0\n...\n0\n\n =\n\n\n0\n0\n...\n0\n\n ;\n\nthat is,\nPA · ej = 0, j = 1, . . . , n,\n\nwhich proves that PA(f) = 0, as claimed.\n\nIfK is a field, then the characteristic polynomial of a linear map f : E → E is independent\nof the basis (e1, . . . , en) chosen in E. To prove this, observe that the matrix of f over another\nbasis will be of the form P−1AP , for some inverible matrix P , and then\n\ndet(XI − P−1AP ) = det(XP−1IP − P−1AP )\n\n= det(P−1(XI − A)P )\n\n= det(P−1) det(XI − A) det(P )\n\n= det(XI − A).\n\nTherefore, the characteristic polynomial of a linear map is intrinsic to f , and it is denoted\nby Pf .\n\nThe zeros (roots) of the characteristic polynomial of a linear map f are called the eigen-\nvalues of f . They play an important role in theory and applications. We will come back to\nthis topic later on.\n\n\n\n7.8. PERMANENTS 209\n\n7.8 Permanents\n\nRecall that the explicit formula for the determinant of an n× n matrix is\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n.\n\nIf we drop the sign ε(π) of every permutation from the above formula, we obtain a quantity\nknown as the permanent :\n\nper(A) =\n∑\nπ∈Sn\n\naπ(1) 1 · · · aπ(n)n.\n\nPermanents and determinants were investigated as early as 1812 by Cauchy. It is clear from\nthe above definition that the permanent is a multilinear symmetric form. We also have\n\nper(A) = per(A>),\n\nand the following unsigned version of the Laplace expansion formula:\n\nper(A) = ai 1per(Ai 1) + · · ·+ ai jper(Ai j) + · · ·+ ai nper(Ai n),\n\nfor i = 1, . . . , n. However, unlike determinants which have a clear geometric interpretation as\nsigned volumes, permanents do not have any natural geometric interpretation. Furthermore,\ndeterminants can be evaluated efficiently, for example using the conversion to row reduced\nechelon form, but computing the permanent is hard.\n\nPermanents turn out to have various combinatorial interpretations. One of these is in\nterms of perfect matchings of bipartite graphs which we now discuss.\n\nSee Definition 20.5 for the definition of an undirected graph. A bipartite (undirected)\ngraph G = (V,E) is a graph whose set of nodes V can be partitioned into two nonempty\ndisjoint subsets V1 and V2, such that every edge e ∈ E has one endpoint in V1 and one\nendpoint in V2.\n\nAn example of a bipartite graph with 14 nodes is shown in Figure 7.3; its nodes are\npartitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and {y1, y2, y3, y4, y5, y6, y7}.\n\nA matching in a graph G = (V,E) (bipartite or not) is a set M of pairwise non-adjacent\nedges, which means that no two edges in M share a common vertex. A perfect matching is\na matching such that every node in V is incident to some edge in the matching M (every\nnode in V is an endpoint of some edge in M). Figure 7.4 shows a perfect matching (in red)\nin the bipartite graph G.\n\nObviously, a perfect matching in a bipartite graph can exist only if its set of nodes has\na partition in two blocks of equal size, say {x1, . . . , xm} and {y1, . . . , ym}. Then there is\na bijection between perfect matchings and bijections π : {x1, . . . , xm} → {y1, . . . , ym} such\nthat π(xi) = yj iff there is an edge between xi and yj.\n\nNow every bipartite graph G with a partition of its nodes into two sets of equal size as\nabove is represented by an m × m matrix A = (aij) such that aij = 1 iff there is an edge\n\n\n\n210 CHAPTER 7. DETERMINANTS\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.3: A bipartite graph G.\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.4: A perfect matching in the bipartite graph G.\n\nbetween xi and yj, and aij = 0 otherwise. Using the interpretation of perfect matchings as\nbijections π : {x1, . . . , xm} → {y1, . . . , ym}, we see that the permanent per(A) of the (0, 1)-\nmatrix A representing the bipartite graph G counts the number of perfect matchings in G.\n\nIn a famous paper published in 1979, Leslie Valiant proves that computing the permanent\nis a #P-complete problem. Such problems are suspected to be intractable. It is known that\nif a polynomial-time algorithm existed to solve a #P-complete problem, then we would have\nP = NP , which is believed to be very unlikely.\n\nAnother combinatorial interpretation of the permanent can be given in terms of systems\nof distinct representatives. Given a finite set S, let (A1, . . . , An) be any sequence of nonempty\nsubsets of S (not necessarily distinct). A system of distinct representatives (for short SDR)\nof the sets A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈ Ai for i =\n1, . . . , n. The number of SDR’s of a sequence of sets plays an important role in combinatorics.\nNow, if S = {1, 2, . . . , n} and if we associate to any sequence (A1, . . . , An) of nonempty\nsubsets of S the matrix A = (aij) defined such that aij = 1 if j ∈ Ai and aij = 0 otherwise,\nthen the permanent per(A) counts the number of SDR’s of the sets A1, . . . , An.\n\nThis interpretation of permanents in terms of SDR’s can be used to prove bounds for the\npermanents of various classes of matrices. Interested readers are referred to van Lint and\n\n\n\n7.9. SUMMARY 211\n\nWilson [178] (Chapters 11 and 12). In particular, a proof of a theorem known as Van der\nWaerden conjecture is given in Chapter 12. This theorem states that for any n × n matrix\nA with nonnegative entries in which all row-sums and column-sums are 1 (doubly stochastic\nmatrices), we have\n\nper(A) ≥ n!\n\nnn\n,\n\nwith equality for the matrix in which all entries are equal to 1/n.\n\n7.9 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Permutations , transpositions , basics transpositions .\n\n• Every permutation can be written as a composition of permutations.\n\n• The parity of the number of transpositions involved in any decomposition of a permu-\ntation σ is an invariant; it is the signature ε(σ) of the permutation σ.\n\n• Multilinear maps (also called n-linear maps); bilinear maps .\n\n• Symmetric and alternating multilinear maps.\n\n• A basic property of alternating multilinear maps (Lemma 7.4) and the introduction of\nthe formula expressing a determinant.\n\n• Definition of a determinant as a multlinear alternating map D : Mn(K)→ K such that\nD(I) = 1.\n\n• We define the set of algorithms Dn, to compute the determinant of an n× n matrix.\n\n• Laplace expansion according to the ith row ; cofactors .\n\n• We prove that the algorithms in Dn compute determinants (Lemma 7.5).\n\n• We prove that all algorithms in Dn compute the same determinant (Theorem 7.6).\n\n• We give an interpretation of determinants as signed volumes .\n\n• We prove that det(A) = det(A>).\n\n• We prove that det(AB) = det(A) det(B).\n\n• The adjugate Ã of a matrix A.\n\n• Formula for the inverse in terms of the adjugate.\n\n\n\n212 CHAPTER 7. DETERMINANTS\n\n• A matrix A is invertible iff det(A) 6= 0.\n\n• Solving linear equations using Cramer’s rules .\n\n• Determinant of a linear map.\n\n• The characteristic polynomial of a matrix.\n\n• The Cayley–Hamilton theorem.\n\n• The action of the polynomial ring induced by a linear map on a vector space.\n\n• Permanents .\n\n• Permanents count the number of perfect matchings in bipartite graphs.\n\n• Computing the permanent is a #P-perfect problem (L. Valiant).\n\n• Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3–6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R2 of coordinates (c1−a1, c2−a2) and (b1−a1, b2−a2),\nprove that they are linearly dependent iff∣∣∣∣∣∣\n\na1 b1 c1\n\na2 b2 c2\n\n1 1 1\n\n∣∣∣∣∣∣ = 0.\n\n(2) Given three vectors in R3 of coordinates (d1−a1, d2−a2, d3−a3), (c1−a1, c2−a2, c3−a3),\nand (b1 − a1, b2 − a2, b3 − a3), prove that they are linearly dependent iff∣∣∣∣∣∣∣∣\n\na1 b1 c1 d1\n\na2 b2 c2 d2\n\na3 b3 c3 d3\n\n1 1 1 1\n\n∣∣∣∣∣∣∣∣ = 0.\n\n212 CHAPTER 7. DETERMINANTS\n\nA matrix A is invertible iff det(A) ¥ 0.\n\ne Solving linear equations using Cramer’s rules.\n\ne Determinant of a linear map.\n\ne The characteristic polynomial of a matrix.\n\ne The Cayley—Hamilton theorem.\n\ne The action of the polynomial ring induced by a linear map on a vector space.\ne Permanents.\n\ne Permanents count the number of perfect matchings in bipartite graphs.\n\ne Computing the permanent is a #P-perfect problem (L. Valiant).\n\ne Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3-6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R? of coordinates (cj —a,, C2—a@2) and (bj —a, by—a2),\nprove that they are linearly dependent iff\n\nay by Cy\nag bo C2) = 0.\n1 11\n\n(2) Given three vectors in R® of coordinates (d;—a1, d2—a2, d3—a3), (c1—a1, C2—2, C343),\nand (b1 — a1, bg — dg, b3 — a3), prove that they are linearly dependent iff\n\nay by Cc dy\naz by C2 dy\naz bg cz dg\n\n1 1 1éiéid1\n\n= 0.\n\n\n\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+ n)× (m+ n) block matrix (over any field K) given by\n\nA =\n\n(\nA1 A2\n\n0 A4\n\n)\n,\n\nwhere A1 is an m×m matrix, A2 is an m×n matrix, and A4 is an n×n matrix. Prove that\ndet(A) = det(A1) det(A4).\n\nUse the above result to prove that if A is an upper triangular n×n matrix, then det(A) =\na11a22 · · · ann.\n\nProblem 7.4. Prove that if n ≥ 3, then\n\ndet\n\n\n1 + x1y1 1 + x1y2 · · · 1 + x1yn\n1 + x2y1 1 + x2y2 · · · 1 + x2yn\n\n...\n...\n\n...\n...\n\n1 + xny1 1 + xny2 · · · 1 + xnyn\n\n = 0.\n\nProblem 7.5. Prove that ∣∣∣∣∣∣∣∣\n1 4 9 16\n4 9 16 25\n9 16 25 36\n16 25 36 49\n\n∣∣∣∣∣∣∣∣ = 0.\n\nProblem 7.6. Consider the n× n symmetric matrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n2 5 2 0 . . . 0 0\n0 2 5 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 2 5 2 0\n0 0 . . . 0 2 5 2\n0 0 . . . 0 0 2 5\n\n\n.\n\n(1) Find an upper-triangular matrix R such that A = R>R.\n\n(2) Prove that det(A) = 1.\n\n(3) Consider the sequence\n\np0(λ) = 1\n\np1(λ) = 1− λ\npk(λ) = (5− λ)pk−1(λ)− 4pk−2(λ) 2 ≤ k ≤ n.\n\nProve that\ndet(A− λI) = pn(λ).\n\nRemark: It can be shown that pn(λ) has n distinct (real) roots and that the roots of pk(λ)\nseparate the roots of pk+1(λ).\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+n) x (m+n) block matrix (over any field AK’) given by\n\nA, Ape\na=(o 4).\nwhere A, is an m Xm matrix, Ag is an m X n matrix, and A, is an n x n matrix. Prove that\ndet(A) = det(A;) det(Ay).\nUse the above result to prove that if A is an upper triangular n x n matrix, then det(A) =\n\n411422 °°* Ann-\n\nProblem 7.4. Prove that if n > 3, then\n\nL+ayyr Llt+ayye +++ 1+21y,\n1 + T2Y1 1 + T2Y2 °°\" 1 + T2Un\ndet . . . . = 0.\nL+apy1 L4+%nyo +++ 14+ 2nyn\nProblem 7.5. Prove that\n1 4 9 16\n4 9 16 25) _ 0\n9 16 25 36, ©\n16 25 36 49\nProblem 7.6. Consider the n x n symmetric matrix\n12 0 0 . 0 0\n25 2 O 0 0\n02 5 2 . 0 0\nA=]: : tet\n0 0 2 5 2 0\n0 0 0 2 5 2\n0 0 0 0 2 5\n\n(1) Find an upper-triangular matrix R such that A = R'R.\n(2) Prove that det(A) = 1.\n(3) Consider the sequence\npo(A) = 1\npi(A) =1-A\npr(A) = (5 — A)pe_-1(A) — 4pp-2(A) 2<k <n.\n\nProve that\ndet(A — AT) = pp (A).\n\nRemark: It can be shown that p,(A) has n distinct (real) roots and that the roots of pz (A)\nseparate the roots of pz41(A).\n\n\n\n\n214 CHAPTER 7. DETERMINANTS\n\nProblem 7.7. Let B be the n× n matrix (n ≥ 3) given by\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n.\n\nProve that\ndet(B) = (−1)n(n− 2)2n−1.\n\nProblem 7.8. Given a field K (say K = R or K = C), given any two polynomials\np(X), q(X) ∈ K[X], we says that q(X) divides p(X) (and that p(X) is a multiple of q(X))\niff there is some polynomial s(X) ∈ K[X] such that\n\np(X) = q(X)s(X).\n\nIn this case we say that q(X) is a factor of p(X), and if q(X) has degree at least one, we\nsay that q(X) is a nontrivial factor of p(X).\n\nLet f(X) and g(X) be two polynomials in K[X] with\n\nf(X) = a0X\nm + a1X\n\nm−1 + · · ·+ am\n\nof degree m ≥ 1 and\ng(X) = b0X\n\nn + b1X\nn−1 + · · ·+ bn\n\nof degree n ≥ 1 (with a0, b0 6= 0).\n\nYou will need the following result which you need not prove:\n\nTwo polynomials f(X) and g(X) with deg(f) = m ≥ 1 and deg(g) = n ≥ 1 have some\ncommon nontrivial factor iff there exist two nonzero polynomials p(X) and q(X) such that\n\nfp = gq,\n\nwith deg(p) ≤ n− 1 and deg(q) ≤ m− 1.\n\n(1) Let Pm denote the vector space of all polynomials in K[X] of degree at most m− 1,\nand let T : Pn × Pm → Pm+n be the map given by\n\nT (p, q) = fp+ gq, p ∈ Pn, q ∈ Pm,\n\nwhere f and g are some fixed polynomials of degree m ≥ 1 and n ≥ 1.\n\nProve that the map T is linear.\n\n\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f, g) = 0, where R(f, g) is\nthe determinant given by\n\nR(f, g) =\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\na0 a1 · · · · · · am 0 · · · · · · · · · · · · 0\n0 a0 a1 · · · · · · am 0 · · · · · · · · · 0\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · · · · · · · · · · 0 a0 a1 · · · · · · am\nb0 b1 · · · · · · · · · · · · · · · bn 0 · · · 0\n0 b0 b1 · · · · · · · · · · · · · · · bn 0 · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · 0 b0 b1 · · · · · · · · · · · · · · · bn\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\n.\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n+m)× (n+m) matrix, with the first row\n(involving the ais) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the bjs) occurring m times, each time shifted over to the right\nby one column.\n\nHint . Express the matrix of T over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.\n\n(b) m = 1 and n ≥ 2 arbitrary.\n\n(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.\n\n(5) Compute the resultant of f(X) = X3 + pX + q and g(X) = 3X2 + p, and\n\nf(X) = a0X\n2 + a1X + a2\n\ng(X) = b0X\n2 + b1X + b2.\n\nIn the second case, you should get\n\n4R(f, g) = (2a0b2 − a1b1 + 2a2b0)2 − (4a0a2 − a2\n1)(4b0b2 − b2\n\n1).\n\nProblem 7.9. Let A,B,C,D be n× n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\ndet\n\n(\nA B\nC D\n\n)\n= det(AD − CB).\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f,g) =0, where R(f,g) is\nthe determinant given by\n\nao ay eee wee Am 0 cee eee eee eee 0\n\n0 ag Gy cts tt Gm Oo ret tee eee O\nRg) =f\n\n0 eee eee eee wee 0 ao ay eee eee Am\n\na | 0\n\nOS | 0\n\nOe 0 dg Bp tet tte tet tee ee Dy\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n +m) x (n +m) matrix, with the first row\n(involving the a;s) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the b;s) occurring m times, each time shifted over to the right\nby one column.\n\nHint. Express the matrix of T’ over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a\n\n)\n(b) m=1 and n > 2 arbitrary.\n)\n\n(c\n\n3\n\n= n=l, and write f(X) =aX +b and g(X) =cX +d.\n\nf(X) =aX? + bX +c and g(X) = 2aX +b.\n\n(5) Compute the resultant of f(X) = X°+pX +q and g(X) = 3X? +p, and\n\nf(X) = ag.X? +a,X + ag\ng(X) => by X? + bX + bo.\n\nIn the second case, you should get\nAR(f, g) = (2Qagbe _ a,b, + 2aybp)? _ (daga2 —_ a*) (4bgbo _ bi).\n\nProblem 7.9. Let A,B,C, D be n x n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\nA B\ndet (< b) = det(AD — CB).\n\n\n\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then | det(H)| = nn/2.\n\n(3) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then\n\ndet\n\n(\nH H\nH −H\n\n)\n= (2n)n.\n\nProblem 7.10. Compute the product of the following determinants∣∣∣∣∣∣∣∣\na −b −c −d\nb a −d c\nc d a −b\nd −c b a\n\n∣∣∣∣∣∣∣∣\n∣∣∣∣∣∣∣∣\nx −y −z −t\ny x −t z\nz t x −y\nt −z y x\n\n∣∣∣∣∣∣∣∣\nto prove the following identity (due to Euler):\n\n(a2 + b2 + c2 + d2)(x2 + y2 + z2 + t2) = (ax+ by + cz + dt)2 + (ay − bx+ ct− dz)2\n\n+ (az − bt− cx+ dy)2 + (at+ bz − cy + dx)2.\n\nProblem 7.11. Let A be an n × n matrix with integer entries. Prove that A−1 exists and\nhas integer entries if and only if det(A) = ±1.\n\nProblem 7.12. Let A be an n× n real or complex matrix.\n\n(1) Prove that if A> = −A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n\n(2) Prove that ∣∣∣∣∣∣∣∣\n0 a b c\n−a 0 d e\n−b −d 0 f\n−c −e −f 0\n\n∣∣∣∣∣∣∣∣ = (af − be+ dc)2.\n\nProblem 7.13. A Cauchy matrix is a matrix of the form\n\n1\n\nλ1 − σ1\n\n1\n\nλ1 − σ2\n\n· · · 1\n\nλ1 − σn\n1\n\nλ2 − σ1\n\n1\n\nλ2 − σ2\n\n· · · 1\n\nλ2 − σn\n...\n\n...\n...\n\n...\n1\n\nλn − σ1\n\n1\n\nλn − σ2\n\n· · · 1\n\nλn − σn\n\n\nwhere λi 6= σj, for all i, j, with 1 ≤ i, j ≤ n. Prove that the determinant Cn of a Cauchy\nmatrix as above is given by\n\nCn =\n\n∏n\ni=2\n\n∏i−1\nj=1(λi − λj)(σj − σi)∏n\ni=1\n\n∏n\nj=1(λi − σj)\n\n.\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n x n Hadamard matrix (n > 2), then | det(H)| =n\".\n(3) Prove that if H is ann x n Hadamard matrix (n > 2), then\n\ndet (ji \") = (2n)\".\n\nProblem 7.10. Compute the product of the following determinants\n\na —b -—c —d\\|x -y -z -t\nb a -d clly a -t 2z\nc dad a —bl|z t aw -y\nd -c b allt -z y «\n\nto prove the following identity (due to Euler):\n\n(P4+RP4C4P)(a? +y? +240) = (ax + by + cz +dt) + (ay — bx + ct — dz)?\n+ (az — bt — cx + dy)? + (at + bz — cy + dx)’.\n\nProblem 7.11. Let A be an n x n matrix with integer entries. Prove that A! exists and\nhas integer entries if and only if det(A) = +1.\n\nProblem 7.12. Let A be an n x n real or complex matrix.\n(1) Prove that if A' = —A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n(2) Prove that\n\n0 a ob e\n—a 0 de} _ 9\npb -d 0 f = (af — be + dc)’.\n—c -e -f 0\nProblem 7.13. A Cauchy matrix is a matrix of the form\n1 1 1\nAy —- 0, ATO At — On\n1 1 ALT 972 Fy\nAy — O71 Ay — J2 A2 — On\n1 1 | 1\nMn — 01 An — 02 An — On\n\nwhere \\; 4 o;, for all 7,7, with 1 < 7,7 <n. Prove that the determinant C,, of a Cauchy\nmatrix as above is given by\n\nC= [Tj-2 Wai — r;)(a; — 9)\n\nTie Tj — 9j)\n\n\n\n\n7.11. PROBLEMS 217\n\nProblem 7.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct scalars in R and let\n(β1, . . . , βm+1) be any sequence of scalars in R, not necessarily distinct.\n\n(1) Prove that there is a unique polynomial P of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\nHint . Remember Vandermonde!\n\n(2) Let Li(X) be the polynomial of degree m given by\n\nLi(X) =\n(X − α1) · · · (X − αi−1)(X − αi+1) · · · (X − αm+1)\n\n(αi − α1) · · · (αi − αi−1)(αi − αi+1) · · · (αi − αm+1)\n, 1 ≤ i ≤ m+ 1.\n\nThe polynomials Li(X) are known as Lagrange polynomial interpolants . Prove that\n\nLi(αj) = δi j 1 ≤ i, j ≤ m+ 1.\n\nProve that\nP (X) = β1L1(X) + · · ·+ βm+1Lm+1(X)\n\nis the unique polynomial of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\n(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that they form a basis\nof all polynomials of degree at most m.\n\nHow is 1 (the constant polynomial 1) expressed over the basis (L1(X), . . . , Lm+1(X))?\n\nGive the expression of every polynomial P (X) of degree at most m over the basis\n(L1(X), . . . , Lm+1(X)).\n\n(4) Prove that the dual basis (L∗1, . . . , L\n∗\nm+1) of the basis (L1(X), . . . , Lm+1(X)) consists\n\nof the linear forms L∗i given by\nL∗i (P ) = P (αi),\n\nfor every polynomial P of degree at most m; this is simply evaluation at αi.\n\n\n\n218 CHAPTER 7. DETERMINANTS\n\n\n\nChapter 8\n\nGaussian Elimination,\nLU-Factorization, Cholesky\nFactorization, Reduced Row Echelon\nForm\n\nIn this chapter we assume that all vector spaces are over the field R. All results that do not\nrely on the ordering on R or on taking square roots hold for arbitrary fields.\n\n8.1 Motivating Example: Curve Interpolation\n\nCurve interpolation is a problem that arises frequently in computer graphics and in robotics\n(path planning). There are many ways of tackling this problem and in this section we will\ndescribe a solution using cubic splines . Such splines consist of cubic Bézier curves. They\nare often used because they are cheap to implement and give more flexibility than quadratic\nBézier curves.\n\nA cubic Bézier curve C(t) (in R2 or R3) is specified by a list of four control points\n(b0, b2, b2, b3) and is given parametrically by the equation\n\nC(t) = (1− t)3 b0 + 3(1− t)2t b1 + 3(1− t)t2 b2 + t3 b3.\n\nClearly, C(0) = b0, C(1) = b3, and for t ∈ [0, 1], the point C(t) belongs to the convex hull of\nthe control points b0, b1, b2, b3. The polynomials\n\n(1− t)3, 3(1− t)2t, 3(1− t)t2, t3\n\nare the Bernstein polynomials of degree 3.\n\nTypically, we are only interested in the curve segment corresponding to the values of t in\nthe interval [0, 1]. Still, the placement of the control points drastically affects the shape of the\ncurve segment, which can even have a self-intersection; See Figures 8.1, 8.2, 8.3 illustrating\nvarious configurations.\n\n219\n\n\n\n220 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.1: A “standard” Bézier curve.\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.2: A Bézier curve with an inflection point.\n\n\n\n8.1. MOTIVATING EXAMPLE: CURVE INTERPOLATION 221\n\nb0\n\nb1b2\n\nb3\n\nFigure 8.3: A self-intersecting Bézier curve.\n\nInterpolation problems require finding curves passing through some given data points and\npossibly satisfying some extra constraints.\n\nA Bézier spline curve F is a curve which is made up of curve segments which are Bézier\ncurves, say C1, . . . , Cm (m ≥ 2). We will assume that F defined on [0,m], so that for\ni = 1, . . . ,m,\n\nF (t) = Ci(t− i+ 1), i− 1 ≤ t ≤ i.\n\nTypically, some smoothness is required between any two junction points, that is, between\nany two points Ci(1) and Ci+1(0), for i = 1, . . . ,m − 1. We require that Ci(1) = Ci+1(0)\n(C0-continuity), and typically that the derivatives of Ci at 1 and of Ci+1 at 0 agree up to\nsecond order derivatives. This is called C2-continuity , and it ensures that the tangents agree\nas well as the curvatures.\n\nThere are a number of interpolation problems, and we consider one of the most common\nproblems which can be stated as follows:\n\nProblem: Given N + 1 data points x0, . . . , xN , find a C2 cubic spline curve F such that\nF (i) = xi for all i, 0 ≤ i ≤ N (N ≥ 2).\n\nA way to solve this problem is to find N + 3 auxiliary points d−1, . . . , dN+1, called de\nBoor control points , from which N Bézier curves can be found. Actually,\n\nd−1 = x0 and dN+1 = xN\n\n\n\n222 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nso we only need to find N + 1 points d0, . . . , dN .\n\nIt turns out that the C2-continuity constraints on the N Bézier curves yield only N − 1\nequations, so d0 and dN can be chosen arbitrarily. In practice, d0 and dN are chosen according\nto various end conditions, such as prescribed velocities at x0 and xN . For the time being, we\nwill assume that d0 and dN are given.\n\nFigure 8.4 illustrates an interpolation problem involving N + 1 = 7 + 1 = 8 data points.\nThe control points d0 and d7 were chosen arbitrarily.\n\nx0 = d−1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\nFigure 8.4: A C2 cubic interpolation spline curve passing through the points x0, x1, x2, x3,\nx4, x5, x6, x7.\n\nIt can be shown that d1, . . . , dN−1 are given by the linear system\n7\n2\n\n1\n1 4 1 0\n\n. . . . . . . . .\n\n0 1 4 1\n1 7\n\n2\n\n\n\n\nd1\n\nd2\n...\n\ndN−2\n\ndN−1\n\n =\n\n\n6x1 − 3\n\n2\nd0\n\n6x2\n...\n\n6xN−2\n\n6xN−1 − 3\n2\ndN\n\n .\n\nWe will show later that the above matrix is invertible because it is strictly diagonally\ndominant.\n\n\n\n8.2. GAUSSIAN ELIMINATION 223\n\nOnce the above system is solved, the Bézier cubics C1, . . ., CN are determined as follows\n(we assume N ≥ 2): For 2 ≤ i ≤ N − 1, the control points (bi0, b\n\ni\n1, b\n\ni\n2, b\n\ni\n3) of Ci are given by\n\nbi0 = xi−1\n\nbi1 =\n2\n\n3\ndi−1 +\n\n1\n\n3\ndi\n\nbi2 =\n1\n\n3\ndi−1 +\n\n2\n\n3\ndi\n\nbi3 = xi.\n\nThe control points (b1\n0, b\n\n1\n1, b\n\n1\n2, b\n\n1\n3) of C1 are given by\n\nb1\n0 = x0\n\nb1\n1 = d0\n\nb1\n2 =\n\n1\n\n2\nd0 +\n\n1\n\n2\nd1\n\nb1\n3 = x1,\n\nand the control points (bN0 , b\nN\n1 , b\n\nN\n2 , b\n\nN\n3 ) of CN are given by\n\nbN0 = xN−1\n\nbN1 =\n1\n\n2\ndN−1 +\n\n1\n\n2\ndN\n\nbN2 = dN\n\nbN3 = xN .\n\nFigure 8.5 illustrates this process spline interpolation for N = 7.\n\nWe will now describe various methods for solving linear systems. Since the matrix of the\nabove system is tridiagonal, there are specialized methods which are more efficient than the\ngeneral methods. We will discuss a few of these methods.\n\n8.2 Gaussian Elimination\n\nLet A be an n × n matrix, let b ∈ Rn be an n-dimensional vector and assume that A is\ninvertible. Our goal is to solve the system Ax = b. Since A is assumed to be invertible,\nwe know that this system has a unique solution x = A−1b. Experience shows that two\ncounter-intuitive facts are revealed:\n\n(1) One should avoid computing the inverse A−1 of A explicitly. This is inefficient since\nit would amount to solving the n linear systems Au(j) = ej for j = 1, . . . , n, where\nej = (0, . . . , 1, . . . , 0) is the jth canonical basis vector of Rn (with a 1 is the jth slot).\nBy doing so, we would replace the resolution of a single system by the resolution of n\nsystems, and we would still have to multiply A−1 by b.\n\n\n\n224 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nx0 = d1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\n1\n1b =\n\n1\n2b\n\nb\n2\n1\n\nb\n2\n2\n\nb\n\nb1\n3\n\nb2\n3\n\nb1\n4\n\nb2\n4\n\nb1\n5\n\nb2\n5\n\nb1\n6\n\nb2\n6\n\n1\n7\n\nb\n7\n2=\n\nFigure 8.5: A C2 cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color\ncoded Bézier cubics.\n\n(2) One does not solve (large) linear systems by computing determinants (using Cramer’s\nformulae) since this method requires a number of additions (resp. multiplications)\nproportional to (n+ 1)! (resp. (n+ 2)!).\n\nThe key idea on which most direct methods (as opposed to iterative methods, that look\nfor an approximation of the solution) are based is that if A is an upper-triangular matrix,\nwhich means that aij = 0 for 1 ≤ j < i ≤ n (resp. lower-triangular, which means that\naij = 0 for 1 ≤ i < j ≤ n), then computing the solution x is trivial. Indeed, say A is an\nupper-triangular matrix\n\nA =\n\n\n\na1 1 a1 2 · · · a1n−2 a1n−1 a1n\n\n0 a2 2 · · · a2n−2 a2n−1 a2n\n\n0 0\n. . .\n\n...\n...\n\n...\n. . .\n\n...\n...\n\n0 0 · · · 0 an−1n−1 an−1n\n\n0 0 · · · 0 0 ann\n\n\n.\n\nThen det(A) = a1 1a2 2 · · · ann 6= 0, which implies that ai i 6= 0 for i = 1, . . . , n, and we can\nsolve the system Ax = b from bottom-up by back-substitution. That is, first we compute\n\n\n\n8.2. GAUSSIAN ELIMINATION 225\n\nxn from the last equation, next plug this value of xn into the next to the last equation and\ncompute xn−1 from it, etc. This yields\n\nxn = a−1\nnnbn\n\nxn−1 = a−1\nn−1n−1(bn−1 − an−1nxn)\n\n...\n\nx1 = a−1\n1 1 (b1 − a1 2x2 − · · · − a1nxn).\n\nNote that the use of determinants can be avoided to prove that if A is invertible then\nai i 6= 0 for i = 1, . . . , n. Indeed, it can be shown directly (by induction) that an upper (or\nlower) triangular matrix is invertible iff all its diagonal entries are nonzero.\n\nIf A is lower-triangular, we solve the system from top-down by forward-substitution.\n\nThus, what we need is a method for transforming a matrix to an equivalent one in upper-\ntriangular form. This can be done by elimination. Let us illustrate this method on the\nfollowing example:\n\n2x + y + z = 5\n4x − 6y = −2\n−2x + 7y + 2z = 9.\n\nWe can eliminate the variable x from the second and the third equation as follows: Subtract\ntwice the first equation from the second and add the first equation to the third. We get the\nnew system\n\n2x + y + z = 5\n− 8y − 2z = −12\n\n8y + 3z = 14.\n\nThis time we can eliminate the variable y from the third equation by adding the second\nequation to the third:\n\n2x + y + z = 5\n− 8y − 2z = −12\n\nz = 2.\n\nThis last system is upper-triangular. Using back-substitution, we find the solution: z = 2,\ny = 1, x = 1.\n\nObserve that we have performed only row operations. The general method is to iteratively\neliminate variables using simple row operations (namely, adding or subtracting a multiple of\na row to another row of the matrix) while simultaneously applying these operations to the\nvector b, to obtain a system, MAx = Mb, where MA is upper-triangular. Such a method is\ncalled Gaussian elimination. However, one extra twist is needed for the method to work in\nall cases: It may be necessary to permute rows, as illustrated by the following example:\n\nx + y + z = 1\nx + y + 3z = 1\n2x + 5y + 8z = 1.\n\n\n\n226 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nIn order to eliminate x from the second and third row, we subtract the first row from the\nsecond and we subtract twice the first row from the third:\n\nx + y + z = 1\n2z = 0\n\n3y + 6z = −1.\n\nNow the trouble is that y does not occur in the second row; so, we can’t eliminate y from\nthe third row by adding or subtracting a multiple of the second row to it. The remedy is\nsimple: Permute the second and the third row! We get the system:\n\nx + y + z = 1\n3y + 6z = −1\n\n2z = 0,\n\nwhich is already in triangular form. Another example where some permutations are needed\nis:\n\nz = 1\n−2x + 7y + 2z = 1\n4x − 6y = −1.\n\nFirst we permute the first and the second row, obtaining\n\n−2x + 7y + 2z = 1\nz = 1\n\n4x − 6y = −1,\n\nand then we add twice the first row to the third, obtaining:\n\n−2x + 7y + 2z = 1\nz = 1\n\n8y + 4z = 1.\n\nAgain we permute the second and the third row, getting\n\n−2x + 7y + 2z = 1\n8y + 4z = 1\n\nz = 1,\n\nan upper-triangular system. Of course, in this example, z is already solved and we could\nhave eliminated it first, but for the general method, we need to proceed in a systematic\nfashion.\n\nWe now describe the method of Gaussian elimination applied to a linear system Ax = b,\nwhere A is assumed to be invertible. We use the variable k to keep track of the stages of\nelimination. Initially, k = 1.\n\n\n\n8.2. GAUSSIAN ELIMINATION 227\n\n(1) The first step is to pick some nonzero entry ai 1 in the first column of A. Such an\nentry must exist, since A is invertible (otherwise, the first column of A would be the\nzero vector, and the columns of A would not be linearly independent. Equivalently, we\nwould have det(A) = 0). The actual choice of such an element has some impact on the\nnumerical stability of the method, but this will be examined later. For the time being,\nwe assume that some arbitrary choice is made. This chosen element is called the pivot\nof the elimination step and is denoted π1 (so, in this first step, π1 = ai 1).\n\n(2) Next we permute the row (i) corresponding to the pivot with the first row. Such a\nstep is called pivoting . So after this permutation, the first element of the first row is\nnonzero.\n\n(3) We now eliminate the variable x1 from all rows except the first by adding suitable\nmultiples of the first row to these rows. More precisely we add −ai 1/π1 times the first\nrow to the ith row for i = 2, . . . , n. At the end of this step, all entries in the first\ncolumn are zero except the first.\n\n(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively repeat Steps\n(1), (2), (3) on the (n− k + 1)× (n− k + 1) subsystem obtained by deleting the first\nk − 1 rows and k − 1 columns from the current system.\n\nIf we let A1 = A and Ak = (a\n(k)\ni j ) be the matrix obtained after k − 1 elimination steps\n\n(2 ≤ k ≤ n), then the kth elimination step is applied to the matrix Ak of the form\n\nAk =\n\n\n\na\n(k)\n1 1 a\n\n(k)\n1 2 · · · · · · · · · a\n\n(k)\n1n\n\n0 a\n(k)\n2 2 · · · · · · · · · a\n\n(k)\n2n\n\n...\n. . . . . .\n\n...\n...\n\n0 0 0 a\n(k)\nk k · · · a\n\n(k)\nk n\n\n...\n...\n\n...\n...\n\n...\n\n0 0 0 a\n(k)\nnk · · · a\n\n(k)\nnn\n\n\n.\n\nActually, note that\na\n\n(k)\ni j = a\n\n(i)\ni j\n\nfor all i, j with 1 ≤ i ≤ k − 2 and i ≤ j ≤ n, since the first k − 1 rows remain unchanged\nafter the (k − 1)th step.\n\nWe will prove later that det(Ak) = ± det(A). Consequently, Ak is invertible. The fact\nthat Ak is invertible iff A is invertible can also be shown without determinants from the fact\nthat there is some invertible matrix Mk such that Ak = MkA, as we will see shortly.\n\nSince Ak is invertible, some entry a\n(k)\ni k with k ≤ i ≤ n is nonzero. Otherwise, the last\n\nn − k + 1 entries in the first k columns of Ak would be zero, and the first k columns of\nAk would yield k vectors in Rk−1. But then the first k columns of Ak would be linearly\n\n\n\n228 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\ndependent and Ak would not be invertible, a contradiction. This situation is illustrated by\nthe following matrix for n = 5 and k = 3:\n\na\n(3)\n1 1 a\n\n(3)\n1 2 a\n\n(3)\n1 3 a\n\n(3)\n1 3 a\n\n(3)\n1 5\n\n0 a\n(3)\n2 2 a\n\n(3)\n2 3 a\n\n(3)\n2 4 a\n\n(3)\n2 5\n\n0 0 0 a\n(3)\n3 4 a\n\n(3)\n3 5\n\n0 0 0 a\n(3)\n4 4 a\n\n(3)\n4n\n\n0 0 0 a\n(3)\n5 4 a\n\n(3)\n5 5\n\n .\n\nThe first three columns of the above matrix are linearly dependent.\n\nSo one of the entries a\n(k)\ni k with k ≤ i ≤ n can be chosen as pivot, and we permute the kth\n\nrow with the ith row, obtaining the matrix α(k) = (α\n(k)\nj l ). The new pivot is πk = α\n\n(k)\nk k , and\n\nwe zero the entries i = k + 1, . . . , n in column k by adding −α(k)\ni k /πk times row k to row i.\n\nAt the end of this step, we have Ak+1. Observe that the first k − 1 rows of Ak are identical\nto the first k − 1 rows of Ak+1.\n\nThe process of Gaussian elimination is illustrated in schematic form below:\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 × × ×\n0 × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 0 ×\n\n .\n\n8.3 Elementary Matrices and Row Operations\n\nIt is easy to figure out what kind of matrices perform the elementary row operations used\nduring Gaussian elimination. The key point is that if A = PB, where A,B are m × n\nmatrices and P is a square matrix of dimension m, if (as usual) we denote the rows of A and\nB by A1, . . . , Am and B1, . . . , Bm, then the formula\n\naij =\nm∑\nk=1\n\npikbkj\n\ngiving the (i, j)th entry in A shows that the ith row of A is a linear combination of the rows\nof B:\n\nAi = pi1B1 + · · ·+ pimBm.\n\nTherefore, multiplication of a matrix on the left by a square matrix performs row opera-\ntions . Similarly, multiplication of a matrix on the right by a square matrix performs column\noperations\n\nThe permutation of the kth row with the ith row is achieved by multiplying A on the left\nby the transposition matrix P (i, k), which is the matrix obtained from the identity matrix\n\n\n\n8.3. ELEMENTARY MATRICES AND ROW OPERATIONS 229\n\nby permuting rows i and k, i.e.,\n\nP (i, k) =\n\n\n\n1\n1\n\n0 1\n1\n\n. . .\n\n1\n1 0\n\n1\n1\n\n\n.\n\nFor example, if m = 3,\n\nP (1, 3) =\n\n0 0 1\n0 1 0\n1 0 0\n\n ,\n\nthen\n\nP (1, 3)B =\n\n0 0 1\n0 1 0\n1 0 0\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n =\n\nb31 b32 · · · · · · · · · b3n\n\nb21 b22 · · · · · · · · · b2n\n\nb11 b12 · · · · · · · · · b1n\n\n .\n\nObserve that det(P (i, k)) = −1. Furthermore, P (i, k) is symmetric (P (i, k)> = P (i, k)), and\n\nP (i, k)−1 = P (i, k).\n\nDuring the permutation Step (2), if row k and row i need to be permuted, the matrix A\nis multiplied on the left by the matrix Pk such that Pk = P (i, k), else we set Pk = I.\n\nAdding β times row j to row i (with i 6= j) is achieved by multiplying A on the left by\nthe elementary matrix ,\n\nEi,j;β = I + βei j,\n\nwhere\n\n(ei j)k l =\n\n{\n1 if k = i and l = j\n0 if k 6= i or l 6= j,\n\ni.e.,\n\nEi,j;β =\n\n\n\n1\n1\n\n1\n1\n\n. . .\n\n1\nβ 1\n\n1\n1\n\n\nor Ei,j;β =\n\n\n\n1\n1\n\n1 β\n1\n\n. . .\n\n1\n1\n\n1\n1\n\n\n,\n\n\n\n230 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\non the left, i > j, and on the right, i < j. The index i is the index of the row that is changed\nby the multiplication. For example, if m = 3 and we want to add twice row 1 to row 3, since\nβ = 2, j = 1 and i = 3, we form\n\nE3,1;2 = I + 2e31 =\n\n1 0 0\n0 1 0\n0 0 1\n\n+\n\n0 0 0\n0 0 0\n2 0 0\n\n =\n\n1 0 0\n0 1 0\n2 0 1\n\n ,\n\nand calculate\n\nE3,1;2B =\n\n1 0 0\n0 1 0\n2 0 1\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n\n=\n\n b11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\n2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n\n\n .\n\nObserve that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I − βei j and that det(Ei,j;β) = 1.\nTherefore, during Step 3 (the elimination step), the matrix A is multiplied on the left by a\nproduct Ek of matrices of the form Ei,k;βi,k , with i > k.\n\nConsequently, we see that\nAk+1 = EkPkAk,\n\nand then\nAk = Ek−1Pk−1 · · ·E1P1A.\n\nThis justifies the claim made earlier that Ak = MkA for some invertible matrix Mk; we can\npick\n\nMk = Ek−1Pk−1 · · ·E1P1,\n\na product of invertible matrices.\n\nThe fact that det(P (i, k)) = −1 and that det(Ei,j;β) = 1 implies immediately the fact\nclaimed above: We always have\n\ndet(Ak) = ± det(A).\n\nFurthermore, since\nAk = Ek−1Pk−1 · · ·E1P1A\n\nand since Gaussian elimination stops for k = n, the matrix\n\nAn = En−1Pn−1 · · ·E2P2E1P1A\n\nis upper-triangular. Also note that if we letM = En−1Pn−1 · · ·E2P2E1P1, then det(M) = ±1,\nand\n\ndet(A) = ± det(An).\n\nThe matrices P (i, k) and Ei,j;β are called elementary matrices . We can summarize the\nabove discussion in the following theorem:\n\n\n\n8.4. LU -FACTORIZATION 231\n\nTheorem 8.1. (Gaussian elimination) Let A be an n× n matrix (invertible or not). Then\nthere is some invertible matrix M so that U = MA is upper-triangular. The pivots are all\nnonzero iff A is invertible.\n\nProof. We already proved the theorem when A is invertible, as well as the last assertion.\nNow A is singular iff some pivot is zero, say at Stage k of the elimination. If so, we must\nhave a\n\n(k)\ni k = 0 for i = k, . . . , n; but in this case, Ak+1 = Ak and we may pick Pk = Ek = I.\n\nRemark: Obviously, the matrix M can be computed as\n\nM = En−1Pn−1 · · ·E2P2E1P1,\n\nbut this expression is of no use. Indeed, what we need is M−1; when no permutations are\nneeded, it turns out that M−1 can be obtained immediately from the matrices Ek’s, in fact,\nfrom their inverses, and no multiplications are necessary.\n\nRemark: Instead of looking for an invertible matrix M so that MA is upper-triangular, we\ncan look for an invertible matrix M so that MA is a diagonal matrix. Only a simple change\nto Gaussian elimination is needed. At every Stage k, after the pivot has been found and\npivoting been performed, if necessary, in addition to adding suitable multiples of the kth\nrow to the rows below row k in order to zero the entries in column k for i = k + 1, . . . , n,\nalso add suitable multiples of the kth row to the rows above row k in order to zero the\nentries in column k for i = 1, . . . , k − 1. Such steps are also achieved by multiplying on\nthe left by elementary matrices Ei,k;βi,k , except that i < k, so that these matrices are not\nlower-triangular matrices. Nevertheless, at the end of the process, we find that An = MA,\nis a diagonal matrix.\n\nThis method is called the Gauss-Jordan factorization. Because it is more expensive than\nGaussian elimination, this method is not used much in practice. However, Gauss-Jordan\nfactorization can be used to compute the inverse of a matrix A. Indeed, we find the jth\ncolumn of A−1 by solving the system Ax(j) = ej (where ej is the jth canonical basis vector\nof Rn). By applying Gauss-Jordan, we are led to a system of the form Djx\n\n(j) = Mjej, where\nDj is a diagonal matrix, and we can immediately compute x(j).\n\nIt remains to discuss the choice of the pivot, and also conditions that guarantee that no\npermutations are needed during the Gaussian elimination process. We begin by stating a\nnecessary and sufficient condition for an invertible matrix to have an LU -factorization (i.e.,\nGaussian elimination does not require pivoting).\n\n8.4 LU-Factorization\n\nDefinition 8.1. We say that an invertible matrix A has an LU-factorization if it can be\nwritten as A = LU , where U is upper-triangular invertible and L is lower-triangular, with\nLi i = 1 for i = 1, . . . , n.\n\n\n\n232 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nA lower-triangular matrix with diagonal entries equal to 1 is called a unit lower-triangular\nmatrix. Given an n×n matrix A = (ai j), for any k with 1 ≤ k ≤ n, let A(1 : k, 1 : k) denote\nthe submatrix of A whose entries are ai j, where 1 ≤ i, j ≤ k.1 For example, if A is the 5× 5\nmatrix\n\nA =\n\n\na11 a12 a13 a14 a15\n\na21 a22 a23 a24 a25\n\na31 a32 a33 a34 a35\n\na41 a42 a43 a44 a45\n\na51 a52 a53 a54 a55\n\n ,\n\nthen\n\nA(1 : 3, 1 : 3) =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n .\n\nProposition 8.2. Let A be an invertible n × n-matrix. Then A has an LU-factorization\nA = LU iff every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n. Furthermore, when A\nhas an LU-factorization, we have\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nwhere πk is the pivot obtained after k− 1 elimination steps. Therefore, the kth pivot is given\nby\n\nπk =\n\na11 = det(A(1 : 1, 1 : 1)) if k = 1\ndet(A(1 : k, 1 : k))\n\ndet(A(1 : k − 1, 1 : k − 1))\nif k = 2, . . . , n.\n\nProof. First assume that A = LU is an LU -factorization of A. We can write\n\nA =\n\n(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nL1 0\nL3 L4\n\n)(\nU1 U2\n\n0 U4\n\n)\n=\n\n(\nL1U1 L1U2\n\nL3U1 L3U2 + L4U4\n\n)\n,\n\nwhere L1, L4 are unit lower-triangular and U1, U4 are upper-triangular. (Note, A(1 : k, 1 : k),\nL1, and U1 are k×k matrices; A2 and U2 are k× (n−k) matrices; A3 and L3 are (n−k)×k\nmatrices; A4, L4, and U4 are (n− k)× (n− k) matrices.) Thus,\n\nA(1 : k, 1 : k) = L1U1,\n\nand since U is invertible, U1 is also invertible (the determinant of U is the product of the\ndiagonal entries in U , which is the product of the diagonal entries in U1 and U4). As L1 is\ninvertible (since its diagonal entries are equal to 1), we see that A(1 : k, 1 : k) is invertible\nfor k = 1, . . . , n.\n\nConversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We just need to\nshow that Gaussian elimination does not need pivoting. We prove by induction on k that\nthe kth step does not need pivoting.\n\n1We are using Matlab’s notation.\n\n\n\n8.4. LU -FACTORIZATION 233\n\nThis holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 6= 0. Assume that no pivoting\nwas necessary for the first k − 1 steps (2 ≤ k ≤ n− 1). In this case, we have\n\nEk−1 · · ·E2E1A = Ak,\n\nwhere L = Ek−1 · · ·E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 : k) is upper-\ntriangular, so that LA = Ak can be written as(\n\nL1 0\nL3 L4\n\n)(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nU1 B2\n\n0 B4\n\n)\n,\n\nwhere L1 is unit lower-triangular and U1 is upper-triangular. (Once again A(1 : k, 1 : k), L1,\nand U1 are k × k matrices; A2 and B2 are k × (n− k) matrices; A3 and L3 are (n− k)× k\nmatrices; A4, L4, and B4 are (n− k)× (n− k) matrices.) But then,\n\nL1A(1 : k, 1 : k)) = U1,\n\nwhere L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 : k, 1 : k) is\ninvertible, U1 is also invertible, which implies that (U1)kk 6= 0, since U1 is upper-triangular.\nTherefore, no pivoting is needed in Step k, establishing the induction step. Since det(L1) = 1,\nwe also have\n\ndet(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k)) = det(A(1 : k, 1 : k)),\n\nand since U1 is upper-triangular and has the pivots π1, . . . , πk on its diagonal, we get\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nas claimed.\n\nRemark: The use of determinants in the first part of the proof of Proposition 8.2 can be\navoided if we use the fact that a triangular matrix is invertible iff all its diagonal entries are\nnonzero.\n\nCorollary 8.3. (LU-Factorization) Let A be an invertible n × n-matrix. If every matrix\nA(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian elimination requires no pivoting\nand yields an LU-factorization A = LU .\n\nProof. We proved in Proposition 8.2 that in this case Gaussian elimination requires no\npivoting. Then since every elementary matrix Ei,k;β is lower-triangular (since we always\narrange that the pivot πk occurs above the rows that it operates on), since E−1\n\ni,k;β = Ei,k;−β\nand the Eks are products of Ei,k;βi,ks, from\n\nEn−1 · · ·E2E1A = U,\n\nwhere U is an upper-triangular matrix, we get\n\nA = LU,\n\nwhere L = E−1\n1 E−1\n\n2 · · ·E−1\nn−1 is a lower-triangular matrix. Furthermore, as the diagonal\n\nentries of each Ei,k;β are 1, the diagonal entries of each Ek are also 1.\n\n\n\n234 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nExample 8.1. The reader should verify that\n2 1 1 0\n4 3 3 1\n8 7 9 5\n6 7 9 8\n\n =\n\n\n1 0 0 0\n2 1 0 0\n4 3 1 0\n3 4 1 1\n\n\n\n\n2 1 1 0\n0 1 1 1\n0 0 2 2\n0 0 0 2\n\n\nis an LU -factorization.\n\nOne of the main reasons why the existence of an LU -factorization for a matrix A is\ninteresting is that if we need to solve several linear systems Ax = b corresponding to the\nsame matrix A, we can do this cheaply by solving the two triangular systems\n\nLw = b, and Ux = w.\n\nThere is a certain asymmetry in the LU -decomposition A = LU of an invertible matrix A.\nIndeed, the diagonal entries of L are all 1, but this is generally false for U . This asymmetry\ncan be eliminated as follows: if\n\nD = diag(u11, u22, . . . , unn)\n\nis the diagonal matrix consisting of the diagonal entries in U (the pivots), then we if let\nU ′ = D−1U , we can write\n\nA = LDU ′,\n\nwhere L is lower- triangular, U ′ is upper-triangular, all diagonal entries of both L and U ′\n\nare 1, and D is a diagonal matrix of pivots. Such a decomposition leads to the following\ndefinition.\n\nDefinition 8.2. We say that an invertible n×n matrix A has an LDU -factorization if it can\nbe written as A = LDU ′, where L is lower- triangular, U ′ is upper-triangular, all diagonal\nentries of both L and U ′ are 1, and D is a diagonal matrix.\n\nWe will see shortly than if A is real symmetric, then U ′ = L>.\n\nAs we will see a bit later, real symmetric positive definite matrices satisfy the condition of\nProposition 8.2. Therefore, linear systems involving real symmetric positive definite matrices\ncan be solved by Gaussian elimination without pivoting. Actually, it is possible to do better:\nthis is the Cholesky factorization.\n\nIf a square invertible matrix A has an LU -factorization, then it is possible to find L and U\nwhile performing Gaussian elimination. Recall that at Step k, we pick a pivot πk = a\n\n(k)\nik 6= 0\n\nin the portion consisting of the entries of index j ≥ k of the k-th column of the matrix Ak\nobtained so far, we swap rows i and k if necessary (the pivoting step), and then we zero the\nentries of index j = k + 1, . . . , n in column k. Schematically, we have the following steps:\n\n× × × × ×\n0 × × × ×\n0 × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n\n pivot\n=⇒\n\n\n× × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n0 × × × ×\n0 × × × ×\n\n elim\n=⇒\n\n\n× × × × ×\n0 × × × ×\n0 0 × × ×\n0 0 × × ×\n0 0 × × ×\n\n .\n\n\n\n8.4. LU -FACTORIZATION 235\n\nMore precisely, after permuting row k and row i (the pivoting step), if the entries in column\nk below row k are αk+1k, . . . , αnk, then we add −αjk/πk times row k to row j; this process\nis illustrated below: \n\na\n(k)\nkk\n\na\n(k)\nk+1k\n...\n\na\n(k)\nik\n...\n\na\n(k)\nnk\n\n\npivot\n=⇒\n\n\n\na\n(k)\nik\n\na\n(k)\nk+1k\n...\n\na\n(k)\nkk\n...\n\na\n(k)\nnk\n\n\n=\n\n\n\nπk\nαk+1k\n\n...\nαik\n...\nαnk\n\n\nelim\n=⇒\n\n\n\nπk\n0\n...\n0\n...\n0\n\n\n.\n\nThen if we write `jk = αjk/πk for j = k + 1, . . . , n, the kth column of L is\n\n0\n...\n0\n1\n\n`k+1k\n...\n`nk\n\n\n.\n\nObserve that the signs of the multipliers −αjk/πk have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n .\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the `ij:\n\nL−1 =\n\n\n1 0 0 · · · 0\n−`21 1 0 · · · 0\n−`31 −`32 1 · · · 0\n\n...\n...\n\n...\n. . . 0\n\n−`n1 −`n2 −`n3 · · · 1\n\n .\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = En−1 · · ·E1A,\n\n8.4. LU-FACTORIZATION 235\n\nMore precisely, after permuting row & and row 7 (the pivoting step), if the entries in column\nk below row k are Qp+iz,---,Qnk, then we add —a,;,/m, times row k to row j; this process\nis illustrated below:\n\n‘s i) (me) fe\nk+1k Qesik Ok+1k 0\npivot _ elim\na () al) Qik 0\n(k) (k) ray 0)\nOnk Onk mk\n\nThen if we write Cj, = ajx/m, for j =k+1,...,n, the kth column of L is\n\nObserve that the signs of the multipliers —a,,/a, have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\n1 0 0\nly, 1 0\nL=|n 2 1\n\nFo OOO\n\nCnt ln2 lng\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the ¢,;:\n\n1 0 0 0\n\nbo} 1 0 0\n\nLot =| —f31 —l32 1 0\n: : 0\n\n—lny —ln2 ng 1\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = E,_,--- FA,\n\n\n\n\n236 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nthen\n\nEk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · −`k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · −`nk 0 · · · 1\n\n\nand E−1\n\nk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · `k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · `nk 0 · · · 1\n\n\n,\n\nso the kth column of Ek is the kth column of L−1.\n\nHere is an example illustrating the method.\n\nExample 8.2. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: The first pivot is π1 = 1 in row 1, and we substract\nrow 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n L1 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 0 0 1\n\n .\n\nThe next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and add 0 times row\n2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L2 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n .\n\n\n\n8.5. PA = LU FACTORIZATION 237\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n = A.\n\nWe now show how to extend the above method to deal with pivoting efficiently. This is\nthe PA = LU factorization.\n\n8.5 PA = LU Factorization\n\nThe following easy proposition shows that, in principle, A can be premultiplied by some\npermutation matrix P , so that PA can be converted to upper-triangular form without using\nany pivoting. Permutations are discussed in some detail in Section 30.3, but for now we\njust need this definition. For the precise connection between the notion of permutation (as\ndiscussed in Section 30.3) and permutation matrices, see Problem 8.16.\n\nDefinition 8.3. A permutation matrix is a square matrix that has a single 1 in every row\nand every column and zeros everywhere else.\n\nIt is shown in Section 30.3 that every permutation matrix is a product of transposition\nmatrices (the P (i, k)s), and that P is invertible with inverse P>.\n\nProposition 8.4. Let A be an invertible n × n-matrix. There is some permutation matrix\nP so that (PA)(1 : k, 1 : k) is invertible for k = 1, . . . , n.\n\nProof. The case n = 1 is trivial, and so is the case n = 2 (we swap the rows if necessary). If\nn ≥ 3, we proceed by induction. Since A is invertible, its columns are linearly independent;\nin particular, its first n− 1 columns are also linearly independent. Delete the last column of\nA. Since the remaining n− 1 columns are linearly independent, there are also n− 1 linearly\nindependent rows in the corresponding n × (n − 1) matrix. Thus, there is a permutation\nof these n rows so that the (n − 1) × (n − 1) matrix consisting of the first n − 1 rows is\ninvertible. But then there is a corresponding permutation matrix P1, so that the first n− 1\nrows and columns of P1A form an invertible matrix A′. Applying the induction hypothesis\nto the (n− 1)× (n− 1) matrix A′, we see that there some permutation matrix P2 (leaving\nthe nth row fixed), so that (P2P1A)(1 : k, 1 : k) is invertible, for k = 1, . . . , n − 1. Since A\nis invertible in the first place and P1 and P2 are invertible, P1P2A is also invertible, and we\nare done.\n\nRemark: One can also prove Proposition 8.4 using a clever reordering of the Gaussian\nelimination steps suggested by Trefethen and Bau [174] (Lecture 21). Indeed, we know that\n\n\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices Pi and products of elementary matrices\nEi, so that\n\nAn = En−1Pn−1 · · ·E2P2E1P1A,\n\nwhere U = An is upper-triangular. For example, when n = 4, we have E3P3E2P2E1P1A = U .\nWe can define new matrices E ′1, E\n\n′\n2, E\n\n′\n3 which are still products of elementary matrices so\n\nthat we have\nE ′3E\n\n′\n2E\n′\n1P3P2P1A = U.\n\nIndeed, if we let E ′3 = E3, E ′2 = P3E2P\n−1\n3 , and E ′1 = P3P2E1P\n\n−1\n2 P−1\n\n3 , we easily verify that\neach E ′k is a product of elementary matrices and that\n\nE ′3E\n′\n2E\n′\n1P3P2P1 = E3(P3E2P\n\n−1\n3 )(P3P2E1P\n\n−1\n2 P−1\n\n3 )P3P2P1 = E3P3E2P2E1P1.\n\nIt can also be proven that E ′1, E\n′\n2, E\n\n′\n3 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nE ′k = Pn−1 · · ·Pk+1EkP\n\n−1\nk+1 · · ·P−1\n\nn−1,\n\nand we have\nE ′n−1 · · ·E ′1Pn−1 · · ·P1A = U,\n\nwhere each E ′j is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU -factorization yields the matrices L,U ,\nand P , such that PA = LU . To describe this new method, since the diagonal entries of L\nare 1s, it is convenient to write\n\nL = I + Λ.\n\nThen in assembling the matrix Λ while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of Λ (really Λk−1) that we make on the rows of A\n(really Ak) during a pivoting step involving row k and row i. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand Λ. Here is an example illustrating this method.\n\nExample 8.3. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: We initialize Λ0 = 0 and P0 = I4. The first pivot is\nπ1 = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 0 −2 0\n0 −2 −1 1\n0 −2 −1 −1\n\n Λ1 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices P; and products of elementary matrices\nE;, so that\nAn = n—1P n—1 ute Fo P, EK, P,A,\n\nwhere U = A, is upper-triangular. For example, when n = 4, we have £3 P3E)P,E,P,A = U.\nWe can define new matrices E}, £5, £3 which are still products of elementary matrices so\nthat we have\n\nEEE P3P,P,A = U.\nIndeed, if we let E, = E3, BE, = P3E,P;', and E} = P3P,E,P;'P;*, we easily verify that\neach Ej, is a product of elementary matrices and that\n\nEEE) P3P2P, = E3(P3E2P;')(P3P)E, Py! Ps')P3P)P, = F3P3E2P)E,P,.\n\nIt can also be proven that FE}, £5, E4 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nEy = Pyrite ++ Pei EePay Poh,\n\nand we have\n\nEly EL Py: RA=U,\nwhere each E* is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU-factorization yields the matrices L, U,\nand P, such that PA = LU. To describe this new method, since the diagonal entries of L\nare ls, it is convenient to write\n\nL=I+A.\n\nThen in assembling the matrix A while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of A (really A;,_1) that we make on the rows of A\n(really A;,) during a pivoting step involving row k and row 7. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand A. Here is an example illustrating this method.\n\nExample 8.3. Given\n\n11 1 £0\n\n1 1 —-1 0\nA= A, = 1-1 0 14?\n\n1-1 0 —-1\n\nwe have the following sequence of steps: We initialize Ag = 0 and Po = Jy. The first pivot is\nm7, = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\n11 1 +0 0000 100 0\n0 0 -2 0 100 0 0100\nAg 0 -2 -1 1 Ay 1000 B= 159 1 0\n0 —2 -1 -l 100 0 0001\n\n\n\n\n8.5. PA = LU FACTORIZATION 239\n\nThe next pivot is π2 = −2 in row 3, so we permute row 2 and 3; we also apply this permutation\nto Λ and P :\n\nA′3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n Λ′2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nNext we subtract row 2 from row 4 (and add 0 times row 2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ3 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = Λ3 + I =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n P = P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n\nand\n\nPA =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n .\n\nUsing the idea in the remark before the above example, we can prove the theorem below\nwhich shows the correctness of the algorithm for computing P,L and U using a simple\nadaptation of Gaussian elimination.\n\n\n\n240 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nWe are not aware of a detailed proof of Theorem 8.5 in the standard texts. Although\nGolub and Van Loan [80] state a version of this theorem as their Theorem 3.1.4, they say\nthat “The proof is a messy subscripting argument.” Meyer [124] also provides a sketch of\nproof (see the end of Section 3.10). In view of this situation, we offer a complete proof.\nIt does involve a lot of subscripts and superscripts, but in our opinion, it contains some\ntechniques that go far beyond symbol manipulation.\n\nTheorem 8.5. For every invertible n× n-matrix A, the following hold:\n\n(1) There is some permutation matrix P , some upper-triangular matrix U , and some unit\nlower-triangular matrix L, so that PA = LU (recall, Li i = 1 for i = 1, . . . , n). Fur-\nthermore, if P = I, then L and U are unique and they are produced as a result of\nGaussian elimination without pivoting.\n\n(2) If En−1 . . . E1A = U is the result of Gaussian elimination without pivoting, write as\n\nusual Ak = Ek−1 . . . E1A (with Ak = (a\n(k)\nij )), and let `ik = a\n\n(k)\nik /a\n\n(k)\nkk , with 1 ≤ k ≤ n− 1\n\nand k + 1 ≤ i ≤ n. Then\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n ,\n\nwhere the kth column of L is the kth column of E−1\nk , for k = 1, . . . , n− 1.\n\n(3) If En−1Pn−1 · · ·E1P1A = U is the result of Gaussian elimination with some pivoting,\nwrite Ak = Ek−1Pk−1 · · ·E1P1A, and define Ek\n\nj , with 1 ≤ j ≤ n− 1 and j ≤ k ≤ n− 1,\nsuch that, for j = 1, . . . , n− 2,\n\nEj\nj = Ej\n\nEk\nj = PkE\n\nk−1\nj Pk, for k = j + 1, . . . , n− 1,\n\nand\nEn−1\nn−1 = En−1.\n\nThen,\n\nEk\nj = PkPk−1 · · ·Pj+1EjPj+1 · · ·Pk−1Pk\n\nU = En−1\nn−1 · · ·En−1\n\n1 Pn−1 · · ·P1A,\n\nand if we set\n\nP = Pn−1 · · ·P1\n\nL = (En−1\n1 )−1 · · · (En−1\n\nn−1)−1,\n\n\n\n8.5. PA = LU FACTORIZATION 241\n\nthen\n\nPA = LU. (†1)\n\nFurthermore,\n\n(Ek\nj )−1 = I + Ekj , 1 ≤ j ≤ n− 1, j ≤ k ≤ n− 1,\n\nwhere Ekj is a lower triangular matrix of the form\n\nEkj =\n\n\n\n0 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 0 0 · · · 0\n\n0 · · · `\n(k)\nj+1j 0 · · · 0\n\n...\n...\n\n...\n...\n\n. . .\n...\n\n0 · · · `\n(k)\nnj 0 · · · 0\n\n\n,\n\nwe have\n\nEk\nj = I − Ekj ,\n\nand\n\nEkj = PkEk−1\nj , 1 ≤ j ≤ n− 2, j + 1 ≤ k ≤ n− 1,\n\nwhere Pk = I or else Pk = P (k, i) for some i such that k + 1 ≤ i ≤ n; if Pk 6= I, this\nmeans that (Ek\n\nj )−1 is obtained from (Ek−1\nj )−1 by permuting the entries on rows i and\n\nk in column j. Because the matrices (Ek\nj )−1 are all","extracted_metadata":{"xmp:MetadataDate":["2020-03-10T20:05:09Z"],"access_permission:assemble_document":["true"],"xmp:ModifyDate":["2020-03-10T20:05:09Z"],"access_permission:can_modify":["true"],"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"dcterms:created":["2019-08-02T23:26:13Z"],"pdf:charsPerPage":["264","1","1263","2199","2131","2172","2059","1963","1915","2250","2056","1853","2008","2098","1054","10","22","23","1330","1775","1213","1058","1293","984","1576","1795","1042","1384","833","728","1496","1390","971","1656","1435","1217","1623","1155","1581","1434","1514","1191","20","0","659","1398","944","800","1316","1428","1296","2192","843","572","533","1323","1637","1545","1495","1940","2321","1417","1100","1421","1230","2050","1727","1956","618","952","1103","1573","1942","2105","2520","1646","1622","2210","1774","1636","1365","1283","1378","1691","1142","875","1677","1253","1163","1970","966","2065","1371","1692","1147","1112","1283","1196","929","1279","1080","1344","936","876","1158","41","1256","1172","775","1475","1080","1589","614","1132","1576","1159","2095","1710","1580","1247","1323","872","1335","1106","751","1060","1004","1307","1442","500","956","1539","547","1234","1239","779","1264","951","766","1556","1174","845","1477","983","1187","1773","537","119","1228","2084","1054","912","1109","1049","1101","1267","1445","1091","1337","1143","1455","642","1517","1909","1086","1505","1157","1061","1268","1366","1685","893","946","1377","1260","1457","1634","1491","437","22","1792","1695","1854","1639","1384","1458","1277","1167","1375","1241","1167","1699","1412","923","608","920","886","861","1146","1309","1511","1833","1614","1061","622","1298","1124","1198","1984","1373","1313","1243","850","1038","1276","954","1011","24","1187","149","1040","838","1218","970","1531","1036","1866","1517","771","1239","2200","1542","1736","1877","891","950","1922","1576","1015","1358"],"pdf:docinfo:trapped":["False"],"xmpMM:DocumentID":["uuid:ce41e11f-d564-4756-b319-b610f878631b"],"Content-Length":["20812100"],"pdf:hasMarkedContent":["false"],"pdf:encrypted":["false"],"dcterms:modified":["2020-03-10T20:05:09Z"],"Content-Type":["application/pdf"],"PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"],"pdf:docinfo:created":["2019-08-02T23:26:13Z"],"pdf:hasCollection":["false"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"pdf:hasXFA":["false"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"pdf:unmappedUnicodeCharsPerPage":["1","0","1","1","1","1","0","0","0","0","3","0","0","2","2","0","0","0","0","3","31","30","0","9","0","2","20","15","23","25","25","10","9","1","0","1","2","4","6","4","3","1","0","0","0","1","14","0","3","26","0","37","0","0","0","3","8","2","0","1","3","22","83","47","51","0","6","11","0","1","11","13","7","2","10","7","7","7","11","11","0","7","16","3","20","16","0","15","15","6","3","0","0","0","6","6","0","0","0","0","0","2","2","1","1","0","1","3","9","1","1","3","10","3","0","1","2","7","25","32","22","6","9","0","0","11","14","7","2","1","0","6","0","5","8","0","6","12","11","7","0","20","28","2","2","0","10","0","7","9","5","3","33","16","40","88","55","0","7","0","9","10","14","17","8","3","0","1","6","15","3","39","10","15","38","3","15","24","5","0","1","4","3","8","13","3","2","11","6","0","21","27","9","12","23","23","0","80","48","7","8","7","2","25","1","2","4","9","5","0","3","29","19","1","46","56","0","0","0","0","0","0","0","2","1","0","0","1","5","0","0","8","8","10","16","4","3","20","2","7"],"xmp:CreatorTool":["TeX"],"access_permission:extract_content":["true"],"access_permission:modify_annotations":["true"],"pdf:docinfo:creator_tool":["TeX"],"pdf:hasXMP":["true"],"xmp:CreateDate":["2019-08-02T23:26:13Z"],"xmpTPg:NPages":["1962"],"access_permission:fill_in_form":["true"],"access_permission:can_print_degraded":["true"],"dc:format":["application/pdf; version=1.5"],"access_permission:extract_for_accessibility":["true"],"pdf:docinfo:producer":["pdfTeX-1.40.17"],"pdf:PDFVersion":["1.5"],"pdf:docinfo:modified":["2020-03-10T20:05:09Z"],"resourceName":["Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf"],"access_permission:can_print":["true"],"pdf:producer":["pdfTeX-1.40.17"],"pdf:docinfo:custom:PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"]},"metadata_field_count":39,"attempts":1,"timestamp":1754065116.0970864,"platform":"Linux","python_version":"3.13.5"},{"file_path":"test_documents/pdfs/Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf","file_size":35493143,"file_type":"pdf","category":"large","framework":"extractous","iteration":2,"extraction_time":29.03472399711609,"startup_time":null,"peak_memory_mb":477.671875,"avg_memory_mb":486.66640625,"peak_cpu_percent":31.9,"avg_cpu_percent":6.38,"total_io_mb":null,"status":"success","character_count":500000,"word_count":81636,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":81636,"sentence_count":5185,"paragraph_count":3135,"avg_word_length":5.082439119996081,"avg_sentence_length":15.972999035679846,"extraction_completeness":1.0,"text_coherence":0.751147842056933,"noise_ratio":0.31442000000000003,"gibberish_ratio":0.021897810218978103,"flesch_reading_ease":40.31429394808873,"gunning_fog_index":15.859068527521277,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":false,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5587323279493441,"extracted_text":"\nThis electronic edition is for non-commercial purposes only.\n\nBayesian Data Analysis\n\nThird edition\n\n(with errors fixed as of 13 February 2020)\n\nAndrew Gelman\n\nColumbia University\n\nJohn B. Carlin\n\nUniversity of Melbourne\n\nHal S. Stern\n\nUniversity of California, Irvine\n\nDavid B. Dunson\n\nDuke University\n\nAki Vehtari\n\nAalto University\n\nDonald B. Rubin\n\nHarvard University\n\nCopyright c©1995–2020, by Andrew Gelman, John Carlin, Hal Stern,\n\nDonald Rubin, David Dunson, and Aki Vehtari\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nContents\n\nPreface xiii\n\nPart I: Fundamentals of Bayesian Inference 1\n\n1 Probability and inference 3\n1.1 The three steps of Bayesian data analysis 3\n1.2 General notation for statistical inference 4\n1.3 Bayesian inference 6\n1.4 Discrete examples: genetics and spell checking 8\n1.5 Probability as a measure of uncertainty 11\n1.6 Example: probabilities from football point spreads 13\n1.7 Example: calibration for record linkage 16\n1.8 Some useful results from probability theory 19\n1.9 Computation and software 22\n1.10 Bayesian inference in applied statistics 24\n1.11 Bibliographic note 25\n1.12 Exercises 27\n\n2 Single-parameter models 29\n2.1 Estimating a probability from binomial data 29\n2.2 Posterior as compromise between data and prior information 32\n2.3 Summarizing posterior inference 32\n2.4 Informative prior distributions 34\n2.5 Normal distribution with known variance 39\n2.6 Other standard single-parameter models 42\n2.7 Example: informative prior distribution for cancer rates 46\n2.8 Noninformative prior distributions 51\n2.9 Weakly informative prior distributions 55\n2.10 Bibliographic note 56\n2.11 Exercises 57\n\n3 Introduction to multiparameter models 63\n3.1 Averaging over ‘nuisance parameters’ 63\n3.2 Normal data with a noninformative prior distribution 64\n3.3 Normal data with a conjugate prior distribution 67\n3.4 Multinomial model for categorical data 69\n3.5 Multivariate normal model with known variance 70\n3.6 Multivariate normal with unknown mean and variance 72\n3.7 Example: analysis of a bioassay experiment 74\n3.8 Summary of elementary modeling and computation 78\n3.9 Bibliographic note 78\n3.10 Exercises 79\n\nvii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nviii CONTENTS\n\n4 Asymptotics and connections to non-Bayesian approaches 83\n4.1 Normal approximations to the posterior distribution 83\n4.2 Large-sample theory 87\n4.3 Counterexamples to the theorems 89\n4.4 Frequency evaluations of Bayesian inferences 91\n4.5 Bayesian interpretations of other statistical methods 92\n4.6 Bibliographic note 97\n4.7 Exercises 98\n\n5 Hierarchical models 101\n5.1 Constructing a parameterized prior distribution 102\n5.2 Exchangeability and hierarchical models 104\n5.3 Bayesian analysis of conjugate hierarchical models 108\n5.4 Normal model with exchangeable parameters 113\n5.5 Example: parallel experiments in eight schools 119\n5.6 Hierarchical modeling applied to a meta-analysis 124\n5.7 Weakly informative priors for variance parameters 128\n5.8 Bibliographic note 132\n5.9 Exercises 134\n\nPart II: Fundamentals of Bayesian Data Analysis 139\n\n6 Model checking 141\n6.1 The place of model checking in applied Bayesian statistics 141\n6.2 Do the inferences from the model make sense? 142\n6.3 Posterior predictive checking 143\n6.4 Graphical posterior predictive checks 153\n6.5 Model checking for the educational testing example 159\n6.6 Bibliographic note 161\n6.7 Exercises 163\n\n7 Evaluating, comparing, and expanding models 165\n7.1 Measures of predictive accuracy 166\n7.2 Information criteria and cross-validation 169\n7.3 Model comparison based on predictive performance 178\n7.4 Model comparison using Bayes factors 182\n7.5 Continuous model expansion 184\n7.6 Implicit assumptions and model expansion: an example 187\n7.7 Bibliographic note 192\n7.8 Exercises 194\n\n8 Modeling accounting for data collection 197\n8.1 Bayesian inference requires a model for data collection 197\n8.2 Data-collection models and ignorability 199\n8.3 Sample surveys 205\n8.4 Designed experiments 214\n8.5 Sensitivity and the role of randomization 218\n8.6 Observational studies 220\n8.7 Censoring and truncation 224\n8.8 Discussion 229\n8.9 Bibliographic note 229\n8.10 Exercises 230\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS ix\n\n9 Decision analysis 237\n9.1 Bayesian decision theory in different contexts 237\n9.2 Using regression predictions: survey incentives 239\n9.3 Multistage decision making: medical screening 245\n9.4 Hierarchical decision analysis for home radon 246\n9.5 Personal vs. institutional decision analysis 256\n9.6 Bibliographic note 257\n9.7 Exercises 257\n\nPart III: Advanced Computation 259\n\n10 Introduction to Bayesian computation 261\n10.1 Numerical integration 261\n10.2 Distributional approximations 262\n10.3 Direct simulation and rejection sampling 263\n10.4 Importance sampling 265\n10.5 How many simulation draws are needed? 267\n10.6 Computing environments 268\n10.7 Debugging Bayesian computing 270\n10.8 Bibliographic note 271\n10.9 Exercises 272\n\n11 Basics of Markov chain simulation 275\n11.1 Gibbs sampler 276\n11.2 Metropolis and Metropolis-Hastings algorithms 278\n11.3 Using Gibbs and Metropolis as building blocks 280\n11.4 Inference and assessing convergence 281\n11.5 Effective number of simulation draws 286\n11.6 Example: hierarchical normal model 288\n11.7 Bibliographic note 291\n11.8 Exercises 291\n\n12 Computationally efficient Markov chain simulation 293\n12.1 Efficient Gibbs samplers 293\n12.2 Efficient Metropolis jumping rules 295\n12.3 Further extensions to Gibbs and Metropolis 297\n12.4 Hamiltonian Monte Carlo 300\n12.5 Hamiltonian Monte Carlo for a hierarchical model 305\n12.6 Stan: developing a computing environment 307\n12.7 Bibliographic note 308\n12.8 Exercises 309\n\n13 Modal and distributional approximations 311\n13.1 Finding posterior modes 311\n13.2 Boundary-avoiding priors for modal summaries 313\n13.3 Normal and related mixture approximations 318\n13.4 Finding marginal posterior modes using EM 320\n13.5 Conditional and marginal posterior approximations 325\n13.6 Example: hierarchical normal model (continued) 326\n13.7 Variational inference 331\n13.8 Expectation propagation 338\n13.9 Other approximations 343\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nx CONTENTS\n\n13.10 Unknown normalizing factors 345\n\n13.11 Bibliographic note 348\n\n13.12 Exercises 349\n\nPart IV: Regression Models 351\n\n14 Introduction to regression models 353\n\n14.1 Conditional modeling 353\n\n14.2 Bayesian analysis of classical regression 354\n\n14.3 Regression for causal inference: incumbency and voting 358\n\n14.4 Goals of regression analysis 364\n\n14.5 Assembling the matrix of explanatory variables 365\n\n14.6 Regularization and dimension reduction 367\n\n14.7 Unequal variances and correlations 369\n\n14.8 Including numerical prior information 376\n\n14.9 Bibliographic note 378\n\n14.10 Exercises 378\n\n15 Hierarchical linear models 381\n\n15.1 Regression coefficients exchangeable in batches 382\n\n15.2 Example: forecasting U.S. presidential elections 383\n\n15.3 Interpreting a normal prior distribution as extra data 388\n\n15.4 Varying intercepts and slopes 390\n\n15.5 Computation: batching and transformation 392\n\n15.6 Analysis of variance and the batching of coefficients 395\n\n15.7 Hierarchical models for batches of variance components 398\n\n15.8 Bibliographic note 400\n\n15.9 Exercises 402\n\n16 Generalized linear models 405\n\n16.1 Standard generalized linear model likelihoods 406\n\n16.2 Working with generalized linear models 407\n\n16.3 Weakly informative priors for logistic regression 412\n\n16.4 Overdispersed Poisson regression for police stops 420\n\n16.5 State-level opinons from national polls 422\n\n16.6 Models for multivariate and multinomial responses 423\n\n16.7 Loglinear models for multivariate discrete data 428\n\n16.8 Bibliographic note 431\n\n16.9 Exercises 432\n\n17 Models for robust inference 435\n\n17.1 Aspects of robustness 435\n\n17.2 Overdispersed versions of standard models 437\n\n17.3 Posterior inference and computation 439\n\n17.4 Robust inference for the eight schools 441\n\n17.5 Robust regression using t-distributed errors 444\n\n17.6 Bibliographic note 445\n\n17.7 Exercises 446\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nCONTENTS xi\n\n18 Models for missing data 449\n18.1 Notation 449\n18.2 Multiple imputation 451\n18.3 Missing data in the multivariate normal and t models 454\n18.4 Example: multiple imputation for a series of polls 456\n18.5 Missing values with counted data 462\n18.6 Example: an opinion poll in Slovenia 463\n18.7 Bibliographic note 466\n18.8 Exercises 467\n\nPart V: Nonlinear and Nonparametric Models 469\n\n19 Parametric nonlinear models 471\n19.1 Example: serial dilution assay 471\n19.2 Example: population toxicokinetics 477\n19.3 Bibliographic note 485\n19.4 Exercises 486\n\n20 Basis function models 487\n20.1 Splines and weighted sums of basis functions 487\n20.2 Basis selection and shrinkage of coefficients 490\n20.3 Non-normal models and regression surfaces 494\n20.4 Bibliographic note 498\n20.5 Exercises 498\n\n21 Gaussian process models 501\n21.1 Gaussian process regression 501\n21.2 Example: birthdays and birthdates 505\n21.3 Latent Gaussian process models 510\n21.4 Functional data analysis 512\n21.5 Density estimation and regression 513\n21.6 Bibliographic note 516\n21.7 Exercises 516\n\n22 Finite mixture models 519\n22.1 Setting up and interpreting mixture models 519\n22.2 Example: reaction times and schizophrenia 524\n22.3 Label switching and posterior computation 533\n22.4 Unspecified number of mixture components 536\n22.5 Mixture models for classification and regression 539\n22.6 Bibliographic note 542\n22.7 Exercises 543\n\n23 Dirichlet process models 545\n23.1 Bayesian histograms 545\n23.2 Dirichlet process prior distributions 546\n23.3 Dirichlet process mixtures 549\n23.4 Beyond density estimation 557\n23.5 Hierarchical dependence 560\n23.6 Density regression 568\n23.7 Bibliographic note 571\n23.8 Exercises 573\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxii CONTENTS\n\nAppendixes 575\n\nA Standard probability distributions 577\nA.1 Continuous distributions 577\nA.2 Discrete distributions 585\nA.3 Bibliographic note 586\n\nB Outline of proofs of limit theorems 587\nB.1 Bibliographic note 590\n\nC Computation in R and Stan 591\nC.1 Getting started with R and Stan 591\nC.2 Fitting a hierarchical model in Stan 592\nC.3 Direct simulation, Gibbs, and Metropolis in R 596\nC.4 Programming Hamiltonian Monte Carlo in R 603\nC.5 Further comments on computation 607\nC.6 Bibliographic note 608\n\nReferences 609\n\nAuthor Index 643\n\nSubject Index 654\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPreface\n\nThis book is intended to have three roles and to serve three associated audiences: an\nintroductory text on Bayesian inference starting from first principles, a graduate text on\neffective current approaches to Bayesian modeling and computation in statistics and related\nfields, and a handbook of Bayesian methods in applied statistics for general users of and\nresearchers in applied statistics. Although introductory in its early sections, the book is\ndefinitely not elementary in the sense of a first text in statistics. The mathematics used\nin our book is basic probability and statistics, elementary calculus, and linear algebra. A\nreview of probability notation is given in Chapter 1 along with a more detailed list of topics\nassumed to have been studied. The practical orientation of the book means that the reader’s\nprevious experience in probability, statistics, and linear algebra should ideally have included\nstrong computational components.\n\nTo write an introductory text alone would leave many readers with only a taste of the\nconceptual elements but no guidance for venturing into genuine practical applications, be-\nyond those where Bayesian methods agree essentially with standard non-Bayesian analyses.\nOn the other hand, we feel it would be a mistake to present the advanced methods with-\nout first introducing the basic concepts from our data-analytic perspective. Furthermore,\ndue to the nature of applied statistics, a text on current Bayesian methodology would be\nincomplete without a variety of worked examples drawn from real applications. To avoid\ncluttering the main narrative, there are bibliographic notes at the end of each chapter and\nreferences at the end of the book.\n\nExamples of real statistical analyses appear throughout the book, and we hope thereby\nto give an applied flavor to the entire development. Indeed, given the conceptual simplicity\nof the Bayesian approach, it is only in the intricacy of specific applications that novelty\narises. Non-Bayesian approaches dominated statistical theory and practice for most of the\nlast century, but the last few decades have seen a re-emergence of Bayesian methods. This\nhas been driven more by the availability of new computational techniques than by what\nmany would see as the theoretical and logical advantages of Bayesian thinking.\n\nIn our treatment of Bayesian inference, we focus on practice rather than philosophy. We\ndemonstrate our attitudes via examples that have arisen in the applied research of ourselves\nand others. Chapter 1 presents our views on the foundations of probability as empirical\nand measurable; see in particular Sections 1.4–1.7.\n\nChanges for the third edition\n\nThe biggest change for this new edition is the addition of Chapters 20–23 on nonparametric\nmodeling. Other major changes include weakly informative priors in Chapters 2, 5, and\nelsewhere; boundary-avoiding priors in Chapter 13; an updated discussion of cross-validation\nand predictive information criteria in the new Chapter 7; improved convergence monitoring\nand effective sample size calculations for iterative simulation in Chapter 11; presentations of\nHamiltonian Monte Carlo, variational Bayes, and expectation propagation in Chapters 12\nand 13; and new and revised code in Appendix C. We have made other changes throughout.\n\nDuring the eighteen years since completing the first edition of Bayesian Data Analysis,\nwe have worked on dozens of interesting applications which, for reasons of space, we are not\nable to add to this new edition. Many of these examples appear in our book, Data Analysis\n\nxiii\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nxiv PREFACE\n\nUsing Regression and Hierarchical/Multilevel Models, as well as in our published research\narticles.\n\nWe have made some small corrections and updates for the second printing of the third\nedition.\n\nOnline information\n\nAdditional materials, including the data used in the examples, solutions to many of the\nend-of-chapter exercises, and any errors found after the book goes to press, are posted at\nhttp://www.stat.columbia.edu/∼gelman/book/. Feel free to send any comments to us\ndirectly.\n\nAcknowledgments\n\nWe thank many students, colleagues, and friends for comments and advice and also ac-\nknowledge the public funding that made much of this work possible.\n\nIn particular, we thank Stephen Ansolabehere, Adriano Azevedo, Jarrett Barber, Richard\nBarker, Tom Belin, Michael Betancourt, Suzette Blanchard, Rob Calver, Brad Carlin, Bob\nCarpenter, Alicia Carriquiry, Samantha Cook, Alex Damour, Victor De Oliveira, Vince\nDorie, David Draper, Greg Dropkin, John Emerson, Steve Fienberg, Alex Franks, Byron\nGajewski, Yuanjun Gao, Daniel Gianola, Yuri Goegebeur, David Hammill, Chad Heilig,\nMatt Hoffman, Chuanpu Hu, Zaiying Huang, Shane Jensen, Yoon-Sook Jeon, Pasi Jy-\nlanki, Jay Kadane, Jouni Kerman, Gary King, Lucien Le Cam, Yew Jin Lim, Rod Little,\nTom Little, Chuanhai Liu, Xuecheng Liu, Tomoki Matsumoto, Peter McCullagh, Mary\nSara McPeek, Xiao-Li Meng, Baback Moghaddam, Sergei Morozov, Jarad Niemi, Olivier\nNimeskern, Peter Norvig, Ali Rahimi, Thomas Richardson, Christian Robert, Scott Schmi-\ndler, Matt Schofield, Andrea Siegel, Raghav Singal, Sandip Sinharay, Elizabeth Stuart,\nDwight Sunada, Andrew Swift, Eric Tassone, Francis Tuerlinckx, Iven Van Mechelen,\nAmos Waterland, Rob Weiss, Lo-Hua Yuan, and Alan Zaslavsky. We especially thank\nJohn Boscardin, Jessica Hwang, Daniel Lee, Phillip Price, and Radford Neal.\n\nThis work was partially supported by research grants from the National Science Foun-\ndation, National Institutes of Health, Institute of Education Sciences, National Security\nAgency, Department of Energy, and Academy of Finland.\n\nMany of our examples have appeared in books and articles written by ourselves and\nothers, as we indicate in the bibliographic notes and exercises in the chapters where they\nappear.1\n\nFinally, we thank Caroline, Nancy, Hara, Amy, Ilona, and other family and friends for\ntheir love and support during the writing and revision of this book.\n\n1In particular: Figures 1.3–1.5 are adapted from the Journal of the American Statistical Association 90\n(1995), pp. 696, 702, and 703, and are reprinted with permission of the American Statistical Association.\nFigures 2.6 and 2.7 come from Gelman, A., and Nolan, D., Teaching Statistics: A Bag of Tricks, Oxford\nUniversity Press (1992), pp. 14 and 15, and are reprinted with permission of Oxford University Press.\nFigures 19.8–19.10 come from the Journal of the American Statistical Association 91 (1996), pp. 1407 and\n1409, and are reprinted with permission of the American Statistical Association. Table 19.1 comes from\nBerry, D., Statistics: A Bayesian Perspective, first edition, copyright 1996 Wadsworth, a part of Cengage\nLearning, Inc. Reproduced by permission. www.cengage.com/permissions. Figures 18.1 and 18.2 come\nfrom the Journal of the American Statistical Association 93 (1998), pp. 851 and 853, and are reprinted\nwith permission of the American Statistical Association. Figures 9.1–9.3 are adapted from the Journal of\n\nBusiness and Economic Statistics 21 (2003), pp. 219 and 223, and are reprinted with permission of the\nAmerican Statistical Association. We thank Jack Taylor for the data used to produce Figure 23.4.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart I: Fundamentals of Bayesian Inference\n\nBayesian inference is the process of fitting a probability model to a set of data and sum-\nmarizing the result by a probability distribution on the parameters of the model and on\nunobserved quantities such as predictions for new observations. In Chapters 1–3, we in-\ntroduce several useful families of models and illustrate their application in the analysis of\nrelatively simple data structures. Some mathematics arises in the analytical manipulation of\nthe probability distributions, notably in transformation and integration in multiparameter\nproblems. We differ somewhat from other introductions to Bayesian inference by emphasiz-\ning stochastic simulation, and the combination of mathematical analysis and simulation, as\ngeneral methods for summarizing distributions. Chapter 4 outlines the fundamental con-\nnections between Bayesian and other approaches to statistical inference. The early chapters\nfocus on simple examples to develop the basic ideas of Bayesian inference; examples in which\nthe Bayesian approach makes a practical difference relative to more traditional approaches\nbegin to appear in Chapter 3. The major practical advantages of the Bayesian approach\nappear in Chapter 5, where we introduce hierarchical models, which allow the parameters\nof a prior, or population, distribution themselves to be estimated from data.\n\n1\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 1\n\nProbability and inference\n\n1.1 The three steps of Bayesian data analysis\n\nThis book is concerned with practical methods for making inferences from data using prob-\nability models for quantities we observe and for quantities about which we wish to learn.\nThe essential characteristic of Bayesian methods is their explicit use of probability for quan-\ntifying uncertainty in inferences based on statistical data analysis.\n\nThe process of Bayesian data analysis can be idealized by dividing it into the following\nthree steps:\n\n1. Setting up a full probability model—a joint probability distribution for all observable and\nunobservable quantities in a problem. The model should be consistent with knowledge\nabout the underlying scientific problem and the data collection process.\n\n2. Conditioning on observed data: calculating and interpreting the appropriate posterior\ndistribution—the conditional probability distribution of the unobserved quantities of ul-\ntimate interest, given the observed data.\n\n3. Evaluating the fit of the model and the implications of the resulting posterior distribution:\nhow well does the model fit the data, are the substantive conclusions reasonable, and\nhow sensitive are the results to the modeling assumptions in step 1? In response, one\ncan alter or expand the model and repeat the three steps.\n\nGreat advances in all these areas have been made in the last forty years, and many\nof these are reviewed and used in examples throughout the book. Our treatment covers\nall three steps, the second involving computational methodology and the third a delicate\nbalance of technique and judgment, guided by the applied context of the problem. The first\nstep remains a major stumbling block for much Bayesian analysis: just where do our models\ncome from? How do we go about constructing appropriate probability specifications? We\nprovide some guidance on these issues and illustrate the importance of the third step in\nretrospectively evaluating the fit of models. Along with the improved techniques available\nfor computing conditional probability distributions in the second step, advances in carrying\nout the third step alleviate to some degree the need to assume correct model specification at\nthe first attempt. In particular, the much-feared dependence of conclusions on ‘subjective’\nprior distributions can be examined and explored.\n\nA primary motivation for Bayesian thinking is that it facilitates a common-sense in-\nterpretation of statistical conclusions. For instance, a Bayesian (probability) interval for\nan unknown quantity of interest can be directly regarded as having a high probability of\ncontaining the unknown quantity, in contrast to a frequentist (confidence) interval, which\nmay strictly be interpreted only in relation to a sequence of similar inferences that might\nbe made in repeated practice. Recently in applied statistics, increased emphasis has been\nplaced on interval estimation rather than hypothesis testing, and this provides a strong im-\npetus to the Bayesian viewpoint, since it seems likely that most users of standard confidence\nintervals give them a common-sense Bayesian interpretation. One of our aims in this book\n\n3\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4 1. PROBABILITY AND INFERENCE\n\nis to indicate the extent to which Bayesian interpretations of common simple statistical\nprocedures are justified.\n\nRather than argue the foundations of statistics—see the bibliographic note at the end\nof this chapter for references to foundational debates—we prefer to concentrate on the\npragmatic advantages of the Bayesian framework, whose flexibility and generality allow\nit to cope with complex problems. The central feature of Bayesian inference, the direct\nquantification of uncertainty, means that there is no impediment in principle to fitting\nmodels with many parameters and complicated multilayered probability specifications. In\npractice, the problems are ones of setting up and computing with such large models, and\na large part of this book focuses on recently developed and still developing techniques\nfor handling these modeling and computational challenges. The freedom to set up complex\nmodels arises in large part from the fact that the Bayesian paradigm provides a conceptually\nsimple method for coping with multiple parameters, as we discuss in detail from Chapter 3\non.\n\n1.2 General notation for statistical inference\n\nStatistical inference is concerned with drawing conclusions, from numerical data, about\nquantities that are not observed. For example, a clinical trial of a new cancer drug might\nbe designed to compare the five-year survival probability in a population given the new drug\nto that in a population under standard treatment. These survival probabilities refer to a\nlarge population of patients, and it is neither feasible nor ethically acceptable to experiment\non an entire population. Therefore inferences about the true probabilities and, in particular,\ntheir differences must be based on a sample of patients. In this example, even if it were\npossible to expose the entire population to one or the other treatment, it is never possible to\nexpose anyone to both treatments, and therefore statistical inference would still be needed to\nassess the causal inference—the comparison between the observed outcome in each patient\nand that patient’s unobserved outcome if exposed to the other treatment.\n\nWe distinguish between two kinds of estimands—unobserved quantities for which sta-\ntistical inferences are made—first, potentially observable quantities, such as future obser-\nvations of a process, or the outcome under the treatment not received in the clinical trial\nexample; and second, quantities that are not directly observable, that is, parameters that\ngovern the hypothetical process leading to the observed data (for example, regression coef-\nficients). The distinction between these two kinds of estimands is not always precise, but is\ngenerally useful as a way of understanding how a statistical model for a particular problem\nfits into the real world.\n\nParameters, data, and predictions\n\nAs general notation, we let θ denote unobservable vector quantities or population parameters\nof interest (such as the probabilities of survival under each treatment for randomly chosen\nmembers of the population in the example of the clinical trial), y denote the observed\ndata (such as the numbers of survivors and deaths in each treatment group), and ỹ denote\nunknown, but potentially observable, quantities (such as the outcomes of the patients under\nthe other treatment, or the outcome under each of the treatments for a new patient similar\nto those already in the trial). In general these symbols represent multivariate quantities.\nWe generally use Greek letters for parameters, lower case Roman letters for observed or\nobservable scalars and vectors (and sometimes matrices), and upper case Roman letters\nfor observed or observable matrices. When using matrix notation, we consider vectors as\ncolumn vectors throughout; for example, if u is a vector with n components, then uTu is a\nscalar and uuT an n× n matrix.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.2. GENERAL NOTATION FOR STATISTICAL INFERENCE 5\n\nObservational units and variables\n\nIn many statistical studies, data are gathered on each of a set of n objects or units, and\nwe can write the data as a vector, y = (y1, . . . , yn). In the clinical trial example, we might\nlabel yi as 1 if patient i is alive after five years or 0 if the patient dies. If several variables\nare measured on each unit, then each yi is actually a vector, and the entire dataset y is a\nmatrix (usually taken to have n rows). The y variables are called the ‘outcomes’ and are\nconsidered ‘random’ in the sense that, when making inferences, we wish to allow for the\npossibility that the observed values of the variables could have turned out otherwise, due\nto the sampling process and the natural variation of the population.\n\nExchangeability\n\nThe usual starting point of a statistical analysis is the (often tacit) assumption that the\nn values yi may be regarded as exchangeable, meaning that we express uncertainty as a\njoint probability density p(y1, . . . , yn) that is invariant to permutations of the indexes. A\nnonexchangeable model would be appropriate if information relevant to the outcome were\nconveyed in the unit indexes rather than by explanatory variables (see below). The idea of\nexchangeability is fundamental to statistics, and we return to it repeatedly throughout the\nbook.\n\nWe commonly model data from an exchangeable distribution as independently and iden-\ntically distributed (iid) given some unknown parameter vector θ with distribution p(θ). In\nthe clinical trial example, we might model the outcomes yi as iid, given θ, the unknown\nprobability of survival.\n\nExplanatory variables\n\nIt is common to have observations on each unit that we do not bother to model as random.\nIn the clinical trial example, such variables might include the age and previous health status\nof each patient in the study. We call this second class of variables explanatory variables, or\ncovariates, and label them x. We use X to denote the entire set of explanatory variables\nfor all n units; if there are k explanatory variables, then X is a matrix with n rows and k\ncolumns. Treating X as random, the notion of exchangeability can be extended to require\nthe distribution of the n values of (x, y)i to be unchanged by arbitrary permutations of\nthe indexes. It is always appropriate to assume an exchangeable model after incorporating\nsufficient relevant information inX that the indexes can be thought of as randomly assigned.\nIt follows from the assumption of exchangeability that the distribution of y, given x, is the\nsame for all units in the study in the sense that if two units have the same value of x, then\ntheir distributions of y are the same. Any of the explanatory variables x can be moved into\nthe y category if we wish to model them. We discuss the role of explanatory variables (also\ncalled predictors) in detail in Chapter 8 in the context of analyzing surveys, experiments,\nand observational studies, and in the later parts of this book in the context of regression\nmodels.\n\nHierarchical modeling\n\nIn Chapter 5 and subsequent chapters, we focus on hierarchical models (also called mul-\ntilevel models), which are used when information is available on several different levels of\nobservational units. In a hierarchical model, it is possible to speak of exchangeability at\neach level of units. For example, suppose two medical treatments are applied, in separate\nrandomized experiments, to patients in several different cities. Then, if no other information\nwere available, it would be reasonable to treat the patients within each city as exchangeable\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6 1. PROBABILITY AND INFERENCE\n\nand also treat the results from different cities as themselves exchangeable. In practice it\nwould make sense to include, as explanatory variables at the city level, whatever relevant\ninformation we have on each city, as well as the explanatory variables mentioned before at\nthe individual level, and then the conditional distributions given these explanatory variables\nwould be exchangeable.\n\n1.3 Bayesian inference\n\nBayesian statistical conclusions about a parameter θ, or unobserved data ỹ, are made in\nterms of probability statements. These probability statements are conditional on the ob-\nserved value of y, and in our notation are written simply as p(θ|y) or p(ỹ|y). We also\nimplicitly condition on the known values of any covariates, x. It is at the fundamental\nlevel of conditioning on observed data that Bayesian inference departs from the approach\nto statistical inference described in many textbooks, which is based on a retrospective eval-\nuation of the procedure used to estimate θ (or ỹ) over the distribution of possible y values\nconditional on the true unknown value of θ. Despite this difference, it will be seen that\nin many simple analyses, superficially similar conclusions result from the two approaches\nto statistical inference. However, analyses obtained using Bayesian methods can be easily\nextended to more complex problems. In this section, we present the basic mathematics and\nnotation of Bayesian inference, followed in the next section by an example from genetics.\n\nProbability notation\nSome comments on notation are needed at this point. First, p(·|·) denotes a conditional\nprobability density with the arguments determined by the context, and similarly for\np(·), which denotes a marginal distribution. We use the terms ‘distribution’ and\n‘density’ interchangeably. The same notation is used for continuous density functions\nand discrete probability mass functions. Different distributions in the same equation\n(or expression) will each be denoted by p(·), as in (1.1) below, for example. Although\nan abuse of standard mathematical notation, this method is compact and similar to\nthe standard practice of using p(·) for the probability of any discrete event, where\nthe sample space is also suppressed in the notation. Depending on context, to avoid\nconfusion, we may use the notation Pr(·) for the probability of an event; for example,\nPr(θ > 2) =\n\n∫\nθ>2\n\np(θ)dθ. When using a standard distribution, we use a notation based\non the name of the distribution; for example, if θ has a normal distribution with mean\nµ and variance σ2, we write θ ∼ N(µ, σ2) or p(θ) = N(θ|µ, σ2) or, to be even more\nexplicit, p(θ|µ, σ2) = N(θ|µ, σ2). Throughout, we use notation such as N(µ, σ2) for\nrandom variables and N(θ|µ, σ2) for density functions. Notation and formulas for\nseveral standard distributions appear in Appendix A.\nWe also occasionally use the following expressions for random variables θ: the coeffi-\ncient of variation is defined as sd(θ)/E(θ), the geometric mean is exp(E[log(θ)]), and\nthe geometric standard deviation is exp(sd[log(θ)]).\n\nBayes’ rule\n\nIn order to make probability statements about θ given y, we must begin with a model\nproviding a joint probability distribution for θ and y. The joint probability mass or density\nfunction can be written as a product of two densities that are often referred to as the prior\ndistribution p(θ) and the sampling distribution (or data distribution) p(y|θ), respectively:\n\np(θ, y) = p(θ)p(y|θ).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.3. BAYESIAN INFERENCE 7\n\nSimply conditioning on the known value of the data y, using the basic property of conditional\nprobability known as Bayes’ rule, yields the posterior density:\n\np(θ|y) = p(θ, y)\n\np(y)\n=\np(θ)p(y|θ)\np(y)\n\n, (1.1)\n\nwhere p(y) =\n∑\n\nθp(θ)p(y|θ), and the sum is over all possible values of θ (or p(y) =∫\np(θ)p(y|θ)dθ in the case of continuous θ). An equivalent form of (1.1) omits the fac-\n\ntor p(y), which does not depend on θ and, with fixed y, can thus be considered a constant,\nyielding the unnormalized posterior density, which is the right side of (1.2):\n\np(θ|y) ∝ p(θ)p(y|θ). (1.2)\n\nThe second term in this expression, p(y|θ), is taken here as a function of θ, not of y. These\nsimple formulas encapsulate the technical core of Bayesian inference: the primary task of\nany specific application is to develop the model p(θ, y) and perform the computations to\nsummarize p(θ|y) in appropriate ways.\n\nPrediction\n\nTo make inferences about an unknown observable, often called predictive inferences, we\nfollow a similar logic. Before the data y are considered, the distribution of the unknown\nbut observable y is\n\np(y) =\n\n∫\np(y, θ)dθ =\n\n∫\np(θ)p(y|θ)dθ. (1.3)\n\nThis is often called the marginal distribution of y, but a more informative name is the prior\npredictive distribution: prior because it is not conditional on a previous observation of the\nprocess, and predictive because it is the distribution for a quantity that is observable.\n\nAfter the data y have been observed, we can predict an unknown observable, ỹ, from\nthe same process. For example, y = (y1, . . . , yn) may be the vector of recorded weights of\nan object weighed n times on a scale, θ = (µ, σ2) may be the unknown true weight of the\nobject and the measurement variance of the scale, and ỹ may be the yet to be recorded\nweight of the object in a planned new weighing. The distribution of ỹ is called the posterior\npredictive distribution, posterior because it is conditional on the observed y and predictive\nbecause it is a prediction for an observable ỹ:\n\np(ỹ|y) =\n\n∫\np(ỹ, θ|y)dθ\n\n=\n\n∫\np(ỹ|θ, y)p(θ|y)dθ\n\n=\n\n∫\np(ỹ|θ)p(θ|y)dθ. (1.4)\n\nThe second and third lines display the posterior predictive distribution as an average of\nconditional predictions over the posterior distribution of θ. The last step follows from the\nassumed conditional independence of y and ỹ given θ.\n\nLikelihood\n\nUsing Bayes’ rule with a chosen probability model means that the data y affect the posterior\ninference (1.2) only through p(y|θ), which, when regarded as a function of θ, for fixed y, is\ncalled the likelihood function. In this way Bayesian inference is obeying what is sometimes\ncalled the likelihood principle.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n8 1. PROBABILITY AND INFERENCE\n\nThe likelihood principle is reasonable, but only within the framework of the model or\nfamily of models adopted for a particular analysis. In practice, one can rarely be confident\nthat the chosen model is correct. We shall see in Chapter 6 that sampling distributions\n(imagining repeated realizations of our data) can play an important role in checking model\nassumptions. In fact, our view of an applied Bayesian statistician is one who is willing to\napply Bayes’ rule under a variety of possible models.\n\nLikelihood and odds ratios\n\nThe ratio of the posterior density p(θ|y) evaluated at the points θ1 and θ2 under a given\nmodel is called the posterior odds for θ1 compared to θ2. The most familiar application of\nthis concept is with discrete parameters, with θ2 taken to be the complement of θ1. Odds\nprovide an alternative representation of probabilities and have the attractive property that\nBayes’ rule takes a particularly simple form when expressed in terms of them:\n\np(θ1|y)\np(θ2|y)\n\n=\np(θ1)p(y|θ1)/p(y)\np(θ2)p(y|θ2)/p(y)\n\n=\np(θ1)\n\np(θ2)\n\np(y|θ1)\np(y|θ2)\n\n. (1.5)\n\nIn words, the posterior odds are equal to the prior odds multiplied by the likelihood ratio,\np(y|θ1)/p(y|θ2).\n\n1.4 Discrete examples: genetics and spell checking\n\nWe next demonstrate Bayes’ theorem with two examples in which the immediate goal is\ninference about a particular discrete quantity rather than with the estimation of a parameter\nthat describes an entire population. These discrete examples allow us to see the prior,\nlikelihood, and posterior probabilities directly.\n\nInference about a genetic status\n\nHuman males have one X-chromosome and one Y-chromosome, whereas females have two\nX-chromosomes, each chromosome being inherited from one parent. Hemophilia is a disease\nthat exhibits X-chromosome-linked recessive inheritance, meaning that a male who inherits\nthe gene that causes the disease on the X-chromosome is affected, whereas a female carrying\nthe gene on only one of her two X-chromosomes is not affected. The disease is generally fatal\nfor women who inherit two such genes, and this is rare, since the frequency of occurrence\nof the gene is low in human populations.\n\nPrior distribution. Consider a woman who has an affected brother, which implies that her\nmother must be a carrier of the hemophilia gene with one ‘good’ and one ‘bad’ hemophilia\ngene. We are also told that her father is not affected; thus the woman herself has a fifty-fifty\nchance of having the gene. The unknown quantity of interest, the state of the woman, has\njust two values: the woman is either a carrier of the gene (θ = 1) or not (θ = 0). Based on\nthe information provided thus far, the prior distribution for the unknown θ can be expressed\nsimply as Pr(θ = 1) = Pr(θ = 0) = 1\n\n2 .\n\nData model and likelihood. The data used to update the prior information consist of the\naffection status of the woman’s sons. Suppose she has two sons, neither of whom is affected.\nLet yi=1 or 0 denote an affected or unaffected son, respectively. The outcomes of the two\nsons are exchangeable and, conditional on the unknown θ, are independent; we assume the\nsons are not identical twins. The two items of independent data generate the following\nlikelihood function:\n\nPr(y1=0, y2 = 0 | θ=1) = (0.5)(0.5) = 0.25\n\nPr(y1=0, y2 = 0 | θ=0) = (1)(1) = 1.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.4. DISCRETE EXAMPLES: GENETICS AND SPELL CHECKING 9\n\nThese expressions follow from the fact that if the woman is a carrier, then each of her sons\nwill have a 50% chance of inheriting the gene and so being affected, whereas if she is not a\ncarrier then there is a probability close to 1 that a son of hers will be unaffected. (In fact,\nthere is a nonzero probability of being affected even if the mother is not a carrier, but this\nrisk—the mutation rate—is small and can be ignored for this example.)\n\nPosterior distribution. Bayes’ rule can now be used to combine the information in the\ndata with the prior probability; in particular, interest is likely to focus on the posterior\nprobability that the woman is a carrier. Using y to denote the joint data (y1, y2), this is\nsimply\n\nPr(θ = 1|y) =\np(y|θ = 1)Pr(θ = 1)\n\np(y|θ = 1)Pr(θ = 1) + p(y|θ = 0)Pr(θ = 0)\n\n=\n(0.25)(0.5)\n\n(0.25)(0.5) + (1.0)(0.5)\n=\n\n0.125\n\n0.625\n= 0.20.\n\nIntuitively it is clear that if a woman has unaffected children, it is less probable that she is\na carrier, and Bayes’ rule provides a formal mechanism for determining the extent of the\ncorrection. The results can also be described in terms of prior and posterior odds. The\nprior odds of the woman being a carrier are 0.5/0.5 = 1. The likelihood ratio based on\nthe information about her two unaffected sons is 0.25/1 = 0.25, so the posterior odds are\n1 ·0.25 = 0.25. Converting back to a probability, we obtain 0.25/(1+0.25) = 0.2, as before.\n\nAdding more data. A key aspect of Bayesian analysis is the ease with which sequential\nanalyses can be performed. For example, suppose that the woman has a third son, who\nis also unaffected. The entire calculation does not need to be redone; rather we use the\nprevious posterior distribution as the new prior distribution, to obtain:\n\nPr(θ = 1|y1, y2, y3) =\n(0.5)(0.20)\n\n(0.5)(0.20) + (1)(0.8)\n= 0.111.\n\nAlternatively, if we suppose that the third son is affected, it is easy to check that the\nposterior probability of the woman being a carrier becomes 1 (again ignoring the possibility\nof a mutation).\n\nSpelling correction\n\nClassification of words is a problem of managing uncertainty. For example, suppose someone\ntypes ‘radom.’ How should that be read? It could be a misspelling or mistyping of ‘random’\nor ‘radon’ or some other alternative, or it could be the intentional typing of ‘radom’ (as\nin its first use in this paragraph). What is the probability that ‘radom’ actually means\nrandom? If we label y as the data and θ as the word that the person was intending to type,\nthen\n\nPr(θ | y=‘radom’) ∝ p(θ) Pr(y=‘radom’ | θ). (1.6)\n\nThis product is the unnormalized posterior density. In this case, if for simplicity we consider\nonly three possibilities for the intended word, θ (random, radon, or radom), we can compute\nthe posterior probability of interest by first computing the unnormalized density for all three\nvalues of theta and then normalizing:\n\np(random|‘radom’) =\np(θ1)p(‘radom’|θ1)∑3\nj=1 p(θj)p(‘radom’|θj)\n\n,\n\nwhere θ1=random, θ2=radon, and θ3=radom. The prior probabilities p(θj) can most simply\ncome from frequencies of these words in some large database, ideally one that is adapted\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n10 1. PROBABILITY AND INFERENCE\n\nto the problem at hand (for example, a database of recent student emails if the word in\nquestion is appearing in such a document). The likelihoods p(y|θj) can come from some\nmodeling of spelling and typing errors, perhaps fit using some study in which people were\nfollowed up after writing emails to identify any questionable words.\n\nPrior distribution. Without any other context, it makes sense to assign the prior proba-\nbilities p(θj) based on the relative frequencies of these three words in some databases. Here\nare probabilities supplied by researchers at Google:\n\nθ p(θ)\nrandom 7.60× 10−5\n\nradon 6.05× 10−6\n\nradom 3.12× 10−7\n\nSince we are considering only these possibilities, we could renormalize the three numbers to\nsum to 1 (p(random) = 760\n\n760+60.5+3.12 , etc.) but there is no need, as the adjustment would\nmerely be absorbed into the proportionality constant in (1.6).\n\nReturning to the table above, we were surprised to see the probability of ‘radom’ in the\ncorpus being as high as it was. We looked up the word in Wikipedia and found that it is a\nmedium-sized city: home to ‘the largest and best-attended air show in Poland . . . also the\npopular unofficial name for a semiautomatic 9 mm Para pistol of Polish design . . . ’ For\nthe documents that we encounter, the relative probability of ‘radom’ seems much too high.\nIf the probabilities above do not seem appropriate for our application, this implies that we\nhave prior information or beliefs that have not yet been included in the model. We shall\nreturn to this point after first working out the model’s implications for this example.\n\nLikelihood. Here are some conditional probabilities from Google’s model of spelling and\ntyping errors:\n\nθ p(‘radom’|θ)\nrandom 0.00193\nradon 0.000143\nradom 0.975\n\nWe emphasize that this likelihood function is not a probability distribution. Rather, it is a\nset of conditional probabilities of a particular outcome (‘radom’) from three different proba-\nbility distributions, corresponding to three different possibilities for the unknown parameter\nθ.\n\nThese particular values look reasonable enough—a 97% chance that this particular five-\nletter word will be typed correctly, a 0.2% chance of obtaining this character string by\nmistakenly dropping a letter from ‘random,’ and a much lower chance of obtaining it by\nmistyping the final letter of ‘radon.’ We have no strong intuition about these probabilities\nand will trust the Google engineers here.\n\nPosterior distribution. We multiply the prior probability and the likelihood to get joint\nprobabilities and then renormalize to get posterior probabilities:\n\nθ p(θ)p(‘radom’|θ) p(θ|‘radom’)\nrandom 1.47× 10−7 0.325\nradon 8.65× 10−10 0.002\nradom 3.04× 10−7 0.673\n\nThus, conditional on the model, the typed word ‘radom’ is about twice as likely to be correct\nas to be a typographical error for ‘random,’ and it is very unlikely to be a mistaken instance\nof ‘radon.’ A fuller analysis would include possibilities beyond these three words, but the\nbasic idea is the same.\n\nDecision making, model checking, and model improvement. We can envision two directions\nto go from here. The first approach is to accept the two-thirds probability that the word\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.5. PROBABILITY AS A MEASURE OF UNCERTAINTY 11\n\nwas typed correctly or even to simply declare ‘radom’ as correct on first pass. The second\noption would be to question this probability by saying, for example, that ‘radom’ looks like\na typo and that the estimated probability of it being correct seems much too high.\n\nWhen we dispute the claims of a posterior distribution, we are saying that the model\ndoes not fit the data or that we have additional prior information not included in the model\nso far. In this case, we are only examining one word so lack of fit is not the issue; thus a\ndispute over the posterior must correspond to a claim of additional information, either in\nthe prior or the likelihood.\n\nFor this problem we have no particular grounds on which to criticize the likelihood. The\nprior probabilities, on the other hand, are highly context dependent. The word ‘random’ is\nof course highly frequent in our own writing on statistics, ‘radon’ occurs occasionally (see\nSection 9.4), while ‘radom’ was entirely new to us. Our surprise at the high probability of\n‘radom’ represents additional knowledge relevant to our particular problem.\n\nThe model can be elaborated most immediately by including contextual information in\nthe prior probabilities. For example, if the document under study is a statistics book, then\nit becomes more likely that the person intended to type ‘random.’ If we label x as the\ncontextual information used by the model, the Bayesian calculation then becomes,\n\np(θ|x, y) ∝ p(θ|x)p(y|θ, x).\n\nTo first approximation, we can simplify that last term to p(y|θ), so that the probability\nof any particular error (that is, the probability of typing a particular string y given the\nintended word θ) does not depend on context. This is not a perfect assumption but could\nreduce the burden of modeling and computation.\n\nThe practical challenges in Bayesian inference involve setting up models to estimate all\nthese probabilities from data. At that point, as shown above, Bayes’ rule can be easily\napplied to determine the implications of the model for the problem at hand.\n\n1.5 Probability as a measure of uncertainty\n\nWe have already used concepts such as probability density, and indeed we assume that the\nreader has a fair degree of familiarity with basic probability theory (although in Section\n1.8 we provide a brief technical review of some probability calculations that often arise\nin Bayesian analysis). But since the uses of probability within a Bayesian framework are\nmuch broader than within non-Bayesian statistics, it is important to consider at least briefly\nthe foundations of the concept of probability before considering more detailed statistical\nexamples. We take for granted a common understanding on the part of the reader of the\nmathematical definition of probability: that probabilities are numerical quantities, defined\non a set of ‘outcomes,’ that are nonnegative, additive over mutually exclusive outcomes,\nand sum to 1 over all possible mutually exclusive outcomes.\n\nIn Bayesian statistics, probability is used as the fundamental measure or yardstick of\nuncertainty. Within this paradigm, it is equally legitimate to discuss the probability of\n‘rain tomorrow’ or of a Brazilian victory in the soccer World Cup as it is to discuss the\nprobability that a coin toss will land heads. Hence, it becomes as natural to consider the\nprobability that an unknown estimand lies in a particular range of values as it is to consider\nthe probability that the mean of a random sample of 10 items from a known fixed population\nof size 100 will lie in a certain range. The first of these two probabilities is of more interest\nafter data have been acquired whereas the second is more relevant beforehand. Bayesian\nmethods enable statements to be made about the partial knowledge available (based on\ndata) concerning some situation or ‘state of nature’ (unobservable or as yet unobserved) in\na systematic way, using probability as the yardstick. The guiding principle is that the state\nof knowledge about anything unknown is described by a probability distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n12 1. PROBABILITY AND INFERENCE\n\nWhat is meant by a numerical measure of uncertainty? For example, the probability of\n‘heads’ in a coin toss is widely agreed to be 1\n\n2 . Why is this so? Two justifications seem to\nbe commonly given:\n\n1. Symmetry or exchangeability argument:\n\nprobability =\nnumber of favorable cases\n\nnumber of possibilities\n,\n\nassuming equally likely possibilities. For a coin toss this is really a physical argument,\nbased on assumptions about the forces at work in determining the manner in which the\ncoin will fall, as well as the initial physical conditions of the toss.\n\n2. Frequency argument: probability = relative frequency obtained in a long sequence of\ntosses, assumed to be performed in an identical manner, physically independently of\neach other.\n\nBoth the above arguments are in a sense subjective, in that they require judgments about\nthe nature of the coin and the tossing procedure, and both involve semantic arguments\nabout the meaning of equally likely events, identical measurements, and independence.\nThe frequency argument may be perceived to have certain special difficulties, in that it\ninvolves the hypothetical notion of a long sequence of identical tosses. If taken strictly, this\npoint of view does not allow a statement of probability for a single coin toss that does not\nhappen to be embedded, at least conceptually, in a long sequence of identical events.\n\nThe following examples illustrate how probability judgments can be increasingly subjec-\ntive. First, consider the following modified coin experiment. Suppose that a particular coin\nis stated to be either double-headed or double-tailed, with no further information provided.\nCan one still talk of the probability of heads? It seems clear that in common parlance one\ncertainly can. It is less clear, perhaps, how to assess this new probability, but many would\nagree on the same value of 1\n\n2 , perhaps based on the exchangeability of the labels ‘heads’\nand ‘tails.’\n\nNow consider some further examples. Suppose Colombia plays Brazil in soccer to-\nmorrow: what is the probability of Colombia winning? What is the probability of rain\ntomorrow? What is the probability that Colombia wins, if it rains tomorrow? What is\nthe probability that a specified rocket launch will fail? Although each of these questions\nseems reasonable in a common-sense way, it is difficult to contemplate strong frequency\ninterpretations for the probabilities being referenced. Frequency interpretations can usually\nbe constructed, however, and this is an extremely useful tool in statistics. For example, one\ncan consider the future rocket launch as a sample from the population of potential launches\nof the same type, and look at the frequency of past launches that have failed (see the bib-\nliographic note at the end of this chapter for more details on this example). Doing this\nsort of thing scientifically means creating a probability model (or, at least, a ‘reference set’\nof comparable events), and this brings us back to a situation analogous to the simple coin\ntoss, where we must consider the outcomes in question as exchangeable and thus equally\nlikely.\n\nWhy is probability a reasonable way of quantifying uncertainty? The following reasons\nare often advanced.\n\n1. By analogy: physical randomness induces uncertainty, so it seems reasonable to describe\nuncertainty in the language of random events. Common speech uses many terms such\nas ‘probably’ and ‘unlikely,’ and it appears consistent with such usage to extend a more\nformal probability calculus to problems of scientific inference.\n\n2. Axiomatic or normative approach: related to decision theory, this approach places all sta-\ntistical inference in the context of decision-making with gains and losses. Then reasonable\naxioms (ordering, transitivity, and so on) imply that uncertainty must be represented in\nterms of probability. We view this normative rationale as suggestive but not compelling.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 13\n\n3. Coherence of bets. Define the probability p attached (by you) to an event E as the\nfraction (p ∈ [0, 1]) at which you would exchange (that is, bet) $p for a return of $1 if E\noccurs. That is, if E occurs, you gain $(1− p); if the complement of E occurs, you lose\n$p. For example:\n\n• Coin toss: thinking of the coin toss as a fair bet suggests even odds corresponding to\np = 1\n\n2 .\n\n• Odds for a game: if you are willing to bet on team A to win a game at 10 to 1 odds\nagainst team B (that is, you bet 1 to win 10), your ‘probability’ for team A winning\nis at least 1\n\n11 .\n\nThe principle of coherence states that your assignment of probabilities to all possible\nevents should be such that it is not possible to make a definite gain by betting with you.\nIt can be proved that probabilities constructed under this principle must satisfy the basic\naxioms of probability theory.\nThe betting rationale has some fundamental difficulties:\n\n• Exact odds are required, on which you would be willing to bet in either direction, for\nall events. How can you assign exact odds if you are not sure?\n\n• If a person is willing to bet with you, and has information you do not, it might not\nbe wise for you to take the bet. In practice, probability is an incomplete (necessary\nbut not sufficient) guide to betting.\n\nAll of these considerations suggest that probabilities may be a reasonable approach to\nsummarizing uncertainty in applied statistics, but the ultimate proof is in the success of the\napplications. The remaining chapters of this book demonstrate that probability provides a\nrich and flexible framework for handling uncertainty in statistical applications.\n\nSubjectivity and objectivity\n\nAll statistical methods that use probability are subjective in the sense of relying on math-\nematical idealizations of the world. Bayesian methods are sometimes said to be especially\nsubjective because of their reliance on a prior distribution, but in most problems, scientific\njudgment is necessary to specify both the ‘likelihood’ and the ‘prior’ parts of the model. For\nexample, linear regression models are generally at least as suspect as any prior distribution\nthat might be assumed about the regression parameters. A general principle is at work\nhere: whenever there is replication, in the sense of many exchangeable units observed, there\nis scope for estimating features of a probability distribution from data and thus making the\nanalysis more ‘objective.’ If an experiment as a whole is replicated several times, then the\nparameters of the prior distribution can themselves be estimated from data, as discussed in\nChapter 5. In any case, however, certain elements requiring scientific judgment will remain,\nnotably the choice of data included in the analysis, the parametric forms assumed for the\ndistributions, and the ways in which the model is checked.\n\n1.6 Example: probabilities from football point spreads\n\nAs an example of how probabilities might be assigned using empirical data and plausible\nsubstantive assumptions, we consider methods of estimating the probabilities of certain\noutcomes in professional (American) football games. This is an example only of probability\nassignment, not of Bayesian inference. A number of approaches to assigning probabilities\nfor football game outcomes are illustrated: making subjective assessments, using empirical\nprobabilities based on observed data, and constructing a parametric probability model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n14 1. PROBABILITY AND INFERENCE\n\nFigure 1.1 Scatterplot of actual outcome vs. point spread for each of 672 professional football games.\nThe x and y coordinates are jittered by adding uniform random numbers to each point’s coordinates\n(between −0.1 and 0.1 for the x coordinate; between −0.2 and 0.2 for the y coordinate) in order to\ndisplay multiple values but preserve the discrete-valued nature of each.\n\nFootball point spreads and game outcomes\n\nFootball experts provide a point spread for every football game as a measure of the difference\nin ability between the two teams. For example, team A might be a 3.5-point favorite to\ndefeat team B. The implication of this point spread is that the proposition that team A,\nthe favorite, defeats team B, the underdog, by 4 or more points is considered a fair bet; in\nother words, the probability that A wins by more than 3.5 points is 1\n\n2 . If the point spread\nis an integer, then the implication is that team A is as likely to win by more points than\nthe point spread as it is to win by fewer points than the point spread (or to lose); there is\npositive probability that A will win by exactly the point spread, in which case neither side\nis paid off. The assignment of point spreads is itself an interesting exercise in probabilistic\nreasoning; one interpretation is that the point spread is the median of the distribution of\nthe gambling population’s beliefs about the possible outcomes of the game. For the rest\nof this example, we treat point spreads as given and do not worry about how they were\nderived.\n\nThe point spread and actual game outcome for 672 professional football games played\nduring the 1981, 1983, and 1984 seasons are graphed in Figure 1.1. (Much of the 1982\nseason was canceled due to a labor dispute.) Each point in the scatterplot displays the\npoint spread, x, and the actual outcome (favorite’s score minus underdog’s score), y. (In\ngames with a point spread of zero, the labels ‘favorite’ and ‘underdog’ were assigned at\nrandom.) A small random jitter is added to the x and y coordinate of each point on the\ngraph so that multiple points do not fall exactly on top of each other.\n\nAssigning probabilities based on observed frequencies\n\nIt is of interest to assign probabilities to particular events: Pr(favorite wins), Pr(favorite\nwins | point spread is 3.5 points), Pr(favorite wins by more than the point spread), Pr(favorite\nwins by more than the point spread | point spread is 3.5 points), and so forth. We might\nreport a subjective probability based on informal experience gathered by reading the news-\npaper and watching football games. The probability that the favored team wins a game\nshould certainly be greater than 0.5, perhaps between 0.6 and 0.75? More complex events\nrequire more intuition or knowledge on our part. A more systematic approach is to assign\nprobabilities based on the data in Figure 1.1. Counting a tied game as one-half win and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.6. EXAMPLE: PROBABILITIES FROM FOOTBALL POINT SPREADS 15\n\nFigure 1.2 (a) Scatterplot of (actual outcome − point spread) vs. point spread for each of 672\nprofessional football games (with uniform random jitter added to x and y coordinates). (b) His-\ntogram of the differences between the game outcome and the point spread, with the N(0, 142) density\nsuperimposed.\n\none-half loss, and ignoring games for which the point spread is zero (and thus there is no\nfavorite), we obtain empirical estimates such as:\n\n• Pr(favorite wins) = 410.5\n655 = 0.63\n\n• Pr(favorite wins |x = 3.5) = 36\n59 = 0.61\n\n• Pr(favorite wins by more than the point spread) = 308\n655 = 0.47\n\n• Pr(favorite wins by more than the point spread |x = 3.5) = 32\n59 = 0.54.\n\nThese empirical probability assignments all seem sensible in that they match the intu-\nition of knowledgeable football fans. However, such probability assignments are problematic\nfor events with few directly relevant data points. For example, 8.5-point favorites won five\nout of five times during this three-year period, whereas 9-point favorites won thirteen out of\ntwenty times. However, we realistically expect the probability of winning to be greater for\na 9-point favorite than for an 8.5-point favorite. The small sample size with point spread\n8.5 leads to imprecise probability assignments. We consider an alternative method using a\nparametric model.\n\nA parametric model for the difference between outcome and point spread\n\nFigure 1.2a displays the differences y−x between the observed game outcome and the point\nspread, plotted versus the point spread, for the games in the football dataset. (Once again,\nrandom jitter was added to both coordinates.) This plot suggests that it may be roughly\nreasonable to model the distribution of y − x as independent of x. (See Exercise 6.10.)\nFigure 1.2b is a histogram of the differences y − x for all the football games, with a fitted\nnormal density superimposed. This plot suggests that it may be reasonable to approximate\nthe marginal distribution of the random variable d = y − x by a normal distribution. The\nsample mean of the 672 values of d is 0.07, and the sample standard deviation is 13.86,\nsuggesting that the results of football games are approximately normal with mean equal to\nthe point spread and standard deviation nearly 14 points (two converted touchdowns). For\nthe remainder of the discussion we take the distribution of d to be independent of x and\nnormal with mean zero and standard deviation 14 for each x; that is,\n\nd|x ∼ N(0, 142),\n\nas displayed in Figure 1.2b. The assigned probability model is not perfect: it does not fit\nthe data exactly, and, as is often the case with real data, neither football scores nor point\nspreads are continuous-valued quantities.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n16 1. PROBABILITY AND INFERENCE\n\nAssigning probabilities using the parametric model\n\nNevertheless, the model provides a convenient approximation that can be used to assign\nprobabilities to events. If d has a normal distribution with mean zero and is independent of\nthe point spread, then the probability that the favorite wins by more than the point spread\nis 1\n\n2 , conditional on any value of the point spread, and therefore unconditionally as well.\nDenoting probabilities obtained by the normal model as Prnorm, the probability that an\nx-point favorite wins the game can be computed, assuming the normal model, as follows:\n\nPrnorm(y>0 |x) = Prnorm(d>−x |x) = 1− Φ\n(\n− x\n\n14\n\n)\n,\n\nwhere Φ is the standard normal cumulative distribution function. For example,\n\n• Prnorm(favorite wins |x = 3.5) = 0.60\n\n• Prnorm(favorite wins |x = 8.5) = 0.73\n\n• Prnorm(favorite wins |x = 9.0) = 0.74.\n\nThe probability for a 3.5-point favorite agrees with the empirical value given earlier, whereas\nthe probabilities for 8.5- and 9-point favorites make more intuitive sense than the empirical\nvalues based on small samples.\n\n1.7 Example: calibration for record linkage\n\nWe emphasize the essentially empirical (not ‘subjective’ or ‘personal’) nature of probabilities\nwith another example in which they are estimated from data.\n\nRecord linkage refers to the use of an algorithmic technique to identify records from\ndifferent databases that correspond to the same individual. Record-linkage techniques are\nused in a variety of settings. The work described here was formulated and first applied in\nthe context of record linkage between the U.S. Census and a large-scale post-enumeration\nsurvey, which is the first step of an extensive matching operation conducted to evaluate\ncensus coverage for subgroups of the population. The goal of this first step is to declare as\nmany records as possible ‘matched’ by computer without an excessive rate of error, thereby\navoiding the cost of the resulting manual processing for all records not declared ‘matched.’\n\nExisting methods for assigning scores to potential matches\n\nMuch attention has been paid in the record-linkage literature to the problem of assigning\n‘weights’ to individual fields of information in a multivariate record and obtaining a com-\nposite ‘score,’ which we call y, that summarizes the closeness of agreement between two\nrecords. Here, we assume that this step is complete in the sense that these rules have been\nchosen. The next step is the assignment of candidate matched pairs, where each pair of\nrecords consists of the best potential match for each other from the respective databases.\nThe specified weighting rules then order the candidate matched pairs. In the motivating\nproblem at the Census Bureau, a binary choice is made between the alternatives ‘declare\nmatched’ vs. ‘send to followup,’ where a cutoff score is needed above which records are\ndeclared matched. The false-match rate is then defined as the number of falsely matched\npairs divided by the number of declared matched pairs.\n\nParticularly relevant for any such decision problem is an accurate method for assessing\nthe probability that a candidate matched pair is a correct match as a function of its score.\nSimple methods exist for converting the scores into probabilities, but these lead to extremely\ninaccurate, typically grossly optimistic, estimates of false-match rates. For example, a\nmanual check of a set of records with nominal false-match probabilities ranging from 10−3\n\nto 10−7 (that is, pairs deemed almost certain to be matches) found actual false-match rates\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.7. EXAMPLE: CALIBRATION FOR RECORD LINKAGE 17\n\nFigure 1.3 Histograms of weight scores y for true and false matches in a sample of records from\nthe 1988 test Census. Most of the matches in the sample are true (because a pre-screening process\nhas already picked these as the best potential match for each case), and the two distributions are\nmostly, but not completely, separated.\n\ncloser to the 1% range. Records with nominal false-match probabilities of 1% had an actual\nfalse-match rate of 5%.\n\nWe would like to use Bayesian methods to recalibrate these to obtain objective proba-\nbilities of matching for a given decision rule—in the same way that in the football example,\nwe used past data to estimate the probabilities of different game outcomes conditional on\nthe point spread. Our approach is to work with the scores y and empirically estimate the\nprobability of a match as a function of y.\n\nEstimating match probabilities empirically\n\nWe obtain accurate match probabilities using mixture modeling, a topic we discuss in detail\nin Chapter 22. The distribution of previously obtained scores for the candidate matches\nis considered a ‘mixture’ of a distribution of scores for true matches and a distribution for\nnon-matches. The parameters of the mixture model are estimated from the data. The\nestimated parameters allow us to calculate an estimate of the probability of a false match\n(a pair declared matched that is not a true match) for any given decision threshold on the\nscores. In the procedure that was actually used, some elements of the mixture model (for\nexample, the optimal transformation required to allow a mixture of normal distributions\nto apply) were fit using ‘training’ data with known match status (separate from the data\nto which we apply our calibration procedure), but we do not describe those details here.\nInstead we focus on how the method would be used with a set of data with unknown match\nstatus.\n\nSupport for this approach is provided in Figure 1.3, which displays the distribution of\nscores for the matches and non-matches in a particular dataset obtained from 2300 records\nfrom a ‘test Census’ survey conducted in a single local area two years before the 1990 Census.\nThe two distributions, p(y|match) and p(y|non-match), are mostly distinct—meaning that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n18 1. PROBABILITY AND INFERENCE\n\nFigure 1.4 Lines show expected false-match rate (and 95% bounds) as a function of the proportion\nof cases declared matches, based on the mixture model for record linkage. Dots show the actual\nfalse-match rate for the data.\n\nin most cases it is possible to identify a candidate as a match or not given the score alone—\nbut with some overlap.\n\nIn our application dataset, we do not know the match status. Thus we are faced with a\nsingle combined histogram from which we estimate the two component distributions and the\nproportion of the population of scores that belong to each component. Under the mixture\nmodel, the distribution of scores can be written as,\n\np(y) = Pr(match) p(y|match) + Pr(non-match) p(y|non-match). (1.7)\n\nThe mixture probability (Pr(match)) and the parameters of the distributions of matches\n(p(y|match)) and non-matches (p(y|non-match)) are estimated using the mixture model\napproach (as described in Chapter 22) applied to the combined histogram from the data\nwith unknown match status.\n\nTo use the method to make record-linkage decisions, we construct a curve giving the\nfalse-match rate as a function of the decision threshold, the score above which pairs will\nbe ‘declared’ a match. For a given decision threshold, the probability distributions in (1.7)\ncan be used to estimate the probability of a false match, a score y above the threshold\noriginating from the distribution p(y|non-match). The lower the threshold, the more pairs\nwe will declare as matches. As we declare more matches, the proportion of errors increases.\nThe approach described here should provide an objective error estimate for each threshold.\n(See the validation in the next paragraph.) Then a decision maker can determine the\nthreshold that provides an acceptable balance between the goals of declaring more matches\nautomatically (thus reducing the clerical labor) and making fewer mistakes.\n\nExternal validation of the probabilities using test data\n\nThe approach described above was externally validated using data for which the match\nstatus is known. The method was applied to data from three different locations of the 1988\ntest Census, and so three tests of the methods were possible. We provide detailed results\nfor one; results for the other two were similar. The mixture model was fitted to the scores\nof all the candidate pairs at a test site. Then the estimated model was used to create the\nlines in Figure 1.4, which show the expected false-match rate (and uncertainty bounds) in\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 19\n\nFigure 1.5 Expansion of Figure 1.4 in the region where the estimated and actual match rates change\nrapidly. In this case, it would seem a good idea to match about 88% of the cases and send the rest\nto followup.\n\nterms of the proportion of cases declared matched, as the threshold varies from high (thus\nallowing no matches) to low (thus declaring almost all the candidate pairs to be matches).\nThe false-match proportion is an increasing function of the number of declared matches,\nwhich makes sense: as we move rightward on the graph, we are declaring weaker and weaker\ncases to be matches.\n\nThe lines on Figure 1.4 display the expected proportion of false matches and 95% pos-\nterior bounds for the false-match rate as estimated from the model. (These bounds give\nthe estimated range within which there is 95% posterior probability that the false-match\nrate lies. The concept of posterior intervals is discussed in more detail in the next chapter.)\nThe dots in the graph display the actual false-match proportions, which track well with the\nmodel. In particular, the model would suggest a recommendation of declaring something\nless than 90% of cases as matched and giving up on the other 10% or so, so as to avoid\nmost of the false matches, and the dots show a similar pattern.\n\nIt is clearly possible to match large proportions of the files with little or no error. Also,\nthe quality of candidate matches becomes dramatically worse at some point where the\nfalse-match rate accelerates. Figure 1.5 takes a magnifying glass to the previous display\nto highlight the behavior of the calibration procedure in the region of interest where the\nfalse-match rate accelerates. The predicted false-match rate curves bend upward, close to\nthe points where the observed false-match rate curves rise steeply, which is a particularly\nencouraging feature of the calibration method. The calibration procedure performs well\nfrom the standpoint of providing predicted probabilities that are close to the true probabili-\nties and interval estimates that are informative and include the true values. By comparison,\nthe original estimates of match probabilities, constructed by multiplying weights without\nempirical calibration, were highly inaccurate.\n\n1.8 Some useful results from probability theory\n\nWe assume the reader is familiar with elementary manipulations involving probabilities\nand probability distributions. In particular, basic probability background that must be\nwell understood for key parts of the book includes the manipulation of joint densities, the\ndefinition of simple moments, the transformation of variables, and methods of simulation. In\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n20 1. PROBABILITY AND INFERENCE\n\nthis section we briefly review these assumed prerequisites and clarify some further notational\nconventions used in the remainder of the book. Appendix A provides information on some\ncommonly used probability distributions.\n\nAs introduced in Section 1.3, we generally represent joint distributions by their joint\nprobability mass or density function, with dummy arguments reflecting the name given to\neach variable being considered. Thus for two quantities u and v, we write the joint density\nas p(u, v); if specific values need to be referenced, this notation will be further abused as\nwith, for example, p(u, v=1).\n\nIn Bayesian calculations relating to a joint density p(u, v), we will often refer to a\nconditional distribution or density function such as p(u|v) and a marginal density such as\np(u) =\n\n∫\np(u, v)dv. In this notation, either or both u and v can be vectors. Typically\n\nit will be clear from the context that the range of integration in the latter expression\nrefers to the entire range of the variable being integrated out. It is also often useful to\nfactor a joint density as a product of marginal and conditional densities; for example,\np(u, v, w) = p(u|v, w)p(v|w)p(w).\n\nSome authors use different notations for distributions on parameters and observables—\nfor example, π(θ), f(y|θ)—but this obscures the fact that all probability distributions have\nthe same logical status in Bayesian inference. We must always be careful, though, to in-\ndicate appropriate conditioning; for example, p(y|θ) is different from p(y). In the inter-\nests of conciseness, however, our notation hides the conditioning on hypotheses that hold\nthroughout—no probability judgments can be made in a vacuum—and to be more explicit\none might use a notation such as the following:\n\np(θ, y|H) = p(θ|H)p(y|θ,H),\n\nwhere H refers to the set of hypotheses or assumptions used to define the model. Also, we\nsometimes suppress explicit conditioning on known explanatory variables, x.\n\nWe use the standard notations, E(·) and var(·), for mean and variance, respectively:\n\nE(u) =\n\n∫\nup(u)du, var(u) =\n\n∫\n(u− E(u))2p(u)du.\n\nFor a vector parameter u, the expression for the mean is the same, and the covariance\nmatrix is defined as\n\nvar(u) =\n\n∫\n(u− E(u))(u− E(u))T p(u)du,\n\nwhere u is considered a column vector. (We use the terms ‘variance matrix’ and ‘covariance\nmatrix’ interchangeably.) This notation is slightly imprecise, because E(u) and var(u) are\nreally functions of the distribution function, p(u), not of the variable u. In an expression\ninvolving an expectation, any variable that does not appear explicitly as a conditioning\nvariable is assumed to be integrated out in the expectation; for example, E(u|v) refers to\nthe conditional expectation of u with v held fixed—that is, the conditional expectation as\na function of v—whereas E(u) is the expectation of u, averaging over v (as well as u).\n\nModeling using conditional probability\n\nUseful probability models often express the distribution of observables conditionally or hier-\narchically rather than through more complicated unconditional distributions. For example,\nsuppose y is the height of a university student selected at random. The marginal distri-\nbution p(y) is (essentially) a mixture of two approximately normal distributions centered\naround 160 and 175 centimeters. A more useful description of the distribution of y would\nbe based on the joint distribution of height and sex: p(male) ≈ p(female) ≈ 1\n\n2 , along with\nthe conditional specifications that p(y|female) and p(y|male) are each approximately normal\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(u|v)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over v. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nE(u) =\n\n∫ ∫\nup(u, v)dudv =\n\n∫ ∫\nu p(u|v)du p(v)dv =\n\n∫\nE(u|v)p(v)dv.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(u|v)) + var(E(u|v)). (1.9)\n\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var (E(u|v)) = E\n(\nE(u2|v)− (E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E (E(u|v)))2\n\n= E(u2)− E\n(\n(E(u|v))2\n\n)\n+ E\n\n(\n(E(u|v))2\n\n)\n− (E(u))2\n\n= E(u2)− (E(u))2\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if u is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(·). Suppose pu(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf pu is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\npv(v) = pu(f\n−1(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for pv(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf pu is a continuous distribution, and v = f(u) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\npv(v) = |J | pu(f−1(v))\n\nThis electronic edition is for non-commercial purposes only.\n\n1.8. SOME USEFUL RESULTS FROM PROBABILITY THEORY 21\n\nwith means 160 and 175 cm, respectively. If the conditional variances are not too large,\nthe marginal distribution of y is bimodal. In general, we prefer to model complexity with\na hierarchical structure using additional variables rather than with complicated marginal\ndistributions, even when the additional variables are unobserved or even unobservable; this\ntheme underlies mixture models, as discussed in Chapter 22. We repeatedly return to the\ntheme of conditional modeling throughout the book.\n\nMeans and variances of conditional distributions\n\nIt is often useful to express the mean and variance of a random variable u in terms of\nthe conditional mean and variance given some related quantity v. The mean of u can be\nobtained by averaging the conditional mean over the marginal distribution of v,\n\nE(u) = E(E(uly)), (1.8)\n\nwhere the inner expectation averages over u, conditional on v, and the outer expectation\naverages over vu. Identity (1.8) is easy to derive by writing the expectation in terms of the\njoint distribution of u and v and then factoring the joint distribution:\n\nu)= / / up(u, v)dudv = / / up(ulv)dup(v)dv = / E(ulv)p(w)do.\n\nThe corresponding result for the variance includes two terms, the mean of the conditional\nvariance and the variance of the conditional mean:\n\nvar(u) = E(var(ulv)) + var(E(ulv)). (1.9)\nThis result can be derived by expanding the terms on the right side of (1.9):\n\nE (var(u|v)) + var(E(ulv)) = E(E(u*|v) — (E(ulv))*) + E ((E(ulv))*) — (E (E(u|v)))°\nE(u’) — E ((E(u|v))*) + E ((E(ulv))*) = (E(u)?\n= E(u*) — (E(u)?\n\n= var(u).\n\nIdentities (1.8) and (1.9) also hold if wu is a vector, in which case E(u) is a vector and var(u)\na matrix.\n\nTransformation of variables\n\nIt is common to transform a probability distribution from one parameterization to another.\nWe review the basic result here for a probability density on a transformed space. For\nclarity, we use subscripts here instead of our usual generic notation, p(-). Suppose p,(u) is\nthe density of the vector u, and we transform to v = f(u), where v has the same number\nof components as u.\n\nIf p, is a discrete distribution, and f is a one-to-one function, then the density of v is\ngiven by\n\nPo(v) = Pu(f(v)).\n\nIf f is a many-to-one function, then a sum of terms appears on the right side of this\nexpression for p,(v), with one term corresponding to each of the branches of the inverse\nfunction.\n\nIf p,, is a continuous distribution, and v = f(wu) is a one-to-one transformation, then the\njoint density of the transformed vector is\n\nPo(v) = |J| pul(f~*(v))\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n22 1. PROBABILITY AND INFERENCE\n\nwhere |J | is the absolute value of the determinant of the Jacobian of the transformation\nu = f−1(v) as a function of v; the Jacobian J is the square matrix of partial derivatives\n(with dimension given by the number of components of u), with the (i, j)th entry equal to\n∂ui/∂vj. Once again, if f is many-to-one, then pv(v) is a sum or integral of terms.\n\nIn one dimension, we commonly use the logarithm to transform the parameter space\nfrom (0,∞) to (−∞,∞). When working with parameters defined on the open unit interval,\n(0, 1), we often use the logistic transformation:\n\nlogit(u) = log\n\n(\nu\n\n1− u\n\n)\n, (1.10)\n\nwhose inverse transformation is\n\nlogit−1(v) =\nev\n\n1 + ev\n.\n\nAnother common choice is the probit transformation, Φ−1(u), where Φ is the standard\nnormal cumulative distribution function, to transform from (0, 1) to (−∞,∞).\n\n1.9 Computation and software\n\nAt the time of writing, the authors rely primarily on the software package R for graphs and\nbasic simulations, fitting of classical simple models (including regression, generalized linear\nmodels, and nonparametric methods such as locally weighted regression), optimization, and\nsome simple programming. We use the Bayesian inference package Stan (see Appendix C)\nfor fitting most models, but for teaching purposes in this book we describe how to perform\nmost of the computations from first principles. Even when using Stan, we typically work\nwithin R to plot and transform the data before model fitting, and to display inferences and\nmodel checks afterwards.\n\nSpecific computational tasks that arise in Bayesian data analysis include:\n\n• Vector and matrix manipulations (see Table 1.1)\n\n• Computing probability density functions (see Appendix A)\n\n• Drawing simulations from probability distributions (see Appendix A for standard distri-\nbutions and Exercise 1.9 for an example of a simple stochastic process)\n\n• Structured programming (including looping and customized functions)\n\n• Calculating the linear regression estimate and variance matrix (see Chapter 14)\n\n• Graphics, including scatterplots with overlain lines and multiple graphs per page (see\nChapter 6 for examples).\n\nOur general approach to computation is to fit many models, gradually increasing the\ncomplexity. We do not recommend the strategy of writing a model and then letting the\ncomputer run overnight to estimate it perfectly. Rather, we prefer to fit each model rela-\ntively quickly, using inferences from the previously fitted simpler models as starting values,\nand displaying inferences and comparing to data before continuing.\n\nWe discuss computation in detail in Part III of this book after first introducing the\nfundamental concepts of Bayesian modeling, inference, and model checking. Appendix C\nillustrates how to perform computations in R and Stan in several different ways for a single\nexample.\n\nSummarizing inferences by simulation\n\nSimulation forms a central part of much applied Bayesian analysis, because of the relative\nease with which samples can often be generated from a probability distribution, even when\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.9. COMPUTATION AND SOFTWARE 23\n\nthe density function cannot be explicitly integrated. In performing simulations, it is helpful\nto consider the duality between a probability density function and a histogram of a set of\nrandom draws from the distribution: given a large enough sample, the histogram can pro-\nvide practically complete information about the density, and in particular, various sample\nmoments, percentiles, and other summary statistics provide estimates of any aspect of the\ndistribution, to a level of precision that can be estimated. For example, to estimate the\n95th percentile of the distribution of θ, draw a random sample of size S from p(θ) and use\nthe 0.95Sth order statistic. For most purposes, S = 1000 is adequate for estimating the\n95th percentile in this way.\n\nAnother advantage of simulation is that extremely large or small simulated values often\nflag a problem with model specification or parameterization (for example, see Figure 4.2)\nthat might not be noticed if estimates and probability statements were obtained in analytic\nform.\n\nGenerating values from a probability distribution is often straightforward with modern\ncomputing techniques based on (pseudo)random number sequences. A well-designed pseu-\ndorandom number generator yields a deterministic sequence that appears to have the same\nproperties as a sequence of independent random draws from the uniform distribution on\n[0, 1]. Appendix A describes methods for drawing random samples from some commonly\nused distributions.\n\nSampling using the inverse cumulative distribution function\n\nAs an introduction to the ideas of simulation, we describe a method for sampling from\ndiscrete and continuous distributions using the inverse cumulative distribution function.\nThe cumulative distribution function, or cdf, F , of a one-dimensional distribution, p(v), is\ndefined by\n\nF (v∗) = Pr(v ≤ v∗)\n\n=\n\n{ ∑\nv≤v∗ p(v) if p is discrete∫ v∗\n\n−∞ p(v)dv if p is continuous.\n\nThe inverse cdf can be used to obtain random samples from the distribution p, as\nfollows. First draw a random value, U , from the uniform distribution on [0, 1], using a table\nof random numbers or, more likely, a random number function on the computer. Now let\nv = F−1(U). The function F is not necessarily one-to-one—certainly not if the distribution\nis discrete—but F−1(U) is unique with probability 1. The value v will be a random draw\nfrom p, and is easy to compute as long as F−1(U) is simple. For a discrete distribution,\nF−1 can simply be tabulated.\n\nFor a continuous example, suppose v has an exponential distribution with parameter λ\n(see Appendix A); then its cdf is F (v) = 1− e−λv, and the value of v for which U=F (v) is\n\nv = − log(1−U)\nλ . Then, recognizing that 1−U also has the uniform distribution on [0, 1], we\n\nsee we can obtain random draws from the exponential distribution as − logU\nλ . We discuss\n\nother methods of simulation in Part III of the book and Appendix A.\n\nSimulation of posterior and posterior predictive quantities\n\nIn practice, we are most often interested in simulating draws from the posterior distribu-\ntion of the model parameters θ, and perhaps from the posterior predictive distribution of\nunknown observables ỹ. Results from a set of S simulation draws can be stored in the\ncomputer in an array, as illustrated in Table 1.1. We use the notation s = 1, . . . , S to in-\ndex simulation draws; (θs, ỹs) is the corresponding joint draw of parameters and predicted\nquantities from their joint posterior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n24 1. PROBABILITY AND INFERENCE\n\nSimulation Parameters Predictive\ndraw quantities\n\nθ1 . . . θk ỹ1 . . . ỹn\n1 θ11 . . . θ1k ỹ11 . . . ỹ1n\n...\n\n...\n. . .\n\n...\n...\n\n. . .\n...\n\nS θS1 . . . θSk ỹS1 . . . ỹSn\n\nTable 1.1 Structure of posterior and posterior predictive simulations. The superscripts are indexes,\nnot powers.\n\nFrom these simulated values, we can estimate the posterior distribution of any quantity\nof interest, such as θ1/θ3, by just computing a new column in Table 1.1 using the existing S\ndraws of (θ, ỹ). We can estimate the posterior probability of any event, such as Pr(ỹ1+ ỹ2 >\neθ1), by the proportion of the S simulations for which it is true. We are often interested in\nposterior intervals; for example, the central 95% posterior interval [a, b] for the parameter\nθj , for which Pr(θj < a) = 0.025 and Pr(θj > b) = 0.025. These values can be directly\nestimated by the appropriate simulated values of θj , for example, the 25th and 976th order\nstatistics if S=1000. We commonly summarize inferences by 50% and 95% intervals.\n\nWe return to the accuracy of simulation inferences in Section 10.5 after we have gained\nsome experience using simulations of posterior distributions in some simple examples.\n\n1.10 Bayesian inference in applied statistics\n\nA pragmatic rationale for the use of Bayesian methods is the inherent flexibility introduced\nby their incorporation of multiple levels of randomness and the resultant ability to combine\ninformation from different sources, while incorporating all reasonable sources of uncertainty\nin inferential summaries. Such methods naturally lead to smoothed estimates in complicated\ndata structures and consequently have the ability to obtain better real-world answers.\n\nAnother reason for focusing on Bayesian methods is more psychological, and involves the\nrelationship between the statistician and the client or specialist in the subject matter area\nwho is the consumer of the statistician’s work. In many practical cases, clients will interpret\ninterval estimates provided by statisticians as Bayesian intervals, that is, as probability\nstatements about the likely values of unknown quantities conditional on the evidence in\nthe data. Such direct probability statements require prior probability specifications for\nunknown quantities (or more generally, probability models for vectors of unknowns), and\nthus the kinds of answers clients will assume are being provided by statisticians, Bayesian\nanswers, require full probability models—explicit or implicit.\n\nFinally, Bayesian inferences are conditional on probability models that invariably contain\napproximations in their attempt to represent complicated real-world relationships. If the\nBayesian answers vary dramatically over a range of scientifically reasonable assumptions\nthat are unassailable by the data, then the resultant range of possible conclusions must be\nentertained as legitimate, and we believe that the statistician has the responsibility to make\nthe client aware of this fact.\n\nIn this book, we focus on the construction of models (especially hierarchical ones, as\ndiscussed in Chapter 5 onward) to relate complicated data structures to scientific questions,\nchecking the fit of such models, and investigating the sensitivity of conclusions to reasonable\nmodeling assumptions. From this point of view, the strength of the Bayesian approach lies in\n(1) its ability to combine information from multiple sources (thereby in fact allowing greater\n‘objectivity’ in final conclusions), and (2) its more encompassing accounting of uncertainty\nabout the unknowns in a statistical problem.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.11. BIBLIOGRAPHIC NOTE 25\n\nOther important themes, many of which are common to much modern applied statistical\npractice, whether formally Bayesian or not, are the following:\n\n• a willingness to use many parameters\n\n• hierarchical structuring of models, which is the essential tool for achieving partial pool-\ning of estimates and compromising in a scientific way between alternative sources of\ninformation\n\n• model checking—not only by examining the internal goodness of fit of models to ob-\nserved and possible future data, but also by comparing inferences about estimands and\npredictions of interest to substantive knowledge\n\n• an emphasis on inference in the form of distributions or at least interval estimates rather\nthan simple point estimates\n\n• the use of simulation as the primary method of computation; the modern computational\ncounterpart to a ‘joint probability distribution’ is a set of randomly drawn values, and a\nkey tool for dealing with missing data is the method of multiple imputation (computation\nand multiple imputation are discussed in more detail in later chapters)\n\n• the use of probability models as tools for understanding and possibly improving data-\nanalytic techniques that may not explicitly invoke a Bayesian model\n\n• the importance of including in the analysis as much background information as possible,\nso as to approximate the goal that data can be viewed as a random sample, conditional\non all the variables in the model\n\n• the importance of designing studies to have the property that inferences for estimands\nof interest will be robust to model assumptions.\n\n1.11 Bibliographic note\n\nSeveral good introductory books have been written on Bayesian statistics, beginning with\nLindley (1965), and continuing through Hoff (2009). Berry (1996) presents, from a Bayesian\nperspective, many of the standard topics for an introductory statistics textbook. Gill\n(2002) and Jackman (2009) introduce applied Bayesian statistics for social scientists, Kr-\nuschke (2011) introduces Bayesian methods for psychology researchers, and Christensen et\nal. (2010) supply a general introduction. Carlin and Louis (2008) cover the theory and\napplications of Bayesian inference, focusing on biological applications and connections to\nclassical methods. Some resources for teaching Bayesian statistics include Sedlmeier and\nGigerenzer (2001) and Gelman (1998, 2008b).\n\nThe bibliographic notes at the ends of the chapters in this book refer to a variety of\nspecific applications of Bayesian data analysis. Several review articles in the statistical\nliterature, such as Breslow (1990) and Racine et al. (1986), have appeared that discuss,\nin general terms, areas of application in which Bayesian methods have been useful. The\nvolumes edited by Gatsonis et al. (1993–2002) are collections of Bayesian analyses, including\nextensive discussions about choices in the modeling process and the relations between the\nstatistical methods and the applications.\n\nThe foundations of probability and Bayesian statistics are an important topic that we\ntreat only briefly. Bernardo and Smith (1994) give a thorough review of the foundations\nof Bayesian models and inference with a comprehensive list of references. Jeffreys (1961) is\na self-contained book about Bayesian statistics that comprehensively presents an inductive\nview of inference; Good (1950) is another important early work. Jaynes (1983) is a collection\nof reprinted articles that present a deductive view of Bayesian inference that we believe is\nsimilar to ours. Both Jeffreys and Jaynes focus on applications in the physical sciences.\nJaynes (2003) focuses on connections between statistical inference and the philosophy of\nscience and includes several examples of physical probability.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n26 1. PROBABILITY AND INFERENCE\n\nGigerenzer and Hoffrage (1995) discuss the connections between Bayesian probability\nand frequency probabilities from a perspective similar to ours, and provide evidence that\npeople can typically understand and compute best with probabilities that are expressed\nin the form of relative frequency. Gelman (1998) presents some classroom activities for\nteaching Bayesian ideas.\n\nDe Finetti (1974) is an influential work that focuses on the crucial role of exchange-\nability. More approachable discussions of the role of exchangeability in Bayesian inference\nare provided by Lindley and Novick (1981) and Rubin (1978a, 1987a). The non-Bayesian\narticle by Draper et al. (1993) makes an interesting attempt to explain how exchangeable\nprobability models can be justified in data analysis. Berger and Wolpert (1984) give a\ncomprehensive discussion and review of the likelihood principle, and Berger (1985, Sections\n1.6, 4.1, and 4.12) reviews a range of philosophical issues from the perspective of Bayesian\ndecision theory.\n\nOur own philosophy of Bayesian statistics appears in Gelman (2011) and Gelman and\nShalizi (2013); for some contrasting views, see the discussion of that article, along with\nEfron (1986) and the discussions following Gelman (2008a).\n\nPratt (1965) and Rubin (1984) discuss the relevance of Bayesian methods for applied\nstatistics and make many connections between Bayesian and non-Bayesian approaches to\ninference. Further references on the foundations of statistical inference appear in Shafer\n(1982) and the accompanying discussion. Kahneman and Tversky (1972) and Alpert and\nRaiffa (1982) present the results of psychological experiments that assess the meaning of\n‘subjective probability’ as measured by people’s stated beliefs and observed actions. Lindley\n(1971a) surveys many different statistical ideas, all from the Bayesian perspective. Box and\nTiao (1973) is an early book on applied Bayesian methods. They give an extensive treatment\nof inference based on normal distributions, and their first chapter, a broad introduction to\nBayesian inference, provides a good counterpart to Chapters 1 and 2 of this book.\n\nThe iterative process involving modeling, inference, and model checking that we present\nin Section 1.1 is discussed at length in the first chapter of Box and Tiao (1973) and also\nin Box (1980). Cox and Snell (1981) provide a more introductory treatment of these ideas\nfrom a less model-based perspective.\n\nMany good books on the mathematical aspects of probability theory are available, such\nas Feller (1968) and Ross (1983); these are useful when constructing probability models\nand working with them. O’Hagan (1988) has written an interesting introductory text on\nprobability from an explicitly Bayesian point of view.\n\nPhysical probability models for coin tossing are discussed by Keller (1986), Jaynes\n(2003), and Gelman and Nolan (2002b). The football example of Section 1.6 is discussed\nin more detail in Stern (1991); see also Harville (1980) and Glickman (1993) and Glickman\nand Stern (1998) for analyses of football scores not using the point spread. Related analyses\nof sports scores and betting odds appear in Stern (1997, 1998). For more background on\nsports betting, see Snyder (1975) and Rombola (1984).\n\nAn interesting real-world example of probability assignment arose with the explosion\nof the Challenger space shuttle in 1986; Martz and Zimmer (1992), Dalal, Fowlkes, and\nHoadley (1989), and Lavine (1991) present and compare various methods for assigning\nprobabilities for space shuttle failures. (At the time of writing we are not aware of similar\ncontributions relating to the more recent space accident in 2003.) The record-linkage ex-\nample in Section 1.7 appears in Belin and Rubin (1995b), who discuss the mixture models\nand calibration techniques in more detail. The Census problem that motivated the record\nlinkage is described by Hogan (1992).\n\nIn all our examples, probabilities are assigned using statistical modeling and estimation,\nnot by ‘subjective’ assessment. Dawid (1986) provides a general discussion of probability\nassignment, and Dawid (1982) discusses the connections between calibration and Bayesian\nprobability assignment.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n1.12. EXERCISES 27\n\nThe graphical method of jittering, used in Figures 1.1 and 1.2 and elsewhere in this\nbook, is discussed in Chambers et al. (1983). For information on the statistical packages R\nand Bugs, see Becker, Chambers, and Wilks (1988), R Project (2002), Fox (2002), Venables\nand Ripley (2002), and Spiegelhalter et al. (1994, 2003).\n\nNorvig (2007) describes the principles and details of the Bayesian spelling corrector.\n\n1.12 Exercises\n\n1. Conditional probability: suppose that if θ = 1, then y has a normal distribution with\nmean 1 and standard deviation σ, and if θ = 2, then y has a normal distribution with\nmean 2 and standard deviation σ. Also, suppose Pr(θ = 1) = 0.5 and Pr(θ = 2) = 0.5.\n\n(a) For σ = 2, write the formula for the marginal probability density for y and sketch it.\n\n(b) What is Pr(θ = 1|y = 1), again supposing σ = 2?\n\n(c) Describe how the posterior density of θ changes in shape as σ is increased and as it is\ndecreased.\n\n2. Conditional means and variances: show that (1.8) and (1.9) hold if u is a vector.\n\n3. Probability calculation for genetics (from Lindley, 1965): suppose that in each individual\nof a large population there is a pair of genes, each of which can be either x or X, that\ncontrols eye color: those with xx have blue eyes, while heterozygotes (those with Xx or\nxX) and those with XX have brown eyes. The proportion of blue-eyed individuals is p2\n\nand of heterozygotes is 2p(1 − p), where 0 < p < 1. Each parent transmits one of its\nown genes to the child; if a parent is a heterozygote, the probability that it transmits the\ngene of type X is 1\n\n2 . Assuming random mating, show that among brown-eyed children\nof brown-eyed parents, the expected proportion of heterozygotes is 2p/(1+2p). Suppose\nJudy, a brown-eyed child of brown-eyed parents, marries a heterozygote, and they have\nn children, all brown-eyed. Find the posterior probability that Judy is a heterozygote\nand the probability that her first grandchild has blue eyes.\n\n4. Probability assignment: we will use the football dataset to estimate some conditional\nprobabilities about professional football games. There were twelve games with point\nspreads of 8 points; the outcomes in those games were: −7, −5, −3, −3, 1, 6, 7, 13, 15,\n16, 20, and 21, with positive values indicating wins by the favorite and negative values\nindicating wins by the underdog. Consider the following conditional probabilities:\n\nPr(favorite wins | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8),\n\nPr(favorite wins by at least 8 | point spread = 8 and favorite wins).\n\n(a) Estimate each of these using the relative frequencies of games with a point spread of\n8.\n\n(b) Estimate each using the normal approximation for the distribution of (outcome −\npoint spread).\n\n5. Probability assignment: the 435 U.S. Congressmembers are elected to two-year terms;\nthe number of voters in an individual congressional election varies from about 50,000 to\n350,000. We will use various sources of information to estimate roughly the probability\nthat at least one congressional election is tied in the next national election.\n\n(a) Use any knowledge you have about U.S. politics. Specify clearly what information you\nare using to construct this conditional probability, even if your answer is just a guess.\n\n(b) Use the following information: in the period 1900–1992, there were 20,597 congres-\nsional elections, out of which 6 were decided by fewer than 10 votes and 49 decided\nby fewer than 100 votes.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n28 1. PROBABILITY AND INFERENCE\n\nSee Gelman, King, and Boscardin (1998), Mulligan and Hunter (2001), and Gelman,\nKatz, and Tuerlinckx (2002) for more on this topic.\n\n6. Conditional probability: approximately 1/125 of all births are fraternal twins and 1/300\nof births are identical twins. Elvis Presley had a twin brother (who died at birth). What\nis the probability that Elvis was an identical twin? (You may approximate the probability\nof a boy or girl birth as 1\n\n2 .)\n\n7. Conditional probability: the following problem is loosely based on the television game\nshow Let’s Make a Deal. At the end of the show, a contestant is asked to choose one of\nthree large boxes, where one box contains a fabulous prize and the other two boxes contain\nlesser prizes. After the contestant chooses a box, Monty Hall, the host of the show,\nopens one of the two boxes containing smaller prizes. (In order to keep the conclusion\nsuspenseful, Monty does not open the box selected by the contestant.) Monty offers the\ncontestant the opportunity to switch from the chosen box to the remaining unopened box.\nShould the contestant switch or stay with the original choice? Calculate the probability\nthat the contestant wins under each strategy. This is an exercise in being clear about the\ninformation that should be conditioned on when constructing a probability judgment.\nSee Selvin (1975) and Morgan et al. (1991) for further discussion of this problem.\n\n8. Subjective probability: discuss the following statement. ‘The probability of event E is\nconsidered “subjective” if two rational persons A and B can assign unequal probabilities\nto E, PA(E) and PB(E). These probabilities can also be interpreted as “conditional”:\nPA(E) = P (E|IA) and PB(E) = P (E|IB), where IA and IB represent the knowledge\navailable to persons A and B, respectively.’ Apply this idea to the following examples.\n\n(a) The probability that a ‘6’ appears when a fair die is rolled, where A observes the\noutcome of the die roll and B does not.\n\n(b) The probability that Brazil wins the next World Cup, where A is ignorant of soccer\nand B is a knowledgeable sports fan.\n\n9. Simulation of a queuing problem: a clinic has three doctors. Patients come into the\nclinic at random, starting at 9 a.m., according to a Poisson process with time parameter\n10 minutes: that is, the time after opening at which the first patient appears follows an\nexponential distribution with expectation 10 minutes and then, after each patient arrives,\nthe waiting time until the next patient is independently exponentially distributed, also\nwith expectation 10 minutes. When a patient arrives, he or she waits until a doctor\nis available. The amount of time spent by each doctor with each patient is a random\nvariable, uniformly distributed between 5 and 20 minutes. The office stops admitting\nnew patients at 4 p.m. and closes when the last patient is through with the doctor.\n\n(a) Simulate this process once. How many patients came to the office? How many had to\nwait for a doctor? What was their average wait? When did the office close?\n\n(b) Simulate the process 100 times and estimate the median and 50% interval for each of\nthe summaries in (a).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 2\n\nSingle-parameter models\n\nOur first detailed discussion of Bayesian inference is in the context of statistical models\nwhere only a single scalar parameter is to be estimated; that is, the estimand θ is one-\ndimensional. In this chapter, we consider four fundamental and widely used one-dimensional\nmodels—the binomial, normal, Poisson, and exponential—and at the same time introduce\nimportant concepts and computational methods for Bayesian data analysis.\n\n2.1 Estimating a probability from binomial data\n\nIn the simple binomial model, the aim is to estimate an unknown population proportion\nfrom the results of a sequence of ‘Bernoulli trials’; that is, data y1, . . . , yn, each of which is\neither 0 or 1. This problem provides a relatively simple but important starting point for\nthe discussion of Bayesian inference. By starting with the binomial model, our discussion\nalso parallels the very first published Bayesian analysis by Thomas Bayes in 1763, and his\nseminal contribution is still of interest.\n\nThe binomial distribution provides a natural model for data that arise from a sequence\nof n exchangeable trials or draws from a large population where each trial gives rise to\none of two possible outcomes, conventionally labeled ‘success’ and ‘failure.’ Because of the\nexchangeability, the data can be summarized by the total number of successes in the n\ntrials, which we denote here by y. Converting from a formulation in terms of exchangeable\ntrials to one using independent and identically distributed random variables is achieved\nnaturally by letting the parameter θ represent the proportion of successes in the population\nor, equivalently, the probability of success in each trial. The binomial sampling model is,\n\np(y|θ) = Bin(y|n, θ) =\n(\nn\n\ny\n\n)\nθy(1 − θ)n−y, (2.1)\n\nwhere on the left side we suppress the dependence on n because it is regarded as part of the\nexperimental design that is considered fixed; all the probabilities discussed for this problem\nare assumed to be conditional on n.\n\nExample. Estimating the probability of a female birth\nAs a specific application of the binomial model, we consider the estimation of the\nsex ratio within a population of human births. The proportion of births that are\nfemale has long been a topic of interest both scientifically and to the lay public. Two\nhundred years ago it was established that the proportion of female births in European\npopulations was less than 0.5 (see Historical Note below), while in this century interest\nhas focused on factors that may influence the sex ratio. The currently accepted value\nof the proportion of female births in large European-race populations is 0.485.\nFor this example we define the parameter θ to be the proportion of female births, but\nan alternative way of reporting this parameter is as a ratio of male to female birth\nrates, φ = (1− θ)/θ.\nLet y be the number of girls in n recorded births. By applying the binomial model\n\n29\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n30 2. SINGLE-PARAMETER MODELS\n\nFigure 2.1 Unnormalized posterior density for binomial parameter θ, based on uniform prior dis-\ntribution and y successes out of n trials. Curves displayed for several values of n and y.\n\n(2.1), we are assuming that the n births are conditionally independent given θ, with\nthe probability of a female birth equal to θ for all cases. This modeling assumption\nis motivated by the exchangeability that may be judged to arise when we have no\nexplanatory information (for example, distinguishing multiple births or births within\nthe same family) that might affect the sex of the baby.\n\nTo perform Bayesian inference in the binomial model, we must specify a prior distribu-\ntion for θ. We will discuss issues associated with specifying prior distributions many times\nthroughout this book, but for simplicity at this point, we assume that the prior distribution\nfor θ is uniform on the interval [0, 1].\n\nElementary application of Bayes’ rule as displayed in (1.2), applied to (2.1), then gives\nthe posterior density for θ as\n\np(θ|y) ∝ θy(1 − θ)n−y. (2.2)\n\nWith fixed n and y, the factor\n(\nn\ny\n\n)\ndoes not depend on the unknown parameter θ, and so it\n\ncan be treated as a constant when calculating the posterior distribution of θ. As is typical\nof many examples, the posterior density can be written immediately in closed form, up to a\nconstant of proportionality. In single-parameter problems, this allows immediate graphical\npresentation of the posterior distribution. For example, in Figure 2.1, the unnormalized\ndensity (2.2) is displayed for several different experiments, that is, different values of n and\ny. Each of the four experiments has the same proportion of successes, but the sample sizes\nvary. In the present case, we can recognize (2.2) as the unnormalized form of the beta\ndistribution (see Appendix A),\n\nθ|y ∼ Beta(y + 1, n− y + 1). (2.3)\n\nHistorical note: Bayes and Laplace\nMany early writers on probability dealt with the elementary binomial model. The first\ncontributions of lasting significance, in the 17th and early 18th centuries, concentrated\non the ‘pre-data’ question: given θ, what are the probabilities of the various possible\noutcomes of the random variable y? For example, the ‘weak law of large numbers’ of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.1. ESTIMATING A PROBABILITY FROM BINOMIAL DATA 31\n\nJacob Bernoulli states that if y ∼ Bin(n, θ), then Pr( | yn − θ|>ǫ\n∣∣ θ) → 0 as n → ∞,\n\nfor any θ and any fixed value of ǫ > 0. The Reverend Thomas Bayes, an English\npart-time mathematician whose work was unpublished during his lifetime, and Pierre\nSimon Laplace, an inventive and productive mathematical scientist whose massive\noutput spanned the Napoleonic era in France, receive independent credit as the first\nto invert the probability statement and obtain probability statements about θ, given\nobserved y.\nIn his famous paper, published in 1763, Bayes sought, in our notation, the probability\nPr(θ∈ (θ1, θ2)|y); his solution was based on a physical analogy of a probability space\nto a rectangular table (such as a billiard table):\n\n1. (Prior distribution) A ballW is randomly thrown (according to a uniform distribu-\ntion on the table). The horizontal position of the ball on the table is θ, expressed\nas a fraction of the table width.\n\n2. (Likelihood) A ball O is randomly thrown n times. The value of y is the number\nof times O lands to the right of W .\n\nThus, θ is assumed to have a (prior) uniform distribution on [0, 1]. Using direct\nprobability calculations which he derived in the paper, Bayes then obtained\n\nPr(θ∈(θ1, θ2)|y) =\nPr(θ∈(θ1, θ2), y)\n\np(y)\n\n=\n\n∫ θ2\nθ1\np(y|θ)p(θ)dθ\np(y)\n\n=\n\n∫ θ2\nθ1\n\n(\nn\ny\n\n)\nθy(1− θ)n−ydθ\n\np(y)\n. (2.4)\n\nBayes succeeded in evaluating the denominator, showing that\n\np(y) =\n\n∫ 1\n\n0\n\n(\nn\n\ny\n\n)\nθy(1− θ)n−ydθ (2.5)\n\n=\n1\n\nn+ 1\nfor y = 0, . . . , n.\n\nThis calculation shows that all possible values of y are equally likely a priori.\nThe numerator of (2.4) is an incomplete beta integral with no closed-form expression\nfor large values of y and (n− y), a fact that apparently presented some difficulties for\nBayes.\nLaplace, however, independently ‘discovered’ Bayes’ theorem, and developed new ana-\nlytic tools for computing integrals. For example, he expanded the function θy(1− θ)n−y\naround its maximum at θ = y/n and evaluated the incomplete beta integral using what\nwe now know as the normal approximation.\nIn analyzing the binomial model, Laplace also used the uniform prior distribution. His\nfirst serious application was to estimate the proportion of girl births in a population.\nA total of 241,945 girls and 251,527 boys were born in Paris from 1745 to 1770. Letting\nθ be the probability that any birth is female, Laplace showed that\n\nPr(θ ≥ 0.5|y = 241,945, n = 251,527+ 241,945) ≈ 1.15× 10−42,\n\nand so he was ‘morally certain’ that θ < 0.5.\n\nPrediction\n\nIn the binomial example with the uniform prior distribution, the prior predictive distribution\ncan be evaluated explicitly, as we have already noted in (2.5). Under the model, all possible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n32 2. SINGLE-PARAMETER MODELS\n\nvalues of y are equally likely, a priori. For posterior prediction from this model, we might\nbe more interested in the outcome of one new trial, rather than another set of n new trials.\nLetting ỹ denote the result of a new trial, exchangeable with the first n,\n\nPr(ỹ = 1|y) =\n\n∫ 1\n\n0\n\nPr(ỹ = 1|θ, y)p(θ|y)dθ\n\n=\n\n∫ 1\n\n0\n\nθp(θ|y)dθ = E(θ|y) = y + 1\n\nn+ 2\n, (2.6)\n\nfrom the properties of the beta distribution (see Appendix A). It is left as an exercise to\nreproduce this result using direct integration of (2.6). This result, based on the uniform\nprior distribution, is known as ‘Laplace’s law of succession.’ At the extreme observations\ny = 0 and y = n, Laplace’s law predicts probabilities of 1\n\nn+2 and n+1\nn+2 , respectively.\n\n2.2 Posterior as compromise between data and prior information\n\nThe process of Bayesian inference involves passing from a prior distribution, p(θ), to a\nposterior distribution, p(θ|y), and it is natural to expect that some general relations might\nhold between these two distributions. For example, we might expect that, because the\nposterior distribution incorporates the information from the data, it will be less variable than\nthe prior distribution. This notion is formalized in the second of the following expressions:\n\nE(θ) = E(E(θ|y)) (2.7)\n\nand\nvar(θ) = E(var(θ|y)) + var(E(θ|y)), (2.8)\n\nwhich are obtained by substituting (θ, y) for the generic (u, v) in (1.8) and (1.9). The result\nexpressed by Equation (2.7) is scarcely surprising: the prior mean of θ is the average of all\npossible posterior means over the distribution of possible data. The variance formula (2.8)\nis more interesting because it says that the posterior variance is on average smaller than\nthe prior variance, by an amount that depends on the variation in posterior means over\nthe distribution of possible data. The greater the latter variation, the more the potential\nfor reducing our uncertainty with regard to θ, as we shall see in detail for the binomial\nand normal models in the next chapter. The mean and variance relations only describe\nexpectations, and in particular situations the posterior variance can be similar to or even\nlarger than the prior variance (although this can be an indication of conflict or inconsistency\nbetween the sampling model and prior distribution).\n\nIn the binomial example with the uniform prior distribution, the prior mean is 1\n2 , and\n\nthe prior variance is 1\n12 . The posterior mean, y+1\n\nn+2 , is a compromise between the prior mean\nand the sample proportion, yn , where clearly the prior mean has a smaller and smaller role\nas the size of the data sample increases. This is a general feature of Bayesian inference: the\nposterior distribution is centered at a point that represents a compromise between the prior\ninformation and the data, and the compromise is controlled to a greater extent by the data\nas the sample size increases.\n\n2.3 Summarizing posterior inference\n\nThe posterior probability distribution contains all the current information about the pa-\nrameter θ. Ideally one might report the entire posterior distribution p(θ|y); as we have seen\nin Figure 2.1, a graphical display is useful. In Chapter 3, we use contour plots and scat-\nterplots to display posterior distributions in multiparameter problems. A key advantage of\nthe Bayesian approach, as implemented by simulation, is the flexibility with which posterior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.3. SUMMARIZING POSTERIOR INFERENCE 33\n\nFigure 2.2 Hypothetical density for which the 95% central interval and 95% highest posterior density\nregion dramatically differ: (a) central posterior interval, (b) highest posterior density region.\n\ninferences can be summarized, even after complicated transformations. This advantage is\nmost directly seen through examples, some of which will be presented shortly.\n\nFor many practical purposes, however, various numerical summaries of the distribu-\ntion are desirable. Commonly used summaries of location are the mean, median, and\nmode(s) of the distribution; variation is commonly summarized by the standard deviation,\nthe interquartile range, and other quantiles. Each summary has its own interpretation: for\nexample, the mean is the posterior expectation of the parameter, and the mode may be\ninterpreted as the single ‘most likely’ value, given the data (and the model). Furthermore,\nas we shall see, much practical inference relies on the use of normal approximations, often\nimproved by applying a symmetrizing transformation to θ, and here the mean and the stan-\ndard deviation play key roles. The mode is important in computational strategies for more\ncomplex problems because it is often easier to compute than the mean or median.\n\nWhen the posterior distribution has a closed form, such as the beta distribution in\nthe current example, summaries such as the mean, median, and standard deviation of\nthe posterior distribution are often available in closed form. For example, applying the\ndistributional results in Appendix A, the mean of the beta distribution in (2.3) is y+1\n\nn+2 , and\nthe mode is y\n\nn , which is well known from different points of view as the maximum likelihood\nand (minimum variance) unbiased estimate of θ.\n\nPosterior quantiles and intervals\n\nIn addition to point summaries, it is nearly always important to report posterior uncertainty.\nOur usual approach is to present quantiles of the posterior distribution of estimands of\ninterest or, if an interval summary is desired, a central interval of posterior probability,\nwhich corresponds, in the case of a 100(1− α)% interval, to the range of values above and\nbelow which lies exactly 100(α/2)% of the posterior probability. Such interval estimates\nare referred to as posterior intervals. For simple models, such as the binomial and normal,\nposterior intervals can be computed directly from cumulative distribution functions, often\nusing calls to standard computer functions, as we illustrate in Section 2.4 with the example\nof the human sex ratio. In general, intervals can be computed using computer simulations\nfrom the posterior distribution, as described at the end of Section 1.9.\n\nA slightly different summary of posterior uncertainty is the highest posterior density\nregion: the set of values that contains 100(1 − α)% of the posterior probability and also\nhas the characteristic that the density within the region is never lower than that outside.\nSuch a region is identical to a central posterior interval if the posterior distribution is\nunimodal and symmetric. In current practice, the central posterior interval is in common\nuse, partly because it has a direct interpretation as the posterior α/2 and 1−α/2 quantiles,\nand partly because it is directly computed using posterior simulations. Figure 2.2 shows\na case where different posterior summaries look much different: the 95% central interval\nincludes the area of zero probability in the center of the distribution, whereas the 95%\nhighest posterior density region comprises two disjoint intervals. In this situation, the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n34 2. SINGLE-PARAMETER MODELS\n\nhighest posterior density region is more cumbersome but conveys more information than\nthe central interval; however, it is probably better not to try to summarize this bimodal\ndensity by any single interval. The central interval and the highest posterior density region\ncan also differ substantially when the posterior density is highly skewed.\n\n2.4 Informative prior distributions\n\nIn the binomial example, we have so far considered only the uniform prior distribution for\nθ. How can this specification be justified, and how in general do we approach the problem\nof constructing prior distributions?\n\nWe consider two basic interpretations that can be given to prior distributions. In the\npopulation interpretation, the prior distribution represents a population of possible parame-\nter values, from which the θ of current interest has been drawn. In the more subjective state\nof knowledge interpretation, the guiding principle is that we must express our knowledge\n(and uncertainty) about θ as if its value could be thought of as a random realization from\nthe prior distribution. For many problems, such as estimating the probability of failure in\na new industrial process, there is no perfectly relevant population of θ’s from which the\ncurrent θ has been drawn, except in hypothetical contemplation. Typically, the prior distri-\nbution should include all plausible values of θ, but the distribution need not be realistically\nconcentrated around the true value, because often the information about θ contained in the\ndata will far outweigh any reasonable prior probability specification.\n\nIn the binomial example, we have seen that the uniform prior distribution for θ im-\nplies that the prior predictive distribution for y (given n) is uniform on the discrete set\n{0, 1, . . . , n}, giving equal probability to the n+1 possible values. In his original treatment\nof this problem (described in the Historical Note in Section 2.1), Bayes’ justification for the\nuniform prior distribution appears to have been based on this observation; the argument\nis appealing because it is expressed entirely in terms of the observable quantities y and n.\nLaplace’s rationale for the uniform prior density was less clear, but subsequent interpre-\ntations ascribe to him the so-called ‘principle of insufficient reason,’ which claims that a\nuniform specification is appropriate if nothing is known about θ. We shall discuss in Section\n2.8 the weaknesses of the principle of insufficient reason as a general approach for assigning\nprobability distributions.\n\nAt this point, we discuss some of the issues that arise in assigning a prior distribution\nthat reflects substantive information.\n\nBinomial example with different prior distributions\n\nWe first pursue the binomial model in further detail using a parametric family of prior\ndistributions that includes the uniform as a special case. For mathematical convenience, we\nconstruct a family of prior densities that lead to simple posterior densities.\n\nConsidered as a function of θ, the likelihood (2.1) is of the form,\n\np(y|θ) ∝ θa(1 − θ)b.\n\nThus, if the prior density is of the same form, with its own values a and b, then the posterior\ndensity will also be of this form. We will parameterize such a prior density as\n\np(θ) ∝ θα−1(1− θ)β−1,\n\nwhich is a beta distribution with parameters α and β: θ ∼ Beta(α, β). Comparing p(θ) and\np(y|θ) suggests that this prior density is equivalent to α− 1 prior successes and β − 1 prior\nfailures. The parameters of the prior distribution are often referred to as hyperparameters.\nThe beta prior distribution is indexed by two hyperparameters, which means we can specify\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 35\n\na particular prior distribution by fixing two features of the distribution, for example its mean\nand variance; see (A.3) on page 585.\n\nFor now, assume that we can select reasonable values α and β. Appropriate methods\nfor working with unknown hyperparameters in certain problems are described in Chapter\n5. The posterior density for θ is\n\np(θ|y) ∝ θy(1− θ)n−yθα−1(1− θ)β−1\n\n= θy+α−1(1− θ)n−y+β−1\n\n= Beta(θ|α + y, β + n− y).\n\nThe property that the posterior distribution follows the same parametric form as the\nprior distribution is called conjugacy; the beta prior distribution is a conjugate family for\nthe binomial likelihood. The conjugate family is mathematically convenient in that the\nposterior distribution follows a known parametric form. If information is available that\ncontradicts the conjugate parametric family, it may be necessary to use a more realistic, if\ninconvenient, prior distribution (just as the binomial likelihood may need to be replaced by\na more realistic likelihood in some cases).\n\nTo continue with the binomial model with beta prior distribution, the posterior mean of\nθ, which may be interpreted as the posterior probability of success for a future draw from\nthe population, is now\n\nE(θ|y) = α+ y\n\nα+ β + n\n,\n\nwhich always lies between the sample proportion, y/n, and the prior mean, α/(α+ β); see\nExercise 2.5b. The posterior variance is\n\nvar(θ|y) = (α+ y)(β + n− y)\n(α+ β + n)2(α+ β + n+ 1)\n\n=\nE(θ|y)[1− E(θ|y)]\nα+ β + n+ 1\n\n.\n\nAs y and n− y become large with fixed α and β, E(θ|y) ≈ y/n and var(θ|y) ≈ 1\nn\ny\nn (1−\n\ny\nn ),\n\nwhich approaches zero at the rate 1/n. In the limit, the parameters of the prior distribution\nhave no influence on the posterior distribution.\n\nIn fact, as we shall see in more detail in Chapter 4, the central limit theorem of proba-\nbility theory can be put in a Bayesian context to show:\n\n(\nθ − E(θ|y)√\n\nvar(θ|y)\n\n∣∣∣∣∣ y\n)\n→ N(0, 1).\n\nThis result is often used to justify approximating the posterior distribution with a normal\ndistribution. For the binomial parameter θ, the normal distribution is a more accurate\napproximation in practice if we transform θ to the logit scale; that is, performing inference\nfor log(θ/(1 − θ)) instead of θ itself, thus expanding the probability space from [0, 1] to\n(−∞,∞), which is more fitting for a normal approximation.\n\nConjugate prior distributions\n\nConjugacy is formally defined as follows. If F is a class of sampling distributions p(y|θ),\nand P is a class of prior distributions for θ, then the class P is conjugate for F if\n\np(θ|y) ∈ P for all p(·|θ) ∈ F and p(·) ∈ P .\n\nThis definition is formally vague since if we choose P as the class of all distributions, then\nP is always conjugate no matter what class of sampling distributions is used. We are most\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n36 2. SINGLE-PARAMETER MODELS\n\ninterested in natural conjugate prior families, which arise by taking P to be the set of all\ndensities having the same functional form as the likelihood.\n\nConjugate prior distributions have the practical advantage, in addition to computational\nconvenience, of being interpretable as additional data, as we have seen for the binomial\nexample and will also see for the normal and other standard models in Sections 2.5 and 2.6.\n\nNonconjugate prior distributions\n\nThe basic justification for the use of conjugate prior distributions is similar to that for using\nstandard models (such as binomial and normal) for the likelihood: it is easy to understand\nthe results, which can often be put in analytic form, they are often a good approximation,\nand they simplify computations. Also, they will be useful later as building blocks for more\ncomplicated models, including in many dimensions, where conjugacy is typically impossible.\nFor these reasons, conjugate models can be good starting points; for example, mixtures of\nconjugate families can sometimes be useful when simple conjugate distributions are not\nreasonable (see Exercise 2.4).\n\nAlthough they can make interpretations of posterior inferences less transparent and\ncomputation more difficult, nonconjugate prior distributions do not pose any new conceptual\nproblems. In practice, for complicated models, conjugate prior distributions may not even\nbe possible. Section 2.4 and Exercises 2.10 and 2.11 present examples of nonconjugate\ncomputation; a more extensive nonconjugate example, an analysis of a bioassay experiment,\nappears in Section 3.7.\n\nConjugate prior distributions, exponential families, and sufficient statistics\n\nWe close this section by relating conjugate families of distributions to the classical concepts\nof exponential families and sufficient statistics. Readers who are unfamiliar with these\nconcepts can skip ahead to the example with no loss.\n\nProbability distributions that belong to an exponential family have natural conjugate\nprior distributions, so we digress at this point to review the definition of exponential families;\nfor complete generality in this section, we allow data points yi and parameters θ to be\nmultidimensional. The class F is an exponential family if all its members have the form,\n\np(yi|θ) = f(yi)g(θ)e\nφ(θ)Tu(yi).\n\nThe factors φ(θ) and u(yi) are, in general, vectors of equal dimension to that of θ. The\nvector φ(θ) is called the ‘natural parameter’ of the family F . The likelihood corresponding\nto a sequence y = (y1, . . . , yn) of independent and identically distributed observations is\n\np(y|θ) =\n(\n\nn∏\n\ni=1\n\nf(yi)\n\n)\ng(θ)n exp\n\n(\nφ(θ)T\n\nn∑\n\ni=1\n\nu(yi)\n\n)\n.\n\nFor all n and y, this has a fixed form (as a function of θ):\n\np(y|θ) ∝ g(θ)neφ(θ)T t(y), where t(y) =\n\nn∑\n\ni=1\n\nu(yi).\n\nThe quantity t(y) is said to be a sufficient statistic for θ, because the likelihood for θ\ndepends on the data y only through the value of t(y). Sufficient statistics are useful in\nalgebraic manipulations of likelihoods and posterior distributions. If the prior density is\nspecified as\n\np(θ) ∝ g(θ)ηeφ(θ)T ν ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.4. INFORMATIVE PRIOR DISTRIBUTIONS 37\n\nthen the posterior density is\n\np(θ|y) ∝ g(θ)η+neφ(θ)T (ν+t(y)),\n\nwhich shows that this choice of prior density is conjugate. It has been shown that, in general,\nthe exponential families are the only classes of distributions that have natural conjugate\nprior distributions, since, apart from certain irregular cases, the only distributions having\na fixed number of sufficient statistics for all n are of the exponential type. We have already\ndiscussed the binomial distribution, where for the likelihood p(y|θ, n) = Bin(y|n, θ) with n\nknown, the conjugate prior distributions on θ are beta distributions. It is left as an exercise\nto show that the binomial is an exponential family with natural parameter logit(θ).\n\nExample. Probability of a girl birth given placenta previa\nAs a specific example of a factor that may influence the sex ratio, we consider the\nmaternal condition placenta previa, an unusual condition of pregnancy in which the\nplacenta is implanted low in the uterus, obstructing the fetus from a normal vaginal\ndelivery. An early study concerning the sex of placenta previa births in Germany found\nthat of a total of 980 births, 437 were female. How much evidence does this provide\nfor the claim that the proportion of female births in the population of placenta previa\nbirths is less than 0.485, the proportion of female births in the general population?\n\nAnalysis using a uniform prior distribution. Under a uniform prior distribution for\nthe probability of a girl birth, the posterior distribution is Beta(438, 544). Exact\nsummaries of the posterior distribution can be obtained from the properties of the\nbeta distribution (Appendix A): the posterior mean of θ is 0.446 and the posterior\nstandard deviation is 0.016. Exact posterior quantiles can be obtained using numerical\nintegration of the beta density, which in practice we perform by a computer function\ncall; the median is 0.446 and the central 95% posterior interval is [0.415, 0.477]. This\n95% posterior interval matches, to three decimal places, the interval that would be\nobtained by using a normal approximation with the calculated posterior mean and\nstandard deviation. Further discussion of the approximate normality of the posterior\ndistribution is given in Chapter 4.\nIn many situations it is not feasible to perform calculations on the posterior density\nfunction directly. In such cases it can be particularly useful to use simulation from the\nposterior distribution to obtain inferences. The first histogram in Figure 2.3 shows the\ndistribution of 1000 draws from the Beta(438, 544) posterior distribution. An estimate\nof the 95% posterior interval, obtained by taking the 25th and 976th of the 1000\nordered draws, is [0.415, 0.476], and the median of the 1000 draws from the posterior\ndistribution is 0.446. The sample mean and standard deviation of the 1000 draws are\n0.445 and 0.016, almost identical to the exact results. A normal approximation to the\n95% posterior interval is [0.445 ± 1.96 · 0.016] = [0.414, 0.476]. Because of the large\nsample and the fact that the distribution of θ is concentrated away from zero and one,\nthe normal approximation works well in this example.\nAs already noted, when estimating a proportion, the normal approximation is gener-\nally improved by applying it to the logit transform, log( θ\n\n1−θ ), which transforms the\nparameter space from the unit interval to the real line. The second histogram in Figure\n2.3 shows the distribution of the transformed draws. The estimated posterior mean\nand standard deviation on the logit scale based on 1000 draws are −0.220 and 0.065.\nA normal approximation to the 95% posterior interval for θ is obtained by inverting\nthe 95% interval on the logit scale [−0.220± 1.96 · 0.065], which yields [0.414, 0.477]\non the original scale. The improvement from using the logit scale is most noticeable\nwhen the sample size is small or the distribution of θ includes values near zero or one.\nIn any real data analysis, it is important to keep the applied context in mind. The pa-\nrameter of interest in this example is traditionally expressed as the ‘sex ratio,’ (1−θ)/θ,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n38 2. SINGLE-PARAMETER MODELS\n\nFigure 2.3 Draws from the posterior distribution of (a) the probability of female birth, θ; (b) the\nlogit transform, logit(θ); (c) the male-to-female sex ratio, φ = (1− θ)/θ.\n\nParameters of the Summaries of the\nprior distribution posterior distribution\n\nPosterior 95% posterior\nα\n\nα+β\nα+ β median of θ interval for θ\n\n0.500 2 0.446 [0.415, 0.477]\n0.485 2 0.446 [0.415, 0.477]\n0.485 5 0.446 [0.415, 0.477]\n0.485 10 0.446 [0.415, 0.477]\n0.485 20 0.447 [0.416, 0.478]\n0.485 100 0.450 [0.420, 0.479]\n0.485 200 0.453 [0.424, 0.481]\n\nTable 2.1 Summaries of the posterior distribution of θ, the probability of a girl birth given placenta\nprevia, under a variety of conjugate prior distributions.\n\nthe ratio of male to female births. The posterior distribution of the ratio is illustrated\nin the third histogram. The posterior median of the sex ratio is 1.24, and the 95%\nposterior interval is [1.10, 1.41]. The posterior distribution is concentrated on values\nfar above the usual European-race sex ratio of 1.06, implying that the probability of\na female birth given placenta previa is less than in the general population.\n\nAnalysis using different conjugate prior distributions. The sensitivity of posterior\ninference about θ to the proposed prior distribution is exhibited in Table 2.1. The\nfirst row corresponds to the uniform prior distribution, α=1, β=1, and subsequent\nrows of the table use prior distributions that are increasingly concentrated around\n0.485, the proportion of female births in the general population. The first column\nshows the prior mean for θ, and the second column indexes the amount of prior\ninformation, as measured by α+ β; recall that α+ β − 2 is, in some sense, equivalent\nto the number of prior observations. Posterior inferences based on a large sample are\nnot particularly sensitive to the prior distribution. Only at the bottom of the table,\nwhere the prior distribution contains information equivalent to 100 or 200 births, are\nthe posterior intervals pulled noticeably toward the prior distribution, and even then,\nthe 95% posterior intervals still exclude the prior mean.\n\nAnalysis using a nonconjugate prior distribution. As an alternative to the conjugate\nbeta family for this problem, we might prefer a prior distribution that is centered\naround 0.485 but is flat far away from this value to admit the possibility that the\ntruth is far away. The piecewise linear prior density in Figure 2.4a is an example\nof a prior distribution of this form; 40% of the probability mass is outside the inter-\nval [0.385, 0.585]. This prior distribution has mean 0.493 and standard deviation 0.21,\nsimilar to the standard deviation of a beta distribution with α+β = 5. The unnormal-\nized posterior distribution is obtained at a grid of θ values, (0.000, 0.001, . . . , 1.000),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 39\n\nFigure 2.4 (a) Prior density for θ in an example nonconjugate analysis of birth ratio example; (b)\nhistogram of 1000 draws from a discrete approximation to the posterior density. Figures are plotted\non different scales.\n\nby multiplying the prior density and the binomial likelihood at each point. Poste-\nrior simulations can be obtained by normalizing the distribution on the discrete grid\nof θ values. Figure 2.4b is a histogram of 1000 draws from the discrete posterior\ndistribution. The posterior median is 0.448, and the 95% central posterior interval is\n[0.419, 0.480]. Because the prior distribution is overwhelmed by the data, these results\nmatch those in Table 2.1 based on beta distributions. In taking the grid approach, it\nis important to avoid grids that are too coarse and distort a significant portion of the\nposterior mass.\n\n2.5 Normal distribution with known variance\n\nThe normal distribution is fundamental to most statistical modeling. The central limit\ntheorem helps to justify using the normal likelihood in many statistical problems, as an\napproximation to a less analytically convenient actual likelihood. Also, as we shall see in\nlater chapters, even when the normal distribution does not itself provide a good model fit,\nit can be useful as a component of a more complicated model involving t or finite mixture\ndistributions. For now, we simply work through the Bayesian results assuming the normal\nmodel is appropriate. We derive results first for a single data point and then for the general\ncase of many data points.\n\nLikelihood of one data point\n\nAs the simplest first case, consider a single scalar observation y from a normal distribution\nparameterized by a mean θ and variance σ2, where for this initial development we assume\nthat σ2 is known. The sampling distribution is\n\np(y|θ) = 1√\n2πσ\n\ne−\n1\n\n2σ2 (y−θ)2.\n\nConjugate prior and posterior distributions\n\nConsidered as a function of θ, the likelihood is an exponential of a quadratic form in θ, so\nthe family of conjugate prior densities looks like\n\np(θ) = eAθ\n2+Bθ+C .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n40 2. SINGLE-PARAMETER MODELS\n\nWe parameterize this family as\n\np(θ) ∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n)\n;\n\nthat is, θ ∼ N(µ0, τ\n2\n0 ), with hyperparameters µ0 and τ20 . As usual in this preliminary\n\ndevelopment, we assume that the hyperparameters are known.\nThe conjugate prior density implies that the posterior distribution for θ is the exponential\n\nof a quadratic form and thus normal, but some algebra is required to reveal its specific\nform. In the posterior density, all variables except θ are regarded as constants, giving the\nconditional density,\n\np(θ|y) ∝ exp\n\n(\n−1\n\n2\n\n(\n(y − θ)2\nσ2\n\n+\n(θ − µ0)\n\n2\n\nτ20\n\n))\n.\n\nExpanding the exponents, collecting terms and then completing the square in θ (see Exercise\n2.14(a) for details) gives\n\np(θ|y) ∝ exp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\n, (2.9)\n\nthat is, θ|y ∼ N(µ1, τ\n2\n1 ), where\n\nµ1 =\n\n1\nτ2\n0\nµ0 +\n\n1\nσ2 y\n\n1\nτ2\n0\n+ 1\n\nσ2\n\nand\n1\n\nτ21\n=\n\n1\n\nτ20\n+\n\n1\n\nσ2\n. (2.10)\n\nPrecisions of the prior and posterior distributions. In manipulating normal distributions,\nthe inverse of the variance plays a prominent role and is called the precision. The algebra\nabove demonstrates that for normal data and normal prior distribution (each with known\nprecision), the posterior precision equals the prior precision plus the data precision.\n\nThere are several different ways of interpreting the form of the posterior mean, µ1. In\n(2.10), the posterior mean is expressed as a weighted average of the prior mean and the\nobserved value, y, with weights proportional to the precisions. Alternatively, we can express\nµ1 as the prior mean adjusted toward the observed y,\n\nµ1 = µ0 + (y − µ0)\nτ20\n\nσ2 + τ20\n,\n\nor as the data ‘shrunk’ toward the prior mean,\n\nµ1 = y − (y − µ0)\nσ2\n\nσ2 + τ20\n.\n\nEach formulation represents the posterior mean as a compromise between the prior mean\nand the observed value.\n\nAt the extremes, the posterior mean equals the prior mean or the observed data:\n\nµ1 = µ0 if y = µ0 or τ20 = 0;\n\nµ1 = y if y = µ0 or σ2 = 0.\n\nIf τ20 = 0, the prior distribution is infinitely more precise than the data, and so the posterior\nand prior distributions are identical and concentrated at the value µ0. If σ2 = 0, the data\nare perfectly precise, and the posterior distribution is concentrated at the observed value,\ny. If y = µ0, the prior and data means coincide, and the posterior mean must also fall at\nthis point.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\n\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, ỹ, p(ỹ|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ỹ|y) =\n\n∫\np(ỹ|θ)p(θ|y)dθ\n\n∝\n∫\n\nexp\n\n(\n− 1\n\n2σ2\n(ỹ − θ)2\n\n)\nexp\n\n(\n− 1\n\n2τ21\n(θ − µ1)\n\n2\n\n)\ndθ.\n\nThe first line above holds because the distribution of the future observation, ỹ, given θ,\ndoes not depend on the past data, y. We can determine the distribution of ỹ more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (ỹ, θ); hence ỹ and θ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of ỹ is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(ỹ|θ) = θ and var(ỹ|θ) = σ2, along\nwith identities (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|θ, y)|y) = E(θ|y) = µ1,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|θ, y)|y) + var(E(ỹ|θ, y)|y)\n= E(σ2|y) + var(θ|y)\n= σ2 + τ21 .\n\nThus, the posterior predictive distribution of ỹ has mean equal to the posterior mean of θ\nand two components of variance: the predictive variance σ2 from the model and the variance\nτ21 due to posterior uncertainty in θ.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (y1, . . . , yn) is available. Proceeding formally, the posterior density is\n\np(θ|y) ∝ p(θ)p(y|θ)\n\n= p(θ)\n\nn∏\n\ni=1\n\np(yi|θ)\n\n∝ exp\n\n(\n− 1\n\n2τ20\n(θ − µ0)\n\n2\n\n) n∏\n\ni=1\n\nexp\n\n(\n− 1\n\n2σ2\n(yi − θ)2\n\n)\n\n∝ exp\n\n(\n−1\n\n2\n\n(\n1\n\nτ20\n(θ − µ0)\n\n2 +\n1\n\nσ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n))\n\n.\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = 1\n\nn\n\n∑\ni yi; that is, y is a sufficient statistic\n\nin this model. In fact, since y|θ, σ2 ∼ N(θ, σ2/n), the results derived for the single normal\nobservation apply immediately (treating y as the single observation) to give\n\np(θ|y1 . . . , yn) = p(θ|y) = N(θ|µn, τ2n), (2.11)\n\nThis electronic edition is for non-commercial purposes only.\n\n2.5. NORMAL DISTRIBUTION WITH KNOWN VARIANCE 41\nPosterior predictive distribution\n\nThe posterior predictive distribution of a future observation, y, p(y|y), can be calculated\ndirectly by integration, using (1.4):\n\np(ily) = / p(Gl0)p(6ly)a6\n\nx few(-daa-0\") an (-y0-mr)an\n\nThe first line above holds because the distribution of the future observation, y, given 6,\ndoes not depend on the past data, y. We can determine the distribution of y more easily\nusing the properties of the bivariate normal distribution. The product in the integrand is\nthe exponential of a quadratic function of (g, 8); hence 7 and @ have a joint normal posterior\ndistribution, and so the marginal posterior distribution of y is normal.\n\nWe can determine the mean and variance of the posterior predictive distribution using\nthe knowledge from the posterior distribution that E(g|@) = 6 and var(g|9) = 07, along\nwith identities (2.7) and (2.8):\n\nE(gly) = E(E(g|@, y)ly) = Ely) = 11,\nand\n\nvar(yly) = E(var(g|@,y)|y) + var(E(g/6, y)|y)\nE(o?|y) + var(4|y)\n\nor +77.\n\nThus, the posterior predictive distribution of y has mean equal to the posterior mean of 0\nand two components of variance: the predictive variance a? from the model and the variance\n7? due to posterior uncertainty in 0.\n\nNormal model with multiple observations\n\nThis development of the normal model with a single observation can be easily extended\nto the more realistic situation where a sample of independent and identically distributed\nobservations y = (yi,---, Yn) is available. Proceeding formally, the posterior density is\n\nPly) x p(O)p(yl@)\n\nR\noO)\n%\nso}\n“~\nwo |\naN\nOw]\nsS\n|\n=\nS\n“—\"\nbo\n+4\nSI —\nil =\nmn\nS\n|\nS\niw)\nNe\nNe\n\nAlgebraic simplification of this expression (along similar lines to those used in the single\nobservation case, as explicated in Exercise 2.14(b)) shows that the posterior distribution\ndepends on y only through the sample mean, y = + 2, wii that is, 7 is a sufficient statistic\nin this model. In fact, since 9|9,07 ~ N(0,07/n), the results derived for the single normal\nobservation apply immediately (treating 7 as the single observation) to give\n\nP(O\\y1 -- Yn) = PO) = N(O|kins Tr): (2.11)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n42 2. SINGLE-PARAMETER MODELS\n\nwhere\n\nµn =\n\n1\nτ2\n0\nµ0 +\n\nn\nσ2 y\n\n1\nτ2\n0\n+ n\n\nσ2\n\nand\n1\n\nτ2n\n=\n\n1\n\nτ20\n+\n\nn\n\nσ2\n. (2.12)\n\nIncidentally, the same result is obtained by adding information for the data y1, y2, . . . , yn\none point at a time, using the posterior distribution at each step as the prior distribution\nfor the next (see Exercise 2.14(c)).\n\nIn the expressions for the posterior mean and variance, the prior precision, 1/τ20 , and\nthe data precision, n/σ2, play equivalent roles, so if n is large, the posterior distribution\nis largely determined by σ2 and the sample value y. For example, if τ20 = σ2, then the\nprior distribution has the same weight as one extra observation with the value µ0. More\nspecifically, as τ0 →∞ with n fixed, or as n→∞ with τ20 fixed, we have:\n\np(θ|y) ≈ N(θ|y, σ2/n), (2.13)\n\nwhich is, in practice, a good approximation whenever prior beliefs are relatively diffuse over\nthe range of θ where the likelihood is substantial.\n\n2.6 Other standard single-parameter models\n\nRecall that, in general, the posterior density, p(θ|y), has no closed-form expression; the\nnormalizing constant, p(y), is often especially difficult to compute due to the integral (1.3).\nMuch formal Bayesian analysis concentrates on situations where closed forms are available;\nsuch models are sometimes unrealistic, but their analysis often provides a useful starting\npoint when it comes to constructing more realistic models.\n\nThe standard distributions—binomial, normal, Poisson, and exponential—have natural\nderivations from simple probability models. As we have already discussed, the binomial\ndistribution is motivated from counting exchangeable outcomes, and the normal distribu-\ntion applies to a random variable that is the sum of many exchangeable or independent\nterms. We will also have occasion to apply the normal distribution to the logarithm of all-\npositive data, which would naturally apply to observations that are modeled as the product\nof many independent multiplicative factors. The Poisson and exponential distributions arise\nas the number of counts and the waiting times, respectively, for events modeled as occur-\nring exchangeably in all time intervals; that is, independently in time, with a constant rate\nof occurrence. We will generally construct realistic probability models for more compli-\ncated outcomes by combinations of these basic distributions. For example, in Section 22.2,\nwe model the reaction times of schizophrenic patients in a psychological experiment as a\nbinomial mixture of normal distributions on the logarithmic scale.\n\nEach of these standard models has an associated family of conjugate prior distributions,\nwhich we discuss in turn.\n\nNormal distribution with known mean but unknown variance\n\nThe normal model with known mean θ and unknown variance is an important example,\nnot necessarily for its direct applied value, but as a building block for more complicated,\nuseful models, most immediately the normal distribution with unknown mean and variance,\nwhich we cover in Section 3.2. In addition, the normal distribution with known mean but\nunknown variance provides an introductory example of the estimation of a scale parameter.\n\nFor p(y|θ, σ2) = N(y|θ, σ2), with θ known and σ2 unknown, the likelihood for a vector\ny of n independent and identically distributed observations is\n\np(y|σ2) ∝ σ−n exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − θ)2\n)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43\n\n= (σ2)−n/2 exp\n(\n− n\n\n2σ2\nv\n)\n.\n\nThe sufficient statistic is\n\nv =\n1\n\nn\n\nn∑\n\ni=1\n\n(yi − θ)2.\n\nThe corresponding conjugate prior density is the inverse-gamma,\n\np(σ2) ∝ (σ2)−(α+1)e−β/σ\n2\n\n,\n\nwhich has hyperparameters (α, β). A convenient parameterization is as a scaled inverse-χ2\n\ndistribution with scale σ2\n0 and ν0 degrees of freedom (see Appendix A); that is, the prior\n\ndistribution of σ2 is taken to be the distribution of σ2\n0ν0/X , where X is a χ2\n\nν0 random\nvariable. We use the convenient but nonstandard notation, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0).\n\nThe resulting posterior density for σ2 is\n\np(σ2|y) ∝ p(σ2)p(y|σ2)\n\n∝\n(\nσ2\n0\n\nσ2\n\n)ν0/2+1\n\nexp\n\n(\n−ν0σ\n\n2\n0\n\n2σ2\n\n)\n· (σ2)−n/2 exp\n\n(\n−n\n2\n\nv\n\nσ2\n\n)\n\n∝ (σ2)−((n+ν0)/2+1) exp\n\n(\n− 1\n\n2σ2\n(ν0σ\n\n2\n0 + nv)\n\n)\n.\n\nThus,\n\nσ2|y ∼ Inv-χ2\n\n(\nν0 + n,\n\nν0σ\n2\n0 + nv\n\nν0 + n\n\n)\n,\n\nwhich is a scaled inverse-χ2 distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to ν0 observations with average squared deviation σ2\n\n0 .\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate θ, then the probability\ndistribution of a single observation y is\n\np(y|θ) = θye−θ\n\ny!\n, for y = 0, 1, 2, . . . ,\n\nand for a vector y = (y1, . . . , yn) of independent and identically distributed observations,\nthe likelihood is\n\np(y|θ) =\n\nn∏\n\ni=1\n\n1\n\nyi!\nθyie−θ\n\n∝ θt(y)e−nθ,\n\nwhere t(y) =\n∑n\ni=1 yi is the sufficient statistic. We can rewrite the likelihood in exponential\n\nfamily form as\n\np(y|θ) ∝ e−nθet(y) log θ,\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 43,\nThe sufficient statistic is\n\nThe corresponding conjugate prior density is the inverse-gamma,\npo?) x (0?) De Ble\",\n\nwhich has hyperparameters (a, 3). A convenient parameterization is as a scaled inverse-y?\ndistribution with scale 2 and vo degrees of freedom (see Appendix A); that is, the prior\ndistribution of 0? is taken to be the distribution of o§vo/X, where X is a x7, random\nvariable. We use the convenient but nonstandard notation, 0? ~ Inv-x?(vo, 08).\n\nThe resulting posterior density for a? is\n\np(o|y) x p(o)p(y|o”)\n2 Vo /2+1 2\n90 Y009 2\\—n/2 nv\n(3) exp ( pa) (a) exp (Fa)\n\n—Un+V 1\nx (a?) (( + 0) /2+1) exp (— spate? + nv) .\n\nThus,\n\nYoo + nu\nYo +n ,\n\no* ly ~ Inv-x? (» +n,\n\nwhich is a scaled inverse-? distribution with scale equal to the degrees-of-freedom-weighted\naverage of the prior and data scales and degrees of freedom equal to the sum of the prior\nand data degrees of freedom. The prior distribution can be thought of as providing the\ninformation equivalent to vp observations with average squared deviation o@.\n\nPoisson model\n\nThe Poisson distribution arises naturally in the study of data taking the form of counts;\nfor instance, a major area of application is epidemiology, where the incidence of diseases is\nstudied.\n\nIf a data point y follows the Poisson distribution with rate 0, then the probability\ndistribution of a single observation y is\n\nWe?\np(ylO) = Tn for y =0,1,2,...,\n\nand for a vector y = (y1,...,Yn) of independent and identically distributed observations,\nthe likelihood is\n\nnm\n\nLoy\np(y) = [[—e%e~®\n\nja Yt\n\n« Gy e—nF\n\nwhere t(y) = >>\", y; is the sufficient statistic. We can rewrite the likelihood in exponential\nfamily form as\n\np(y|0) x e Mel lee?\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n44 2. SINGLE-PARAMETER MODELS\n\nrevealing that the natural parameter is φ(θ) = log θ, and the natural conjugate prior distri-\nbution is\n\np(θ) ∝ (e−θ)ηeν log θ,\n\nindexed by hyperparameters (η, ν). To put this argument another way, the likelihood is of\nthe form θae−bθ, and so the conjugate prior density must be of the form p(θ) ∝ θAe−Bθ. In\na more conventional parameterization,\n\np(θ) ∝ e−βθθα−1,\n\nwhich is a gamma density with parameters α and β, Gamma(α, β); see Appendix A. Com-\nparing p(y|θ) and p(θ) reveals that the prior density is, in some sense, equivalent to a total\ncount of α− 1 in β prior observations. With this conjugate prior distribution, the posterior\ndistribution is\n\nθ|y ∼ Gamma(α + ny, β + n).\n\nThe negative binomial distribution. With conjugate families, the known form of the prior\nand posterior densities can be used to find the marginal distribution, p(y), using the formula\n\np(y) =\np(y|θ)p(θ)\np(θ|y) .\n\nFor instance, the Poisson model for a single observation, y, has prior predictive distribution\n\np(y) =\nPoisson(y|θ)Gamma(θ|α, β)\nGamma(θ|α+ y, 1 + β)\n\n=\nΓ(α+ y)βα\n\nΓ(α)y!(1 + β)α+y\n,\n\nwhich reduces to\n\np(y) =\n\n(\nα+ y − 1\n\ny\n\n)(\nβ\n\nβ + 1\n\n)α(\n1\n\nβ + 1\n\n)y\n,\n\nwhich is known as the negative binomial density:\n\ny ∼ Neg-bin(α, β).\n\nThe above derivation shows that the negative binomial distribution is a mixture of Poisson\ndistributions with rates, θ, that follow the gamma distribution:\n\nNeg-bin(y|α, β) =\n∫\n\nPoisson(y|θ)Gamma(θ|α, β)dθ.\n\nWe return to the negative binomial distribution in Section 17.2 as a robust alternative to\nthe Poisson distribution.\n\nPoisson model parameterized in terms of rate and exposure\n\nIn many applications, it is convenient to extend the Poisson model for data points y1, . . . , yn\nto the form\n\nyi ∼ Poisson(xiθ), (2.14)\n\nwhere the values xi are known positive values of an explanatory variable, x, and θ is the\nunknown parameter of interest. In epidemiology, the parameter θ is often called the rate,\nand xi is called the exposure of the ith unit. This model is not exchangeable in the yi’s but\nis exchangeable in the pairs (x, y)i. The likelihood for θ in the extended Poisson model is\n\np(y|θ) ∝ θ(\n∑n\n\ni=1\nyi)e−(\n\n∑n\n\ni=1\nxi)θ\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.6. OTHER STANDARD SINGLE-PARAMETER MODELS 45\n\n(ignoring factors that do not depend on θ), and so the gamma distribution for θ is conjugate.\nWith prior distribution\n\nθ ∼ Gamma(α, β),\n\nthe resulting posterior distribution is\n\nθ|y ∼ Gamma\n\n(\nα+\n\nn∑\n\ni=1\n\nyi, β +\nn∑\n\ni=1\n\nxi\n\n)\n. (2.15)\n\nEstimating a rate from Poisson data: an idealized example\nSuppose that causes of death are reviewed in detail for a city in the United States for a\nsingle year. It is found that 3 persons, out of a population of 200,000, died of asthma,\ngiving a crude estimated asthma mortality rate in the city of 1.5 cases per 100,000\npersons per year. A Poisson sampling model is often used for epidemiological data of\nthis form. The Poisson model derives from an assumption of exchangeability among\nall small intervals of exposure. Under the Poisson model, the sampling distribution\nof y, the number of deaths in a city of 200,000 in one year, may be expressed as\nPoisson(2.0θ), where θ represents the true underlying long-term asthma mortality rate\nin our city (measured in cases per 100,000 persons per year). In the above notation,\ny = 3 is a single observation with exposure x = 2.0 (since θ is defined in units of\n100,000 people) and unknown rate θ. We can use knowledge about asthma mortality\nrates around the world to construct a prior distribution for θ and then combine the\ndatum y = 3 with that prior distribution to obtain a posterior distribution.\n\nSetting up a prior distribution. What is a sensible prior distribution for θ? Reviews\nof asthma mortality rates around the world suggest that mortality rates above 1.5\nper 100,000 people are rare in Western countries, with typical asthma mortality rates\naround 0.6 per 100,000. Trial-and-error exploration of the properties of the gamma dis-\ntribution, the conjugate prior family for this problem, reveals that a Gamma(3.0, 5.0)\ndensity provides a plausible prior density for the asthma mortality rate in this example\nif we assume exchangeability between this city and other cities and this year and other\nyears. The mean of this prior distribution is 0.6 (with a mode of 0.4), and 97.5% of\nthe mass of the density lies below 1.44. In practice, specifying a prior mean sets the\nratio of the two gamma parameters, and then the shape parameter can be altered by\ntrial and error to match the prior knowledge about the tail of the distribution.\n\nPosterior distribution. The result in (2.15) shows that the posterior distribution\nof θ for a Gamma(α, β) prior distribution is Gamma(α + y, β + x) in this case.\nWith the prior distribution and data described, the posterior distribution for θ is\nGamma(6.0, 7.0), which has mean 0.86—substantial shrinkage has occurred toward\nthe prior distribution. A histogram of 1000 draws from the posterior distribution for\nθ is shown as Figure 2.5a. For example, the posterior probability that the long-term\ndeath rate from asthma in our city is more than 1.0 per 100,000 per year, computed\nfrom the gamma posterior density, is 0.30.\n\nPosterior distribution with additional data. To consider the effect of additional data,\nsuppose that ten years of data are obtained for the city in our example, instead of just\none, and it is found that the mortality rate of 1.5 per 100,000 is maintained; we find\ny = 30 deaths over 10 years. Assuming the population is constant at 200,000, and\nassuming the outcomes in the ten years are independent with constant long-term rate\nθ, the posterior distribution of θ is then Gamma(33.0, 25.0); Figure 2.5b displays 1000\ndraws from this distribution. The posterior distribution is much more concentrated\nthan before, and it still lies between the prior distribution and the data. After ten\nyears of data, the posterior mean of θ is 1.32, and the posterior probability that θ\nexceeds 1.0 is 0.93.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n46 2. SINGLE-PARAMETER MODELS\n\nFigure 2.5 Posterior density for θ, the asthma mortality rate in cases per 100,000 persons per year,\nwith a Gamma(3.0, 5.0) prior distribution: (a) given y = 3 deaths out of 200,000 persons; (b) given\ny = 30 deaths in 10 years for a constant population of 200,000. The histograms appear jagged\nbecause they are constructed from only 1000 random draws from the posterior distribution in each\ncase.\n\nExponential model\n\nThe exponential distribution is commonly used to model ‘waiting times’ and other continu-\nous, positive, real-valued random variables, often measured on a time scale. The sampling\ndistribution of an outcome y, given parameter θ, is\n\np(y|θ) = θ exp(−yθ), for y > 0,\n\nand θ = 1/E(y|θ) is called the ‘rate.’ Mathematically, the exponential is a special case of the\ngamma distribution with the parameters (α, β) = (1, θ). In this case, however, it is being\nused as a sampling distribution for an outcome y, not a prior distribution for a parameter\nθ, as in the Poisson example.\n\nThe exponential distribution has a ‘memoryless’ property that makes it a natural model\nfor survival or lifetime data; the probability that an object survives an additional length of\ntime t is independent of the time elapsed to this point: Pr(y>t+s | y>s, θ) = Pr(y>t | θ) for\nany s, t. The conjugate prior distribution for the exponential parameter θ, as for the Poisson\nmean, is Gamma(θ|α, β) with corresponding posterior distribution Gamma(θ|α+1, β+y).\nThe sampling distribution of n independent exponential observations, y = (y1, . . . , yn), with\nconstant rate θ is\n\np(y|θ) = θn exp(−nyθ), for y ≥ 0,\n\nwhich when viewed as the likelihood of θ, for fixed y, is proportional to a Gamma(n+1, ny)\ndensity. Thus the Gamma(α, β) prior distribution for θ can be viewed as α−1 exponential\nobservations with total waiting time β (see Exercise 2.19).\n\n2.7 Example: informative prior distribution for cancer rates\n\nAt the end of Section 2.4, we considered the effect of the prior distribution on inference\ngiven a fixed quantity of data. Here, in contrast, we consider a large set of inferences, each\nbased on different data but with a common prior distribution. In addition to illustrating\nthe role of the prior distribution, this example introduces hierarchical modeling, to which\nwe return in Chapter 5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 47\n\nFigure 2.6 The counties of the United States with the highest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Why are most of the shaded counties in\nthe middle of the country? See Section 2.7 for discussion.\n\nFigure 2.7 The counties of the United States with the lowest 10% age-standardized death rates for\ncancer of kidney/ureter for U.S. white males, 1980–1989. Surprisingly, the pattern is somewhat\nsimilar to the map of the highest rates, shown in Figure 2.6.\n\nA puzzling pattern in a map\n\nFigure 2.6 shows the counties in the United States with the highest kidney cancer death\nrates during the 1980s.1 The most noticeable pattern in the map is that many of the\ncounties in the Great Plains in the middle of the country, but relatively few counties near\nthe coasts, are shaded.\n\nWhen shown the map, people come up with many theories to explain the dispropor-\ntionate shading in the Great Plains: perhaps the air or the water is polluted, or the people\ntend not to seek medical care so the cancers get detected too late to treat, or perhaps their\ndiet is unhealthy . . . These conjectures may all be true but they are not actually needed\nto explain the patterns in Figure 2.6. To see this, look at Figure 2.7, which plots the 10%\nof counties with the lowest kidney cancer death rates. These are also mostly in the middle\nof the country. So now we need to explain why these areas have the lowest, as well as the\nhighest, rates.\n\n1The rates are age-adjusted and restricted to white males, issues which need not concern us here.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n48 2. SINGLE-PARAMETER MODELS\n\nThe issue is sample size. Consider a county of population 1000. Kidney cancer is a\nrare disease, and, in any ten-year period, a county of 1000 will probably have zero kidney\ncancer deaths, so that it will be tied for the lowest rate in the country and will be shaded\nin Figure 2.7. However, there is a chance the county will have one kidney cancer death\nduring the decade. If so, it will have a rate of 1 per 10,000 per year, which is high enough\nto put it in the top 10% so that it will be shaded in Figure 2.6. The Great Plains has many\nlow-population counties, and so it is overrepresented in both maps. There is no evidence\nfrom these maps that cancer rates are particularly high there.\n\nBayesian inference for the cancer death rates\n\nThe misleading patterns in the maps of raw rates suggest that a model-based approach to\nestimating the true underlying rates might be helpful. In particular, it is natural to estimate\nthe underlying cancer death rate in each county j using the model\n\nyj ∼ Poisson(10njθj), (2.16)\n\nwhere yj is the number of kidney cancer deaths in county j from 1980–1989, nj is the\npopulation of the county, and θj is the underlying rate in units of deaths per person per\nyear. In this notation, the maps in Figures 2.6 and 2.7 are plotting the raw rates,\n\nyj\n10nj\n\n.\n\n(Here we are ignoring the age-standardization, although a generalization of the model to\nallow for this would be possible.)\n\nThis model differs from (2.14) in that θj varies between counties, so that (2.16) is a\nseparate model for each of the counties in the U.S. We use the subscript j (rather than i)\nin (2.16) to emphasize that these are separate parameters, each being estimated from its\nown data. Were we performing inference for just one of the counties, we would simply write\ny ∼ Poisson(10nθ).\n\nTo perform Bayesian inference, we need a prior distribution for the unknown rate θj .\nFor convenience we use a gamma distribution, which is conjugate to the Poisson. As we\nshall discuss later, a gamma distribution with parameters α = 20 and β = 430,000 is a\nreasonable prior distribution for underlying kidney cancer death rates in the counties of\nthe U.S. during this period. This prior distribution has a mean of α\n\nβ = 4.65 × 10−5 and\n\nstandard deviation\n√\nα\nβ = 1.04× 10−5.\n\nThe posterior distribution of θj is then,\n\nθj |yj ∼ Gamma(20 + yj , 430,000+ 10nj),\n\nwhich has mean and variance,\n\nE(θj |yj) =\n20 + yj\n\n430,000 + 10nj\n\nvar(θj |yj) =\n20 + yj\n\n(430,000 + 10nj)2\n.\n\nThe posterior mean can be viewed as a weighted average of the raw rate,\nyj\n\n10nj\n, and the\n\nprior mean, αβ = 4.65× 10−5. (For a similar calculation, see Exercise 2.5.)\n\nRelative importance of the local data and the prior distribution\n\nInference for a small county. The relative weighting of prior information and data depends\non the population size nj . For example, consider a small county with nj = 1000:\n\n• For this county, if yj = 0, then the raw death rate is 0 but the posterior mean is\n20\n\n440,000 = 4.55× 10−5.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.7. EXAMPLE: INFORMATIVE PRIOR DISTRIBUTION FOR CANCER RATES 49\n\nFigure 2.8 (a) Kidney cancer death rates yj/(10nj) vs. population size nj. (b) Replotted on the\nscale of log10 population to see the data more clearly. The patterns come from the discreteness of\nthe data (nj = 0, 1, 2, . . .).\n\n• If yj = 1, then the raw death rate is 1 per 1000 per 10 years, or 10−4 per person-year\n(about twice as high as the national mean), but the posterior mean is only 21\n\n440,000 =\n\n4.77× 10−5.\n\n• If yj = 2, then the raw death rate is an extremely high 2 × 10−4 per person-year, but\nthe posterior mean is still only 22\n\n440,000 = 5.00× 10−5.\n\nWith such a small population size, the data are dominated by the prior distribution.\n\nBut how likely, a priori, is it that yj will equal 0, 1, 2, and so forth, for this county with\nnj = 1000? This is determined by the predictive distribution, the marginal distribution\nof yj , averaging over the prior distribution of θj . As discussed in Section 2.6, the Poisson\nmodel with gamma prior distribution has a negative binomial predictive distribution:\n\nyj ∼ Neg-bin\n\n(\nα,\n\nβ\n\n10nj\n\n)\n.\n\nIt is perhaps even simpler to simulate directly the predictive distribution of yj as follows:\n(1) draw 500 (say) values of θj from the Gamma(20, 430,000) distribution; (2) for each of\nthese, draw one value yj from the Poisson distribution with parameter 10,000 θj. Of 500\nsimulations of yj produced in this way, 319 were 0’s, 141 were 1’s, 33 were 2’s, and 5 were\n3’s.\n\nInference for a large county. Now consider a large county with nj = 1 million. How\nmany cancer deaths yj might we expect to see in a ten-year period? Again we can use\nthe Gamma(20, 430,000) and Poisson(107 θj) distributions to simulate 500 values yj from\nthe predictive distribution. Doing this we found a median of 473 and a 50% interval of\n[393, 545]. The raw death rate in such a county is then as likely or not to fall between\n3.93× 10−5 and 5.45× 10−5.\n\nWhat about the Bayesianly estimated or ‘Bayes-adjusted’ death rate? For example, if\nyj takes on the low value of 393, then the raw death rate is 3.93× 10−5 and the posterior\nmean of θj is 20+393\n\n107+430,000 = 3.96× 10−5, and if yj = 545, then the raw rate is 5.45× 10−5\n\nand the posterior mean is 5.41 × 10−5. In this large county, the data dominate the prior\ndistribution.\n\nComparing counties of different sizes. In the Poisson model (2.16), the variance of\nyj\n\n10nj\n\nis inversely proportional to the exposure parameter nj , which can thus be considered a\n‘sample size’ for county j. Figure 2.8 shows how the raw kidney cancer death rates vary by\npopulation. The extremely high and extremely low rates are all in low-population counties.\nBy comparison, Figure 2.9a shows that the Bayes-estimated rates are much less variable.\nFinally, Figure 2.9b displays 50% interval estimates for a sample of counties (chosen because\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n50 2. SINGLE-PARAMETER MODELS\n\nFigure 2.9 (a) Bayes-estimated posterior mean kidney cancer death rates, E(θj |yj) = 20+yj\n\n430,000+10nj\n\nvs. logarithm of population size nj , the 3071 counties in the U.S. (b) Posterior medians and 50%\nintervals for θj for a sample of 100 counties j. The scales on the y-axes differ from the plots in\nFigure 2.8b.\n\nit would be hard to display all 3071 in a single plot). The smaller counties supply less\ninformation and thus have wider posterior intervals.\n\nConstructing a prior distribution\n\nWe now step back and discuss where we got the Gamma(20, 430,000) prior distribution for\nthe underlying rates. As we discussed when introducing the model, we picked the gamma\ndistribution for mathematical convenience. We now explain how the two parameters α, β\ncan be estimated from data to match the distribution of the observed cancer death rates\nyj\n\n10nj\n. It might seem inappropriate to use the data to set the prior distribution, but we\n\nview this as a useful approximation to our preferred approach of hierarchical modeling\n(introduced in Chapter 5), in which distributional parameters such as α, β in this example\nare treated as unknowns to be estimated.\n\nUnder the model, the observed count yj for any county j comes from the predictive dis-\n\ntribution, p(yj) =\n∫\np(yj |θj)p(θj)dθj , which in this case is Neg-bin(α, β\n\n10nj\n). From Appendix\n\nA, we can find the mean and variance of this distribution:\n\nE(yj) = 10nj\nα\n\nβ\n\nvar(yj) = 10nj\nα\n\nβ\n+ (10nj)\n\n2 α\n\nβ2\n. (2.17)\n\nThese can also be derived directly using the mean and variance formulas (1.8) and (1.9);\nsee Exercise 2.6.\n\nMatching the observed mean and variance to their expectations and solving for α and β\nyields the parameters of the prior distribution. The actual computation is more complicated\nbecause we must deal with the age adjustment and it also is more efficient to work with the\nmean and variance of the rates\n\nyj\n10nj\n\n:\n\nE\n\n(\nyj\n\n10nj\n\n)\n=\n\nα\n\nβ\n\nvar\n\n(\nyj\n\n10nj\n\n)\n=\n\n1\n\n10nj\n\nα\n\nβ\n+\n\nα\n\nβ2\n. (2.18)\n\nAfter dealing with the age adjustments, we equate the observed and theoretical moments,\nsetting the mean of the values of\n\nyj\n10nj\n\nto α\nβ and setting the variance of the values of\n\nyj\n10nj\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 51\n\nFigure 2.10 Empirical distribution of the age-adjusted kidney cancer death rates,\nyj\n\n10nj\n, for the 3071\n\ncounties in the U.S., along with the Gamma(20, 430,000) prior distribution for the underlying cancer\nrates θj.\n\nto E\n(\n\n1\n10nj\n\n)\nα\nβ + α\n\nβ2 , using the sample average of the values 1\n10nj\n\nin place of E\n(\n\n1\n10nj\n\n)\nin that\n\nlast expression.\nFigure 2.10 shows the empirical distribution of the raw cancer rates, along with the\n\nestimated Gamma(20, 430,000) prior distribution for the underlying cancer rates θj . The\ndistribution of the raw rates is much broader, which makes sense since they include the\nPoisson variability as well as the variation between counties.\n\nOur prior distribution is reasonable in this example, but this method of constructing\nit—by matching moments—is somewhat sloppy and can be difficult to apply in general. In\nChapter 5, we discuss how to estimate this and other prior distributions in a more direct\nBayesian manner, using hierarchical models.\n\nA more important way this model could be improved is by including information at the\ncounty level that could predict variation in the cancer rates. This would move the model\ntoward a hierarchical Poisson regression of the sort discussed in Chapter 16.\n\n2.8 Noninformative prior distributions\n\nWhen prior distributions have no population basis, they can be difficult to construct, and\nthere has long been a desire for prior distributions that can be guaranteed to play a minimal\nrole in the posterior distribution. Such distributions are sometimes called ‘reference prior\ndistributions,’ and the prior density is described as vague, flat, diffuse or noninformative.\nThe rationale for using noninformative prior distributions is often said to be ‘to let the\ndata speak for themselves,’ so that inferences are unaffected by information external to the\ncurrent data.\n\nA related idea is the weakly informative prior distribution, which contains some informa-\ntion—enough to ‘regularize’ the posterior distribution, that is, to keep it roughly within rea-\nsonable bounds—but without attempting to fully capture one’s scientific knowledge about\nthe underlying parameter.\n\nProper and improper prior distributions\n\nWe return to the problem of estimating the mean θ of a normal model with known variance\nσ2, with a N(µ0, τ\n\n2\n0 ) prior distribution on θ. If the prior precision, 1/τ20 , is small relative to\n\nthe data precision, n/σ2, then the posterior distribution is approximately as if τ20 =∞:\n\np(θ|y) ≈ N(θ|y, σ2/n).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(θ) is proportional to a constant for θ ∈ (−∞,∞). Such a distribution is\nnot strictly possible, since the integral of the assumed p(θ) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(θ) proper if it\ndoes not depend on data and integrates to 1. (If p(θ) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-χ2 prior distri-\nbution. If the prior degrees of freedom, ν0, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if ν0 = 0:\n\np(σ2|y) ≈ Inv-χ2(σ2|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for σ2 as p(σ2) ∝ 1/σ2, which is improper, having an infinite integral over the range\n(0,∞).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, θ). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(θ|y) ∝ p(y|θ)p(θ).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,∫\np(θ|y)dθ is finite for all y. Posterior distributions obtained from improper prior distri-\n\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis as approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\nφ = h(θ). By transformation of variables, the prior density p(θ) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on φ:\n\np(φ) = p(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣ = p(θ)|h′(θ)|−1. (2.19)\n\nJeffreys’ general principle is that any rule for determining the prior density p(θ) should\nyield an equivalent result if applied to the transformed parameter; that is, p(φ) computed\nby determining p(θ) and applying (2.19) should match the distribution that is obtained by\ndetermining p(φ) directly using the transformed model, p(y, φ) = p(φ)p(y|φ).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(θ) ∝ [J(θ)]1/2,\nwhere J(θ) is the Fisher information for θ:\n\nJ(θ) = E\n\n((\nd log p(y|θ)\n\ndθ\n\n)2\n∣∣∣∣∣ θ\n)\n\n= −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n. (2.20)\n\nThis electronic edition is for non-commercial purposes only.\n\n52 2. SINGLE-PARAMETER MODELS\n\nPutting this another way, the posterior distribution is approximately that which would result\nfrom assuming p(9) is proportional to a constant for 6 € (—oo, 00). Such a distribution is\nnot strictly possible, since the integral of the assumed p(@) is infinity, which violates the\nassumption that probabilities sum to 1. In general, we call a prior density p(0) proper if it\ndoes not depend on data and integrates to 1. (If p(@) integrates to any positive finite value,\nit is called an unnormalized density and can be renormalized—multiplied by a constant—\nto integrate to 1.) The prior distribution is improper in this example, but the posterior\ndistribution is proper, given at least one data point.\n\nAs a second example of a noninformative prior distribution, consider the normal model\nwith known mean but unknown variance, with the conjugate scaled inverse-y? prior distri-\nbution. If the prior degrees of freedom, vo, are small relative to the data degrees of freedom,\nn, then the posterior distribution is approximately as if vp = 0:\n\np(o\"|y) © Inv-x?(o*|n, v).\n\nThis limiting form of the posterior distribution can also be derived by defining the prior\ndensity for 0? as p(a?) « 1/07, which is improper, having an infinite integral over the range\n(0, co).\n\nImproper prior distributions can lead to proper posterior distributions\n\nIn neither of the above two examples does the prior density combine with the likelihood to\ndefine a proper joint probability model, p(y, @). However, we can proceed with the algebra\nof Bayesian inference and define an unnormalized posterior density function by\n\np(y) x p(y|O)p(9).\n\nIn the above examples (but not always!), the posterior density is in fact proper; that is,\n{p(Oly)d@ is finite for all y. Posterior distributions obtained from improper prior distri-\nbutions must be interpreted with great care—one must always check that the posterior\ndistribution has a finite integral and a sensible form. Their most reasonable interpretation\nis aS approximations in situations where the likelihood dominates the prior density. We\ndiscuss this aspect of Bayesian analysis more completely in Chapter 4.\n\nJeffreys’ invariance principle\n\nOne approach that is sometimes used to define noninformative prior distributions was in-\ntroduced by Jeffreys, based on considering one-to-one transformations of the parameter:\n@ = h(@). By transformation of variables, the prior density p(@) is equivalent, in terms of\nexpressing the same beliefs, to the following prior density on ¢:\n\ndo\n\ndd| —\nJeffreys’ general principle is that any rule for determining the prior density p(@) should\nyield an equivalent result if applied to the transformed parameter; that is, p(¢@) computed\nby determining p(@) and applying (2.19) should match the distribution that is obtained by\ndetermining p(#) directly using the transformed model, p(y, ¢) = p(¢)p(yl¢).\n\nJeffreys’ principle leads to defining the noninformative prior density as p(6) « [J(6)]\nwhere J(@) is the Fisher information for 0:\n\n_ dlog p(y|9) \\”\nsay = (et\n\nrd) = v(0) p()|h!(a)|—2. (2.19)\n\n1/2\n?\n\ndé?\n\n) =-5 (eeu) 0). (2.20)\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\n\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(φ) at θ =\nh−1(φ):\n\nJ(φ) = −E\n(\nd2 log p(y|φ)\n\ndφ2\n\n)\n\n= −E\n(\nd2 log p(y|θ=h−1(φ))\n\ndθ2\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n)\n\n= J(θ)\n\n∣∣∣∣\ndθ\n\ndφ\n\n∣∣∣∣\n2\n\n;\n\nthus, J(φ)1/2 = J(θ)1/2\n∣∣∣ dθdφ\n∣∣∣, as required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter θ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ∼ Bin(n, θ), which has log-likelihood\n\nlog p(y|θ) = constant + y log θ + (n− y) log(1− θ).\n\nRoutine evaluation of the second derivative and substitution of E(y|θ) = nθ yields the\nFisher information:\n\nJ(θ) = −E\n(\nd2 log p(y|θ)\n\ndθ2\n\n∣∣∣∣ θ\n)\n\n=\nn\n\nθ(1 − θ) .\n\nJeffreys’ prior density is then p(θ) ∝ θ−1/2(1 − θ)−1/2, which is a Beta(12 ,\n1\n2 ) density. By\n\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\nθ ∼ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(θ)) ∝ constant (see\nExercise 2.7), which corresponds to the improper Beta(0, 0) density on θ. In practice,\nthe difference between these alternatives is often small, since to get from θ ∼ Beta(0, 0)\nto θ ∼ Beta(1, 1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0, 0) prior distribution—if\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y− θ|θ) is a function that is free of θ and y, say,\nf(u), where u = y − θ, then y − θ is a pivotal quantity, and θ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for θ\nwould give f(y−θ) for the posterior distribution, p(y−θ|y). That is, under the posterior\ndistribution, y − θ should still be a pivotal quantity, whose distribution is free of both\nθ and y. Under this condition, using Bayes’ rule, p(y−θ|y) ∝ p(θ)p(y−θ|θ), thereby\n\nThis electronic edition is for non-commercial purposes only.\n\n2.8. NONINFORMATIVE PRIOR DISTRIBUTIONS 53\nTo see that Jeffreys’ prior model is invariant to parameterization, evaluate J(¢) at 0 =\nh-*(¢):\n@ log p(yl¢)\n= —K a\nH(0) Ger\n_ _p{ Plogpyl@=h\\(6)) | d0 |?\ndo? do\ndo |\n= J(0)|/—| ;\n0)\n\nagp |> 2 required.\n\nJeffreys’ principle can be extended to multiparameter models, but the results are more\ncontroversial. Simpler approaches based on assuming independent noninformative prior\ndistributions for the components of the vector parameter @ can give different results than\nare obtained with Jeffreys’ principle. When the number of parameters in a problem is large,\nwe find it useful to abandon pure noninformative prior distributions in favor of hierarchical\nmodels, as we discuss in Chapter 5.\n\nthus, J(¢)!/2 = J(@)!/2 \\\n\nVarious noninformative prior distributions for the binomial parameter\n\nConsider the binomial distribution: y ~ Bin(n, 6), which has log-likelihood\nlog p(y|@) = constant + y log 6 + (n — y) log(1 — @).\nRoutine evaluation of the second derivative and substitution of E(y|0) = n@ yields the\n\nFisher information: 5 (v0)\nd° log p(y|@ n\n0) = —E | ——~——_] 0 ) = —_~..\n10) = 8 ee) = aaa\n\nJeffreys’ prior density is then p(@) « 6~'/2(1 — 0)~1/?, which is a Beta(4, 4) density. By\ncomparison, recall the Bayes-Laplace uniform prior density, which can be expressed as\n@ ~ Beta(1, 1). On the other hand, the prior density that is uniform in the natural parameter\nof the exponential family representation of the distribution is p(logit(@)) o constant (see\nExercise 2.7), which corresponds to the improper Beta(0,0) density on @. In practice,\nthe difference between these alternatives is often small, since to get from 0 ~ Beta(0, 0)\nto @ ~ Beta(1,1) is equivalent to passing from prior to posterior distribution given one\nmore success and one more failure, and usually 2 is a small fraction of the total number of\nobservations. But one must be careful with the improper Beta(0,0) prior distribution—if\n\ny = 0 or n, the resulting posterior distribution is improper!\n\nPivotal quantities\n\nFor the binomial and other single-parameter models, different principles give (slightly) dif-\nferent noninformative prior distributions. But for two cases—location parameters and scale\nparameters—all principles seem to agree.\n\n1. If the density of y is such that p(y—0|@) is a function that is free of 0 and y, say,\nf(u), where u = y — 0, then y — 6 is a pivotal quantity, and @ is called a pure location\nparameter. In such a case, it is reasonable that a noninformative prior distribution for 0\nwould give f(y—6@) for the posterior distribution, p(y—9|y). That is, under the posterior\ndistribution, y — 0 should still be a pivotal quantity, whose distribution is free of both\n6 and y. Under this condition, using Bayes’ rule, p(y—9@ly) « p(@)p(y—6|@), thereby\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n54 2. SINGLE-PARAMETER MODELS\n\nimplying that the noninformative prior density is uniform on θ; that is, p(θ) ∝ constant\nover the range (−∞,∞).\n\n2. If the density of y is such that p(yθ |θ) is a function that is free of θ and y—say, g(u), where\nu = y\n\nθ—then u = y\nθ is a pivotal quantity and θ is called a pure scale parameter. In such\n\na case, it is reasonable that a noninformative prior distribution for θ would give g(yθ )\nfor the posterior distribution, p(yθ |y). By transformation of variables, the conditional\ndistribution of y given θ can be expressed in terms of the distribution of u given θ,\n\np(y|θ) = 1\n\nθ\np(u|θ),\n\nand similarly,\n\np(θ|y) = y\n\nθ2\np(u|y).\n\nAfter letting both p(u|θ) and p(u|y) equal g(u), we have the identity p(θ|y) = y\nθ p(y|θ).\n\nThus, in this case, the reference prior distribution is p(θ) ∝ 1\nθ or, equivalently, p(log θ) ∝ 1\n\nor p(θ2) ∝ 1\nθ2 .\n\nThis approach, in which the sampling distribution of the pivot is used as its posterior\ndistribution, can be applied to sufficient statistics in more complicated examples, such as\nhierarchical normal models.\n\nEven these principles can be misleading in some problems, in the critical sense of suggest-\ning prior distributions that can lead to improper posterior distributions. For example, the\nuniform prior density does not work for the logarithm of a hierarchical variance parameter,\nas we discuss in Section 5.4.\n\nDifficulties with noninformative prior distributions\n\nThe search for noninformative priors has several problems, including:\n\n1. Searching for a prior distribution that is always vague seems misguided: if the likelihood\nis truly dominant in a given problem, then the choice among a range of relatively flat\nprior densities cannot matter. Establishing a particular specification as the reference\nprior distribution seems to encourage its automatic, and possibly inappropriate, use.\n\n2. For many problems, there is no clear choice for a vague prior distribution, since a density\nthat is flat or uniform in one parameterization will not be in another. This is the\nessential difficulty with Laplace’s principle of insufficient reason—on what scale should\nthe principle apply? For example, the ‘reasonable’ prior density on the normal mean θ\nabove is uniform, while for σ2, the density p(σ2) ∝ 1/σ2 seems reasonable. However, if\nwe define φ = log σ2, then the prior density on φ is\n\np(φ) = p(σ2)\n\n∣∣∣∣\ndσ2\n\ndφ\n\n∣∣∣∣ ∝\n1\n\nσ2\nσ2 = 1;\n\nthat is, uniform on φ = log σ2. With discrete distributions, there is the analogous\ndifficulty of deciding how to subdivide outcomes into ‘atoms’ of equal probability.\n\n3. Further difficulties arise when averaging over a set of competing models that have im-\nproper prior distributions, as we discuss in Section 7.3.\n\nNevertheless, noninformative and reference prior densities are often useful when it does\nnot seem to be worth the effort to quantify one’s real prior knowledge as a probability\ndistribution, as long as one is willing to perform the mathematical work to check that\nthe posterior density is proper and to determine the sensitivity of posterior inferences to\nmodeling assumptions of convenience.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.9. WEAKLY INFORMATIVE PRIOR DISTRIBUTIONS 55\n\n2.9 Weakly informative prior distributions\n\nWe characterize a prior distribution as weakly informative if it is proper but is set up so that\nthe information it does provide is intentionally weaker than whatever actual prior knowledge\nis available. We will discuss this further in the context of a specific example, but in general\nany problem has some natural constraints that would allow a weakly informative model.\nFor example, for regression models on the logarithmic or logistic scale, with predictors that\nare binary or scaled to have standard deviation 1, we can be sure for most applications that\neffect sizes will be less than 10, given that a difference of 10 on the log scale changes the\nexpected value by a factor of exp(10) = 20,000, and on the logit scale shifts a probability\nof logit−1(−5) = 0.01 to logit−1(5) = 0.99.\n\nRather than trying to model complete ignorance, we prefer in most problems to use\nweakly informative prior distributions that include a small amount of real-world information,\nenough to ensure that the posterior distribution makes sense. For example, in the sex\nratio example from Sections 2.1 and 2.4, one could use a prior distribution concentrated\nbetween 0.4 and 0.6, for example N(0.5, 0.12) or, to keep the mathematical convenience of\nconjugacy, Beta(20, 20).2 In the general problem of estimating a normal mean from Section\n2.5, a N(0, A2) prior distribution is weakly informative, with A set to some large value that\ndepends on the context of the problem.\n\nIn almost every real problem, the data analyst will have more information than can\nbe conveniently included in the statistical model. This is an issue with the likelihood as\nwell as the prior distribution. In practice, there is always compromise for a number of\nreasons: to describe the model more conveniently; because it may be difficult to express\nknowledge accurately in probabilistic form; to simplify computations; or perhaps to avoid\nusing a possibly unreliable source of information. Except for the last reason, these are all\narguments for convenience and are best justified by the claim that the answer would not\nhave changed much had we been more accurate. If so few data are available that the choice\nof noninformative prior distribution makes a difference, one should put relevant information\ninto the prior distribution, perhaps using a hierarchical model, as we discuss in Chapter 5.\nWe return to the issue of accuracy vs. convenience in likelihoods and prior distributions in\nthe examples of the later chapters.\n\nConstructing a weakly informative prior distribution\n\nOne might argue that virtually all statistical models are weakly informative: a model always\nconveys some information, if only in its choice of inputs and the functional form of how\nthey are combined, but it is not possible or perhaps even desirable to encode all of one’s\nprior beliefs about a subject into a set of probability distributions. With that in mind, we\noffer two principles for setting up weakly informative priors, going at the problem from two\ndifferent directions:\n\n• Start with some version of a noninformative prior distribution and then add enough\ninformation so that inferences are constrained to be reasonable.\n\n• Start with a strong, highly informative prior and broaden it to account for uncertainty\nin one’s prior beliefs and in the applicability of any historically based prior distribution\nto new data.\n\nNeither of these approaches is pure. In the first case, it can happen that the purportedly\nnoninformative prior distribution used as a starting point is in fact too strong. For example,\nif a U(0, 1) prior distribution is assigned to the probability of some rare disease, then in\nthe presence of weak data the probability can be grossly overestimated (suppose y = 0\n\n2A quick R calculation, pbeta(.6,20,20) - pbeta(.4,20,20), reveals that 80% of the probability mass\nin the Beta(20, 20) falls between 0.4 and 0.6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n56 2. SINGLE-PARAMETER MODELS\n\nincidences out of n = 100 cases, and the true prevalence is known to be less than 1 in\n10,000), and an appropriate weakly informative prior will be such that the posterior in this\ncase will be concentrated in that low range. In the second case, a prior distribution that is\nbelieved to be strongly informative may in fact be too weak along some direction. This is\nnot to say that priors should be made more precise whenever posterior inferences are vague;\nin many cases, our best strategy is simply to acknowledge whatever posterior uncertainty\nwe have. But we should not feel constrained by default noninformative models when we\nhave substantive prior knowledge available.\n\nThere are settings, however, when it can be recommended to not use relevant informa-\ntion, even when it could clearly improve posterior inferences. The concern here is often\nexpressed in terms of fairness and encoded mathematically as a symmetry principle, that\nthe prior distribution should not pull inferences in any predetermined direction. For exam-\nple, consider an experimenter studying an effect that she is fairly sure is positive; perhaps\nher prior distribution is N(0.5, 0.5) on some appropriate scale. Such an assumption might\nbe pefectly reasonable given current scientific information but seems potentially risky if it\nis part of the analysis of an experiment designed to test the scientist’s theory. If anything,\none might want a prior distribution that leans against an experimenter’s hypothesis in order\nto require a higher standard of proof.\n\nUltimately, such concerns can and should be subsumed into decision analysis and some\nsort of model of the entire scientific process, trading off the gains of early identification of\nlarge and real effects against the losses entailed in overestimating the magnitudes of effects\nand overreacting to patterns that could be attributed to chance. In the meantime, though,\nwe know that statistical inferences are taken as evidence of effects, and as guides to future\ndecision making, and for this purpose it can make sense to require models to have certain\nconstraints such as symmetry about 0 for the prior distribution of a single treatment effect.\n\n2.10 Bibliographic note\n\nA fascinating detailed account of the early development of the idea of ‘inverse probability’\n(Bayesian inference) is provided in the book by Stigler (1986), on which our brief accounts\nof Bayes’ and Laplace’s solutions to the problem of estimating an unknown proportion are\nbased. Bayes’ famous 1763 essay in the Philosophical Transactions of the Royal Society of\nLondon has been reprinted as Bayes (1763); see also Laplace (1785, 1810).\n\nIntroductory textbooks providing complementary discussions of the simple models cov-\nered in this chapter were listed at the end of Chapter 1. In particular, Box and Tiao (1973)\nprovide a detailed treatment of Bayesian analysis with the normal model and also discuss\nhighest posterior density regions in some detail. The theory of conjugate prior distributions\nwas developed in detail by Raiffa and Schlaifer (1961). An interesting account of inference\nfor prediction, which also includes extensive details of particular probability models and\nconjugate prior analyses, appears in Aitchison and Dunsmore (1975).\n\nLiu et al. (2013) discuss how to efficiently compute highest posterior density intervals\nusing simulations.\n\nNoninformative and reference prior distributions have been studied by many researchers.\nJeffreys (1961) and Hartigan (1964) discuss invariance principles for noninformative prior\ndistributions. Chapter 1 of Box and Tiao (1973) presents a straightforward and practically\noriented discussion, a brief but detailed survey is given by Berger (1985), and the article by\nBernardo (1979) is accompanied by a wide-ranging discussion. Bernardo and Smith (1994)\ngive an extensive treatment of this topic along with many other matters relevant to the\nconstruction of prior distributions. Barnard (1985) discusses the relation between pivotal\nquantities and noninformative Bayesian inference. Kass and Wasserman (1996) provide a\nreview of many approaches for establishing noninformative prior densities based on Jeffreys’\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 57\n\nrule, and they also discuss the problems that may arise from uncritical use of purportedly\nnoninformative prior specifications. Dawid, Stone, and Zidek (1973) discuss some difficulties\nthat can arise with noninformative prior distributions; also see Jaynes (1980).\n\nKerman (2011) discusses noninformative and informative conjugate prior distributions\nfor the binomial and Poisson models.\n\nJaynes (1983) discusses in several places the idea of objectively constructing prior dis-\ntributions based on invariance principles and maximum entropy. Appendix A of Bretthorst\n(1988) outlines an objective Bayesian approach to assigning prior distributions, as applied\nto the problem of estimating the parameters of a sinusoid from time series data. More\ndiscussions of maximum entropy models appear in Jaynes (1982), Skilling (1989), and Gull\n(1989a); see Titterington (1984) and Donoho et al. (1992) for other views.\n\nFor more on weakly informative prior distributions, see Gelman (2006a) and Gelman,\nJakulin, et al. (2008). Gelman (2004b) discusses connections between parameterization and\nBayesian modeling. Greenland (2001) discusses informative prior distributions in epidemi-\nology.\n\nThe data for the placenta previa example come from a study from 1922 reported in\nJames (1987). For more on the challenges of estimating sex ratios from small samples,\nsee Gelman and Weakliem (2009). The Bayesian analysis of age-adjusted kidney cancer\ndeath rates in Section 2.7 is adapted from Manton et al. (1989); see also Gelman and Nolan\n(2002a) for more on this particular example and Bernardinelli, Clayton, and Montomoli\n(1995) for a general discussion of prior distributions for disease mapping. Gelman and\nPrice (1999) discuss artifacts in maps of parameter estimates, and Louis (1984), Shen and\nLouis (1998), and Louis and Shen (1999) analyze the general problem of estimation of\nensembles of parameters, a topic to which we return in Chapter 5.\n\n2.11 Exercises\n\n1. Posterior inference: suppose you have a Beta(4, 4) prior distribution on the probability θ\nthat a coin will yield a ‘head’ when spun in a specified manner. The coin is independently\nspun ten times, and ‘heads’ appear fewer than 3 times. You are not told how many heads\nwere seen, only that the number is less than 3. Calculate your exact posterior density\n(up to a proportionality constant) for θ and sketch it.\n\n2. Predictive distributions: consider two coins, C1 and C2, with the following characteristics:\nPr(heads|C1) = 0.6 and Pr(heads|C2) = 0.4. Choose one of the coins at random and\nimagine spinning it repeatedly. Given that the first two spins from the chosen coin are\ntails, what is the expectation of the number of additional spins until a head shows up?\n\n3. Predictive distributions: let y be the number of 6’s in 1000 rolls of a fair die.\n\n(a) Sketch the approximate distribution of y, based on the normal approximation.\n\n(b) Using the normal distribution table, give approximate 5%, 25%, 50%, 75%, and 95%\npoints for the distribution of y.\n\n4. Predictive distributions: let y be the number of 6’s in 1000 independent rolls of a par-\nticular real die, which may be unfair. Let θ be the probability that the die lands on ‘6.’\nSuppose your prior distribution for θ is as follows:\n\nPr(θ = 1/12) = 0.25,\n\nPr(θ = 1/6) = 0.5,\n\nPr(θ = 1/4) = 0.25.\n\n(a) Using the normal approximation for the conditional distributions, p(y|θ), sketch your\napproximate prior predictive distribution for y.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n58 2. SINGLE-PARAMETER MODELS\n\n(b) Give approximate 5%, 25%, 50%, 75%, and 95% points for the distribution of y. (Be\ncareful here: y does not have a normal distribution, but you can still use the normal\ndistribution as part of your analysis.)\n\n5. Posterior distribution as a compromise between prior information and data: let y be the\nnumber of heads in n spins of a coin, whose probability of heads is θ.\n\n(a) If your prior distribution for θ is uniform on the range [0, 1], derive your prior predictive\ndistribution for y,\n\nPr(y = k) =\n\n∫ 1\n\n0\n\nPr(y = k|θ)dθ,\n\nfor each k = 0, 1, . . . , n.\n\n(b) Suppose you assign a Beta(α, β) prior distribution for θ, and then you observe y heads\nout of n spins. Show algebraically that your posterior mean of θ always lies between\nyour prior mean, α\n\nα+β , and the observed relative frequency of heads, yn .\n\n(c) Show that, if the prior distribution on θ is uniform, the posterior variance of θ is\nalways less than the prior variance.\n\n(d) Give an example of a Beta(α, β) prior distribution and data y, n, in which the posterior\nvariance of θ is higher than the prior variance.\n\n6. Predictive distributions: Derive the mean and variance (2.17) of the negative binomial\npredictive distribution for the cancer rate example, using the mean and variance formulas\n(1.8) and (1.9).\n\n7. Noninformative prior densities:\n\n(a) For the binomial likelihood, y ∼ Bin(n, θ), show that p(θ) ∝ θ−1(1 − θ)−1 is the\nuniform prior distribution for the natural parameter of the exponential family.\n\n(b) Show that if y = 0 or n, the resulting posterior distribution is improper.\n\n8. Normal distribution with unknown mean: a random sample of n students is drawn\nfrom a large population, and their weights are measured. The average weight of the n\nsampled students is y = 150 pounds. Assume the weights in the population are normally\ndistributed with unknown mean θ and known standard deviation 20 pounds. Suppose\nyour prior distribution for θ is normal with mean 180 and standard deviation 40.\n\n(a) Give your posterior distribution for θ. (Your answer will be a function of n.)\n\n(b) A new student is sampled at random from the same population and has a weight of\nỹ pounds. Give a posterior predictive distribution for ỹ. (Your answer will still be a\nfunction of n.)\n\n(c) For n = 10, give a 95% posterior interval for θ and a 95% posterior predictive interval\nfor ỹ.\n\n(d) Do the same for n = 100.\n\n9. Setting parameters for a beta prior distribution: suppose your prior distribution for θ,\nthe proportion of Californians who support the death penalty, is beta with mean 0.6 and\nstandard deviation 0.3.\n\n(a) Determine the parameters α and β of your prior distribution. Sketch the prior density\nfunction.\n\n(b) A random sample of 1000 Californians is taken, and 65% support the death penalty.\nWhat are your posterior mean and variance for θ? Draw the posterior density function.\n\n(c) Examine the sensitivity of the posterior distribution to different prior means and\nwidths including a non-informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 59\n\nYear Fatal Passenger Death\naccidents deaths rate\n\n1976 24 734 0.19\n1977 25 516 0.12\n1978 31 754 0.15\n1979 31 877 0.16\n1980 22 814 0.14\n1981 21 362 0.06\n1982 26 764 0.13\n1983 20 809 0.13\n1984 16 223 0.03\n1985 22 1066 0.15\n\nTable 2.2 Worldwide airline fatalities, 1976–1985. Death rate is passenger deaths per 100 million\npassenger miles. Source: Statistical Abstract of the United States.\n\n10. Discrete sample spaces: suppose there are N cable cars in San Francisco, numbered\nsequentially from 1 to N . You see a cable car at random; it is numbered 203. You wish\nto estimate N . (See Goodman, 1952, for a discussion and references to several versions of\nthis problem, and Jeffreys, 1961, Lee, 1989, and Jaynes, 2003, for Bayesian treatments.)\n\n(a) Assume your prior distribution on N is geometric with mean 100; that is,\n\np(N) = (1/100)(99/100)N−1, for N = 1, 2, . . . .\n\nWhat is your posterior distribution for N?\n\n(b) What are the posterior mean and standard deviation of N? (Sum the infinite series\nanalytically or approximate them on the computer.)\n\n(c) Choose a reasonable ‘noninformative’ prior distribution for N and give the resulting\nposterior distribution, mean, and standard deviation for N .\n\n11. Computing with a nonconjugate single-parameter model: suppose y1, . . . , y5 are inde-\npendent samples from a Cauchy distribution with unknown center θ and known scale 1:\np(yi|θ) ∝ 1/(1 + (yi − θ)2). Assume, for simplicity, that the prior distribution for θ is\nuniform on [0, 100]. Given the observations (y1, . . . , y5) = (43, 44, 45, 46.5, 47.5):\n\n(a) Compute the unnormalized posterior density function, p(θ)p(y|θ), on a grid of points\nθ = 0, 1\n\nm ,\n2\nm , . . . , 100, for some large integerm. Using the grid approximation, compute\n\nand plot the normalized posterior density function, p(θ|y), as a function of θ.\n\n(b) Sample 1000 draws of θ from the posterior density and plot a histogram of the draws.\n\n(c) Use the 1000 samples of θ to obtain 1000 samples from the predictive distribution of\na future observation, y6, and plot a histogram of the predictive draws.\n\n12. Jeffreys’ prior distributions: suppose y|θ ∼ Poisson(θ). Find Jeffreys’ prior density for θ,\nand then find α and β for which the Gamma(α, β) density is a close match to Jeffreys’\ndensity.\n\n13. Discrete data: Table 2.2 gives the number of fatal accidents and deaths on scheduled\nairline flights per year over a ten-year period. We use these data as a numerical example\nfor fitting discrete data models.\n\n(a) Assume that the numbers of fatal accidents in each year are independent with a\nPoisson(θ) distribution. Set a prior distribution for θ and determine the posterior\ndistribution based on the data from 1976 through 1985. Under this model, give a 95%\npredictive interval for the number of fatal accidents in 1986. You can use the normal\napproximation to the gamma and Poisson or compute using simulation.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n60 2. SINGLE-PARAMETER MODELS\n\n(b) Assume that the numbers of fatal accidents in each year follow independent Poisson\ndistributions with a constant rate and an exposure in each year proportional to the\nnumber of passenger miles flown. Set a prior distribution for θ and determine the\nposterior distribution based on the data for 1976–1985. (Estimate the number of\npassenger miles flown in each year by dividing the appropriate columns of Table 2.2\nand ignoring round-off errors.) Give a 95% predictive interval for the number of fatal\naccidents in 1986 under the assumption that 8 × 1011 passenger miles are flown that\nyear.\n\n(c) Repeat (a) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(d) Repeat (b) above, replacing ‘fatal accidents’ with ‘passenger deaths.’\n\n(e) In which of the cases (a)–(d) above does the Poisson model seem more or less rea-\nsonable? Why? Discuss based on general principles, without specific reference to the\nnumbers in Table 2.2.\n\nIncidentally, in 1986, there were 22 fatal accidents, 546 passenger deaths, and a death\nrate of 0.06 per 100 million miles flown. We return to this example in Exercises 3.12,\n6.2, 6.3, and 8.14.\n\n14. Algebra of the normal model:\n\n(a) Fill in the steps to derive (2.9)–(2.10), and (2.11)–(2.12).\n\n(b) Derive (2.11) and (2.12) by starting with a N(µ0, τ\n2\n0 ) prior distribution and adding\n\ndata points one at a time, using the posterior distribution at each step as the prior\ndistribution for the next.\n\n15. Beta distribution: assume the result, from standard advanced calculus, that\n\n∫ 1\n\n0\n\nuα−1(1− u)β−1du =\nΓ(α)Γ(β)\n\nΓ(α + β)\n.\n\nIf Z has a beta distribution with parameters α and β, find E[Zm(1− Z)n] for any non-\nnegative integers m and n. Hence derive the mean and variance of Z.\n\n16. Beta-binomial distribution and Bayes’ prior distribution: suppose y has a binomial dis-\ntribution for given n and unknown parameter θ, where the prior distribution of θ is\nBeta(α, β).\n\n(a) Find p(y), the marginal distribution of y, for y = 0, . . . , n (unconditional on θ). This\ndiscrete distribution is known as the beta-binomial, for obvious reasons.\n\n(b) Show that if the beta-binomial probability is constant in y, then the prior distribution\nhas to have α = β = 1.\n\n17. Posterior intervals: unlike the central posterior interval, the highest posterior interval\nis not invariant to transformation. For example, suppose that, given σ2, the quantity\nnv/σ2 is distributed as χ2\n\nn, and that σ has the (improper) noninformative prior density\np(σ) ∝ σ−1, σ > 0.\n\n(a) Prove that the corresponding prior density for σ2 is p(σ2) ∝ σ−2.\n\n(b) Show that the 95% highest posterior density region for σ2 is not the same as the region\nobtained by squaring the endpoints of a posterior interval for σ.\n\n18. Poisson model: derive the gamma posterior distribution (2.15) for the Poisson model\nparameterized in terms of rate and exposure with conjugate prior distribution.\n\n19. Exponential model with conjugate prior distribution:\n\n(a) Show that if y|θ is exponentially distributed with rate θ, then the gamma prior dis-\ntribution is conjugate for inferences about θ given an independent and identically\ndistributed sample of y values.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n2.11. EXERCISES 61\n\n(b) Show that the equivalent prior specification for the mean, φ = 1/θ, is inverse-gamma.\n(That is, derive the latter density function.)\n\n(c) The length of life of a light bulb manufactured by a certain process has an exponential\ndistribution with unknown rate θ. Suppose the prior distribution for θ is a gamma\ndistribution with coefficient of variation 0.5. (The coefficient of variation is defined\nas the standard deviation divided by the mean.) A random sample of light bulbs is\nto be tested and the lifetime of each obtained. If the coefficient of variation of the\ndistribution of θ is to be reduced to 0.1, how many light bulbs need to be tested?\n\n(d) In part (c), if the coefficient of variation refers to φ instead of θ, how would your\nanswer be changed?\n\n20. Censored and uncensored data in the exponential model:\n\n(a) Suppose y|θ is exponentially distributed with rate θ, and the marginal (prior) distri-\nbution of θ is Gamma(α, β). Suppose we observe that y ≥ 100, but do not observe\nthe exact value of y. What is the posterior distribution, p(θ|y≥100), as a function of\nα and β? Write down the posterior mean and variance of θ.\n\n(b) In the above problem, suppose that we are now told that y is exactly 100. Now what\nare the posterior mean and variance of θ?\n\n(c) Explain why the posterior variance of θ is higher in part (b) even though more in-\nformation has been observed. Why does this not contradict identity (2.8) on page\n32?\n\n21. Simple hierarchical modeling:\nThe file pew research center june elect wknd data.dta3 has data from Pew Research\nCenter polls taken during the 2008 election campaign. You can read these data into R\nusing the read.dta() function (after first loading the foreign package into R).\nYour task is to estimate the percentage of the (adult) population in each state (excluding\nAlaska, Hawaii, and the District of Columbia) who label themselves as ‘very liberal,’\nfollowing the general procedure that was used in Section 2.7 to estimate cancer rates,\nbut using the binomial and beta rather than Poisson and gamma distributions. But you\ndo not need to make maps; it will be enough to make scatterplots, plotting the estimate\nvs. Barack Obama’s vote share in 2008 (data available at 2008ElectionResult.csv,\nreadable in R using read.csv()).\nMake the following four graphs on a single page:\n\n• Graph proportion very liberal among the survey respondents in each state vs. Obama\nvote share—that is, a scatterplot using the two-letter state abbreviations (see state.abb()\nin R).\n\n• Graph the Bayes posterior mean in each state vs. Obama vote share.\n\n• Repeat graphs (a) and (b) using the number of respondents in the state on the x-axis.\n\nThis exercise has four challenges: first, manipulating the data in order to get the totals\nby state; second, estimating the parameters of the prior distribution; third, doing the\nBayesian analysis by state; and fourth, making the graphs.\n\n22. Prior distributions:\nA (hypothetical) study is performed to estimate the effect of a simple training program\non basketball free-throw shooting. A random sample of 100 college students is recruited\ninto the study. Each student first shoots 100 free-throws to establish a baseline success\nprobability. Each student then takes 50 practice shots each day for a month. At the end\nof that time, he or she takes 100 shots for a final measurement. Let θ be the average\nimprovement in success probability.\nGive three prior distributions for θ (explaining each in a sentence):\n\n3For data for this and other exercises, go to http://www.stat.columbia.edu/∼gelman/book/.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n62 2. SINGLE-PARAMETER MODELS\n\n(a) A noninformative prior,\n\n(b) A subjective prior based on your best knowledge, and\n\n(c) A weakly informative prior.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 3\n\nIntroduction to multiparameter models\n\nVirtually every practical problem in statistics involves more than one unknown or unob-\nservable quantity. It is in dealing with such problems that the simple conceptual framework\nof the Bayesian approach reveals its principal advantages over other methods of inference.\nAlthough a problem can include several parameters of interest, conclusions will often be\ndrawn about one, or only a few, parameters at a time. In this case, the ultimate aim of a\nBayesian analysis is to obtain the marginal posterior distribution of the particular param-\neters of interest. In principle, the route to achieving this aim is clear: we first require the\njoint posterior distribution of all unknowns, and then we integrate this distribution over the\nunknowns that are not of immediate interest to obtain the desired marginal distribution.\nOr equivalently, using simulation, we draw samples from the joint posterior distribution\nand then look at the parameters of interest and ignore the values of the other unknowns.\nIn many problems there is no interest in making inferences about many of the unknown\nparameters, although they are required in order to construct a realistic model. Parameters\nof this kind are often called nuisance parameters. A classic example is the scale of the\nrandom errors in a measurement problem.\n\nWe begin this chapter with a general treatment of nuisance parameters and then cover\nthe normal distribution with unknown mean and variance in Section 3.2. Sections 3.4\nand 3.5 present inference for the multinomial and multivariate normal distributions—the\nsimplest models for discrete and continuous multivariate data, respectively. The chapter\nconcludes with an analysis of a nonconjugate logistic regression model, using numerical\ncomputation of the posterior density on a grid.\n\n3.1 Averaging over ‘nuisance parameters’\n\nTo express the ideas of joint and marginal posterior distributions mathematically, suppose\nθ has two parts, each of which can be a vector, θ = (θ1, θ2), and further suppose that we\nare only interested (at least for the moment) in inference for θ1, so θ2 may be considered a\n‘nuisance’ parameter. For instance, in the simple example,\n\ny|µ, σ2 ∼ N(µ, σ2),\n\nin which both µ (=‘θ1’) and σ\n2 (=‘θ2’) are unknown, interest commonly centers on µ.\n\nWe seek the conditional distribution of the parameter of interest given the observed\ndata; in this case, p(θ1|y). This is derived from the joint posterior density,\n\np(θ1, θ2|y) ∝ p(y|θ1, θ2)p(θ1, θ2),\n\nby averaging over θ2:\n\np(θ1|y) =\n∫\np(θ1, θ2|y)dθ2.\n\n63\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(θ1|y) =\n∫\np(θ1|θ2, y)p(θ2|y)dθ2, (3.1)\n\nwhich shows that the posterior distribution of interest, p(θ1|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, θ2, where p(θ2|y) is a weighting\nfunction for the different possible values of θ2. The weights depend on the posterior density\nof θ2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters θ2 can be interpreted generally; for example, θ2 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing θ2 from\nits marginal posterior distribution and then θ1 from its conditional posterior distribution,\ngiven the drawn value of θ2. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(µ, σ2);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for µ and σ, assuming prior\nindependence of location and scale parameters, is uniform on (µ, log σ) or, equivalently,\n\np(µ, σ2) ∝ (σ2)−1.\n\nThe joint posterior distribution, p(µ, σ2|y)\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/σ2:\n\np(µ, σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\nn∑\n\ni=1\n\n(yi − µ)2\n)\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n\n[\nn∑\n\ni=1\n\n(yi − y)2 + n(y − µ)2\n])\n\n= σ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\n, (3.2)\n\nwhere\n\ns2 =\n1\n\nn− 1\n\nn∑\n\ni=1\n\n(yi − y)2\n\nis the sample variance of the yi’s. The sufficient statistics are y and s2.\n\nThis electronic edition is for non-commercial purposes only.\n\n64 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nAlternatively, the joint posterior density can be factored to yield\n\np(Orly) = / p(61|62, y)p(O2|y) dbo, (3.1)\n\nwhich shows that the posterior distribution of interest, p(@i|y), is a mixture of the condi-\ntional posterior distributions given the nuisance parameter, 02, where p(02|y) is a weighting\nfunction for the different possible values of 62. The weights depend on the posterior density\nof #2 and thus on a combination of evidence from data and prior model. The averaging over\nnuisance parameters 62 can be interpreted generally; for example, 62 can include a discrete\ncomponent representing different possible sub-models.\n\nWe rarely evaluate the integral (3.1) explicitly, but it suggests an important practical\nstrategy for both constructing and computing with multiparameter models. Posterior dis-\ntributions can be computed by marginal and conditional simulation, first drawing 62 from\nits marginal posterior distribution and then 0; from its conditional posterior distribution,\ngiven the drawn value of 02. In this way the integration embodied in (3.1) is performed\nindirectly. A canonical example of this form of analysis is provided by the normal model\nwith unknown mean and variance, to which we now turn.\n\n3.2 Normal data with a noninformative prior distribution\n\nAs the prototype example of estimating the mean of a population from a sample, we consider\na vector y of n independent observations from a univariate normal distribution, N(, 07);\nthe generalization to the multivariate normal distribution appears in Section 3.5. We begin\nby analyzing the model under a noninformative prior distribution, with the understanding\nthat this is no more than a convenient assumption for the purposes of exposition and is\neasily extended to informative prior distributions.\n\nA noninformative prior distribution\n\nWe saw in Chapter 2 that a sensible vague prior density for w and o, assuming prior\nindependence of location and scale parameters, is uniform on (y,logo) or, equivalently,\n\np(u,o7) x (a7) *.\n\nThe joint posterior distribution, p(u,07|y)\n\nUnder this conventional improper prior density, the joint posterior distribution is propor-\ntional to the likelihood function multiplied by the factor 1/c?:\n\nan 1<\nP(u,o*|y) oo\" * exp (-2: Sen -n]\n\nII\nq\n3\ni)\n)\ntal\nue)\n—“~\nto\nPai\nbo\nS\n3\n—\n=\n|\nSl\niw)\n+\n=\nRad]\n|\n=\ni)\n|\nn___”\n\n(3.2)\n\nII\nQ\n3\nbo\nco\ntal\nue)\na ™~\n|\n| H\n=\n3\n|\n—\n“—\nD\niw)\n+\n3\n—\nKa]\n|\n=\n“—\n7\nQe\n\nwhere\n\nis the sample variance of the y;’s. The sufficient statistics are 7 and s?.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(µ|σ2, y), and then the marginal posterior density, p(σ2|y). To determine\nthe posterior distribution of µ, given σ2, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nµ|σ2, y ∼ N(y, σ2/n). (3.3)\n\nThe marginal posterior distribution, p(σ2|y)\n\nTo determine p(σ2|y), we must average the joint distribution (3.2) over µ:\n\np(σ2|y) ∝\n∫\nσ−n−2 exp\n\n(\n− 1\n\n2σ2\n[(n−1)s2 + n(y − µ)2]\n\n)\ndµ.\n\nIntegrating this expression over µ requires evaluating the integral exp\n(\n− 1\n\n2σ2n(y − µ)2\n)\n,\n\nwhich is a simple normal integral; thus,\n\np(σ2|y) ∝ σ−n−2 exp\n\n(\n− 1\n\n2σ2\n(n−1)s2\n\n)√\n2πσ2/n\n\n∝ (σ2)−(n+1)/2 exp\n\n(\n− (n− 1)s2\n\n2σ2\n\n)\n, (3.4)\n\nwhich is a scaled inverse-χ2 density:\n\nσ2|y ∼ Inv-χ2(n− 1, s2). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(µ, σ2|y) = p(µ|σ2, y)p(σ2|y).\n\nThis marginal posterior distribution for σ2 has a remarkable similarity to the analogous\nsampling theory result: conditional on σ2 (and µ), the distribution of the appropriately\n\nscaled sufficient statistic, (n−1)s2\n\nσ2 , is χ2\nn−1. Considering our derivation of the reference prior\n\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw σ2 from (3.5),\nthen draw µ from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of µ\n\nThe population mean, µ, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of µ, which can be obtained by\nintegrating σ2 out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of µ can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-χ2 distribution for the variance, σ2. We can derive the marginal\nposterior density for µ by integrating the joint posterior density over σ2:\n\np(µ|y) =\n∫ ∞\n\n0\n\np(µ, σ2|y)dσ2.\n\nThis electronic edition is for non-commercial purposes only.\n\n3.2. NORMAL DATA WITH A NONINFORMATIVE PRIOR DISTRIBUTION 65\nThe conditional posterior distribution, p(u|o?, y)\n\nIn order to factor the joint posterior density as in (3.1), we consider first the conditional\nposterior density, p(u|o”, y), and then the marginal posterior density, p(a?|y). To determine\nthe posterior distribution of 4, given 07, we simply use the result derived in Section 2.5 for\nthe mean of a normal distribution with known variance and a uniform prior distribution:\n\nulo?,y ~ NG,o7/n). (3.3)\n\nThe marginal posterior distribution, p(o?|y)\n\nTo determine p(o?|y), we must average the joint distribution (3.2) over p:\n2 —n-2 1 2 = 2\nploly) x fo\" exp | —sal(n—1s° +n — 4)\"] } du.\n\nIntegrating this expression over fz requires evaluating the integral exp (-sin(y — L)’),\nwhich is a simple normal integral; thus,\n\npoly) oo\" Pexp (—o5(n—1)s*) VBra%]n\n\n20?\n\nx (0?) TY? exp (-“S*) ; (3.4)\n\nwhich is a scaled inverse-y? density:\no*|y ~ Inv-x?(n — 1, s”). (3.5)\n\nWe have thus factored the joint posterior density (3.2) as the product of conditional and\nmarginal posterior densities: p(u,0?|y) = p(ulo?, y)p(o?|y).\n\nThis marginal posterior distribution for ¢? has a remarkable similarity to the analogous\nsampling theory result: conditional on o? (and jy), the distribution of the appropriately\nscaled sufficient statistic, (nas , is x2_,. Considering our derivation of the reference prior\ndistribution for the scale parameter in Section 2.8, however, this result is not surprising.\n\nSampling from the joint posterior distribution\n\nIt is easy to draw samples from the joint posterior distribution: first draw o? from (3.5),\nthen draw py from (3.3). We also derive some analytical results for the posterior distribution,\nsince this is one of the few multiparameter problems simple enough to solve in closed form.\n\nAnalytic form of the marginal posterior distribution of ju\n\nThe population mean, ju, is typically the estimand of interest, and so the objective of the\nBayesian analysis is the marginal posterior distribution of yu, which can be obtained by\nintegrating 0? out of the joint posterior distribution. The representation (3.1) shows that\nthe posterior distribution of 4: can be regarded as a mixture of normal distributions, mixed\nover the scaled inverse-x? distribution for the variance, 07. We can derive the marginal\n\nposterior density for 4 by integrating the joint posterior density over o?:\n\nP(Hly) = [ p(b, 07 |y)do”.\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\n\nz =\nA\n\n2σ2\n, where A = (n− 1)s2 + n(µ− y)2,\n\nand recognizing that the result is an unnormalized gamma integral:\n\np(µ|y) ∝ A−n/2\n∫ ∞\n\n0\n\nz(n−2)/2 exp(−z)dz\n\n∝ [(n− 1)s2 + n(µ− y)2]−n/2\n\n∝\n[\n1 +\n\nn(µ− y)2\n(n− 1)s2\n\n]−n/2\n.\n\nThis is the tn−1(y, s\n2/n) density (see Appendix A).\n\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (µ, log σ), the posterior distribution of µ has the form\n\nµ− y\ns/\n√\nn\n\n∣∣∣∣ y ∼ tn−1,\n\nwhere tn−1 denotes the standard t density (location 0, scale 1) with n−1 degrees of freedom.\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|µ, σ2), the following relation holds:\n\ny − µ\ns/\n√\nn\n\n∣∣∣∣µ, σ2 ∼ tn−1.\n\nThe sampling distribution of the pivotal quantity (y − µ)/(s/√n) does not depend on the\nnuisance parameter σ2, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, ỹ, can be written as a mixture,\np(ỹ|y) =\n\n∫∫\np(ỹ|µ, σ2, y)p(µ, σ2|y)dµdσ2. The first of the two factors in the integral is just\n\nthe normal distribution for the future observation given the values of (µ, σ2), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw µ, σ2 from\ntheir joint posterior distribution and then simulate ỹ ∼ N(µ, σ2).\n\nIn fact, the posterior predictive distribution of ỹ is a t distribution with location y,\nscale (1 + 1\n\nn )\n1/2s, and n − 1 degrees of freedom. This analytic form is obtained using the\n\nsame techniques as in the derivation of the posterior distribution of µ. Specifically, the\ndistribution can be obtained by integrating out the parameters µ, σ2 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(ỹ|σ2, y) =\n\n∫\np(ỹ|µ, σ2, y)p(µ|σ2, y)dµ leads to p(ỹ|σ2, y) = N(ỹ|y, (1 + 1\n\nn )σ\n2),\n\nwhich is the same, up to a changed scale factor, as the distribution of µ|σ2, y.\n\nExample. Estimating the speed of light\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\nThis electronic edition is for non-commercial purposes only.\n\n66 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThis integral can be evaluated using the substitution\nz= AL where A = (n —1)s? + n(pp— 9)?\n202 >) ?\n\nand recognizing that the result is an unnormalized gamma integral:\n\nply) 2 Av? [0-2 exp(—a)de\n10)\nx [(n—1)s? +n(u— 92?\n\n_\\94—n/2\nnu 9)?”\n«x |1+———> .\n(n — 1)s?\nThis is the t,_1(Y, s?/n) density (see Appendix A).\nTo put it another way, we have shown that, under the noninformative uniform prior\ndistribution on (js, log a), the posterior distribution of y has the form\n\nMay\nwhere t,,_1 denotes the standard ¢ density (location 0, scale 1) with n—1 degrees of freedom.\n\nThis marginal posterior distribution provides another interesting comparison with sampling\ntheory. Under the sampling distribution, p(y|u, 07), the following relation holds:\n\nyYy~ tn—-1;\n\nYH\n\ns/vn\\ i?\n\nThe sampling distribution of the pivotal quantity (y — )/(s/./n) does not depend on the\nnuisance parameter o7, and its posterior distribution does not depend on data. In general,\na pivotal quantity for the estimand is defined as a nontrivial function of the data and the\nestimand whose sampling distribution is independent of all parameters and data.\n\n2 ty.\n\nPosterior predictive distribution for a future observation\n\nThe posterior predictive distribution for a future observation, y, can be written as a mixture,\nply) = f{vG\\u, 0? y)p(u, 07 |y)dudo?. The first of the two factors in the integral is just\nthe normal distribution for the future observation given the values of (1,07), and does not\ndepend on y at all. To draw from the posterior predictive distribution, first draw 4,0? from\ntheir joint posterior distribution and then simulate 7 ~ N(j, 07).\n\nIn fact, the posterior predictive distribution of y is a t distribution with location ¥,\nscale (1 + 4)1/ 2s, and n — 1 degrees of freedom. This analytic form is obtained using the\nsame techniques as in the derivation of the posterior distribution of jz. Specifically, the\ndistribution can be obtained by integrating out the parameters j1,07 according to their\njoint posterior distribution. We can identify the result more easily by noticing that the\nfactorization p(glo”, y) = [p(glu,07, y)p(ulo?, yd leads to p(glo?, y) = N(gly, (1+ 4)o”),\nwhich is the same, up to a changed scale factor, as the distribution of pu\\o?, y.\n\nExample. Estimating the speed of light\n\nSimon Newcomb set up an experiment in 1882 to measure the speed of light. Newcomb\n\nmeasured the amount of time required for light to travel a distance of 7442 meters. A\n\nhistogram of Newcomb’s 66 measurements is shown in Figure 3.1. There are two un-\n\nusually low measurements and then a cluster of measurements that are approximately\nsymmetrically distributed. We (inappropriately) apply the normal model, assuming\nthat all 66 measurements are independent draws from a normal distribution with mean\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.3. NORMAL DATA WITH A CONJUGATE PRIOR DISTRIBUTION 67\n\nFigure 3.1 Histogram of Simon Newcomb’s measurements for estimating the speed of light, from\nStigler (1977). The data are recorded as deviations from 24,800 nanoseconds.\n\nµ and variance σ2. The main substantive goal is posterior inference for µ. The outlying\nmeasurements do not fit the normal model; we discuss Bayesian methods for measur-\ning the lack of fit for these data in Section 6.3. The mean of the 66 measurements is\ny = 26.2, and the sample standard deviation is s = 10.8. Assuming the noninformative\nprior distribution p(µ, σ2) ∝ (σ2)−1, a 95% central posterior interval for µ is obtained\nfrom the t65 marginal posterior distribution of µ as y ± 1.997s/\n\n√\n66 = [23.6, 28.8].\n\nThe posterior interval can also be obtained by simulation. Following the factorization\nof the posterior distribution given by (3.5) and (3.3), we first draw a random value of\nσ2 ∼ Inv-χ2(65, s2) as 65s2 divided by a random draw from the χ2\n\n65 distribution (see\nAppendix A). Then given this value of σ2, we draw µ from its conditional posterior\ndistribution, N(26.2, σ2/66). Based on 1000 simulated values of (µ, σ2), we estimate\nthe posterior median of µ to be 26.2 and a 95% central posterior interval for µ to be\n[23.6, 28.9], close to the analytically calculated interval.\nIncidentally, based on the currently accepted value of the speed of light, the ‘true\nvalue’ for µ in Newcomb’s experiment is 33.0, which falls outside our 95% interval.\nThis reinforces the fact that posterior inferences are only as good as the model and\nthe experiment that produced the data.\n\n3.3 Normal data with a conjugate prior distribution\n\nA family of conjugate prior distributions\n\nA first step toward a more general model is to assume a conjugate prior distribution for\nthe two-parameter univariate normal sampling model in place of the noninformative prior\ndistribution just considered. The form of the likelihood displayed in (3.2) and the subse-\nquent discussion shows that the conjugate prior density must also have the product form\np(σ2)p(µ|σ2), where the marginal distribution of σ2 is scaled inverse-χ2 and the conditional\ndistribution of µ given σ2 is normal (so that marginally µ has a t distribution). A convenient\nparameterization is given by the following specification:\n\nµ|σ2 ∼ N(µ0, σ\n2/κ0)\n\nσ2 ∼ Inv-χ2(ν0, σ\n2\n0),\n\nwhich corresponds to the joint prior density\n\np(µ, σ2) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ0 − µ)2]\n\n)\n. (3.6)\n\nWe label this the N-Inv-χ2(µ, σ2|µ0, σ\n2\n0/κ0; ν0, σ\n\n2\n0) density; its four parameters can be iden-\n\ntified as the location and scale of µ and the degrees of freedom and scale of σ2, respectively.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n68 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nThe appearance of σ2 in the conditional distribution of µ|σ2 means that µ and σ2 are\nnecessarily dependent in their joint conjugate prior density: for example, if σ2 is large, then\na high-variance prior distribution is induced on µ. This dependence is notable, considering\nthat conjugate prior distributions are used largely for convenience. Upon reflection, however,\nit often makes sense for the prior variance of the mean to be tied to σ2, which is the sampling\nvariance of the observation y. In this way, prior belief about µ is calibrated by the scale of\nmeasurement of y and is equivalent to κ0 prior measurements on this scale.\n\nThe joint posterior distribution, p(µ, σ2|y)\n\nMultiplying the prior density (3.6) by the normal likelihood yields the posterior density\n\np(µ, σ2|y) ∝ σ−1(σ2)−(ν0/2+1) exp\n\n(\n− 1\n\n2σ2\n[ν0σ\n\n2\n0 + κ0(µ− µ0)\n\n2]\n\n)\n×\n\n× (σ2)−n/2 exp\n\n(\n− 1\n\n2σ2\n[(n− 1)s2 + n(y − µ)2]\n\n)\n(3.7)\n\n= N-Inv-χ2(µ, σ2|µn, σ2\nn/κn; νn, σ\n\n2\nn),\n\nwhere, after some algebra (see Exercise 3.9), it can be shown that\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nνnσ\n2\nn = ν0σ\n\n2\n0 + (n− 1)s2 +\n\nκ0n\n\nκ0 + n\n(y − µ0)\n\n2.\n\nThe parameters of the posterior distribution combine the prior information and the infor-\nmation contained in the data. For example µn is a weighted average of the prior mean and\nthe sample mean, with weights determined by the relative precision of the two pieces of\ninformation. The posterior degrees of freedom, νn, is the prior degrees of freedom plus the\nsample size. The posterior sum of squares, νnσ\n\n2\nn, combines the prior sum of squares, the\n\nsample sum of squares, and the additional uncertainty conveyed by the difference between\nthe sample mean and the prior mean.\n\nThe conditional posterior distribution, p(µ|σ2, y)\n\nThe conditional posterior density of µ, given σ2, is proportional to the joint posterior density\n(3.7) with σ2 held constant,\n\nµ|σ2, y ∼ N(µn, σ\n2/κn)\n\n= N\n\n( κ0\n\nσ2µ0 +\nn\nσ2 y\n\nκ0\n\nσ2 + n\nσ2\n\n,\n1\n\nκ0\n\nσ2 + n\nσ2\n\n)\n, (3.8)\n\nwhich agrees, as it must, with the analysis in Section 2.5 of µ with σ considered fixed.\n\nThe marginal posterior distribution, p(σ2|y)\n\nThe marginal posterior density of σ2, from (3.7), is scaled inverse-χ2:\n\nσ2|y ∼ Inv-χ2(νn, σ\n2\nn). (3.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.4. MULTINOMIAL MODEL FOR CATEGORICAL DATA 69\n\nSampling from the joint posterior distribution\n\nTo sample from the joint posterior distribution, just as in the previous section, we first draw\nσ2 from its marginal posterior distribution (3.9), then draw µ from its normal conditional\nposterior distribution (3.8), using the simulated value of σ2.\n\nAnalytic form of the marginal posterior distribution of µ\n\nIntegration of the joint posterior density with respect to σ2, in a precisely analogous way\nto that used in the previous section, shows that the marginal posterior density for µ is\n\np(µ|y) ∝\n(\n1 +\n\nκn(µ− µn)2\nνnσ2\n\nn\n\n)−(νn+1)/2\n\n= tνn(µ|µn, σ2\nn/κn).\n\n3.4 Multinomial model for categorical data\n\nThe binomial distribution that was emphasized in Chapter 2 can be generalized to allow\nmore than two possible outcomes. The multinomial sampling distribution is used to describe\ndata for which each observation is one of k possible outcomes. If y is the vector of counts\nof the number of observations of each outcome, then\n\np(y|θ) ∝\nk∏\n\nj=1\n\nθ\nyj\nj ,\n\nwhere the sum of the probabilities,\n∑k\n\nj=1 θj , is 1. The distribution is typically thought of as\n\nimplicitly conditioning on the number of observations,\n∑k\nj=1 yj = n. The conjugate prior\n\ndistribution is a multivariate generalization of the beta distribution known as the Dirichlet,\n\np(θ|α) ∝\nk∏\n\nj=1\n\nθ\nαj−1\nj ,\n\nwhere the distribution is restricted to nonnegative θj ’s with\n∑k\nj=1 θj = 1; see Appendix\n\nA for details. The resulting posterior distribution for the θj ’s is Dirichlet with parameters\nαj + yj .\n\nThe prior distribution expressed on the scale of α is mathematically equivalent to a\nlikelihood resulting from\n\n∑k\nj=1(αj−1) observations with αj−1 observations of the jth out-\n\ncome category. As in the binomial there are several plausible noninformative Dirichlet prior\ndistributions. A uniform density is obtained by setting αj = 1 for all j; this distribution\n\nassigns equal density to any vector θ satisfying\n∑k\nj=1 θj = 1. Setting αj = 0 for all j results\n\nin an improper prior distribution that is uniform in the log(θj)’s. The resulting posterior\ndistribution is proper if there is at least one observation in each of the k categories, so that\neach component of y is positive. The bibliographic note at the end of this chapter points\nto other suggested noninformative prior distributions for the multinomial model.\n\nExample. Pre-election polling\nFor a simple example of a multinomial model, we consider a sample survey question\nwith three possible responses. In late October, 1988, a survey was conducted by CBS\nNews of 1447 adults in the United States to find out their preferences in the upcoming\npresidential election. Out of 1447 persons, y1 = 727 supported George Bush, y2 = 583\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n70 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.2 Histogram of values of (θ1− θ2) for 1000 simulations from the posterior distribution for\nthe election polling example.\n\nsupported Michael Dukakis, and y3 = 137 supported other candidates or expressed no\nopinion. Assuming no other information on the respondents, the 1447 observations\nare exchangeable. If we also assume simple random sampling (that is, 1447 names\n‘drawn out of a hat’), then the data (y1, y2, y3) follow a multinomial distribution, with\nparameters (θ1, θ2, θ3), the proportions of Bush supporters, Dukakis supporters, and\nthose with no opinion in the survey population. An estimand of interest is θ1 − θ2,\nthe population difference in support for the two major candidates.\nWith a noninformative uniform prior distribution on θ, α1=α2=α3=1, the posterior\ndistribution for (θ1, θ2, θ3) is Dirichlet(728, 584, 138). We could compute the posterior\ndistribution of θ1 − θ2 by integration, but it is simpler just to draw 1000 points\n(θ1, θ2, θ3) from the posterior Dirichlet distribution and then compute θ1 − θ2 for\neach. The result is displayed in Figure 3.2. All of the 1000 simulations had θ1 > θ2;\nthus, the estimated posterior probability that Bush had more support than Dukakis\nin the survey population is over 99.9%.\nIn fact, the CBS survey does not use independent random sampling but rather uses a\nvariant of a stratified sampling plan. We discuss an improved analysis of this survey,\nusing some knowledge of the sampling scheme, in Section 8.3 (see Table 8.2 on page\n207).\n\nIn complicated problems—for example, analyzing the results of many survey questions\nsimultaneously—the number of multinomial categories, and thus parameters, becomes so\nlarge that it is hard to usefully analyze a dataset of moderate size without additional\nstructure in the model. Formally, additional information can enter the analysis through\nthe prior distribution or the sampling model. An informative prior distribution might be\nused to improve inference in complicated problems, using the ideas of hierarchical modeling\nintroduced in Chapter 5. Alternatively, loglinear models can be used to impose structure on\nmultinomial parameters that result from cross-classifying several survey questions; Section\n16.7 provides details and an example.\n\n3.5 Multivariate normal model with known variance\n\nHere we give a somewhat formal account of the distributional results of Bayesian inference\nfor the parameters of a multivariate normal distribution. In many ways, these results\nparallel those already given for the univariate normal model, but there are some important\nnew aspects that play a major role in the analysis of linear models, which is the central\nactivity of much applied statistical work (see Chapters 5, 14, and 15). This section can be\nviewed at this point as reference material for future chapters.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\n\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|µ,Σ ∼ N(µ,Σ), (3.10)\n\nwhere µ is a (column) vector of length d and Σ is a d×d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\np(y|µ,Σ) ∝ |Σ|−1/2 exp\n\n(\n−1\n\n2\n(y − µ)TΣ−1(y − µ)\n\n)\n,\n\nand for a sample of n independent and identically distributed observations, y1, . . . , yn, is\n\np(y1, . . . , yn|µ,Σ) ∝ |Σ|−n/2 exp\n(\n−1\n\n2\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n)\n. (3.11)\n\nConjugate analysis\n\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known Σ.\n\nConjugate prior distribution for µ with known Σ. The log-likelihood is a quadratic form\nin µ, and therefore the conjugate prior distribution for µ is the multivariate normal distri-\nbution, which we parameterize as µ ∼ N(µ0,Λ0).\n\nPosterior distribution for µ with known Σ. The posterior distribution of µ is\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n\n(\n(µ− µ0)\n\nTΛ−1\n0 (µ− µ0) +\n\nn∑\n\ni=1\n\n(yi − µ)TΣ−1(yi − µ)\n))\n\n,\n\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\np(µ|y,Σ) ∝ exp\n\n(\n−1\n\n2\n(µ− µn)TΛ−1\n\nn (µ− µn)\n)\n\n= N(µ|µn,Λn),\n\nwhere\n\nµn = (Λ−1\n0 + nΣ−1)−1(Λ−1\n\n0 µ0 + nΣ−1y)\n\nΛ−1\nn = Λ−1\n\n0 + nΣ−1. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nΣ−1 and Λ−1\n\n0 , respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of µ with known Σ. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, µ(1) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\nµn and variance matrix equal to the appropriate submatrix of Λn. Also, the conditional\nposterior distribution of a subset µ(1) given the values of a second subset µ(2) is multivariate\n\nThis electronic edition is for non-commercial purposes only.\n\n3.5. MULTIVARIATE NORMAL MODEL WITH KNOWN VARIANCE 71\nMultivariate normal likelihood\n\nThe basic model to be discussed concerns an observable vector y of d components, with the\nmultivariate normal distribution,\n\ny|w, &~ N(w,%), (3.10)\n\nwhere yu is a (column) vector of length d and © is a dx d variance matrix, which is symmetric\nand positive definite. The likelihood function for a single observation is\n\npul.) x [E12 ep (—S y= w= 1)),\n\nand for a sample of n independent and identically distributed observations, y1,..., Yn, 1S\n\n_n 1X _\nPCY «+++ Yn|ts B) oc [Z|-\"/? exp (-4 Yow w= Mn 0))- (3.11)\n\ni=l\n\nConjugate analysis\nAs with the univariate normal model, we analyze the multivariate normal model by first\nconsidering the case of known ™.\n\nConjugate prior distribution for w with known X. The log-likelihood is a quadratic form\nin ys, and therefore the conjugate prior distribution for yz is the multivariate normal distri-\nbution, which we parameterize as pu ~ N({uo, Ao).\n\nPosterior distribution for with known %. The posterior distribution of ju is\n1 _ . _\np(uly, &) x exp (-} (\\: — po)” Ng (= po) + S3(yi = BPE yi - »))\ni=1\n\nwhich is an exponential of a quadratic form in jz. Completing the quadratic form and pulling\nout constant factors (see Exercise 3.13) gives\n\nplays) 0 exp (—5(00~ nn) A\" — sn)\n\n= N(u| én, An),\nwhere\nHin = (Ag* + n¥7\")7*(Ag to + nE~\"y)\nAyt = Apt+nzc7. (3.12)\n\nThese are similar to the results for the univariate normal model in Section 2.5, the posterior\nmean being a weighted average of the data and the prior mean, with weights given by the\ndata and prior precision matrices, nu~! and Ag 1 respectively. The posterior precision is\nthe sum of the prior and data precisions.\n\nPosterior conditional and marginal distributions of subvectors of 1 with known %. It follows\nfrom the properties of the multivariate normal distribution (see Appendix A) that the\nmarginal posterior distribution of a subset of the parameters, 1“) say, is also multivariate\nnormal, with mean vector equal to the appropriate subvector of the posterior mean vector\n[in and variance matrix equal to the appropriate submatrix of A,. Also, the conditional\nposterior distribution of a subset p“) given the values of a second subset “?) is multivariate\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n72 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nnormal. If we write superscripts in parentheses to indicate appropriate subvectors and\nsubmatrices, then\n\nµ(1)|µ(2), y ∼ N\n(\nµ(1)\nn + β1|2(µ(2) − µ(2)\n\nn ),Λ1|2\n)\n, (3.13)\n\nwhere the regression coefficients β1|2 and conditional variance matrix Λ1|2 are defined by\n\nβ1|2 = Λ(12)\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ1|2 = Λ(11)\nn − Λ(12)\n\nn\n\n(\nΛ(22)\nn\n\n)−1\n\nΛ(21)\nn .\n\nPosterior predictive distribution for new data. We now work out the analytic form of the\nposterior predictive distribution for a new observation ỹ ∼ N(µ,Σ). As with the univariate\nnormal, we first note that the joint distribution, p(ỹ, µ|y) = N(ỹ|µ,Σ)N(µ|µn,Λn), is the\nexponential of a quadratic form in (ỹ, µ); hence (ỹ, µ) have a joint normal posterior distri-\nbution, and so the marginal posterior distribution of ỹ is (multivariate) normal. We are\nstill assuming the variance matrix Σ is known. As in the univariate case, we can determine\nthe posterior mean and variance of ỹ using (2.7) and (2.8):\n\nE(ỹ|y) = E(E(ỹ|µ, y)|y)\n= E(µ|y) = µn,\n\nand\n\nvar(ỹ|y) = E(var(ỹ|µ, y)|y) + var(E(ỹ|µ, y)|y)\n= E(Σ|y) + var(µ|y) = Σ + Λn.\n\nTo sample from the posterior distribution or the posterior predictive distribution, re-\nfer to Appendix A for a method of generating random draws from a multivariate normal\ndistribution with specified mean and variance matrix.\n\nNoninformative prior density for µ. A noninformative uniform prior density for µ is p(µ) ∝\nconstant, obtained in the limit as the prior precision tends to zero in the sense |Λ−1\n\n0 | → 0;\nin the limit of infinite prior variance (zero prior precision), the prior mean is irrelevant.\nThough this choice of prior density does not combine with the likelihood to form a proper\njoint probability model for µ and y, the posterior density obtained by applying Bayes’ rule\nis a proper posterior density. The posterior density is proportional to the likelihood (3.11)\nwhich is an exponential of a quadratic form in µ. Completing the quadratic form and pulling\nout constant terms yields the posterior distribution for µ, given the uniform prior density,\nas µ|Σ, y ∼ N(y,Σ/n).\n\n3.6 Multivariate normal with unknown mean and variance\n\nConjugate inverse-Wishart family of prior distributions\n\nRecall that the conjugate distribution for the univariate normal with unknown mean and\nvariance is the normal-inverse-χ2 distribution (3.6). We can use the inverse-Wishart dis-\ntribution, a multivariate generalization of the scaled inverse-χ2, to describe the prior dis-\ntribution of the matrix Σ. The conjugate prior distribution for (µ,Σ), the normal-inverse-\nWishart, is conveniently parameterized in terms of hyperparameters (µ0,Λ0/κ0; ν0,Λ0):\n\nΣ ∼ Inv-Wishartν0(Λ\n−1\n0 )\n\nµ|Σ ∼ N(µ0,Σ/κ0),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.6. MULTIVARIATE NORMAL WITH UNKNOWN MEAN AND VARIANCE 73\n\nwhich corresponds to the joint prior density\n\np(µ,Σ)∝|Σ|−((ν0+d)/2+1) exp\n\n(\n−1\n\n2\ntr(Λ0Σ\n\n−1)− κ0\n2\n(µ− µ0)\n\nTΣ−1(µ− µ0)\n\n)\n.\n\nThe parameters ν0 and Λ0 describe the degrees of freedom and the scale matrix for the\ninverse-Wishart distribution on Σ. The remaining parameters are the prior mean, µ0, and\nthe number of prior measurements, κ0, on the Σ scale. Multiplying the prior density by the\nnormal likelihood results in a posterior density of the same family with parameters\n\nµn =\nκ0\n\nκ0 + n\nµ0 +\n\nn\n\nκ0 + n\ny\n\nκn = κ0 + n\n\nνn = ν0 + n\n\nΛn = Λ0 + S +\nκ0n\n\nκ0 + n\n(y − µ0)(y − µ0)\n\nT ,\n\nwhere S is the sum of squares matrix about the sample mean,\n\nS =\n\nn∑\n\ni=1\n\n(yi − y)(yi − y)T .\n\nOther results from the univariate normal easily generalize to the multivariate case. The\nmarginal posterior distribution of µ is multivariate tνn−d+1(µn,Λn/(κn(νn − d+ 1))). The\nposterior predictive distribution of a new observation ỹ is also multivariate t with an ad-\nditional factor of κn+1 in the numerator of the scale matrix. Samples from the joint\nposterior distribution of (µ,Σ) are easily obtained using the following procedure: first,\ndraw Σ|y ∼ Inv-Wishartνn(Λ\n\n−1\nn ), then draw µ|Σ, y ∼ N(µn,Σ/κn). See Appendix A for\n\ndrawing from inverse-Wishart and multivariate normal distributions. To draw from the\nposterior predictive distribution of a new observation, draw ỹ|µ,Σ, y ∼ N(µ,Σ), given the\nalready drawn values of µ and Σ.\n\nDifferent noninformative prior distributions\n\nInverse-Wishart with d + 1 degrees of freedom. Setting Σ ∼ Inv-Wishartd+1(I) has the\nappealing feature that each of the correlations in Σ has, marginally, a uniform prior distri-\nbution. (The joint distribution is not uniform, however, because of the constraint that the\ncorrelation matrix be positive definite.)\n\nInverse-Wishart with d−1 degrees of freedom. Another proposed noninformative prior\ndistribution is the multivariate Jeffreys prior density,\n\np(µ,Σ) ∝ |Σ|−(d+1)/2,\n\nwhich is the limit of the conjugate prior density as κ0 → 0, ν0 → −1, |Λ0| → 0. The\ncorresponding posterior distribution can be written as\n\nΣ|y ∼ Inv-Wishartn−1(S\n−1)\n\nµ|Σ, y ∼ N(y,Σ/n).\n\nResults for the marginal distribution of µ and the posterior predictive distribution of ỹ,\nassuming that the posterior distribution is proper, follow from the previous paragraph. For\nexample, the marginal posterior distribution of µ is multivariate tn−d(y, S/(n(n− d))).\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n74 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nDose, xi Number of Number of\n(log g/ml) animals, ni deaths, yi\n\n−0.86 5 0\n−0.30 5 1\n−0.05 5 3\n0.73 5 5\n\nTable 3.1: Bioassay data from Racine et al. (1986).\n\nScaled inverse-Wishart model\n\nWhen modeling covariance matrices it can help to extend the inverse-Wishart model by\nmultiplying by a set of scale parameters that can be modeled separately. This gives flexibility\nin modeling and allows one to set up a uniform or weak prior distribution on correlations\nwithout overly constraining the variance parameters. The scaled inverse-Wishart model for\nΣ has the form,\n\nΣ = Diag(ξ)ΣηDiag(ξ),\n\nwhere Ση is given an inverse-Wishart prior distribution (one choice is Inv-Wishartd+1(I), so\nthat the marginal distributions of the correlations are uniform) and then the scale param-\neters ξ can be given weakly informative priors themselves. We discuss further in Section\n15.4 in the context of varying-intercept, varying-slope hierarchical regression models.\n\n3.7 Example: analysis of a bioassay experiment\n\nBeyond the normal distribution, few multiparameter sampling models allow simple explicit\ncalculation of posterior distributions. Data analysis for such models is possible using the\ncomputational methods described in Part III of this book. Here we present an example\nof a nonconjugate model for a bioassay experiment, drawn from the literature on applied\nBayesian statistics. The model is a two-parameter example from the broad class of general-\nized linear models to be considered more thoroughly in Chapter 16. We use a particularly\nsimple simulation approach, approximating the posterior distribution by a discrete distri-\nbution supported on a two-dimensional grid of points, that provides sufficiently accurate\ninferences for this two-parameter example.\n\nThe scientific problem and the data\n\nIn the development of drugs and other chemical compounds, acute toxicity tests or bioassay\nexperiments are commonly performed on animals. Such experiments proceed by adminis-\ntering various dose levels of the compound to batches of animals. The animals’ responses\nare typically characterized by a dichotomous outcome: for example, alive or dead, tumor\nor no tumor. An experiment of this kind gives rise to data of the form\n\n(xi, ni, yi); i = 1, . . . , k,\n\nwhere xi represents the ith of k dose levels (often measured on a logarithmic scale) given\nto ni animals, of which yi subsequently respond with positive outcome. An example of real\ndata from such an experiment is shown in Table 3.1: twenty animals were tested, five at\neach of four dose levels.\n\nModeling the dose–response relation\n\nGiven what we have seen so far, we must model the outcomes of the five animals within\neach group i as exchangeable, and it seems reasonable to model them as independent with\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 75\n\nequal probabilities, which implies that the data points yi are binomially distributed:\n\nyi|θi ∼ Bin(ni, θi),\n\nwhere θi is the probability of death for animals given dose xi. (An example of a situation\nin which independence and the binomial model would not be appropriate is if the deaths\nwere caused by a contagious disease.) For this experiment, it is also reasonable to treat the\noutcomes in the four groups as independent of each other, given the parameters θ1, . . . , θ4.\n\nThe simplest analysis would treat the four parameters θi as exchangeable in their prior\ndistribution, perhaps using a noninformative density such as p(θ1, . . . , θ4) ∝ 1, in which case\nthe parameters θi would have independent beta posterior distributions. The exchangeable\nprior model for the θi parameters has a serious flaw, however; we know the dose level xi\nfor each group i, and one would expect the probability of death to vary systematically as a\nfunction of dose.\n\nThe simplest model of the dose–response relation—that is, the relation of θi to xi—is\nlinear: θi = α + βxi. Unfortunately, this model has the flaw that at low or high doses,\nxi approaches ±∞ (recall that the dose is measured on the log scale), whereas θi, being a\nprobability, must be constrained to lie between 0 and 1. The standard solution is to use a\ntransformation of the θ’s, such as the logistic, in the dose–response relation:\n\nlogit(θi) = α+ βxi, (3.14)\n\nwhere logit(θi) = log(θi/(1 − θi)) as defined in (1.10). This is called a logistic regression\nmodel.\n\nThe likelihood\n\nUnder the model (3.14), we can write the sampling distribution, or likelihood, for each\ngroup i in terms of the parameters α and β as\n\np(yi|α, β, ni, xi) ∝ [logit−1(α+ βxi)]\nyi [1− logit−1(α+ βxi)]\n\nni−yi .\n\nThe model is characterized by the parameters α and β, whose joint posterior distribution\nis\n\np(α, β|y, n, x) ∝ p(α, β|n, x)p(y|α, β, n, x) (3.15)\n\n∝ p(α, β)\n\nk∏\n\ni=1\n\np(yi|α, β, ni, xi).\n\nWe consider the sample sizes ni and dose levels xi as fixed for this analysis and suppress\nthe conditioning on (n, x) in subsequent notation.\n\nThe prior distribution\n\nWe present an analysis based on a prior distribution for (α, β) that is independent and locally\nuniform in the two parameters; that is, p(α, β) ∝ 1. In practice, we might use a uniform\nprior distribution if we really have no prior knowledge about the parameters, or if we want to\npresent a simple analysis of this experiment alone. If the analysis using the noninformative\nprior distribution is insufficiently precise, we may consider using other sources of substantive\ninformation (for example, from other bioassay experiments) to construct an informative\nprior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n76 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nFigure 3.3 (a) Contour plot for the posterior density of the parameters in the bioassay example.\nContour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of 1000 draws\nfrom the posterior distribution.\n\nA rough estimate of the parameters\n\nWe will compute the joint posterior distribution (3.15) at a grid of points (α, β), but before\ndoing so, it is a good idea to get a rough estimate of (α, β) so we know where to look. To\nobtain the rough estimate, we use existing software to perform a logistic regression; that\nis, finding the maximum likelihood estimate of (α, β) in (3.15) for the four data points in\n\nTable 3.1. The estimate is (α̂, β̂) = (0.8, 7.7), with standard errors of 1.0 and 4.9 for α and\nβ, respectively.\n\nObtaining a contour plot of the joint posterior density\n\nWe are now ready to compute the posterior density at a grid of points (α, β). After some\nexperimentation, we use the range (α, β) ∈ [−5, 10]× [−10, 40], which captures almost all\nthe mass of the posterior distribution. The resulting contour plot appears in Figure 3.3a;\na general justification for setting the lowest contour level at 0.05 for two-dimensional plots\nappears in Section 4.1.\n\nSampling from the joint posterior distribution\n\nHaving computed the unnormalized posterior density at a grid of values that cover the\neffective range of (α, β), we can normalize by approximating the distribution as a step\nfunction over the grid and setting the total probability in the grid to 1. We sample 1000\nrandom draws (αs, βs) from the posterior distribution using the following procedure.\n\n1. Compute the marginal posterior distribution of α by numerically summing over β in the\ndiscrete distribution computed on the grid of Figure 3.3a.\n\n2. For s = 1, . . . , 1000:\n\n(a) Draw αs from the discretely computed p(α|y); this can be viewed as a discrete version\nof the inverse cdf method described in Section 1.9.\n\n(b) Draw βs from the discrete conditional distribution, p(β|α, y), given the just-sampled\nvalue of α.\n\n(c) For each of the sampled α and β, add a uniform random jitter centered at zero with\na width equal to the spacing of the sampling grid. This gives the simulation draws a\ncontinuous distribution.\n\nThe 1000 draws (αs, βs) are displayed on a scatterplot in Figure 3.3b. The scale of the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.7. EXAMPLE: ANALYSIS OF A BIOASSAY EXPERIMENT 77\n\nFigure 3.4 Histogram of the draws from the posterior distribution of the LD50 (on the scale of log\ndose in g/ml) in the bioassay example, conditional on the parameter β being positive.\n\nplot, which is the same as the scale of Figure 3.3a, has been set large enough that all the\n1000 draws would fit on the graph.\n\nThere are a number of practical considerations when applying this two-dimensional grid\napproximation. There can be difficulty finding the correct location and scale for the grid\npoints. A grid that is defined on too small an area may miss important features of the\nposterior distribution that fall outside the grid. A grid defined on a large area with wide\nintervals between points can miss important features that fall between the grid points. It\nis also important to avoid overflow and underflow operations when computing the poste-\nrior distribution. It is usually a good idea to compute the logarithm of the unnormalized\nposterior distribution and subtract off the maximum value before exponentiating. This\ncreates an unnormalized discrete approximation with maximum value 1, which can then be\nnormalized (by setting the total probability in the grid to 1).\n\nThe posterior distribution of the LD50\n\nA parameter of common interest in bioassay studies is the LD50—the dose level at which\nthe probability of death is 50%. In our logistic model, a 50% survival rate means\n\nLD50: E\n\n(\nyi\nni\n\n)\n= logit−1(α+ βxi) = 0.5;\n\nthus, α + βxi = logit(0.5) = 0, and the LD50 is xi = −α/β. Computing the posterior\ndistribution of any summaries in the Bayesian approach is straightforward, as discussed at\nthe end of Section 1.9. Given what we have done so far, simulating the posterior distribution\nof the LD50 is trivial: we just compute −α/β for the 1000 draws of (α, β) pictured in Figure\n3.3b.\n\nDifficulties with the LD50 parameterization if the drug is beneficial. In the context of this\nexample, LD50 is a meaningless concept if β ≤ 0, in which case increasing the dose does not\ncause the probability of death to increase. If we were certain that the drug could not cause\nthe tumor rate to decrease, we should constrain the parameter space to exclude values of β\nless than 0. However, it seems more reasonable here to allow the possibility of β ≤ 0 and\njust note that LD50 is hard to interpret in this case.\n\nWe summarize the inference on the LD50 scale by reporting two results: (1) the posterior\nprobability that β > 0—that is, that the drug is harmful—and (2) the posterior distribution\nfor the LD50 conditional on β > 0. All of the 1000 simulation draws had positive values of\nβ, so the posterior probability that β > 0 is roughly estimated to exceed 0.999. We compute\nthe LD50 for the simulation draws with positive values of β (which happen to be all 1000\ndraws for this example); a histogram is displayed in Figure 3.4. This example illustrates that\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n78 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe marginal posterior mean is not always a good summary of inference about a parameter.\nWe are not, in general, interested in the posterior mean of the LD50, because the posterior\nmean includes the cases in which the dose–response relation is negative.\n\n3.8 Summary of elementary modeling and computation\n\nThe lack of multiparameter models permitting easy calculation of posterior distributions is\nnot a major practical handicap for three main reasons. First, when there are few parame-\nters, posterior inference in nonconjugate multiparameter models can be obtained by simple\nsimulation methods, as we have seen in the bioassay example. Second, sophisticated models\ncan often be represented in a hierarchical or conditional manner, as we shall see in Chapter\n5, for which effective computational strategies are available (as we discuss in general in Part\nIII). Finally, as we discuss in Chapter 4, we can often apply a normal approximation to\nthe posterior distribution, and therefore the conjugate structure of the normal model can\nplay an important role in practice, well beyond its application to explicitly normal sampling\nmodels.\n\nOur successful analysis of the bioassay example suggests the following strategy for com-\nputation of simple Bayesian posterior distributions. What follows is not truly a general ap-\nproach, but it summarizes what we have done so far and foreshadows the general methods—\nbased on successive approximations—presented in Part III.\n\n1. Write the likelihood part of the model, p(y|θ), ignoring any factors that are free of θ.\n\n2. Write the posterior density, p(θ|y) ∝ p(θ)p(y|θ). If prior information is well-formulated,\ninclude it in p(θ). Otherwise use a weakly informative prior distribution or temporarily\nset p(θ) ∝ constant, with the understanding that the prior density can be altered later\nto include additional information or structure.\n\n3. Create a crude estimate of the parameters, θ, for use as a starting point and a comparison\nto the computation in the next step.\n\n4. Draw simulations θ1, . . . , θS , from the posterior distribution. Use the sample draws to\ncompute the posterior density of any functions of θ that may be of interest.\n\n5. If any predictive quantities, ỹ, are of interest, simulate ỹ1, . . . , ỹS by drawing each ỹs\n\nfrom the sampling distribution conditional on the drawn value θs, p(ỹ|θs). In Chapter\n6, we discuss how to use posterior simulations of θ and ỹ to check the fit of the model to\ndata and substantive knowledge.\n\nFor nonconjugate models, step 4 above can be difficult. Various methods have been\ndeveloped to draw posterior simulations in complicated models, as we discuss in Part III.\nOccasionally, high-dimensional problems can be solved by combining analytical and nu-\nmerical simulation methods. If θ has only one or two components, it is possible to draw\nsimulations by computing on a grid, as we illustrated in the previous section for the bioassay\nexample.\n\n3.9 Bibliographic note\n\nChapter 2 of Box and Tiao (1973) thoroughly treats the univariate and multivariate normal\ndistribution problems and also some related problems such as estimating the difference\nbetween two means and the ratio between two variances. At the time that book was\nwritten, computer simulation methods were much less convenient than they are now, and\nso Box and Tiao, and other Bayesian authors of the period, restricted their attention to\nconjugate families and devoted much effort to deriving analytic forms of marginal posterior\ndensities.\n\nMany textbooks on multivariate analysis discuss the unique mathematical features of\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 79\n\nSurvey Bush Dukakis No opinion/other Total\n\npre-debate 294 307 38 639\npost-debate 288 332 19 639\n\nTable 3.2 Number of respondents in each preference category from ABC News pre- and post-debate\nsurveys in 1988.\n\nthe multivariate normal distribution, such as the property that all marginal and conditional\ndistributions of components of a multivariate normal vector are normal; for example, see\nMardia, Kent, and Bibby (1979).\n\nSimon Newcomb’s data, along with a discussion of his experiment, appear in Stigler\n(1977).\n\nThe multinomial model and corresponding informative and noninformative prior distri-\nbutions are discussed by Good (1965) and Fienberg (1977); also see the bibliographic note\non loglinear models at the end of Chapter 16.\n\nThe data and model for the bioassay example appear in Racine et al. (1986), an article\nthat presents several examples of simple Bayesian analyses that have been useful in the\npharmaceutical industry.\n\n3.10 Exercises\n\n1. Binomial and multinomial models: suppose data (y1, . . . , yJ) follow a multinomial distri-\nbution with parameters (θ1, . . . , θJ). Also suppose that θ = (θ1, . . . , θJ ) has a Dirichlet\nprior distribution. Let α = θ1\n\nθ1+θ2\n.\n\n(a) Write the marginal posterior distribution for α.\n\n(b) Show that this distribution is identical to the posterior distribution for α obtained by\ntreating y1 as an observation from the binomial distribution with probability α and\nsample size y1 + y2, ignoring the data y3, . . . , yJ .\n\nThis result justifies the application of the binomial distribution to multinomial problems\nwhen we are only interested in two of the categories; for example, see the next problem.\n\n2. Comparison of two multinomial observations: on September 25, 1988, the evening of a\npresidential campaign debate, ABC News conducted a survey of registered voters in the\nUnited States; 639 persons were polled before the debate, and 639 different persons were\npolled after. The results are displayed in Table 3.2. Assume the surveys are independent\nsimple random samples from the population of registered voters. Model the data with\ntwo different multinomial distributions. For j = 1, 2, let αj be the proportion of voters\nwho preferred Bush, out of those who had a preference for either Bush or Dukakis at\nthe time of survey j. Plot a histogram of the posterior density for α2 − α1. What is the\nposterior probability that there was a shift toward Bush?\n\n3. Estimation from two independent experiments: an experiment was performed on the\neffects of magnetic fields on the flow of calcium out of chicken brains. Two groups\nof chickens were involved: a control group of 32 chickens and an exposed group of 36\nchickens. One measurement was taken on each chicken, and the purpose of the experiment\nwas to measure the average flow µc in untreated (control) chickens and the average flow\nµt in treated chickens. The 32 measurements on the control group had a sample mean of\n1.013 and a sample standard deviation of 0.24. The 36 measurements on the treatment\ngroup had a sample mean of 1.173 and a sample standard deviation of 0.20.\n\n(a) Assuming the control measurements were taken at random from a normal distribution\nwith mean µc and variance σ2\n\nc , what is the posterior distribution of µc? Similarly, use\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n80 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\nthe treatment group measurements to determine the marginal posterior distribution\nof µt. Assume a uniform prior distribution on (µc, µt, log σc, log σt).\n\n(b) What is the posterior distribution for the difference, µt − µc? To get this, you may\nsample from the independent t distributions you obtained in part (a) above. Plot a\nhistogram of your samples and give an approximate 95% posterior interval for µt−µc.\n\nThe problem of estimating two normal means with unknown ratio of variances is called\nthe Behrens–Fisher problem.\n\n4. Inference for a 2× 2 table: an experiment was performed to estimate the effect of beta-\nblockers on mortality of cardiac patients. A group of patients were randomly assigned\nto treatment and control groups: out of 674 patients receiving the control, 39 died, and\nout of 680 receiving the treatment, 22 died. Assume that the outcomes are independent\nand binomially distributed, with probabilities of death of p0 and p1 under the control\nand treatment, respectively. We return to this example in Section 5.6.\n\n(a) Set up a noninformative prior distribution on (p0, p1) and obtain posterior simulations.\n\n(b) Summarize the posterior distribution for the odds ratio, (p1/(1− p1))/(p0/(1− p0)).\n(c) Discuss the sensitivity of your inference to your choice of noninformative prior density.\n\n5. Rounded data: it is a common problem for measurements to be observed in rounded\nform (for a review, see Heitjan, 1989). For a simple example, suppose we weigh an\nobject five times and measure weights, rounded to the nearest pound, of 10, 10, 12, 11,\n9. Assume the unrounded measurements are normally distributed with a noninformative\nprior distribution on the mean µ and variance σ2.\n\n(a) Give the posterior distribution for (µ, σ2) obtained by pretending that the observations\nare exact unrounded measurements.\n\n(b) Give the correct posterior distribution for (µ, σ2) treating the measurements as rounded.\n\n(c) How do the incorrect and correct posterior distributions differ? Compare means,\nvariances, and contour plots.\n\n(d) Let z = (z1, . . . , z5) be the original, unrounded measurements corresponding to the five\nobservations above. Draw simulations from the posterior distribution of z. Compute\nthe posterior mean of (z1 − z2)2.\n\n6. Binomial with unknown probability and sample size: some of the difficulties with setting\nprior distributions in multiparameter models can be illustrated with the simple binomial\ndistribution. Consider data y1, . . . , yn modeled as independent Bin(N, θ), with both N\nand θ unknown. Defining a convenient family of prior distributions on (N, θ) is difficult,\npartly because of the discreteness of N .\nRaftery (1988) considers a hierarchical approach based on assigning the parameter N\na Poisson distribution with unknown mean µ. To define a prior distribution on (θ,N),\nRaftery defines λ = µθ and specifies a prior distribution on (λ, θ). The prior distribution\nis specified in terms of λ rather than µ because ‘it would seem easier to formulate prior\ninformation about λ, the unconditional expectation of the observations, than about µ,\nthe mean of the unobserved quantity N .’\n\n(a) A suggested noninformative prior distribution is p(λ, θ) ∝ λ−1. What is a motivation\nfor this noninformative distribution? Is the distribution improper? Transform to\ndetermine p(N, θ).\n\n(b) The Bayesian method is illustrated on counts of waterbuck obtained by remote pho-\ntography on five separate days in Kruger Park in South Africa. The counts were\n53, 57, 66, 67, and 72. Perform the Bayesian analysis on these data and display a\nscatterplot of posterior simulations of (N, θ). What is the posterior probability that\nN > 100?\n\n(c) Why not simply use a Poisson with fixed µ as a prior distribution for N?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n3.10. EXERCISES 81\n\nType of Bike Counts of bicycles/other vehicles\nstreet route?\n\nResidential yes 16/58, 9/90, 10/48, 13/57, 19/103,\n20/57, 18/86, 17/112, 35/273, 55/64\n\nResidential no 12/113, 1/18, 2/14, 4/44, 9/208,\n7/67, 9/29, 8/154\n\nFairly busy yes 8/29, 35/415, 31/425, 19/42, 38/180,\n47/675, 44/620, 44/437, 29/47, 18/462\n\nFairly busy no 10/557, 43/1258, 5/499, 14/601, 58/1163,\n15/700, 0/90, 47/1093, 51/1459, 32/1086\n\nBusy yes 60/1545, 51/1499, 58/1598, 59/503, 53/407,\n68/1494, 68/1558, 60/1706, 71/476, 63/752\n\nBusy no 8/1248, 9/1246, 6/1596, 9/1765, 19/1290,\n61/2498, 31/2346, 75/3101, 14/1918, 25/2318\n\nTable 3.3 Counts of bicycles and other vehicles in one hour in each of 10 city blocks in each of\nsix categories. (The data for two of the residential blocks were lost.) For example, the first block\nhad 16 bicycles and 58 other vehicles, the second had 9 bicycles and 90 other vehicles, and so on.\nStreets were classified as ‘residential,’ ‘fairly busy,’ or ‘busy’ before the data were gathered.\n\n7. Poisson and binomial distributions: a student sits on a street corner for an hour and\nrecords the number of bicycles b and the number of other vehicles v that go by. Two\nmodels are considered:\n\n• The outcomes b and v have independent Poisson distributions, with unknown means\nθb and θv.\n\n• The outcome b has a binomial distribution, with unknown probability p and sample\nsize b+ v.\n\nShow that the two models have the same likelihood if we define p = θb\nθb+θv\n\n.\n\n8. Analysis of proportions: a survey was done of bicycle and other vehicular traffic in the\nneighborhood of the campus of the University of California, Berkeley, in the spring of\n1993. Sixty city blocks were selected at random; each block was observed for one hour,\nand the numbers of bicycles and other vehicles traveling along that block were recorded.\nThe sampling was stratified into six types of city blocks: busy, fairly busy, and residential\nstreets, with and without bike routes, with ten blocks measured in each stratum. Table\n3.3 displays the number of bicycles and other vehicles recorded in the study. For this\nproblem, restrict your attention to the first four rows of the table: the data on residential\nstreets.\n\n(a) Let y1, . . . , y10 and z1, . . . , z8 be the observed proportion of traffic that was on bicycles\nin the residential streets with bike lanes and with no bike lanes, respectively (so\ny1 = 16/(16 + 58) and z1 = 12/(12 + 113), for example). Set up a model so that the\nyi’s are independent and identically distributed given parameters θy and the zi’s are\nindependent and identically distributed given parameters θz.\n\n(b) Set up a prior distribution that is independent in θy and θz.\n\n(c) Determine the posterior distribution for the parameters in your model and draw 1000\nsimulations from the posterior distribution. (Hint: θy and θz are independent in the\nposterior distribution, so they can be simulated independently.)\n\n(d) Let µy = E(yi|θy) be the mean of the distribution of the yi’s; µy will be a function of\nθy. Similarly, define µz. Using your posterior simulations from (c), plot a histogram of\nthe posterior simulations of µy − µz, the expected difference in proportions in bicycle\ntraffic on residential streets with and without bike lanes.\n\nWe return to this example in Exercise 5.13.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n82 3. INTRODUCTION TO MULTIPARAMETER MODELS\n\n9. Conjugate normal model: suppose y is an independent and identically distributed sam-\nple of size n from the distribution N(µ, σ2), where the prior distribution for (µ, σ2) is\nN-Inv-χ2(µ, σ2|µ0, σ\n\n2\n0/κ0; ν0, σ\n\n2\n0); that is, σ2 ∼ Inv-χ2(ν0, σ\n\n2\n0) and µ|σ2 ∼ N(µ0, σ\n\n2/κ0).\nThe posterior distribution, p(µ, σ2|y), is also normal-inverse-χ2; derive explicitly its pa-\nrameters in terms of the prior parameters and the sufficient statistics of the data.\n\n10. Comparison of normal variances: for j = 1, 2, suppose that\n\nyj1, . . . , yjnj |µj , σ2\nj ∼ iid N(µj , σ\n\n2\nj ),\n\np(µj , σ\n2\nj ) ∝ σ−2\n\nj ,\n\nand (µ1, σ\n2\n1) are independent of (µ2, σ\n\n2\n2) in the prior distribution. Show that the posterior\n\ndistribution of (s21/s\n2\n2)/(σ\n\n2\n1/σ\n\n2\n2) is F with (n1−1) and (n2−1) degrees of freedom. (Hint:\n\nto show the required form of the posterior density, you do not need to carry along all the\nnormalizing constants.)\n\n11. Computation: in the bioassay example, replace the uniform prior density by a joint nor-\nmal prior distribution on (α, β), with α ∼ N(0, 22), β ∼ N(10, 102), and corr(α, β)=0.5.\n\n(a) Repeat all the computations and plots of Section 3.7 with this new prior distribution.\n\n(b) Check that your contour plot and scatterplot look like a compromise between the prior\ndistribution and the likelihood (as displayed in Figure 3.3).\n\n(c) Discuss the effect of this hypothetical prior information on the conclusions in the\napplied context.\n\n12. Poisson regression model: expand the model of Exercise 2.13(a) by assuming that the\nnumber of fatal accidents in year t follows a Poisson distribution with mean α+ βt. You\nwill estimate α and β, following the example of the analysis in Section 3.7.\n\n(a) Discuss various choices for a ‘noninformative’ prior for (α, β). Choose one.\n\n(b) Discuss what would be a realistic informative prior distribution for (α, β). Sketch its\ncontours and then put it aside. Do parts (c)–(h) of this problem using your noninfor-\nmative prior distribution from (a).\n\n(c) Write the posterior density for (α, β). What are the sufficient statistics?\n\n(d) Check that the posterior density is proper.\n\n(e) Calculate crude estimates and uncertainties for (α, β) using linear regression.\n\n(f) Plot the contours and take 1000 draws from the joint posterior density of (α, β).\n\n(g) Using your samples of (α, β), plot a histogram of the posterior density for the expected\nnumber of fatal accidents in 1986, α+ 1986β.\n\n(h) Create simulation draws and obtain a 95% predictive interval for the number of fatal\naccidents in 1986.\n\n(i) How does your hypothetical informative prior distribution in (b) differ from the pos-\nterior distribution in (f) and (g), obtained from the noninformative prior distribution\nand the data? If they disagree, discuss.\n\n13. Multivariate normal model: derive equations (3.12) by completing the square in vector-\nmatrix notation.\n\n14. Improper prior and proper posterior distributions: prove that the posterior density (3.15)\nfor the bioassay example has a finite integral over the range (α, β) ∈ (−∞,∞)×(−∞,∞).\n\n15. Joint distributions: The autoregressive time-series model y1, y2, . . . with mean level 0,\nautocorrelation 0.8, residual standard deviation 1, and normal errors can be written as\n(yt|yt−1, yt−2, . . .) ∼ N(0.8yt−1, 1) for all t.\n\n(a) Prove that the distribution of yt, given the observations at all other integer time points\nt, depends only on yt−1 and yt+1.\n\n(b) What is the distribution of yt given yt−1 and yt+1?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 4\n\nAsymptotics and connections to non-Bayesian\n\napproaches\n\nWe have seen that many simple Bayesian analyses based on noninformative prior distribu-\ntions give similar results to standard non-Bayesian approaches (for example, the posterior t\ninterval for the normal mean with unknown variance). The extent to which a noninforma-\ntive prior distribution can be justified as an objective assumption depends on the amount\nof information available in the data: in the simple cases discussed in Chapters 2 and 3,\nit was clear that as the sample size n increases, the influence of the prior distribution on\nposterior inferences decreases. These ideas, sometimes referred to as asymptotic theory,\nbecause they refer to properties that hold in the limit as n becomes large, will be reviewed\nin the present chapter, along with some more explicit discussion of the connections between\nBayesian and non-Bayesian methods. The large-sample results are not actually necessary\nfor performing Bayesian data analysis but are often useful as approximations and as tools\nfor understanding.\n\nWe begin this chapter with a discussion of the various uses of the normal approximation\nto the posterior distribution. Theorems about consistency and normality of the posterior\ndistribution in large samples are outlined in Section 4.2, followed by several counterexamples\nin Section 4.3; proofs of the theorems are sketched in Appendix B. Finally, we discuss how\nthe methods of frequentist statistics can be used to evaluate the properties of Bayesian\ninferences.\n\n4.1 Normal approximations to the posterior distribution\n\nNormal approximation to the joint posterior distribution\n\nIf the posterior distribution p(θ|y) is unimodal and roughly symmetric, it can be convenient\nto approximate it by a normal distribution; that is, the logarithm of the posterior density\nis approximated by a quadratic function of θ.\n\nHere we consider a quadratic approximation to the log-posterior density that is centered\nat the posterior mode (which in general is easy to compute using off-the-shelf optimization\nroutines); in Chapter 13 we discuss more elaborate approximations which can be effective\nin settings where simple mode-based approximations fail.\n\nA Taylor series expansion of log p(θ|y) centered at the posterior mode, θ̂ (where θ can\n\nbe a vector and θ̂ is assumed to be in the interior of the parameter space), gives\n\nlog p(θ|y) = log p(θ̂|y) + 1\n\n2\n(θ − θ̂)T\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n(θ − θ̂) + · · · , (4.1)\n\nwhere the linear term in the expansion is zero because the log-posterior density has zero\nderivative at its mode. As we discuss in Section 4.2, the remainder terms of higher order fade\nin importance relative to the quadratic term when θ is close to θ̂ and n is large. Considering\n\n83\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n84 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of θ, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\np(θ|y) ≈ N(θ̂, [I(θ̂)]−1), (4.2)\n\nwhere I(θ) is the observed information,\n\nI(θ) = − d2\n\ndθ2\nlog p(θ|y).\n\nIf the mode, θ̂, is in the interior of parameter space, then the matrix I(θ̂) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1, . . . , yn be independent observations from a N(µ, σ2) distribution, and,\nfor simplicity, we assume a uniform prior density for (µ, log σ). We set up a normal\napproximation to the posterior distribution of (µ, log σ), which has the virtue of re-\nstricting σ to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\nlog p(µ, log σ|y) = constant− n log σ − 1\n\n2σ2\n((n− 1)s2 + n(y − µ)2).\n\nThe first derivatives are\n\nd\n\ndµ\nlog p(µ, log σ|y) =\n\nn(y − µ)\nσ2\n\n,\n\nd\n\nd(log σ)\nlog p(µ, log σ|y) = −n+\n\n(n− 1)s2 + n(y − µ)2\nσ2\n\n,\n\nfrom which the posterior mode is readily obtained as\n\n(µ̂, log σ̂) =\n\n(\ny, log\n\n(√\nn− 1\n\nn\ns\n\n))\n.\n\nThe second derivatives of the log posterior density are\n\nd2\n\ndµ2\nlog p(µ, log σ|y) = − n\n\nσ2\n\nd2\n\ndµd(log σ)\nlog p(µ, log σ|y) = −2ny − µ\n\nσ2\n\nd2\n\nd(log σ)2\nlog p(µ, log σ|y) = − 2\n\nσ2\n((n− 1)s2 + n(y − µ)2).\n\nThe matrix of second derivatives at the mode is then\n\n(\n−n/σ̂2 0\n\n0 −2n\n\n)\n. From (4.2),\n\nthe posterior distribution can be approximated as\n\np(µ, log σ|y) ≈ N\n\n((\nµ\n\nlog σ\n\n) ∣∣∣∣\n(\n\ny\nlog σ̂\n\n)\n,\n\n(\nσ̂2/n 0\n0 1/(2n)\n\n))\n.\n\nIf we had instead constructed the normal approximation in terms of p(µ, σ2), the sec-\nond derivative matrix would be multiplied by the Jacobian of the transformation from\nlog σ to σ2 and the mode would change slightly, to σ̃2 = n\n\nn+2 σ̂\n2. The two components,\n\n(µ, σ2), would still be independent in their approximate posterior distribution, and\np(σ2|y) ≈ N(σ2|σ̃2, 2σ̃4/(n+ 2)).\n\n84\n\nThis electronic edition is for non-commercial purposes only.\n\n4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n(4.1) as a function of 0, the first term is a constant, whereas the second term is proportional\nto the logarithm of a normal density, yielding the approximation,\n\nply) = N(8, [L(8)|~*), (4.2)\n\nwhere I(0) is the observed information,\n\n2\n\nd\nI(0) =— 762 log p(4|y).\n\nIf the mode, 6, is in the interior of parameter space, then the matrix I (6) is positive definite.\n\nExample. Normal distribution with unknown mean and variance\n\nWe illustrate the approximate normal distribution with a simple theoretical exam-\nple. Let y1,-.-,Ym be independent observations from a N(,07) distribution, and,\nfor simplicity, we assume a uniform prior density for (u,loga). We set up a normal\napproximation to the posterior distribution of (~,loga), which has the virtue of re-\nstricting o to positive values. To construct the approximation, we need the second\nderivatives of the log posterior density,\n\n—((n— 1s? +n(9 — )?).\n\nlog p(y, log oly) = constant — nlog a — 5\no\n\nThe first derivatives are\n\nd ny = LL\nda log p(u,logaly) = mow)\nd (n —1)s* + n(¥ — pw)?\nl | —\nTogo) og p(t, log oly) n+ 53 ;\n\nfrom which the posterior mode is readily obtained as\n\nvue (ne (=).\n\nThe second derivatives of the log posterior density are\n\n2\n\nd n\nae srt logely) = —Ta\na? y-u\n—— | I = -2\nTud(logo) og p(t, log aly) 13\nd? 2 ne:\nTloga)2 SPH log aly) = ~aa((n— 1s + n(y — 1)”).\n. ar . —n/6* 0\nThe matrix of second derivatives at the mode is then 6 on | From (4.2),\n\nthe posterior distribution can be approximated as\n\nP(L, log oly) ~n(( logo ) ( lone ).( -_ 1/(2n) ))\n\nIf we had instead constructed the normal approximation in terms of p(j1,07), the sec-\n\nond derivative matrix would be multiplied by the Jacobian of the transformation from\n\nlog o to o? and the mode would change slightly, to ¢? = age. The two components,\n\n(u,07), would still be independent in their approximate posterior distribution, and\np(o?|y) © N(o?|a?, 264/(n + 2)).\n\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.1. NORMAL APPROXIMATIONS TO THE POSTERIOR DISTRIBUTION 85\n\nInterpretation of the posterior density function relative to its maximum\n\nIn addition to its direct use as an approximation, the multivariate normal distribution pro-\nvides a benchmark for interpreting the posterior density function and contour plots. In the d-\ndimensional normal distribution, the logarithm of the density function is a constant plus a χ2\n\nd\n\ndistribution divided by −2. For example, the 95th percentile of the χ2\n10 density is 18.31, so if\n\na problem has d = 10 parameters, then approximately 95% of the posterior probability mass\nis associated with the values of θ for which p(θ|y) is no less than exp(−18.31/2) = 1.1×10−4\n\ntimes the density at the mode. Similarly, with d = 2 parameters, approximately 95% of the\nposterior mass corresponds to densities above exp(−5.99/2) = 0.05, relative to the density\nat the mode. In a two-dimensional contour plot of a posterior density (for example, Figure\n3.3a), the 0.05 contour line thus includes approximately 95% of the probability mass.\n\nSummarizing posterior distributions by point estimates and standard errors\n\nThe asymptotic theory outlined in Section 4.2 shows that if n is large enough, a posterior\ndistribution can be approximated by a normal distribution. In many areas of application, a\nstandard inferential summary is the 95% interval obtained by computing a point estimate,\nθ̂, such as the maximum likelihood estimate (which is the posterior mode under a uniform\nprior density), plus or minus two standard errors, with the standard error estimated from\n\nthe information at the estimate, I(θ̂). A different asymptotic argument justifies the non-\nBayesian, frequentist interpretation of this summary, but in many simple situations both\ninterpretations hold. It is difficult to give general guidelines on when the normal approxi-\nmation is likely to be adequate in practice. From the Bayesian point of view, the accuracy\nin any given example can be directly determined by inspecting the posterior distribution.\n\nIn many cases, convergence to normality of the posterior distribution for a parameter\nθ can be dramatically improved by transformation. If φ is a continuous transformation\nof θ, then both p(φ|y) and p(θ|y) approach normal distributions, but the closeness of the\napproximation for finite n can vary substantially with the transformation chosen.\n\nData reduction and summary statistics\n\nUnder the normal approximation, the posterior distribution is summarized by its mode, θ̂,\nand the curvature of the posterior density, I(θ̂); that is, asymptotically, these are sufficient\nstatistics. In the examples at the end of the next chapter, we shall see that it can be\nconvenient to summarize ‘local-level’ or ‘individual-level’ data from a number of sources by\ntheir normal-theory sufficient statistics. This approach using summary statistics allows the\nrelatively easy application of hierarchical modeling techniques to improve each individual\nestimate. For example, in Section 5.5, each of a set of eight experiments is summarized by\na point estimate and a standard error estimated from an earlier linear regression analysis.\nUsing summary statistics is clearly most reasonable when posterior distributions are close\nto normal; the approach can otherwise discard important information and lead to erroneous\ninferences.\n\nLower-dimensional normal approximations\n\nFor a finite sample size n, the normal approximation is typically more accurate for condi-\ntional and marginal distributions of components of θ than for the full joint distribution. For\nexample, if a joint distribution is multivariate normal, all its margins are normal, but the\nconverse is not true. Determining the marginal distribution of a component of θ is equiva-\nlent to averaging over all the other components of θ, and averaging a family of distributions\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n86 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nFigure 4.1 (a) Contour plot of the normal approximation to the posterior distribution of the pa-\nrameters in the bioassay example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the\nmode. Compare to Figure 3.3a. (b) Scatterplot of 1000 draws from the normal approximation to\nthe posterior distribution. Compare to Figure 3.3b.\n\ngenerally brings them closer to normality, by the same logic that underlies the central limit\ntheorem.\n\nThe normal approximation for the posterior distribution of a low-dimensional θ is often\nperfectly acceptable, especially after appropriate transformation. If θ is high-dimensional,\ntwo situations commonly arise. First, the marginal distributions of many individual com-\nponents of θ can be approximately normal; inference about any one of these parameters,\ntaken individually, can then be well summarized by a point estimate and a standard error.\nSecond, it is possible that θ can be partitioned into two subvectors, θ = (θ1, θ2), for which\np(θ2|y) is not necessarily close to normal, but p(θ1|θ2, y) is, perhaps with mean and variance\nthat are functions of θ2. The approach of approximation using conditional distributions is\noften useful, and we consider it more systematically in Section 13.5. Lower-dimensional\napproximations are increasingly popular, for example in computation for latent Gaussian\nmodels.\n\nFinally, approximations based on the normal distribution are often useful for debugging\na computer program or checking a more elaborate method for approximating the posterior\ndistribution.\n\nExample. Bioassay experiment (continued)\nWe illustrate the normal approximation for the model and data from the bioassay\nexperiment of Section 3.7. The sample size in this experiment is relatively small, only\ntwenty animals in all, and we find that the normal approximation is close to the exact\nposterior distribution but with important differences.\n\nThe normal approximation to the joint posterior distribution of (α, β). To begin, we\ncompute the mode of the posterior distribution (using a logistic regression program)\nand the normal approximation (4.2) evaluated at the mode. The posterior mode of\n(α, β) is the same as the maximum likelihood estimate because we have assumed a\nuniform prior density for (α, β). Figure 4.1 shows a contour plot of the bivariate normal\napproximation and a scatterplot of 1000 draws from this approximate distribution.\nThe plots resemble the plots of the actual posterior distribution in Figure 3.3 but\nwithout the skewness in the upper right corner of the earlier plots. The effect of\nthe skewness is apparent when comparing the mean of the normal approximation,\n(α, β) = (0.8, 7.7), to the mean of the actual posterior distribution, (α, β) = (1.4, 11.9),\ncomputed from the simulations displayed in Figure 3.3b.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.2. LARGE-SAMPLE THEORY 87\n\nFigure 4.2 (a) Histogram of the simulations of LD50, conditional on β > 0, in the bioassay example\nbased on the normal approximation p(α, β|y). The wide tails of the histogram correspond to values\nof β close to 0. Omitted from this histogram are five simulation draws with values of LD50 less\nthan −2 and four draws with values greater than 2; the extreme tails are truncated to make the\nhistogram visible. The values of LD50 for the 950 simulation draws corresponding to β > 0 had a\nrange of [−12.4, 5.4]. Compare to Figure 3.4. (b) Histogram of the central 95% of the distribution.\n\nThe posterior distribution for the LD50 using the normal approximation on (α, β).\nFlaws of the normal approximation. The same set of 1000 draws from the normal\napproximation can be used to estimate the probability that β is positive and the\nposterior distribution of the LD50, conditional on β being positive. Out of the 1000\nsimulation draws, 950 had positive values of β, yielding the estimate Pr(β > 0) = 0.95,\na different result than from the exact distribution, where Pr(β > 0) > 0.999. Con-\ntinuing with the analysis based on the normal approximation, we compute the LD50\nas −α/β for each of the 950 draws with β > 0; Figure 4.2a presents a histogram of\nthe LD50 values, excluding some extreme values in both tails. (If the entire range of\nthe simulations were included, the shape of the distribution would be nearly impos-\nsible to see.) To get a better picture of the center of the distribution, we display in\nFigure 4.2b a histogram of the middle 95% of the 950 simulation draws of the LD50.\nThe histograms are centered in approximately the same place as Figure 3.4 but with\nsubstantially more variation, due to the possibility that β is close to zero.\nIn summary, posterior inferences based on the normal approximation here are roughly\nsimilar to the exact results, but because of the small sample, the actual joint posterior\ndistribution is substantially more skewed than the large-sample approximation, and\nthe posterior distribution of the LD50 actually has much shorter tails than implied by\nusing the joint normal approximation. Whether or not these differences imply that\nthe normal approximation is inadequate for practical use in this example depends on\nthe ultimate aim of the analysis.\n\n4.2 Large-sample theory\n\nTo understand why the normal approximation is often reasonable, we review some theory\nof how the posterior distribution behaves as the amount of data, from some fixed sampling\ndistribution, increases.\n\nNotation and mathematical setup\n\nThe basic tool of large sample Bayesian inference is asymptotic normality of the posterior\ndistribution: as more and more data arrive from the same underlying process, the posterior\ndistribution of the parameter vector approaches multivariate normality, even if the true\ndistribution of the data is not within the parametric family under consideration. Mathe-\nmatically, the results apply most directly to observations y1, . . . , yn that are independent\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n88 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\noutcomes sampled from a common distribution, f(y). In many situations, the notion of a\n‘true’ underlying distribution, f(y), for the data is difficult to interpret, but it is necessary\nin order to develop the asymptotic theory. Suppose the data are modeled by a parametric\nfamily, p(y|θ), with a prior distribution p(θ). In general, the data points yi and the parame-\nter θ can be vectors. If the true data distribution is included in the parametric family—that\nis, if f(y) = p(y|θ0) for some θ0—then, in addition to asymptotic normality, the property\nof consistency holds: the posterior distribution converges to a point mass at the true pa-\nrameter value, θ0, as n→∞. When the true distribution is not included in the parametric\nfamily, there is no longer a true value θ0, but its role in the theoretical result is replaced by\na value θ0 that makes the model distribution, p(y|θ), closest to the true distribution, f(y),\nin a technical sense involving Kullback-Leibler divergence, as is explained in Appendix B.\n\nIn discussing the large-sample properties of posterior distributions, the concept of Fisher\ninformation, J(θ), introduced as (2.20) in Section 2.8 in the context of Jeffreys’ prior dis-\ntributions, plays an important role.\n\nAsymptotic normality and consistency\n\nThe fundamental mathematical result given in Appendix B shows that, under some regu-\nlarity conditions (notably that the likelihood is a continuous function of θ and that θ0 is\nnot on the boundary of the parameter space), as n → ∞, the posterior distribution of θ\napproaches normality with mean θ0 and variance (nJ(θ0))\n\n−1. At its simplest level, this\nresult can be understood in terms of the Taylor series expansion (4.1) of the log posterior\ndensity centered about the posterior mode. A preliminary result shows that the posterior\nmode is consistent for θ0, so that as n → ∞, the mass of the posterior distribution p(θ|y)\nbecomes concentrated in smaller and smaller neighborhoods of θ0, and the distance |θ̂− θ0|\napproaches zero.\n\nFurthermore, we can rewrite the coefficient of the quadratic term in (4.1):\n\n[\nd2\n\ndθ2\nlog p(θ|y)\n\n]\n\nθ=θ̂\n\n=\n\n[\nd2\n\ndθ2\nlog p(θ)\n\n]\n\nθ=θ̂\n\n+\n\nn∑\n\ni=1\n\n[\nd2\n\ndθ2\nlog p(yi|θ)\n\n]\n\nθ=θ̂\n\n.\n\nThis coefficient is a single term for the prior plus the sum of n likelihood terms, each of\nwhose expected value under the true sampling distribution of yi, p(y|θ0), is approximately\n\n−J(θ0), as long as θ̂ is close to θ0 (we are assuming now that f(y) = p(y|θ0) for some θ0).\nTherefore, for large n, the curvature of the log posterior density can be approximated by\nthe Fisher information, evaluated at either θ̂ or θ0 (where only the former is available in\npractice).\n\nIn summary, in the limit of large n, in the context of a specified family of models,\nthe posterior mode, θ̂, approaches θ0, and the curvature (the observed information or the\n\nnegative of the coefficient of the second term in the Taylor expansion) approaches nJ(θ̂) or\nnJ(θ0). In addition, as n → ∞, the likelihood dominates the prior distribution, so we can\njust use the likelihood alone to obtain the mode and curvature for the normal approximation.\nMore precise statements of the theorems and outlines of proofs appear in Appendix B.\n\nLikelihood dominating the prior distribution\n\nThe asymptotic results formalize the notion that the importance of the prior distribution\ndiminishes as the sample size increases. One consequence of this result is that in problems\nwith large sample sizes we need not work especially hard to formulate a prior distribution\nthat accurately reflects all available information. When sample sizes are small, the prior\ndistribution is a critical part of the model specification.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.3. COUNTEREXAMPLES TO THE THEOREMS 89\n\n4.3 Counterexamples to the theorems\n\nA good way to understand the limitations of the large-sample results is to consider cases in\nwhich the theorems fail. The normal distribution is usually helpful as a starting approxi-\nmation, but one must examine deviations, especially with unusual parameter spaces and in\nthe extremes of the distribution. The counterexamples to the asymptotic theorems gener-\nally correspond to situations in which the prior distribution has an impact on the posterior\ninference, even in the limit of infinite sample sizes.\n\nUnderidentified models and nonidentified parameters. The model is underidentified given\ndata y if the likelihood, p(y|θ), is equal for a range of values of θ. This may also be called\na flat likelihood (although that term is sometimes also used for likelihoods for parameters\nthat are only weakly identified by the data—so the likelihood function is not strictly equal\nfor a range of values, only almost so). Under such a model, there is no single point θ0 to\nwhich the posterior distribution can converge.\n\nFor example, consider the model,\n(\nu\nv\n\n)\n∼ N\n\n((\n0\n0\n\n)\n,\n\n(\n1 ρ\nρ 1\n\n))\n,\n\nin which only one of u or v is observed from each pair (u, v). Here, the parameter ρ is\nnonidentified. The data supply no information about ρ, so the posterior distribution of ρ is\nthe same as its prior distribution, no matter how large the dataset is.\n\nThe only solution to a problem of nonidentified or underidentified parameters is to\nrecognize that the problem exists and, if there is a desire to estimate these parameters\nmore precisely, gather further information that can enable the parameters to be estimated\n(either from future data collection or from external information that can inform a prior\ndistribution).\n\nNumber of parameters increasing with sample size. In complicated problems, there can\nbe large numbers of parameters, and then we need to distinguish between different types\nof asymptotics. If, as n increases, the model changes so that the number of parameters\nincreases as well, then the simple results outlined in Sections 4.1 and 4.2, which assume a\nfixed model class p(yi|θ), do not apply. For example, sometimes a parameter is assigned\nfor each sampling unit in a study; for example, yi ∼ N(θi, σ\n\n2). The parameters θi generally\ncannot be estimated consistently unless the amount of data collected from each sampling\nunit increases along with the number of units. In nonparametric models such as Gaussian\nprocesses (see Chapter 21) there can be a new latent parameter corresponding to each data\npoint.\n\nAs with underidentified parameters, the posterior distribution for θi will not converge to\na point mass if new data do not bring enough information about θi. Here, the posterior dis-\ntribution will not in general converge to a point in the expanding parameter space (reflecting\nthe increasing dimensionality of θ), and its projection into any fixed space—for example,\nthe marginal posterior distribution of any particular θi—will not necessarily converge to a\npoint either.\n\nAliasing. Aliasing is a special case of underidentified parameters in which the same likeli-\nhood function repeats at a discrete set of points. For example, consider the following normal\nmixture model with independent and identically distributed data y1, . . . , yn and parameter\nvector θ = (µ1, µ2, σ\n\n2\n1 , σ\n\n2\n2 , λ):\n\np(yi|µ1, µ2, σ\n2\n1 , σ\n\n2\n2 , λ)=λ\n\n1√\n2π σ1\n\ne\n− 1\n\n2σ2\n1\n\n(yi−µ1)\n2\n\n+ (1 − λ) 1√\n2π σ2\n\ne\n− 1\n\n2σ2\n2\n\n(yi−µ2)\n2\n\n.\n\nIf we interchange each of (µ1, µ2) and (σ2\n1 , σ\n\n2\n2), and replace λ by (1 − λ), the likelihood of\n\nthe data remains the same. The posterior distribution of this model generally has at least\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n90 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ntwo modes and consists of a (50%, 50%) mixture of two distributions that are mirror images\nof each other; it does not converge to a single point no matter how large the dataset is.\n\nIn general, the problem of aliasing is eliminated by restricting the parameter space so\nthat no duplication appears; in the above example, the aliasing can be removed by restricting\nµ1 to be less than or equal to µ2.\n\nUnbounded likelihoods. If the likelihood function is unbounded, then there might be no\nposterior mode within the parameter space, invalidating both the consistency results and\nthe normal approximation. For example, consider the previous normal mixture model; for\nsimplicity, assume that λ is known (and not equal to 0 or 1). If we set µ1 = yi for any\narbitrary yi, and let σ2\n\n1 → 0, then the likelihood approaches infinity. As n → ∞, the\nnumber of modes of the likelihood increases. If the prior distribution is uniform on σ2\n\n1 and\nσ2\n2 in the region near zero, there will be likewise an increasing number of posterior modes,\n\nwith no corresponding normal approximations. A prior distribution proportional to σ−2\n1 σ−2\n\n2\n\njust makes things worse because this puts more probability near zero, causing the posterior\ndistribution to explode even faster at zero.\n\nIn general, this problem should arise rarely in practice, because the poles of an un-\nbounded likelihood correspond to unrealistic conditions in a model. The problem can be\nsolved by restricting to a plausible set of distributions. When the problem occurs for\nvariance components near zero, it can be resolved in various ways, such as using a prior\ndistribution that declines to zero at the boundary or by assigning an informative prior\ndistribution to the ratio of the variance parameters.\n\nImproper posterior distributions. If the unnormalized posterior density, obtained by multi-\nplying the likelihood by a ‘formal’ prior density representing an improper prior distribution,\nintegrates to infinity, then the asymptotic results, which rely on probabilities summing to\n1, do not follow. An improper posterior distribution cannot occur except with an improper\nprior distribution.\n\nA simple example arises from combining a Beta(0, 0) prior distribution for a binomial\nproportion with data consisting of n successes and 0 failures. More subtle examples, with\nhierarchical binomial and normal models, are discussed in Sections 5.3 and 5.4.\n\nThe solution to this problem is clear. An improper prior distribution is only a convenient\napproximation, and if it does not give rise to a proper posterior distribution then the sought\nconvenience is lost. In this case a proper prior distribution is needed, or at least an improper\nprior density that when combined with the likelihood has a finite integral.\n\nPrior distributions that exclude the point of convergence. If p(θ0) = 0 for a discrete param-\neter space, or if p(θ) = 0 in a neighborhood about θ0 for a continuous parameter space, then\nthe convergence results, which are based on the likelihood dominating the prior distribution,\ndo not hold. The solution is to give positive probability density in the prior distribution to\nall values of θ that are even remotely plausible.\n\nConvergence to the edge of parameter space. If θ0 is on the boundary of the parameter\nspace, then the Taylor series expansion must be truncated in some directions, and the\nnormal distribution will not necessarily be appropriate, even in the limit.\n\nFor example, consider the model, yi ∼ N(θ, 1), with the restriction θ ≥ 0. Suppose that\nthe model is accurate, with θ = 0 as the true value. The posterior distribution for θ is\nnormal, centered at y, truncated to be positive. The shape of the posterior distribution for\nθ, in the limit as n → ∞, is half of a normal distribution, centered about 0, truncated to\nbe positive.\n\nFor another example, consider the same assumed model, but now suppose that the true\nθ is −1, a value outside the assumed parameter space. The limiting posterior distribution\nfor θ has a sharp spike at 0 with no resemblance to a normal distribution at all. The\nsolution in practice is to recognize the difficulties of applying the normal approximation if\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.4. FREQUENCY EVALUATIONS OF BAYESIAN INFERENCES 91\n\none is interested in parameter values near the edge of parameter space. More important,\none should give positive prior probability density to all values of θ that are even remotely\npossible, or in the neighborhood of remotely possible values.\n\nTails of the distribution. The normal approximation can hold for essentially all the mass\nof the posterior distribution but still not be accurate in the tails. For example, suppose\np(θ|y) is proportional to e−c|θ| as |θ| → ∞, for some constant c; by comparison, the normal\n\ndensity is proportional to e−cθ\n2\n\n. The distribution function still converges to normality, but\nfor any finite sample size n the approximation fails far out in the tail. As another example,\nconsider any parameter that is constrained to be positive. For any finite sample size, the\nnormal approximation will admit the possibility of the parameter being negative, because\nthe approximation is simply not appropriate at that point in the tail of the distribution,\nbut that point becomes farther and farther in the tail as n increases.\n\n4.4 Frequency evaluations of Bayesian inferences\n\nJust as the Bayesian paradigm can be seen to justify simple ‘classical’ techniques, the\nmethods of frequentist statistics provide a useful approach for evaluating the properties of\nBayesian inferences—their operating characteristics—when these are regarded as embedded\nin a sequence of repeated samples. We have already used this notion in discussing the ideas\nof consistency and asymptotic normality. The notion of stable estimation, which says that\nfor a fixed model, the posterior distribution approaches a point as more data arrive—\nleading, in the limit, to inferential certainty—is based on the idea of repeated sampling.\nIt is certainly appealing that if the hypothesized family of probability models contains the\ntrue distribution (and assigns it a nonzero prior density), then as more information about\nθ arrives, the posterior distribution converges to the true value of θ.\n\nLarge-sample correspondence\n\nSuppose that the normal approximation (4.2) for the posterior distribution of θ holds; then\nwe can transform to the standard multivariate normal:\n\n[I(θ̂)]1/2(θ − θ̂) | y ∼ N(0, I), (4.3)\n\nwhere θ̂ is the posterior mode and [I(θ̂)]1/2 is any matrix square root of I(θ̂). In addition,\n\nθ̂ → θ0, and so we could just as well write the approximation in terms of I(θ0). If the true\ndata distribution is included in the class of models, so that f(y) ≡ p(y|θ) for some θ, then\nin repeated sampling with fixed θ, in the limit n→∞, it can be proved that\n\n[I(θ̂)]1/2(θ − θ̂) | θ ∼ N(0, I), (4.4)\n\na result from classical statistical theory that is generally proved for θ̂ equal to the maximum\nlikelihood estimate but is easily extended to the case with θ̂ equal to the posterior mode.\nThese results mean that, for any function of (θ− θ̂), the posterior distribution derived from\n(4.3) is asymptotically the same as the repeated sampling distribution derived from (4.4).\nThus, for example, a 95% central posterior interval for θ will cover the true value 95% of\nthe time under repeated sampling with any fixed true θ.\n\nPoint estimation, consistency, and efficiency\n\nIn the Bayesian framework, obtaining an ‘estimate’ of θ makes most sense in large samples\nwhen the posterior mode, θ̂, is the obvious center of the posterior distribution of θ and the\nuncertainty conveyed by nI(θ̂) is so small as to be practically unimportant. More generally,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n92 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nhowever, in smaller samples, it is inappropriate to summarize inference about θ by one\nvalue, especially when the posterior distribution of θ is more variable or even asymmetric.\nFormally, by incorporating loss functions in a decision-theoretic context (see Section 9.1\nand Exercise 9.6), one can define optimal point estimates; for the purposes of Bayesian data\nanalysis, however, we believe that representation of the full posterior distribution (as, for\nexample, with 50% and 95% central posterior intervals) is more useful. In many problems,\nespecially with large samples, a point estimate and its estimated standard error are adequate\nto summarize a posterior inference, but we interpret the estimate as an inferential summary,\nnot as the solution to a decision problem. In any case, the large-sample frequency properties\nof any estimate can be evaluated, without consideration of whether the estimate was derived\nfrom a Bayesian analysis.\n\nA point estimate is said to be consistent in the sampling theory sense if, as samples\nget larger, it converges to the true value of the parameter that it is asserted to estimate.\nThus, if f(y) ≡ p(y|θ0), then a point estimate θ̂ of θ is consistent if its sampling distribution\nconverges to a point mass at θ0 as the data sample size n increases (that is, considering\n\nθ̂ as a function of y, which is a random variable conditional on θ0). A closely related\n\nconcept is asymptotic unbiasedness, where (E(θ̂|θ0) − θ0)/sd(θ̂|θ0) converges to 0 (once\n\nagain, considering θ̂(y) as a random variable whose distribution is determined by p(y|θ0)).\nWhen the truth is included in the family of models being fitted, the posterior mode θ̂, and\nalso the posterior mean and median, are consistent and asymptotically unbiased under mild\nregularity conditions.\n\nA point estimate θ̂ is said to be efficient if there exists no other function of y that\nestimates θ with lower mean squared error, that is, if the expression E((θ̂ − θ0)2|θ0) is at\n\nits optimal, lowest value. More generally, the efficiency of θ̂ is the optimal mean squared\nerror divided by the mean squared error of θ̂. An estimate is asymptotically efficient if its\nefficiency approaches 1 as the sample size n → ∞. Under mild regularity conditions, the\ncenter of the posterior distribution (defined, for example, by the posterior mean, median,\nor mode) is asymptotically efficient.\n\nConfidence coverage\n\nIf a region C(y) includes θ0 at least 100(1 − α)% of the time (given any value of θ0) in\nrepeated samples, then C(y) is called a 100(1− α)% confidence region for the parameter\nθ. The word ‘confidence’ is carefully chosen to distinguish such intervals from probability\nintervals and to convey the following behavioral meaning: if one chooses α to be small\nenough (for example, 0.05 or 0.01), then since confidence regions cover the truth in at least\n(1 − α) of their applications, one should be confident in each application that the truth\nis within the region and therefore act as if it is. We saw previously that asymptotically a\n100(1− α)% central posterior interval for θ has the property that, in repeated samples of\ny, 100(1− α)% of the intervals include the value θ0.\n\n4.5 Bayesian interpretations of other statistical methods\n\nWe consider three levels at which Bayesian statistical methods can be compared with other\nmethods. First, as we have already indicated, Bayesian methods are often similar to other\nstatistical approaches in problems involving large samples from a fixed probability model.\nSecond, even for small samples, many statistical methods can be considered as approxi-\nmations to Bayesian inferences based on particular prior distributions; as a way of under-\nstanding a statistical procedure, it is often useful to determine the implicit underlying prior\ndistribution. Third, some methods from classical statistics (notably hypothesis testing)\ncan give results that differ greatly from those given by Bayesian methods. In this section,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 93\n\nwe briefly consider several statistical concepts—point and interval estimation, likelihood\ninference, unbiased estimation, frequency coverage of confidence intervals, hypothesis test-\ning, multiple comparisons, nonparametric methods, and the jackknife and bootstrap—and\ndiscuss their relation to Bayesian methods.\n\nOne way to develop possible models is to examine the interpretation of crude data-\nanalytic procedures as approximations to Bayesian inference under specific models. For\nexample, a widely used technique in sample surveys is ratio estimation, in which, for exam-\nple, given data from a simple random sample, one estimates R = y/x by yobs/xobs, in the\nnotation of Chapter 8. It can be shown that this estimate corresponds to a summary of\na Bayesian posterior inference given independent observations yi|xi ∼ N(Rxi, σ\n\n2xi) and a\nnoninformative prior distribution. Ratio estimates can be useful in a wide variety of cases\nin which this model does not hold, but when the data deviate greatly from this model, the\nratio estimate generally is not appropriate.\n\nFor another example, standard methods of selecting regression predictors, based on\n‘statistical significance,’ correspond roughly to Bayesian analyses under exchangeable prior\ndistributions on the coefficients in which the prior distribution of each coefficient is a mixture\nof a peak at zero and a widely spread distribution, as we discuss further in Section 14.6. We\nbelieve that understanding this correspondence suggests when such models can be usefully\napplied and how they can be improved. Often, in fact, such procedures can be improved\nby including additional information, for example, in problems involving large numbers of\npredictors, by clustering regression coefficients that are likely to be similar into batches.\n\nMaximum likelihood and other point estimates\n\nFrom the perspective of Bayesian data analysis, we can often interpret classical point esti-\nmates as exact or approximate posterior summaries based on some implicit full probability\nmodel. In the limit of large sample size, in fact, we can use asymptotic theory to con-\nstruct a theoretical Bayesian justification for classical maximum likelihood inference. In the\nlimit (assuming regularity conditions), the maximum likelihood estimate, θ̂, is a sufficient\nstatistic—and so is the posterior mode, mean, or median. That is, for large enough n,\nthe maximum likelihood estimate (or any of the other summaries) supplies essentially all\nthe information about θ available from the data. The asymptotic irrelevance of the prior\ndistribution can be taken to justify the use of convenient noninformative prior models.\n\nIn repeated sampling with θ = θ0,\n\np(θ̂(y)|θ=θ0) ≈ N(θ̂(y)|θ0, (nJ(θ0))−1);\n\nthat is, the sampling distribution of θ̂(y) is approximately normal with mean θ0 and precision\n\nnJ(θ0), where for clarity we emphasize that θ̂ is a function of y. Assuming that the prior\ndistribution is locally uniform (or continuous and nonzero) near the true θ, the simple\nanalysis of the normal mean (Section 3.5) shows that the posterior Bayesian inference is\n\np(θ|θ̂) ≈ N(θ|θ̂, (nJ(θ̂))−1).\n\nThis result appears directly from the asymptotic normality theorem, but deriving it indi-\nrectly through Bayesian inference given θ̂ gives insight into a Bayesian rationale for classical\nasymptotic inference based on point estimates and standard errors.\n\nFor finite n, the above approach is inefficient or wasteful of information to the extent\nthat θ̂ is not a sufficient statistic. When the number of parameters is large, the consistency\nresult is often not helpful, and noninformative prior distributions are hard to justify. As\ndiscussed in Chapter 5, hierarchical models are preferable when dealing with a large number\nof parameters since then their common distribution can be estimated from data. In addi-\ntion, any method of inference based on the likelihood alone can be improved if real prior\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n94 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\ninformation is available that is strong enough to contribute substantially to that contained\nin the likelihood function.\n\nUnbiased estimates\n\nSome non-Bayesian statistical methods place great emphasis on unbiasedness as a desirable\nprinciple of estimation, and it is intuitively appealing that, over repeated sampling, the mean\n(or perhaps the median) of a parameter estimate should be equal to its true value. Formally,\n\nan estimate θ̂(y) is called unbiased if E(θ̂(y)|θ) = θ for any value of θ, where this expectation\nis taken over the data distribution, p(y|θ). From a Bayesian perspective, the principle of\nunbiasedness is reasonable in the limit of large samples (see page 92) but otherwise is\npotentially misleading. The major difficulties arise when there are many parameters to be\nestimated and our knowledge or partial knowledge of some of these parameters is clearly\nrelevant to the estimation of others. Requiring unbiased estimates will often lead to relevant\ninformation being ignored (as we discuss with hierarchical models in Chapter 5). In sampling\ntheory terms, minimizing bias will often lead to counterproductive increases in variance.\n\nOne general problem with unbiasedness (and point estimation in general) is that it is\noften not possible to estimate several parameters at once in an even approximately unbiased\nmanner. For example, unbiased estimates of θ1, . . . , θJ yield an upwardly biased estimate\nof the variance of the θj ’s (except in the trivial case in which the θj’s are known exactly).\n\nAnother problem with the principle of unbiasedness arises when treating a future ob-\nservable value as a parameter in prediction problems.\n\nExample. Prediction using regression\nConsider the problem of estimating θ, the height of an adult daughter, given y, her\nmother’s height. For simplicity, assume that the heights of mothers and daughters\nare jointly normally distributed, with known equal means of 160 centimeters, equal\nvariances, and a known correlation of 0.5. Conditioning on the known value of y (in\nother words, using Bayesian inference), the posterior mean of θ is\n\nE(θ|y) = 160 + 0.5(y − 160). (4.5)\n\nThe posterior mean is not, however, an unbiased estimate of θ, in the sense of repeated\nsampling of y given a fixed θ. Given the daughter’s height, θ, the mother’s height, y,\nhas mean E(y|θ) = 160+0.5(θ− 160). Thus, under repeated sampling of y given fixed\nθ, the posterior mean (4.5) has expectation 160+ 0.25(θ− 160) and is biased towards\nthe grand mean of 160. In contrast, the estimate\n\nθ̂ = 160 + 2(y − 160)\n\nis unbiased under repeated sampling of y, conditional on θ. Unfortunately, the esti-\nmate θ̂ makes no sense for values of y not equal to 160; for example, if a mother is 10\ncentimeters taller than average, it estimates her daughter to be 20 centimeters taller\nthan average!\nIn this simple example, in which θ has an accepted population distribution, a sensible\nnon-Bayesian statistician would not use the unbiased estimate θ̂; instead, this problem\nwould be classified as ‘prediction’ rather than ‘estimation,’ and procedures would not\nbe evaluated conditional on the random variable θ. The example illustrates, however,\nthe limitations of unbiasedness as a general principle: it requires unknown quantities to\nbe characterized either as ‘parameters’ or ‘predictions,’ with different implications for\nestimation but no clear substantive distinction. Chapter 5 considers similar situations\nin which the population distribution of θ must be estimated from data rather than\nconditioning on a particular value.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.5. BAYESIAN INTERPRETATIONS OF OTHER STATISTICAL METHODS 95\n\nThe important principle illustrated by the example is that of regression to the mean:\nfor any given mother, the expected value of her daughter’s height lies between her\nmother’s height and the population mean. This principle was fundamental to the\noriginal use of the term ‘regression’ for this type of analysis by Galton in the late\n19th century. In many ways, Bayesian analysis can be seen as a logical extension of\nthe principle of regression to the mean, ensuring that proper weighting is made of\ninformation from different sources.\n\nConfidence intervals\n\nEven in small samples, Bayesian (1−α) posterior intervals often have close to (1−α) con-\nfidence coverage under repeated samples conditional on θ. But there are some confidence\nintervals, derived purely from sampling-theory arguments, that differ considerably from\nBayesian probability intervals. From our perspective these intervals are of doubtful value.\nFor example, many authors have shown that a general theory based on unconditional behav-\nior can lead to clearly counterintuitive results, for example, the possibilities of confidence\nintervals with zero or infinite length. A simple example is the confidence interval that is\nempty 5% of the time and contains all of the real line 95% of the time: this always contains\nthe true value (of any real-valued parameter) in 95% of repeated samples. Such examples\ndo not imply that there is no value in the concept of confidence coverage but rather show\nthat coverage alone is not a sufficient basis on which to form reasonable inferences.\n\nHypothesis testing\n\nThe perspective of this book has little role for the non-Bayesian concept of hypothesis\ntests, especially where these relate to point null hypotheses of the form θ = θ0. In order\nfor a Bayesian analysis to yield a nonzero probability for a point null hypothesis, it must\nbegin with a nonzero prior probability for that hypothesis; in the case of a continuous\nparameter, such a prior distribution (comprising a discrete mass, of say 0.5, at θ0 mixed\nwith a continuous density elsewhere) usually seems contrived. In fact, most of the difficulties\nin interpreting hypothesis tests arise from the artificial dichotomy that is required between\nθ = θ0 and θ 6= θ0. Difficulties related to this dichotomy are widely acknowledged from all\nperspectives on statistical inference. In problems involving a continuous parameter θ (say\nthe difference between two means), the hypothesis that θ is exactly zero is rarely reasonable,\nand it is of more interest to estimate a posterior distribution or a corresponding interval\nestimate of θ. For a continuous parameter θ, the question ‘Does θ equal 0?’ can generally\nbe rephrased more usefully as ‘What is the posterior distribution for θ?’\n\nIn various simple one-sided hypothesis tests, conventional p-values may correspond with\nposterior probabilities under noninformative prior distributions. For example, suppose we\nobserve y = 1 from the model y ∼ N(θ, 1), with a uniform prior density on θ. One cannot\n‘reject the hypothesis’ that θ = 0: the one-sided p-value is 0.16 and the two-sided p-value\nis 0.32, both greater than the conventionally accepted cutoff value of 0.05 for ‘statistical\nsignificance.’ On the other hand, the posterior probability that θ > 0 is 84%, which is a\nmore satisfactory and informative conclusion than the dichotomous verdict ‘reject’ or ‘do\nnot reject.’\n\nIn contrast to the problem of making inference about a parameter within a particular\nmodel, we do find a form of hypothesis test to be useful when assessing the goodness of fit of\na probability model. In the Bayesian framework, it is useful to check a model by comparing\nobserved data to possible predictive outcomes, as we discuss in detail in Chapter 6.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n96 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nMultiple comparisons and multilevel modeling\n\nConsider a problem with independent measurements, yj ∼ N(θj , 1), on each of J parameters,\nin which the goal is to detect differences among and ordering of the continuous parame-\nters θj . Several competing multiple comparisons procedures have been derived in classical\nstatistics, with rules about when various θj ’s can be declared ‘significantly different.’ In\nthe Bayesian approach, the parameters have a joint posterior distribution. One can com-\npute the posterior probability of each of the J ! orderings if desired. If there is posterior\nuncertainty in the ordering, several permutations will have substantial probabilities, which\nis a more reasonable conclusion than producing a list of θj ’s that can be declared different\n(with the false implication that other θj ’s may be exactly equal). With J large, the exact\nordering is probably not important, and it might be more reasonable to give a posterior\nmedian and interval estimate of the quantile of each θj in the population.\n\nWe prefer to handle multiple comparisons problems using hierarchical models, as we\nshall illustrate in a comparison of treatment effects in eight schools in Section 5.5 (see also\nExercise 5.3). Hierarchical modeling automatically partially pools estimates of different θj ’s\ntoward each other when there is little evidence for real variation. As a result, this Bayesian\nprocedure automatically addresses the key concern of classical multiple comparisons analy-\nsis, which is the possibility of finding large differences as a byproduct of searching through\nso many possibilities. For example, in the educational testing example, the eight schools\ngive 8 · 7/2 = 28 possible comparisons, and none turn out to be close to ‘statistically sig-\nnificant’ (in the sense that zero is contained within the 95% intervals for all the differences\nin effects between pairs of schools), which makes sense since the between-school variation\n(the parameter τ in that model) is estimated to be low.\n\nNonparametric methods, permutation tests, jackknife, bootstrap\n\nMany non-Bayesian methods have been developed that avoid complete probability models,\neven at the sampling level. It is difficult to evaluate many of these from a Bayesian point\nof view. For instance, hypothesis tests for comparing medians based on ranks do not have\ndirect counterparts in Bayesian inference; therefore it is hard to interpret the resulting es-\ntimates and p-values from a Bayesian point of view (for example, as posterior expectations,\nintervals, or probabilities for parameters or predictions of interest). In complicated prob-\nlems, there is often a degree of arbitrariness in the procedures used; for example there is\ngenerally no clear method for constructing a nonparametric inference or an estimator to\njackknife/bootstrap in hypothetical replications. Without a specified probability model,\nit is difficult to see how to test the assumptions underlying a particular nonparametric\nmethod. In such problems, we find it more satisfactory to construct a joint probability\ndistribution and check it against the data (as in Chapter 6) than to construct an estimator\nand evaluate its frequency properties. Nonparametric methods are useful to us as tools for\ndata summary and description that can help us to construct models or help us evaluate\ninferences from a completely different perspective.\n\nFrom a different direction, one might well say that Bayesian methods involve arbitrary\nchoices of models and are difficult to evaluate because in practice there will always be\nimportant aspects of a model that are impossible to check. Our purpose here is not to\ndismiss or disparage classical nonparametric methods but rather to put them in a Bayesian\ncontext to the extent this is possible.\n\nSome nonparametric methods such as permutation tests for experiments and sampling-\ntheory inference for surveys turn out to give similar results in simple problems to Bayesian\ninferences with noninformative prior distributions, if the Bayesian model is constructed\nto fit the data reasonably well. Such simple problems include balanced designs with no\nmissing data and surveys based on simple random sampling. When estimating several pa-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.6. BIBLIOGRAPHIC NOTE 97\n\nrameters at once or including explanatory variables in the analysis (using methods such as\nthe analysis of covariance or regression) or prior information on the parameters, the permu-\ntation/sampling theory methods give no direct answer, and this often provides considerable\npractical incentive to move to a model-based Bayesian approach.\n\nExample. The Wilcoxon rank test\nAnother connection can be made by interpreting nonparametric methods in terms\nof implicit models. For example, the Wilcoxon rank test for comparing two samples\n(y1, . . . , yny ) and (z1, . . . , znz) proceeds by first ranking each of the points in the com-\nbined data from 1 to n = ny +nz, then computing the difference between the average\nranks of the y’s and z’s, and finally computing the p-value of this difference by com-\nparing to a tabulated reference distribution calculated based on the assumption of\nrandom assignment of the n ranks. This can be formulated as a nonlinear transfor-\nmation that replaces each data point by its rank in the combined data, followed by\na comparison of the mean values of the two transformed samples. Even more clear\nwould be to transform the ranks 1, 2, . . . , n to quantiles 1\n\n2n ,\n3\n2n , . . . ,\n\n2n−1\n2n , so that the\n\ndifference between the two means can be interpreted as an average distance in the scale\nof the quantiles of the combined distribution. From the Central Limit Theorem, the\nmean difference is approximately normally distributed, and so classical normal-theory\nconfidence intervals can be interpreted as Bayesian posterior probability statements,\nas discussed at the beginning of this section.\nWe see two major advantages of expressing rank tests as approximate Bayesian infer-\nences. First, the Bayesian framework is more flexible than rank testing for handling\nthe complications that arise, for example, from additional information such as regres-\nsion predictors or from complications such as censored or truncated data. Second,\nsetting up the problem in terms of a nonlinear transformation reveals the general-\nity of the model-based approach—we are free to use any transformation that might\nbe appropriate for the problem, perhaps now treating the combined quantiles as a\nconvenient default choice.\n\n4.6 Bibliographic note\n\nRelatively little has been written on the practical implications of asymptotic theory for\nBayesian analysis. The overview by Edwards, Lindman, and Savage (1963) remains one of\nthe best and includes a detailed discussion of the principle of ‘stable estimation’ or when\nprior information can be satisfactorily approximated by a uniform density function. Much\nmore has been written comparing Bayesian and non-Bayesian approaches to inference, and\nwe have largely ignored the extensive philosophical and logical debates on this subject.\nSome good sources on the topic from the Bayesian point of view include Lindley (1958),\nPratt (1965), and Berger and Wolpert (1984). Jaynes (1976) discusses some disadvantages\nof non-Bayesian methods compared to a particular Bayesian approach.\n\nIn Appendix B we provide references to the asymptotic normality theory. The coun-\nterexamples presented in Section 4.3 have arisen, in various forms, in our own applied\nresearch. Berzuini et al. (1997) discuss Bayesian inference for sequential data problems, in\nwhich the posterior distribution changes as data arrive, thus approaching the asymptotic\nresults dynamically.\n\nAn example of the use of the normal approximation with small samples is provided by\nRubin and Schenker (1987), who approximate the posterior distribution of the logit of the\nbinomial parameter in a real application and evaluate the frequentist operating character-\nistics of their procedure; see also Agresti and Coull (1998). Clogg et al. (1991) provide\nadditional discussion of this approach in a more complicated setting.\n\nMorris (1983) and Rubin (1984) discuss, from two different standpoints, the concept\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n98 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\nof evaluating Bayesian procedures by examining long-run frequency properties (such as\ncoverage of 95% confidence intervals). An example of frequency evaluation of Bayesian\nprocedures in an applied problem is given by Zaslavsky (1993).\n\nKrantz (1999) discusses the strengths and weaknesses of p-values as used in statistical\ndata analysis in practice. Discussions of the role of p-values in Bayesian inference appear\nin Bayarri and Berger (1998, 2000). Earlier work on the Bayesian analysis of hypothesis\ntesting and the problems of interpreting conventional p-values is provided by Berger and\nSellke (1987), which contains a lively discussion and many further references. Gelman\n(2008a) and discussants provide a more recent airing of arguments for and against Bayesian\nstatistics. Gelman (2006b) compares Bayesian inference and the more generalized approach\nknown as belief functions (Dempster, 1967, 1968) using a simple toy example.\n\nGreenland and Poole (2013) and Gelman (2013a) present some more recent discussions\nof the relevance of classical p-values in Bayesian inference.\n\nA simple and pragmatic discussion of the need to consider Bayesian ideas in hypothesis\ntesting in a biostatistical context is given by Browner and Newman (1987), and further dis-\ncussion of the role of Bayesian thinking in medical statistics appears in Goodman (1999a, b)\nand Sterne and Smith (2001). Gelman and Tuerlinckx (2000), Efron and Tibshirani (2002),\nand Gelman, Hill, and Yajima (2012) give a Bayesian perspective on multiple comparisons\nin the context of hierarchical modeling.\n\nStigler (1983) discusses the similarity between Bayesian inference and regression predic-\ntion that we mention in our critique of unbiasedness in Section 4.5; Stigler (1986) discusses\nGalton’s use of regression.\n\nSequential monitoring and analysis of clinical trials in medical research is an important\narea of practical application that has been dominated by frequentist thinking but has re-\ncently seen considerable discussion of the merits of a Bayesian approach; recent reviews\nand examples are provided by Freedman, Spiegelhalter, and Parmar (1994), Parmar et al.\n(2001), and Vail et al. (2001). Thall, Simon, and Estey (1995) consider frequency properties\nof Bayesian analyses of sequential trials. More references on sequential designs appear in\nthe bibliographic note at the end of Chapter 8.\n\nThe non-Bayesian principles and methods mentioned in Section 4.5 are covered in many\nbooks, for example, Lehmann (1983, 1986), Cox and Hinkley (1974), Hastie and Tibshi-\nrani (1990), and Efron and Tibshirani (1993). The connection between ratio estimation\nand modeling alluded to in Section 4.5 is discussed by Brewer (1963), Royall (1970), and,\nfrom our Bayesian approach, Rubin (1987a, p. 46). Conover and Iman (1980) discuss the\nconnection between nonparametric tests and data transformations.\n\n4.7 Exercises\n\n1. Normal approximation: suppose that y1, . . . , y5 are independent samples from a Cauchy\ndistribution with unknown center θ and known scale 1: p(yi|θ) ∝ 1/(1 + (yi − θ)2).\nAssume that the prior distribution for θ is uniform on [0, 1]. Given the observations\n(y1, . . . , y5) = (−2,−1, 0, 1.5, 2.5):\n\n(a) Determine the derivative and the second derivative of the log posterior density.\n\n(b) Find the posterior mode of θ by iteratively solving the equation determined by setting\nthe derivative of the log-likelihood to zero.\n\n(c) Construct the normal approximation based on the second derivative of the log posterior\ndensity at the mode. Plot the approximate normal density and compare to the exact\ndensity as computed using the approach described in Exercise 2.11.\n\n2. Normal approximation: derive the analytic form of the information matrix and the nor-\nmal approximation variance for the bioassay example.\n\n3. Normal approximation to the marginal posterior distribution of an estimand: in the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n4.7. EXERCISES 99\n\nbioassay example, the normal approximation to the joint posterior distribution of (α, β)\nis obtained. The posterior distribution of any estimand, such as the LD50, can be ap-\nproximated by a normal distribution fit to its marginal posterior mode and the curvature\nof the marginal posterior density about the mode. This is sometimes called the ‘delta\nmethod.’ Expand the posterior distribution of the LD50, −α/β, as a Taylor series around\nthe posterior mode and thereby derive the asymptotic posterior median and standard\ndeviation. Compare to the histogram in Figure 4.2.\n\n4. Asymptotic normality: assuming the regularity conditions hold, we know that p(θ|y)\napproaches normality as n → ∞. In addition, if φ = f(θ) is any one-to-one continuous\ntransformation of θ, we can express the Bayesian inference in terms of φ and find that\np(φ|y) also approaches normality. But a nonlinear transformation of a normal distribution\nis no longer normal. How can both limiting normal distributions be valid?\n\n5. Approximate mean and variance:\n\n(a) Suppose x and y are independent normally distributed random variables, where x has\nmean 4 and standard deviation 1, and y has mean 3 and standard deviation 2. What\nare the mean and standard deviation of y/x? Compute this using simulation.\n\n(b) Suppose x and y are independent random variables, where x has mean 4 and standard\ndeviation 1, and y has mean 3 and standard deviation 2. What are the approximate\nmean and standard deviation of y/x? Determine this without using simulation.\n\n(c) What assumptions are required for the approximation in (b) to be reasonable?\n\n6. Statistical decision theory: a decision-theoretic approach to the estimation of an unknown\nparameter θ introduces the loss function L(θ, a) which, loosely speaking, gives the cost of\ndeciding that the parameter has the value a, when it is in fact equal to θ. The estimate\na can be chosen to minimize the posterior expected loss,\n\nE(L(a|y)) =\n∫\nL(θ, a)p(θ|y)dθ.\n\nThis optimal choice of a is called a Bayes estimate for the loss function L. Show that:\n\n(a) If L(θ, a) = (θ − a)2 (squared error loss), then the posterior mean, E(θ|y), if it exists,\nis the unique Bayes estimate of θ.\n\n(b) If L(θ, a) = |θ − a|, then any posterior median of θ is a Bayes estimate of θ.\n\n(c) If k0 and k1 are nonnegative numbers, not both zero, and\n\nL(θ, a) =\n\n{\nk0(θ − a) if θ ≥ a\nk1(a− θ) if θ < a,\n\nthen any k0\nk0+k1\n\nquantile of the posterior distribution p(θ|y) is a Bayes estimate of θ.\n\n7. Unbiasedness: prove that the Bayesian posterior mean, based on a proper prior distri-\nbution, cannot be an unbiased estimator except in degenerate problems (see Bickel and\nBlackwell, 1967, and Lehmann, 1983, p. 244).\n\n8. Regression to the mean: work through the details of the example of mother’s and daugh-\nter’s heights on page 94, illustrating with a sketch of the joint distribution and relevant\nconditional distributions.\n\n9. Point estimation: suppose a measurement y is recorded with a N(θ, σ2) sampling dis-\ntribution, with σ known exactly and θ known to lie in the interval [0, 1]. Consider two\npoint estimates of θ: (1) the maximum likelihood estimate, restricted to the range [0, 1],\nand (2) the posterior mean based on the assumption of a uniform prior distribution on\nθ. Show that if σ is large enough, estimate (1) has a higher mean squared error than\n(2) for any value of θ in [0, 1]. (The unrestricted maximum likelihood estimate has even\nhigher mean squared error.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n100 4. ASYMPTOTICS AND CONNECTIONS TO NON-BAYESIAN APPROACHES\n\n10. Non-Bayesian inference: replicate the analysis of the bioassay example in Section 3.7\nusing non-Bayesian inference. This problem does not have a unique answer, so be clear\non what methods you are using.\n\n(a) Construct an ‘estimator’ of (α, β); that is, a function whose input is a dataset, (x, n, y),\n\nand whose output is a point estimate (α̂, β̂). Compute the value of the estimate for\nthe data given in Table 3.1.\n\n(b) The bias and variance of this estimate are functions of the true values of the parameters\n(α, β) and also of the sampling distribution of the data, given α, β. Assuming the\nbinomial model, estimate the bias and variance of your estimator.\n\n(c) Create approximate 95% confidence intervals for α, β, and the LD50 based on asymp-\ntotic theory and the estimated bias and variance.\n\n(d) Does the inaccuracy of the normal approximation for the posterior distribution (com-\npare Figures 3.3 and 4.1) cast doubt on the coverage properties of your confidence\nintervals in (c)? If so, why?\n\n(e) Create approximate 95% confidence intervals for α, β, and the LD50 using the jack-\nknife or bootstrap (see Efron and Tibshirani, 1993).\n\n(f) Compare your 95% intervals for the LD50 in (c) and (e) to the posterior distribution\ndisplayed in Figure 3.4 and the posterior distribution based on the normal approxima-\ntion, displayed in 4.2b. Comment on the similarities and differences among the four\nintervals. Which do you prefer as an inferential summary about the LD50? Why?\n\n11. Bayesian interpretation of non-Bayesian estimates: consider the following estimation\nprocedure, which is based on classical hypothesis testing. A matched pairs experiment\nis done, and the differences y1, . . . , yn are recorded and modeled as independent draws\nfrom N(θ, σ2). For simplicity, assume σ2 is known. The parameter θ is estimated as the\naverage observed difference if it is ‘statistically significant’ and zero otherwise:\n\nθ̂ =\n\n{\ny if y ≥ 1.96σ/\n\n√\nn\n\n0 otherwise.\n\nCan this be interpreted, in some sense, as an approximate summary (for example, a\nposterior mean or mode) of a Bayesian inference under some prior distribution on θ?\n\n12. Bayesian interpretation of non-Bayesian estimates: repeat the above problem but with\nσ replaced by s, the sample standard deviation of y1, . . . , yn.\n\n13. Objections to Bayesian inference: discuss the criticism, ‘Bayesianism assumes: (a) Either\na weak or uniform prior [distribution], in which case why bother?, (b) Or a strong prior\n[distribution], in which case why collect new data?, (c) Or more realistically, something\nin between, in which case Bayesianism always seems to duck the issue’ (Ehrenberg, 1986).\nFeel free to use any of the examples covered so far to illustrate your points.\n\n14. Objectivity and subjectivity: discuss the statement, ‘People tend to believe results that\nsupport their preconceptions and disbelieve results that surprise them. Bayesian methods\nencourage this undisciplined mode of thinking.’\n\n15. Coverage of posterior intervals:\n\n(a) Consider a model with scalar parameter θ. Prove that, if you draw θ from the prior,\ndraw y|θ from the data model, then perform Bayesian inference for θ given y, that\nthere is a 50% probability that your 50% interval for θ contains the true value.\n\n(b) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is 1. What is the\ncoverage of the posterior 50% interval for θ? (You have to work this one out; it’s not\n50% or any other number you could just guess.)\n\n(c) Suppose θ ∼ N(0, 22) and y|θ ∼ N(θ, 1). Suppose the true value of θ is θ0. Make a\nplot showing the coverage of the posterior 50% interval for θ, as a function of θ0.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 5\n\nHierarchical models\n\nMany statistical applications involve multiple parameters that can be regarded as related\nor connected in some way by the structure of the problem, implying that a joint probability\nmodel for these parameters should reflect their dependence. For example, in a study of the\neffectiveness of cardiac treatments, with the patients in hospital j having survival probability\nθj , it might be reasonable to expect that estimates of the θj ’s, which represent a sample of\nhospitals, should be related to each other. We shall see that this is achieved in a natural\nway if we use a prior distribution in which the θj ’s are viewed as a sample from a common\npopulation distribution. A key feature of such applications is that the observed data, yij ,\nwith units indexed by i within groups indexed by j, can be used to estimate aspects of\nthe population distribution of the θj ’s even though the values of θj are not themselves\nobserved. It is natural to model such a problem hierarchically, with observable outcomes\nmodeled conditionally on certain parameters, which themselves are given a probabilistic\nspecification in terms of further parameters, known as hyperparameters. Such hierarchical\nthinking helps in understanding multiparameter problems and also plays an important role\nin developing computational strategies.\n\nPerhaps even more important in practice is that simple nonhierarchical models are usu-\nally inappropriate for hierarchical data: with few parameters, they generally cannot fit large\ndatasets accurately, whereas with many parameters, they tend to ‘overfit’ such data in the\nsense of producing models that fit the existing data well but lead to inferior predictions for\nnew data. In contrast, hierarchical models can have enough parameters to fit the data well,\nwhile using a population distribution to structure some dependence into the parameters,\nthereby avoiding problems of overfitting. As we show in the examples in this chapter, it is\noften sensible to fit hierarchical models with more parameters than there are data points.\n\nIn Section 5.1, we consider the problem of constructing a prior distribution using hierar-\nchical principles but without fitting a formal probability model for the hierarchical structure.\nWe first consider the analysis of a single experiment, using historical data to create a prior\ndistribution, and then we consider a plausible prior distribution for the parameters of a set\nof experiments. The treatment in Section 5.1 is not fully Bayesian, because, for the purpose\nof simplicity in exposition, we work with a point estimate, rather than a complete joint\nposterior distribution, for the parameters of the population distribution (the hyperparam-\neters). In Section 5.2, we discuss how to construct a hierarchical prior distribution in the\ncontext of a fully Bayesian analysis. Sections 5.3–5.4 present a general approach to compu-\ntation with hierarchical models in conjugate families by combining analytical and numerical\nmethods. We defer details of the most general computational methods to Part III in order\nto explore immediately the important practical and conceptual advantages of hierarchical\nBayesian models. The chapter continues with two extended examples: a hierarchical model\nfor an educational testing experiment and a Bayesian treatment of the method of ‘meta-\nanalysis’ as used in medical research to combine the results of separate studies relating to\nthe same research question. We conclude with a discussion of weakly informative priors,\nwhich become important for hierarchical models fit to data from a small number of groups.\n\n101\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n102 5. HIERARCHICAL MODELS\n\nPrevious experiments:\n\n0/20 0/20 0/20 0/20 0/20 0/20 0/20 0/19 0/19 0/19\n0/19 0/18 0/18 0/17 1/20 1/20 1/20 1/20 1/19 1/19\n1/18 1/18 2/25 2/24 2/23 2/20 2/20 2/20 2/20 2/20\n2/20 1/10 5/49 2/19 5/46 3/27 2/17 7/49 7/47 3/20\n3/20 2/13 9/48 10/50 4/20 4/20 4/20 4/20 4/20 4/20\n4/20 10/48 4/19 4/19 4/19 5/22 11/46 12/49 5/20 5/20\n6/23 5/19 6/22 6/20 6/20 6/20 16/52 15/47 15/46 9/24\n\nCurrent experiment:\n4/14\n\nTable 5.1 Tumor incidence in historical control groups and current group of rats, from Tarone\n(1982). The table displays the values of\n\nyj\n\nnj\n: (number of rats with tumors)/(total number of rats).\n\n5.1 Constructing a parameterized prior distribution\n\nAnalyzing a single experiment in the context of historical data\n\nTo begin our description of hierarchical models, we consider the problem of estimating a\nparameter θ using data from a small experiment and a prior distribution constructed from\nsimilar previous (or historical) experiments. Mathematically, we will consider the current\nand historical experiments to be a random sample from a common population.\n\nExample. Estimating the risk of tumor in a group of rats\nIn the evaluation of drugs for possible clinical application, studies are routinely per-\nformed on rodents. For a particular study drawn from the statistical literature, sup-\npose the immediate aim is to estimate θ, the probability of tumor in a population of\nfemale laboratory rats of type ‘F344’ that receive a zero dose of the drug (a control\ngroup). The data show that 4 out of 14 rats developed endometrial stromal polyps (a\nkind of tumor). It is natural to assume a binomial model for the number of tumors,\ngiven θ. For convenience, we select a prior distribution for θ from the conjugate family,\nθ ∼ Beta(α, β).\n\nAnalysis with a fixed prior distribution. From historical data, suppose we knew that\nthe tumor probabilities θ among groups of female lab rats of type F344 follow an\napproximate beta distribution, with known mean and standard deviation. The tumor\nprobabilities θ vary because of differences in rats and experimental conditions among\nthe experiments. Referring to the expressions for the mean and variance of the beta\ndistribution (see Appendix A), we could find values for α, β that correspond to the\ngiven values for the mean and standard deviation. Then, assuming a Beta(α, β) prior\ndistribution for θ yields a Beta(α+ 4, β + 10) posterior distribution for θ.\n\nApproximate estimate of the population distribution using the historical data. Typ-\nically, the mean and standard deviation of underlying tumor risks are not available.\nRather, historical data are available on previous experiments on similar groups of rats.\nIn the rat tumor example, the historical data were in fact a set of observations of tu-\nmor incidence in 70 groups of rats (Table 5.1). In the jth historical experiment, let the\nnumber of rats with tumors be yj and the total number of rats be nj . We model the\nyj ’s as independent binomial data, given sample sizes nj and study-specific means θj .\nAssuming that the beta prior distribution with parameters (α, β) is a good description\nof the population distribution of the θj ’s in the historical experiments, we can display\nthe hierarchical model schematically as in Figure 5.1, with θ71 and y71 corresponding\nto the current experiment.\nThe observed sample mean and standard deviation of the 70 values\n\nyj\nnj\n\nare 0.136 and\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.1. CONSTRUCTING A PARAMETERIZED PRIOR DISTRIBUTION 103\n\nα, β\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑\n✑\n\n✑✑✰\n\n✚\n✚\n\n✚\n✚\n\n✚\n✚\n\n✚✚❂\n\n�\n�\n\n�\n�\n\n�\n�✠\n\n✁\n✁\n✁\n\n✁\n✁\n✁☛ ❄\n\n❆\n❆\n❆\n❆\n❆\n❆❯\n\n❅\n❅\n❅\n❅\n❅\n❅❘\n\n❩\n❩\n❩\n❩\n❩\n❩\n❩❩⑦\n\n◗\n◗\n◗\n◗\n◗\n◗\n◗\n◗◗s\n\nθ1 θ2 θ3 . . . . . . . . . . . . θ70 θ71\n\n❄ ❄ ❄ ❄ ❄\ny1 y2 y3 . . . . . . . . . . . . y70 y71\n\nFigure 5.1: Structure of the hierarchical model for the rat tumor example.\n\n0.103. If we set the mean and standard deviation of the population distribution to\nthese values, we can solve for α and β—see (A.3) on page 585 in Appendix A. The\nresulting estimate for (α, β) is (1.4, 8.6). This is not a Bayesian calculation because\nit is not based on any specified full probability model. We present a better, fully\nBayesian approach to estimating (α, β) for this example in Section 5.3. The estimate\n(1.4, 8.6) is simply a starting point from which we can explore the idea of estimating\nthe parameters of the population distribution.\nUsing the simple estimate of the historical population distribution as a prior distribu-\ntion for the current experiment yields a Beta(5.4, 18.6) posterior distribution for θ71:\nthe posterior mean is 0.223, and the standard deviation is 0.083. The prior informa-\ntion has resulted in a posterior mean substantially lower than the crude proportion,\n4/14 = 0.286, because the weight of experience indicates that the number of tumors\nin the current experiment is unusually high.\nThese analyses require that the current tumor risk, θ71, and the 70 historical tumor\nrisks, θ1, . . . , θ70, be considered a random sample from a common distribution, an\nassumption that would be invalidated, for example, if it were known that the historical\nexperiments were all done in laboratory A but the current data were gathered in\nlaboratory B, or if time trends were relevant. In practice, a simple, although arbitrary,\nway of accounting for differences between the current and historical data is to inflate\nthe historical variance. For the beta model, inflating the historical variance means\ndecreasing (α+β) while holding α\n\nβ constant. Other systematic differences, such as a\ntime trend in tumor risks, can be incorporated in a more extensive model.\n\nHaving used the 70 historical experiments to form a prior distribution for θ71, we might\nnow like also to use this same prior distribution to obtain Bayesian inferences for the tumor\nprobabilities in the first 70 experiments, θ1, . . . , θ70. There are several logical and practical\nproblems with the approach of directly estimating a prior distribution from existing data:\n\n• If we wanted to use the estimated prior distribution for inference about the first 70\nexperiments, then the data would be used twice: first, all the results together are used to\nestimate the prior distribution, and then each experiment’s results are used to estimate\nits θ. This would seem to cause us to overestimate our precision.\n\n• The point estimate for α and β seems arbitrary, and using any point estimate for α and\nβ necessarily ignores some posterior uncertainty.\n\n• We can also make the opposite point: does it make sense to ‘estimate’ α and β at all?\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n104 5. HIERARCHICAL MODELS\n\nThey are part of the ‘prior’ distribution: should they be known before the data are\ngathered, according to the logic of Bayesian inference?\n\nLogic of combining information\n\nDespite these problems, it clearly makes more sense to try to estimate the population\ndistribution from all the data, and thereby to help estimate each θj , than to estimate all 71\nvalues θj separately. Consider the following thought experiment about inference on two of\nthe parameters, θ26 and θ27, each corresponding to experiments with 2 observed tumors out\nof 20 rats. Suppose our prior distribution for both θ26 and θ27 is centered around 0.15; now\nsuppose that you were told after completing the data analysis that θ26 = 0.1 exactly. This\nshould influence your estimate of θ27; in fact, it would probably make you think that θ27\nis lower than you previously believed, since the data for the two parameters are identical,\nand the postulated value of 0.1 is lower than you previously expected for θ26 from the prior\ndistribution. Thus, θ26 and θ27 should be dependent in the posterior distribution, and they\nshould not be analyzed separately.\n\nWe retain the advantages of using the data to estimate prior parameters and eliminate\nall of the disadvantages just mentioned by putting a probability model on the entire set of\nparameters and experiments and then performing a Bayesian analysis on the joint distribu-\ntion of all the model parameters. A complete Bayesian analysis is described in Section 5.3.\nThe analysis using the data to estimate the prior parameters, which is sometimes called\nempirical Bayes, can be viewed as an approximation to the complete hierarchical Bayesian\nanalysis. We prefer to avoid the term ‘empirical Bayes’ because it misleadingly suggests\nthat the full Bayesian method, which we discuss here and use for the rest of the book, is\nnot ‘empirical.’\n\n5.2 Exchangeability and hierarchical models\n\nGeneralizing from the example of the previous section, consider a set of experiments j =\n1, . . . , J , in which experiment j has data (vector) yj and parameter (vector) θj , with like-\nlihood p(yj |θj). (Throughout this chapter we use the word ‘experiment’ for convenience,\nbut the methods can apply equally well to nonexperimental data.) Some of the parameters\nin different experiments may overlap; for example, each data vector yj may be a sample of\nobservations from a normal distribution with mean µj and common variance σ2, in which\ncase θj = (µj , σ\n\n2). In order to create a joint probability model for all the parameters θ, we\nuse the crucial idea of exchangeability introduced in Chapter 1 and used repeatedly since\nthen.\n\nExchangeability\n\nIf no information—other than the data y—is available to distinguish any of the θj ’s from any\nof the others, and no ordering or grouping of the parameters can be made, one must assume\nsymmetry among the parameters in their prior distribution. This symmetry is represented\nprobabilistically by exchangeability; the parameters (θ1, . . . , θJ) are exchangeable in their\njoint distribution if p(θ1, . . . , θJ) is invariant to permutations of the indexes (1, . . . , J). For\nexample, in the rat tumor problem, suppose we have no information to distinguish the 71\nexperiments, other than the sample sizes nj, which presumably are not related to the values\nof θj ; we therefore use an exchangeable model for the θj ’s.\n\nWe have already encountered the concept of exchangeability in constructing independent\nand identically distributed models for direct data. In practice, ignorance implies exchange-\nability. Generally, the less we know about a problem, the more confidently we can make\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 105\n\nclaims of exchangeability. (This is not, we hasten to add, a good reason to limit our knowl-\nedge of a problem before embarking on statistical analysis!) Consider the analogy to a\nroll of a die: we should initially assign equal probabilities to all six outcomes, but if we\nstudy the measurements of the die and weigh the die carefully, we might eventually notice\nimperfections, which might make us favor one outcome over the others and thus eliminate\nthe symmetry among the six outcomes.\n\nThe simplest form of an exchangeable distribution has each of the parameters θj as an\nindependent sample from a prior (or population) distribution governed by some unknown\nparameter vector φ; thus,\n\np(θ|φ) =\nJ∏\n\nj=1\n\np(θj |φ). (5.1)\n\nIn general, φ is unknown, so our distribution for θ must average over our uncertainty in φ:\n\np(θ) =\n\n∫ ( J∏\n\nj=1\n\np(θj |φ)\n)\np(φ)dφ, (5.2)\n\nThis form, the mixture of independent identical distributions, is usually all that we need to\ncapture exchangeability in practice.\n\nA related theoretical result, de Finetti’s theorem, to which we alluded in Section 1.2,\nstates that in the limit as J → ∞, any suitably well-behaved exchangeable distribution\non (θ1, . . . , θJ) can be expressed as a mixture of independent and identical distributions\nas in (5.2). The theorem does not hold when J is finite (see Exercises 5.1, 5.2, and 5.4).\nStatistically, the mixture model characterizes parameters θ as drawn from a common ‘su-\nperpopulation’ that is determined by the unknown hyperparameters, φ. We are already\nfamiliar with exchangeable models for data, y1, . . . , yn, in the form of likelihoods in which\nthe n observations are independent and identically distributed, given some parameter vector\nθ.\n\nAs a simple counterexample to the above mixture model, consider the probabilities of a\ngiven die landing on each of its six faces. The probabilities θ1, . . . , θ6 are exchangeable, but\nthe six parameters θj are constrained to sum to 1 and so cannot be modeled with a mixture\nof independent identical distributions; nonetheless, they can be modeled exchangeably.\n\nExample. Exchangeability and sampling\nThe following thought experiment illustrates the role of exchangeability in inference\nfrom random sampling. For simplicity, we use a nonhierarchical example with ex-\nchangeability at the level of y rather than θ.\nWe, the authors, have selected eight states out of the United States and recorded the\ndivorce rate per 1000 population in each state in 1981. Call these y1, . . . , y8. What\ncan you, the reader, say about y8, the divorce rate in the eighth state?\nSince you have no information to distinguish any of the eight states from the others,\nyou must model them exchangeably. You might use a beta distribution for the eight\nyj ’s, a logit normal, or some other prior distribution restricted to the range [0, 1].\nUnless you are familiar with divorce statistics in the United States, your distribution\non (y1, . . . , y8) should be fairly vague.\nWe now randomly sample seven states from these eight and tell you their divorce\nrates: 5.8, 6.6, 7.8, 5.6, 7.0, 7.1, 5.4, each in numbers of divorces per 1000 population\n(per year). Based primarily on the data, a reasonable posterior (predictive) distri-\nbution for the remaining value, y8, would probably be centered around 6.5 and have\nmost of its mass between 5.0 and 8.0. Changing the indexing does not change the\njoint distribution. If we relabel the remaining value to be any other yj the posterior\nestimate would be the same. yj are exchangeable but they are not independent as we\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n106 5. HIERARCHICAL MODELS\n\nassume that the divorce rate in the eighth unobserved state is probably similar to the\nobserved rates.\nSuppose initially we had given you the further prior information that the eight states\nare Mountain states: Arizona, Colorado, Idaho, Montana, Nevada, New Mexico, Utah,\nand Wyoming, but selected in a random order; you still are not told which observed\nrate corresponds to which state. Now, before the seven data points were observed,\nthe eight divorce rates should still be modeled exchangeably. However, your prior\ndistribution (that is, before seeing the data), for the eight numbers should change:\nit seems reasonable to assume that Utah, with its large Mormon population, has a\nmuch lower divorce rate, and Nevada, with its liberal divorce laws, has a much higher\ndivorce rate, than the remaining six states. Perhaps, given your expectation of outliers\nin the distribution, your prior distribution should have wide tails. Given this extra\ninformation (the names of the eight states), when you see the seven observed values\nand note that the numbers are so close together, it might seem a reasonable guess that\nthe missing eighth state is Nevada or Utah. Therefore its value might be expected to\nbe much lower or much higher than the seven values observed. This might lead to a\nbimodal or trimodal posterior distribution to account for the two plausible scenarios.\nThe prior distribution on the eight values yj is still exchangeable, however, because\nyou have no information telling which state corresponds to which index number. (See\nExercise 5.6.)\nFinally, we tell you that the state not sampled (corresponding to y8) was Nevada.\nNow, even before seeing the seven observed values, you cannot assign an exchangeable\nprior distribution to the set of eight divorce rates, since you have information that\ndistinguishes y8 from the other seven numbers, here suspecting it is larger than any\nof the others. Once y1, . . . , y7 have been observed, a reasonable posterior distribution\nfor y8 plausibly should have most of its mass above the largest observed rate, that is,\np(y8 > max(y1, . . . , y7)|y1, . . . , y7) should be large.\nIncidentally, Nevada’s divorce rate in 1981 was 13.9 per 1000 population.\n\nExchangeability when additional information is available on the units\n\nOften observations are not fully exchangeable, but are partially or conditionally exchange-\nable:\n\n• If observations can be grouped, we may make hierarchical model, where each group has its\nown submodel, but the group properties are unknown. If we assume that group properties\nare exchangeable, we can use a common prior distribution for the group properties.\n\n• If yi has additional information xi so that yi are not exchangeable but (yi, xi) still are\nexchangeable, then we can make a joint model for (yi, xi) or a conditional model for\nyi|xi.\nIn the rat tumor example, yj were exchangeable as no additional knowledge was available\n\non experimental conditions. If we knew that specific batches of experiments were made in\ndifferent laboratories we could assume partial exchangeability and use two level hierarchical\nmodel to model variation within each laboratory and between laboratories.\n\nIn the divorce example, if we knew xj , the divorce rate in state j last year, for j =\n1, . . . , 8, but not which index corresponded to which state, then we would certainly be able\nto distinguish the eight values of yj, but the joint prior distribution p(xj , yj) would be the\nsame for each state. For states having the same last year divorce rates xj , we could use\ngrouping and assume partial exchangeability or if there are many possible values for xj (as\nwe would assume for divorce rates) we could assume conditional exchangeability and use xj\nas covariate in regression model.\n\nIn general, the usual way to model exchangeability with covariates is through con-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.2. EXCHANGEABILITY AND HIERARCHICAL MODELS 107\n\nditional independence: p(θ1, . . . , θJ |x1, . . . , xJ ) =\n∫\n[\n∏J\nj=1 p(θj |φ, xj)]p(φ|x)dφ, with x =\n\n(x1, . . . , xJ ). In this way, exchangeable models become almost universally applicable, be-\ncause any information available to distinguish different units should be encoded in the x\nand y variables.\n\nIn the rat tumor example, we have already noted that the sample sizes nj are the only\navailable information to distinguish the different experiments. It does not seem likely that\nnj would be a useful variable for modeling tumor rates, but if one were interested, one\ncould create an exchangeable model for the J pairs (n, y)j . A natural first step would be\nto plot\n\nyj\nnj\n\nvs. nj to see any obvious relation that could be modeled. For example, perhaps\n\nsome studies j had larger sample sizes nj because the investigators correctly suspected rarer\nevents; that is, smaller θj and thus smaller expected values of\n\nyj\nnj\n. In fact, the plot of\n\nyj\nnj\n\nversus nj , not shown here, shows no apparent relation between the two variables.\n\nObjections to exchangeable models\n\nIn virtually any statistical application, it is natural to object to exchangeability on the\ngrounds that the units actually differ. For example, the 71 rat tumor experiments were\nperformed at different times, on different rats, and presumably in different laboratories.\nSuch information does not, however, invalidate exchangeability. That the experiments differ\nimplies that the θj ’s differ, but it might be perfectly acceptable to consider them as if drawn\nfrom a common distribution. In fact, with no information available to distinguish them, we\nhave no logical choice but to model the θj ’s exchangeably. Objecting to exchangeability for\nmodeling ignorance is no more reasonable than objecting to an independent and identically\ndistributed model for samples from a common population, objecting to regression models\nin general, or, for that matter, objecting to displaying points in a scatterplot without\nindividual labels. As with regression, the valid concern is not about exchangeability, but\nabout encoding relevant knowledge as explanatory variables where possible.\n\nThe full Bayesian treatment of the hierarchical model\n\nReturning to the problem of inference, the key ‘hierarchical’ part of these models is that\nφ is not known and thus has its own prior distribution, p(φ). The appropriate Bayesian\nposterior distribution is of the vector (φ, θ). The joint prior distribution is\n\np(φ, θ) = p(φ)p(θ|φ),\n\nand the joint posterior distribution is\n\np(φ, θ|y) ∝ p(φ, θ)p(y|φ, θ)\n= p(φ, θ)p(y|θ), (5.3)\n\nwith the latter simplification holding because the data distribution, p(y|φ, θ), depends only\non θ; the hyperparameters φ affect y only through θ. Previously, we assumed φ was known,\nwhich is unrealistic; now we include the uncertainty in φ in the model.\n\nThe hyperprior distribution\n\nIn order to create a joint probability distribution for (φ, θ), we must assign a prior distri-\nbution to φ. If little is known about φ, we can assign a diffuse prior distribution, but we\nmust be careful when using an improper prior density to check that the resulting poste-\nrior distribution is proper, and we should assess whether our conclusions are sensitive to\nthis simplifying assumption. In most real problems, one should have enough substantive\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n108 5. HIERARCHICAL MODELS\n\nknowledge about the parameters in φ at least to constrain the hyperparameters into a finite\nregion, if not to assign a substantive hyperprior distribution. As in nonhierarchical models,\nit is often practical to start with a simple, relatively noninformative, prior distribution on φ\nand seek to add more prior information if there remains too much variation in the posterior\ndistribution.\n\nIn the rat tumor example, the hyperparameters are (α, β), which determine the beta\ndistribution for θ. We illustrate one approach to constructing an appropriate hyperprior\ndistribution in the continuation of that example in the next section.\n\nPosterior predictive distributions\n\nHierarchical models are characterized both by hyperparameters, φ, in our notation, and\nparameters θ. There are two posterior predictive distributions that might be of interest to\nthe data analyst: (1) the distribution of future observations ỹ corresponding to an existing\nθj , or (2) the distribution of observations ỹ corresponding to future θj ’s drawn from the\n\nsame superpopulation. We label the future θj’s as θ̃. Both kinds of replications can be used\nto assess model adequacy, as we discuss in Chapter 6. In the rat tumor example, future\nobservations can be (1) additional rats from an existing experiment, or (2) results from a\nfuture experiment. In the former case, the posterior predictive draws ỹ are based on the\nposterior draws of θj for the existing experiment. In the latter case, one must first draw\n\nθ̃ for the new experiment from the population distribution, given the posterior draws of φ,\nand then draw ỹ given the simulated θ̃.\n\n5.3 Bayesian analysis of conjugate hierarchical models\n\nOur inferential strategy for hierarchical models follows the general approach to multiparam-\neter problems presented in Section 3.8 but is more difficult in practice because of the large\nnumber of parameters that commonly appear in a hierarchical model. In particular, we\ncannot generally plot the contours or display a scatterplot of the simulations from the joint\nposterior distribution of (θ, φ). With care, however, we can follow a similar simulation-based\napproach as before.\n\nIn this section, we present an approach that combines analytical and numerical methods\nto obtain simulations from the joint posterior distribution, p(θ, φ|y), for the beta-binomial\nmodel for the rat-tumor example, for which the population distribution, p(θ|φ), is conjugate\nto the likelihood, p(y|θ). For the many nonconjugate hierarchical models that arise in\npractice, more advanced computational methods, presented in Part III of this book, are\nnecessary. Even for more complicated problems, however, the approach using conjugate\ndistributions is useful for obtaining approximate estimates and starting points for more\naccurate computations.\n\nAnalytic derivation of conditional and marginal distributions\n\nWe first perform the following three steps analytically.\n\n1. Write the joint posterior density, p(θ, φ|y), in unnormalized form as a product of the\nhyperprior distribution p(φ), the population distribution p(θ|φ), and the likelihood p(y|θ).\n\n2. Determine analytically the conditional posterior density of θ given the hyperparameters\nφ; for fixed observed y, this is a function of φ, p(θ|φ, y).\n\n3. Estimate φ using the Bayesian paradigm; that is, obtain its marginal posterior distribu-\ntion, p(φ|y).\nThe first step is immediate, and the second step is easy for conjugate models because,\n\nconditional on φ, the population distribution for θ is just the independent and identically\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 109\n\ndistributed model (5.1), so that the conditional posterior density is a product of conjugate\nposterior densities for the components θj .\n\nThe third step can be performed by brute force by integrating the joint posterior distri-\nbution over θ:\n\np(φ|y) =\n∫\np(θ, φ|y)dθ. (5.4)\n\nFor many standard models, however, including the normal distribution, the marginal pos-\nterior distribution of φ can be computed algebraically using the conditional probability\nformula,\n\np(φ|y) = p(θ, φ|y)\np(θ|φ, y) . (5.5)\n\nThis expression is useful because the numerator is just the joint posterior distribution (5.3),\nand the denominator is the posterior distribution for θ if φ were known. The difficulty in\nusing (5.5), beyond a few standard conjugate models, is that the denominator, p(θ|φ, y),\nregarded as a function of both θ and φ for fixed y, has a normalizing factor that depends on\nφ as well as y. One must be careful with the proportionality ‘constant’ in Bayes’ theorem,\nespecially when using hierarchical models, to make sure it is actually constant. Exercise\n5.11 has an example of a nonconjugate model in which the integral (5.4) has no closed-form\nsolution so that (5.5) is no help.\n\nDrawing simulations from the posterior distribution\n\nThe following strategy is useful for simulating a draw from the joint posterior distribution,\np(θ, φ|y), for simple hierarchical models such as are considered in this chapter.\n\n1. Draw the vector of hyperparameters, φ, from its marginal posterior distribution, p(φ|y).\nIf φ is low-dimensional, the methods discussed in Chapter 3 can be used; for high-\ndimensional φ, more sophisticated methods such as described in Part III may be needed.\n\n2. Draw the parameter vector θ from its conditional posterior distribution, p(θ|φ, y), given\nthe drawn value of φ. For the examples we consider in this chapter, the factorization\np(θ|φ, y) =\n\n∏\nj p(θj |φ, y) holds, and so the components θj can be drawn independently,\n\none at a time.\n\n3. If desired, draw predictive values ỹ from the posterior predictive distribution given the\ndrawn θ. Depending on the problem, it might be necessary first to draw a new value θ̃,\ngiven φ, as discussed at the end of the previous section.\n\nAs usual, the above steps are performed L times in order to obtain a set of L draws. From\nthe joint posterior simulations of θ and ỹ, we can compute the posterior distribution of any\nestimand or predictive quantity of interest.\n\nApplication to the model for rat tumors\n\nWe now perform a full Bayesian analysis of the rat tumor experiments described in Section\n5.1. Once again, the data from experiments j = 1, . . . , J , J = 71, are assumed to follow\nindependent binomial distributions:\n\nyj ∼ Bin(nj , θj),\n\nwith the number of rats, nj, known. The parameters θj are assumed to be independent\nsamples from a beta distribution:\n\nθj ∼ Beta(α, β),\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n110 5. HIERARCHICAL MODELS\n\nand we shall assign a noninformative hyperprior distribution to reflect our ignorance about\nthe unknown hyperparameters. As usual, the word ‘noninformative’ indicates our attitude\ntoward this part of the model and is not intended to imply that this particular distribution\nhas any special properties. If the hyperprior distribution turns out to be crucial for our\ninference, we should report this and if possible seek further substantive knowledge that\ncould be used to construct a more informative prior distribution. If we wish to assign an\nimproper prior distribution for the hyperparameters, (α, β), we must check that the poste-\nrior distribution is proper. We defer the choice of noninformative hyperprior distribution,\na relatively arbitrary and unimportant part of this particular analysis, until we inspect the\nintegrability of the posterior density.\n\nJoint, conditional, and marginal posterior distributions. We first perform the three steps\nfor determining the analytic form of the posterior distribution. The joint posterior distri-\nbution of all parameters is\n\np(θ, α, β|y) ∝ p(α, β)p(θ|α, β)p(y|θ, α, β)\n\n∝ p(α, β)\n\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\nθα−1\nj (1− θj)β−1\n\nJ∏\n\nj=1\n\nθ\nyj\nj (1− θj)nj−yj . (5.6)\n\nGiven (α, β), the components of θ have independent posterior densities that are of the form\nθAj (1− θj)B—that is, beta densities—and the joint density is\n\np(θ|α, β, y) =\nJ∏\n\nj=1\n\nΓ(α+β+nj)\n\nΓ(α+yj)Γ(β+nj−yj)\nθ\nα+yj−1\nj (1− θj)β+nj−yj−1. (5.7)\n\nWe can determine the marginal posterior distribution of (α, β) by substituting (5.6) and\n(5.7) into the conditional probability formula (5.5):\n\np(α, β|y) ∝ p(α, β)\nJ∏\n\nj=1\n\nΓ(α+β)\n\nΓ(α)Γ(β)\n\nΓ(α+ yj)Γ(β + nj − yj)\nΓ(α+β + nj)\n\n. (5.8)\n\nThe product in equation (5.8) cannot be simplified analytically but is easy to compute for\nany specified values of (α, β) using a standard routine to compute the gamma function.\n\nChoosing a standard parameterization and setting up a ‘noninformative’ hyperprior dis-\ntribution. Because we have no immediately available information about the distribution\nof tumor rates in populations of rats, we seek a relatively diffuse hyperprior distribu-\ntion for (α, β). Before assigning a hyperprior distribution, we reparameterize in terms\nof logit( α\n\nα+β ) = log(αβ ) and log(α+β), which are the logit of the mean and the logarithm\nof the ‘sample size’ in the beta population distribution for θ. It would seem reasonable to\nassign independent hyperprior distributions to the prior mean and ‘sample size,’ and we\nuse the logistic and logarithmic transformations to put each on a (−∞,∞) scale. Unfortu-\nnately, a uniform prior density on these newly transformed parameters yields an improper\nposterior density, with an infinite integral in the limit (α+β)→ ∞, and so this particular\nprior density cannot be used here.\n\nIn a problem such as this with a reasonably large amount of data, it is possible to set up a\n‘noninformative’ hyperprior density that is dominated by the likelihood and yields a proper\nposterior distribution. One reasonable choice of diffuse hyperprior density is uniform on\n( α\nα+β , (α+β)\n\n−1/2), which when multiplied by the appropriate Jacobian yields the following\ndensities on the original scale,\n\np(α, β) ∝ (α+β)−5/2, (5.9)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.3. BAYESIAN ANALYSIS OF CONJUGATE HIERARCHICAL MODELS 111\n\nFigure 5.2 First try at a contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for\n\nthe rat tumor example. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode.\n\nand on the natural transformed scale:\n\np\n\n(\nlog(\n\nα\n\nβ\n), log(α+β)\n\n)\n∝ αβ(α+β)−5/2. (5.10)\n\nSee Exercise 5.9 for a discussion of this prior density.\nWe could avoid the mathematical effort of checking the integrability of the posterior\n\ndensity if we were to use a proper hyperprior distribution. Another approach would be\ntentatively to use a flat hyperprior density, such as p( α\n\nα+β , α+β) ∝ 1, or even p(α, β) ∝ 1,\n\nand then compute the contours and simulations from the posterior density (as detailed\nbelow). The result would clearly show the posterior contours drifting off toward infinity,\nindicating that the posterior density is not integrable in that limit. The prior distribution\nwould then have to be altered to obtain an integrable posterior density.\n\nIncidentally, setting the prior distribution for (log(αβ ), log(α+β)) to uniform in a vague\n\nbut finite range, such as [−1010, 1010]× [−1010, 1010], would not be an acceptable solution\nfor this problem, as almost all the posterior mass in this case would be in the range of α\nand β near ‘infinity,’ which corresponds to a Beta(α, β) distribution with a variance of zero,\nmeaning that all the θj parameters would be essentially equal in the posterior distribution.\nWhen the likelihood is not integrable, setting a faraway finite cutoff to a uniform prior\ndensity does not necessarily eliminate the problem.\n\nComputing the marginal posterior density of the hyperparameters. Now that we have estab-\nlished a full probability model for data and parameters, we compute the marginal posterior\ndistribution of the hyperparameters. Figure 5.2 shows a contour plot of the unnormalized\nmarginal posterior density on a grid of values of (log(αβ ), log(α+β)). To create the plot, we\n\nfirst compute the logarithm of the density function (5.8) with prior density (5.9), multiply-\ning by the Jacobian to obtain the density p(log(αβ ), log(α+β)|y). We set a grid in the range\n\n(log(αβ ), log(α+β)) ∈ [−2.5,−1]× [1.5, 3], which is centered near our earlier point estimate\n\n(−1.8, 2.3) (that is, (α, β) = (1.4, 8.6)) and covers a factor of 4 in each parameter. Then, to\navoid computational overflows, we subtract the maximum value of the log density from each\npoint on the grid and exponentiate, yielding values of the unnormalized marginal posterior\ndensity.\n\nThe most obvious features of the contour plot are (1) the mode is not far from the\npoint estimate (as we would expect), and (2) important parts of the marginal posterior\ndistribution lie outside the range of the graph.\n\nWe recompute p(log(αβ ), log(α+ β)|y), this time in the range (log(αβ ), log(α+ β)) ∈\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n112 5. HIERARCHICAL MODELS\n\nFigure 5.3 (a) Contour plot of the marginal posterior density of (log(α\nβ\n), log(α+β)) for the rat tumor\n\nexample. Contour lines are at 0.05, 0.15, . . . , 0.95 times the density at the mode. (b) Scatterplot of\n1000 draws (log(α\n\nβ\n), log(α+β)) from the numerically computed marginal posterior density.\n\n[−2.3,−1.3]× [1, 5]. The resulting grid, shown in Figure 5.3a, displays essentially all of\nthe marginal posterior distribution. Figure 5.3b displays 1000 random draws from the\nnumerically computed posterior distribution. The graphs show that the marginal poste-\nrior distribution of the hyperparameters, under this transformation, is approximately sym-\nmetric about the mode, roughly (−1.75, 2.8). This corresponds to approximate values of\n(α, β) = (2.4, 14.0), which differs somewhat from the crude estimate obtained earlier.\n\nHaving computed the relative posterior density at a grid that covers the effective range\nof (α, β), we normalize by approximating the distribution as a step function over the grid\nand setting the total probability in the grid to 1.\n\nWe can then compute posterior moments based on the grid of (log(αβ ), log(α+β)); for\nexample,\n\nE(α|y) is estimated by\n∑\n\nlog(α\nβ ),log(α+β)\n\nα · p(log(α\nβ\n), log(α+β)|y).\n\nFrom the grid in Figure 5.3, we compute E(α|y) = 2.4 and E(β|y) = 14.3. This is close to the\nestimate based on the mode of Figure 5.3a, given above, because the posterior distribution is\napproximately symmetric on the scale of (log(αβ ), log(α+β)). A more important consequence\n\nof averaging over the grid is to account for the posterior uncertainty in (α, β), which is not\ncaptured in the point estimate.\n\nSampling from the joint posterior distribution of parameters and hyperparameters. We\ndraw 1000 random samples from the joint posterior distribution of (α, β, θ1, . . . , θJ ), as\nfollows.\n\n1. Simulate 1000 draws of (log(αβ ), log(α+β)) from their posterior distribution displayed\n\nin Figure 5.3, using the same discrete-grid sampling procedure used to draw (α, β) for\nFigure 3.3b in the bioassay example of Section 3.8.\n\n2. For l = 1, . . . , 1000:\n\n(a) Transform the lth draw of (log(αβ ), log(α+β)) to the scale (α, β) to yield a draw of\nthe hyperparameters from their marginal posterior distribution.\n\n(b) For each j = 1, . . . , J , sample θj from its conditional posterior distribution, θj |α, β, y ∼\nBeta(α+ yj, β + nj − yj).\n\nDisplaying the results. Figure 5.4 shows posterior medians and 95% intervals for the θj ’s,\ncomputed by simulation. The rates θj are shrunk from their sample point estimates,\n\nyj\nnj\n,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 113\n\nFigure 5.4 Posterior medians and 95% intervals of rat tumor rates, θj (plotted vs. observed tumor\nrates yj/nj), based on simulations from the joint posterior distribution. The 45◦ line corresponds\nto the unpooled estimates, θ̂i = yi/ni. The horizontal positions of the line have been jittered to\nreduce overlap.\n\ntowards the population distribution, with approximate mean 0.14; experiments with fewer\nobservations are shrunk more and have higher posterior variances. The results are superfi-\ncially similar to what would be obtained based on a point estimate of the hyperparameters,\nwhich makes sense in this example, because of the fairly large number of experiments.\nBut key differences remain, notably that posterior variability is higher in the full Bayesian\nanalysis, reflecting posterior uncertainty in the hyperparameters.\n\n5.4 Normal model with exchangeable parameters\n\nWe now present a full treatment of a simple hierarchical model based on the normal distribu-\ntion, in which observed data are normally distributed with a different mean for each ‘group’\nor ‘experiment,’ with known observation variance, and a normal population distribution\nfor the group means. This model is sometimes termed the one-way normal random-effects\nmodel with known data variance and is widely applicable, being an important special case\nof the hierarchical normal linear model, which we treat in some generality in Chapter 15.\nIn this section, we present a general treatment following the computational approach of\nSection 5.3. The following section presents a detailed example; those impatient with the\nalgebraic details may wish to look ahead at the example for motivation.\n\nThe data structure\n\nConsider J independent experiments, with experiment j estimating the parameter θj from\nnj independent normally distributed data points, yij , each with known error variance σ2;\nthat is,\n\nyij |θj ∼ N(θj , σ\n2), for i = 1, . . . , nj; j = 1, . . . , J. (5.11)\n\nUsing standard notation from the analysis of variance, we label the sample mean of each\ngroup j as\n\ny.j =\n1\n\nnj\n\nnj∑\n\ni=1\n\nyij\n\nwith sampling variance\n\nσ2\nj = σ2/nj.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n114 5. HIERARCHICAL MODELS\n\nWe can then write the likelihood for each θj using the sufficient statistics, y.j :\n\ny.j |θj ∼ N(θj , σ\n2\nj ), (5.12)\n\na notation that will prove useful later because of the flexibility in allowing a separate\nvariance σ2\n\nj for the mean of each group j. For the rest of this chapter, all expressions will\n\nbe implicitly conditional on the known values σ2\nj . The problem of estimating a set of means\n\nwith unknown variances will require some additional computational methods, presented in\nSections 11.6 and 13.6. Although rarely strictly true, the assumption of known variances\nat the sampling level of the model is often an adequate approximation.\n\nThe treatment of the model provided in this section is also appropriate for situations\nin which the variances differ for reasons other than the number of data points in the ex-\nperiment. In fact, the likelihood (5.12) can appear in much more general contexts than\nthat stated here. For example, if the group sizes nj are large enough, then the means y.j\nare approximately normally distributed, given θj , even when the data yij are not. Other\napplications where the actual likelihood is well approximated by (5.12) appear in the next\ntwo sections.\n\nConstructing a prior distribution from pragmatic considerations\n\nRather than considering immediately the problem of specifying a prior distribution for the\nparameter vector θ = (θ1, . . . , θJ ), let us consider what sorts of posterior estimates might\nbe reasonable for θ, given data (yij). A simple natural approach is to estimate θj by y.j, the\naverage outcome in experiment j. But what if, for example, there are J = 20 experiments\nwith only nj = 2 observations per experimental group, and the groups are 20 pairs of\nassays taken from the same strain of rat, under essentially identical conditions? The two\nobservations per group do not permit accurate estimates. Since the 20 groups are from the\nsame strain of rat, we might now prefer to estimate each θj by the pooled estimate,\n\ny.. =\n\n∑J\nj=1\n\n1\nσ2\nj\n\ny.j\n∑J\n\nj=1\n1\nσ2\nj\n\n. (5.13)\n\nTo decide which estimate to use, a traditional approach from classical statistics is to\nperform an analysis of variance F test for differences among means: if the J group means\nappear significantly variable, choose separate sample means, and if the variance between\nthe group means is not significantly greater than what could be explained by individual\nvariability within groups, use y... The theoretical analysis of variance table is as follows,\nwhere τ2 is the variance of θ1, . . . , θJ . For simplicity, we present the analysis of variance for\na balanced design in which nj = n and σ2\n\nj = σ2/n for all j.\n\ndf SS MS E(MS|σ2, τ )\n\nBetween groups J − 1\n∑\n\ni\n\n∑\nj\n(y.j − y..)\n\n2 SS/(J − 1) nτ 2 + σ2\n\nWithin groups J(n− 1)\n∑\n\ni\n\n∑\nj\n(yij − y.j)\n\n2 SS/(J(n− 1)) σ2\n\nTotal Jn− 1\n∑\n\ni\n\n∑\nj\n(yij − y..)\n\n2 SS/(Jn − 1)\n\nIn the classical random-effects analysis of variance, one computes the sum of squares (SS)\nand the mean square (MS) columns of the table and uses the ‘between’ and ‘within’ mean\nsquares to estimate τ . If the ratio of between to within mean squares is significantly greater\nthan 1, then the analysis of variance suggests separate estimates, θ̂j = y.j for each j. If\nthe ratio of mean squares is not ‘statistically significant,’ then the F test cannot ‘reject the\nhypothesis’ that τ = 0, and pooling is reasonable: θ̂j = y.., for all j. We discuss Bayesian\nanalysis of variance in Section 15.6 in the context of hierarchical regression models.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 115\n\nBut we are not forced to choose between complete pooling and none at all. An alternative\nis to use a weighted combination:\n\nθ̂j = λjy.j + (1− λj)y..,\n\nwhere λj is between 0 and 1.\nWhat kind of prior models produce these various posterior estimates?\n\n1. The unpooled estimate θ̂j = y.j is the posterior mean if the J values θj have independent\nuniform prior densities on (−∞,∞).\n\n2. The pooled estimate θ̂ = y.. is the posterior mean if the J values θj are restricted to be\nequal, with a uniform prior density on the common θ.\n\n3. The weighted combination is the posterior mean if the J values θj have independent and\nidentically distributed normal prior densities.\n\nAll three of these options are exchangeable in the θj ’s, and options 1 and 2 are special cases\nof option 3. No pooling corresponds to λj ≡ 1 for all j and an infinite prior variance for\nthe θj ’s, and complete pooling corresponds to λj ≡ 0 for all j and a zero prior variance for\nthe θj ’s.\n\nThe hierarchical model\n\nFor the convenience of conjugacy (more accurately, partial conjugacy), we assume that the\nparameters θj are drawn from a normal distribution with hyperparameters (µ, τ):\n\np(θ1, . . . , θJ |µ, τ) =\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2) (5.14)\n\np(θ1, . . . , θJ ) =\n\n∫ J∏\n\nj=1\n\n[\nN(θj |µ, τ2)\n\n]\np(µ, τ)d(µ, τ).\n\nThat is, the θj ’s are conditionally independent given (µ, τ). The hierarchical model also\npermits the interpretation of the θj ’s as a random sample from a shared population distri-\nbution, as illustrated in Figure 5.1 for the rat tumors.\n\nWe assign a noninformative uniform hyperprior distribution to µ, given τ :\n\np(µ, τ) = p(µ|τ)p(τ) ∝ p(τ). (5.15)\n\nThe uniform prior density for µ is generally reasonable for this problem; because the com-\nbined data from all J experiments are generally highly informative about µ, we can afford\nto be vague about its prior distribution. We defer discussion of the prior distribution of\nτ to later in the analysis, although relevant principles have already been discussed in the\ncontext of the rat tumor example. As usual, we first work out the answer conditional on\nthe hyperparameters and then consider their prior and posterior distributions.\n\nThe joint posterior distribution\n\nCombining the sampling model for the observable yij ’s and the prior distribution yields\nthe joint posterior distribution of all the parameters and hyperparameters, which we can\nexpress in terms of the sufficient statistics, y.j:\n\np(θ, µ, τ |y) ∝ p(µ, τ)p(θ|µ, τ)p(y|θ)\n\n∝ p(µ, τ)\n\nJ∏\n\nj=1\n\nN(θj |µ, τ2)\nJ∏\n\nj=1\n\nN(y.j |θj , σ2\nj ), (5.16)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n116 5. HIERARCHICAL MODELS\n\nwhere we can ignore factors that depend only on y and the parameters σj , which are assumed\nknown for this analysis.\n\nThe conditional posterior distribution of the normal means, given the hyperparameters\n\nAs in the general hierarchical structure, the parameters θj are independent in the prior\ndistribution (given µ and τ) and appear in different factors in the likelihood (5.11); thus,\nthe conditional posterior distribution p(θ|µ, τ, y) factors into J components.\n\nConditional on the hyperparameters, we simply have J independent unknown normal\nmeans, given normal prior distributions, so we can use the methods of Section 2.5 inde-\npendently on each θj . The conditional posterior distributions for the θj’s are independent,\nand\n\nθj |µ, τ, y ∼ N(θ̂j , Vj),\n\nwhere\n\nθ̂j =\n\n1\nσ2\nj\n\ny.j +\n1\nτ2µ\n\n1\nσ2\nj\n\n+ 1\nτ2\n\nand Vj =\n1\n\n1\nσ2\nj\n\n+ 1\nτ2\n\n. (5.17)\n\nThe posterior mean is a precision-weighted average of the prior population mean and the\nsample mean of the jth group; these expressions for θ̂j and Vj are functions of µ and τ as\nwell as the data. The conditional posterior density for each θj given µ, τ is proper.\n\nThe marginal posterior distribution of the hyperparameters\n\nThe solution so far is only partial because it depends on the unknown µ and τ . The next step\nin our approach is a full Bayesian treatment for the hyperparameters. Section 5.3 mentions\nintegration or analytic computation as two approaches for obtaining p(µ, τ |y) from the joint\nposterior density p(θ, µ, τ |y). For the hierarchical normal model, we can simply consider\nthe information supplied by the data about the hyperparameters directly:\n\np(µ, τ |y) ∝ p(µ, τ)p(y|µ, τ).\n\nFor many problems, this decomposition is no help, because the ‘marginal likelihood’ factor,\np(y|µ, τ), cannot generally be written in closed form. For the normal distribution, however,\nthe marginal likelihood has a particularly simple form. The marginal distributions of the\ngroup means y.j , averaging over θ, are independent (but not identically distributed) normal:\n\ny.j |µ, τ ∼ N(µ, σ2\nj + τ2).\n\nThus we can write the marginal posterior density as\n\np(µ, τ |y) ∝ p(µ, τ)\nJ∏\n\nj=1\n\nN(y.j|µ, σ2\nj + τ2). (5.18)\n\nPosterior distribution of µ given τ . We could use (5.18) to compute directly the posterior\ndistribution p(µ, τ |y) as a function of two variables and proceed as in the rat tumor example.\nFor the normal model, however, we can further simplify by integrating over µ, leaving a\nsimple univariate numerical computation of p(τ |y). We factor the marginal posterior density\nof the hyperparameters as we did the prior density (5.15):\n\np(µ, τ |y) = p(µ|τ, y)p(τ |y). (5.19)\n\nThe first factor on the right side of (5.19) is just the posterior distribution of µ if τ were\nknown. From inspection of (5.18) with τ assumed known, and with a uniform conditional\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.4. NORMAL MODEL WITH EXCHANGEABLE PARAMETERS 117\n\nprior density p(µ|τ), the log posterior distribution is found to be quadratic in µ; thus,\np(µ|τ, y) must be normal. The mean and variance of this distribution can be obtained\nimmediately by considering the group means y.j as J independent estimates of µ with\nvariances (σ2\n\nj + τ2). Combining the data with the uniform prior density p(µ|τ) yields\n\nµ|τ, y ∼ N(µ̂, Vµ),\n\nwhere µ̂ is the precision-weighted average of the y.j-values, and V\n−1\nµ is the total precision:\n\nµ̂ =\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2 y.j\n\n∑J\nj=1\n\n1\nσ2\nj\n+τ2\n\nand V −1\nµ =\n\nJ∑\n\nj=1\n\n1\n\nσ2\nj + τ2\n\n. (5.20)\n\nThe result is a proper posterior density for µ, given τ .\n\nPosterior distribution of τ . We can now obtain the posterior distribution of τ analyti-\ncally from (5.19) and substitution of (5.18) and (5.20) for the numerator and denominator,\nrespectively:\n\np(τ |y) =\np(µ, τ |y)\np(µ|τ, y)\n\n∝\np(τ)\n\n∏J\nj=1 N(y.j |µ, σ2\n\nj + τ2)\n\nN(µ|µ̂, Vµ)\n.\n\nThis identity must hold for any value of µ (in other words, all the factors of µ must cancel\nwhen the expression is simplified); in particular, it holds if we set µ to µ̂, which makes\nevaluation of the expression simple:\n\np(τ |y) ∝\np(τ)\n\n∏J\nj=1 N(y.j |µ̂, σ2\n\nj + τ2)\n\nN(µ̂|µ̂, Vµ)\n\n∝ p(τ)V 1/2\nµ\n\nJ∏\n\nj=1\n\n(σ2\nj + τ2)−1/2 exp\n\n(\n−\n\n(y.j − µ̂)2\n2(σ2\n\nj + τ2)\n\n)\n, (5.21)\n\nwith µ̂ and Vµ defined in (5.20). Both expressions are functions of τ , which means that\np(τ |y) is a complicated function of τ .\n\nPrior distribution for τ . To complete our analysis, we must assign a prior distribution to\nτ . For convenience, we use a diffuse noninformative prior density for τ and hence must\nexamine the resulting posterior density to ensure it has a finite integral. For our illustrative\nanalysis, we use the uniform prior distribution, p(τ) ∝ 1. We leave it as an exercise to show\nmathematically that the uniform prior density for τ yields a proper posterior density and\nthat, in contrast, the seemingly reasonable ‘noninformative’ prior distribution for a variance\ncomponent, p(log τ) ∝ 1, yields an improper posterior distribution for τ . Alternatively, in\napplications it involves little extra effort to determine a ‘best guess’ and an upper bound\nfor the population variance τ , and a reasonable prior distribution can then be constructed\nfrom the scaled inverse-χ2 family (the natural choice for variance parameters), matching the\n‘best guess’ to the mean of the scaled inverse-χ2 density and the upper bound to an upper\npercentile such as the 99th. Once an initial analysis is performed using the noninformative\n‘uniform’ prior density, a sensitivity analysis with a more realistic prior distribution is often\ndesirable.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n118 5. HIERARCHICAL MODELS\n\nComputation\n\nFor this model, computation of the posterior distribution of θ is most conveniently performed\nvia simulation, following the factorization used above:\n\np(θ, µ, τ |y) = p(τ |y)p(µ|τ, y)p(θ|µ, τ, y).\n\nThe first step, simulating τ , is easily performed numerically using the inverse cdf method\n(see Section 1.9) on a grid of uniformly spaced values of τ , with p(τ |y) computed from\n(5.21). The second and third steps, simulating µ and then θ, can both be done easily by\nsampling from normal distributions, first (5.20) to obtain µ and then (5.17) to obtain the\nθj ’s independently.\n\nPosterior predictive distributions\n\nSampling from the posterior predictive distribution of new data, either from a current or\nnew batch, is straightforward given draws from the posterior distribution of the parameters.\nWe consider two scenarios: (1) future data ỹ from the current set of batches, with means\nθ = (θ1, . . . , θJ), and (2) future data ỹ from J̃ future batches, with means θ̃ = (θ̃1, . . . , θ̃J̃).\n\nIn the latter case, we must also specify the J̃ individual sample sizes ñj for the future\nbatches.\n\nTo obtain a draw from the posterior predictive distribution of new data ỹ from the\ncurrent batch of parameters, θ, first obtain a draw from p(θ, µ, τ |y) and then draw the\npredictive data ỹ from (5.11).\n\nTo obtain posterior predictive simulations of new data ỹ for J̃ new groups, perform the\nfollowing three steps: first, draw (µ, τ) from their posterior distribution; second, draw J̃\nnew parameters θ̃ = (θ̃1, . . . , θ̃J̃) from the population distribution p(θ̃j |µ, τ), which is the\npopulation, or prior, distribution for θ given the hyperparameters (equation (5.14)); and\nthird, draw ỹ given θ̃ from the data distribution (5.11).\n\nDifficulty with a natural non-Bayesian estimate of the hyperparameters\n\nTo see some advantages of our fully Bayesian approach, we compare it to an approximate\nmethod that is sometimes used based on a point estimate of µ and τ from the data. Unbiased\npoint estimates, derived from the analysis of variance presented earlier, are\n\nµ̂ = y..\n\nτ̂2 = (MSB −MSW )/n. (5.22)\n\nThe terms MSB and MSW are the ‘between’ and ‘within’ mean squares, respectively, from\nthe analysis of variance. In this alternative approach, inference for θ1, . . . , θJ is based on\nthe conditional posterior distribution, p(θ|µ̂, τ̂ ), given the point estimates.\n\nAs we saw in the rat tumor example of the previous section, the main problem with\nsubstituting point estimates for the hyperparameters is that it ignores our real uncertainty\nabout them. The resulting inference for θ cannot be interpreted as a Bayesian posterior\nsummary. In addition, the estimate τ̂2 in (5.22) has the flaw that it can be negative! The\nproblem of a negative estimate for a variance component can be avoided by setting τ̂2 to\nzero in the case that MSW exceeds MSB , but this creates new issues. Estimating τ2 = 0\nwhenever MSW > MSB seems too strong a claim: if MSW > MSB, then the sample size is\ntoo small for τ2 to be distinguished from zero, but this is not the same as saying we know\nthat τ2 = 0. The latter claim, made implicitly by the point estimate, implies that all the\ngroup means θj are absolutely identical, which leads to scientifically indefensible claims, as\nwe shall see in the example in the next section. It is possible to construct a point estimate\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 119\n\nof (µ, τ) to avoid this particular difficulty, but it would still have the problem, common to\nall point estimates, of ignoring uncertainty.\n\n5.5 Example: parallel experiments in eight schools\n\nWe illustrate the hierarchical normal model with a problem in which the Bayesian analysis\ngives conclusions that differ in important respects from other methods.\n\nA study was performed for the Educational Testing Service to analyze the effects of\nspecial coaching programs on test scores. Separate randomized experiments were performed\nto estimate the effects of coaching programs for the SAT-V (Scholastic Aptitude Test-\nVerbal) in each of eight high schools. The outcome variable in each study was the score on\na special administration of the SAT-V, a standardized multiple choice test administered by\nthe Educational Testing Service and used to help colleges make admissions decisions; the\nscores can vary between 200 and 800, with mean about 500 and standard deviation about\n100. The SAT examinations are designed to be resistant to short-term efforts directed\nspecifically toward improving performance on the test; instead they are designed to reflect\nknowledge acquired and abilities developed over many years of education. Nevertheless,\neach of the eight schools in this study considered its short-term coaching program to be\nsuccessful at increasing SAT scores. Also, there was no prior reason to believe that any of\nthe eight programs was more effective than any other or that some were more similar in\neffect to each other than to any other.\n\nThe results of the experiments are summarized in Table 5.2. All students in the ex-\nperiments had already taken the PSAT (Preliminary SAT), and allowance was made for\ndifferences in the PSAT-M (Mathematics) and PSAT-V test scores between coached and\nuncoached students. In particular, in each school the estimated coaching effect and its\nstandard error were obtained by an analysis of covariance adjustment (that is, a linear\nregression was performed of SAT-V on treatment group, using PSAT-M and PSAT-V as\ncontrol variables) appropriate for a completely randomized experiment. A separate regres-\nsion was estimated for each school. Although not simple sample means (because of the\ncovariance adjustments), the estimated coaching effects, which we label yj , and their sam-\npling variances, σ2\n\nj , play the same role in our model as y.j and σ2\nj in the previous section.\n\nThe estimates yj are obtained by independent experiments and have approximately normal\nsampling distributions with sampling variances that are known, for all practical purposes,\nbecause the sample sizes in all of the eight experiments were relatively large, over thirty\nstudents in each school (recall the discussion of data reduction in Section 4.1). Incidentally,\nan increase of eight points on the SAT-V corresponds to about one more test item correct.\n\nInferences based on nonhierarchical models and their problems\n\nBefore fitting the hierarchical Bayesian model, we first consider two simpler nonhierarchical\nmethods—estimating the effects from the eight experiments independently, and complete\npooling—and discuss why neither of these approaches is adequate for this example.\n\nSeparate estimates. A cursory examination of Table 5.2 may at first suggest that some\ncoaching programs have moderate effects (in the range 18–28 points), most have small\neffects (0–12 points), and two have small negative effects; however, when we take note\nof the standard errors of these estimated effects, we see that it is difficult statistically\nto distinguish between any of the experiments. For example, treating each experiment\nseparately and applying the simple normal analysis in each yields 95% posterior intervals\nthat all overlap substantially.\n\nA pooled estimate. The general overlap in the posterior intervals based on independent\nanalyses suggests that all experiments might be estimating the same quantity. Under the\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n120 5. HIERARCHICAL MODELS\n\nEstimated Standard error\ntreatment of effect\n\nSchool effect, yj estimate, σj\nA 28 15\nB 8 10\nC −3 16\nD 7 11\nE −1 9\nF 1 11\nG 18 10\nH 12 18\n\nTable 5.2 Observed effects of special preparation on SAT-V scores in eight randomized experiments.\nEstimates are based on separate analyses for the eight experiments.\n\nhypothesis that all experiments have the same effect and produce independent estimates\nof this common effect, we could treat the data in Table 5.2 as eight normally distributed\nobservations with known variances. With a noninformative prior distribution, the posterior\nmean for the common coaching effect in the schools is y.., as defined in equation (5.13) with\n\nyj in place of y.j . This pooled estimate is 7.7, and the posterior variance is (\n∑8\n\nj=1\n1\nσ2\nj\n\n)−1 =\n\n16.6 because the eight experiments are independent. Thus, we would estimate the common\neffect to be 7.7 points with standard error equal to\n\n√\n16.6 = 4.1, which would lead to the\n\n95% posterior interval [−0.5, 15.9], or approximately [8 ± 8]. Supporting this analysis, the\nclassical test of the hypothesis that all θj ’s are estimating the same quantity yields a χ2\n\nstatistic less than its degrees of freedom (seven, in this case):\n∑8\n\nj=1(yj − y..)2/σ2\ni = 4.6. To\n\nput it another way, the estimate τ̂2 from (5.22) is negative.\n\nWould it be possible to have one school’s observed effect be 28 just by chance, if the\ncoaching effects in all eight schools were really the same? To get a feeling for the natural\nvariation that we would expect across eight studies if this assumption were true, suppose\nthe estimated treatment effects are eight independent draws from a normal distribution\nwith mean 8 points and standard deviation 13 points (the square root of the mean of the\neight variances σ2\n\nj ). Then, based on the expected values of normal order statistics, we\nwould expect the largest observed value of yj to be about 26 points and the others, in\ndiminishing order, to be about 19, 14, 10, 6, 2, −3, and −9 points. These expected effect\nsizes are consistent with the set of observed effect sizes in Table 5.2. Thus, it would appear\nimprudent to believe that school A really has an effect as large as 28 points.\n\nDifficulties with the separate and pooled estimates. To see the problems with the two ex-\ntreme attitudes—the separate analyses that consider each θj separately, and the alternative\nview (a single common effect) that leads to the pooled estimate—consider θ1, the effect in\nschool A. The effect in school A is estimated as 28.4 with a standard error of 14.9 under\nthe separate analysis, versus a pooled estimate of 7.7 with a standard error of 4.1 under\nthe common-effect model. The separate analyses of the eight schools imply the following\nposterior statement: ‘the probability is 1\n\n2 that the true effect in A is more than 28.4,’ a\ndoubtful statement, considering the results for the other seven schools. On the other hand,\nthe pooled model implies the following statement: ‘the probability is 1\n\n2 that the true effect\nin A is less than 7.7,’ which, despite the non-significant χ2 test, seems an inaccurate sum-\nmary of our knowledge. The pooled model also implies the statement: ‘the probability is 1\n\n2\nthat the true effect in A is less than the true effect in C,’ which also is difficult to justify\ngiven the data in Table 5.2. As in the theoretical discussion of the previous section, neither\nestimate is fully satisfactory, and we would like a compromise that combines information\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 121\n\nFigure 5.5 Marginal posterior density, p(τ |y), for standard deviation of the population of school\neffects θj in the educational testing example.\n\nfrom all eight experiments without assuming all the θj ’s to be equal. The Bayesian analysis\nunder the hierarchical model provides exactly that.\n\nPosterior simulation under the hierarchical model\n\nConsequently, we compute the posterior distribution of θ1, . . . , θ8, based on the normal\nmodel presented in Section 5.4. (More discussion of the reasonableness of applying this\nmodel in this problem appears in Sections 6.5 and 17.4.) We draw from the posterior\ndistribution for the Bayesian model by simulating the random variables τ , µ, and θ, in that\norder, from their posterior distribution, as discussed at the end of the previous section. The\nsampling standard deviations, σj , are assumed known and equal to the values in Table 5.2,\nand we assume independent uniform prior densities on µ and τ .\n\nResults\n\nThe marginal posterior density function, p(τ |y) from (5.21), is plotted in Figure 5.5. Values\nof τ near zero are most plausible; zero is the most likely value, values of τ larger than 10\nare less than half as likely as τ = 0, and Pr(τ > 25) ≈ 0. Inference regarding the marginal\ndistributions of the other model parameters and the joint distribution are obtained from the\nsimulated values. Illustrations are provided in the discussion that follows this section. In\nthe normal hierarchical model, however, we learn a great deal by considering the conditional\nposterior distributions given τ (and averaged over µ).\n\nThe conditional posterior means E(θj |τ, y) (averaging over µ) are displayed as functions\nof τ in Figure 5.6; the vertical axis displays the scale for the θj ’s. Comparing Figure 5.6\nto Figure 5.5, which has the same scale on the horizontal axis, we see that for most of the\nlikely values of τ , the estimated effects are relatively close together; as τ becomes larger,\ncorresponding to more variability among schools, the estimates become more like the raw\nvalues in Table 5.2.\n\nThe lines in Figure 5.7 show the conditional standard deviations, sd(θj |τ, y), as a func-\ntion of τ . As τ increases, the population distribution allows the eight effects to be more\ndifferent from each other, and hence the posterior uncertainty in each individual θj increases,\napproaching the standard deviations in Table 5.2 in the limit of τ → ∞. (The posterior\nmeans and standard deviations for the components θj , given τ , are computed using the\nmean and variance formulas (2.7) and (2.8), averaging over µ; see Exercise 5.12.)\n\nThe general conclusion from an examination of Figures 5.5–5.7 is that an effect as large\nas 28.4 points in any school is unlikely. For the likely values of τ , the estimates in all\nschools are substantially less than 28 points. For example, even at τ = 10, the probability\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n122 5. HIERARCHICAL MODELS\n\nFigure 5.6 Conditional posterior means of treatment effects, E(θj |τ, y), as functions of the between-\nschool standard deviation τ , for the educational testing example. The line for school C crosses the\nlines for E and F because C has a higher measurement error (see Table 5.2) and its estimate is\ntherefore shrunk more strongly toward the overall mean in the Bayesian analysis.\n\nFigure 5.7 Conditional posterior standard deviations of treatment effects, sd(θj |τ, y), as functions\nof the between-school standard deviation τ , for the educational testing example.\n\nthat the effect in school A is less than 28 points is Φ[(28 − 14.5)/9.1] = 93%, where Φ is\nthe standard normal cumulative distribution function; the corresponding probabilities for\nthe effects being less than 28 points in the other schools are 99.5%, 99.2%, 98.5%, 99.96%,\n99.8%, 97%, and 98%.\n\nOf substantial importance, we do not obtain an accurate summary of the data if we\ncondition on the posterior mode of τ . The technique of conditioning on a modal value (for\nexample, the maximum likelihood estimate) of a hyperparameter such as τ is often used\nin practice (at least as an approximation), but it ignores the uncertainty conveyed by the\nposterior distribution of the hyperparameter. At τ = 0, the inference is that all experiments\nhave the same size effect, 7.7 points, and the same standard error, 4.1 points. Figures 5.5–\n5.7 certainly suggest that this answer represents too much pulling together of the estimates\nin the eight schools. The problem is especially acute in this example because the posterior\nmode of τ is on the boundary of its parameter space. A joint posterior modal estimate of\n(θ1, . . . , θJ , µ, τ) suffers from even worse problems in general.\n\nDiscussion\n\nTable 5.3 summarizes the 200 simulated effect estimates for all eight schools. In one sense,\nthese results are similar to the pooled 95% interval [8± 8], in that the eight Bayesian 95%\nintervals largely overlap and are median-centered between 5 and 10. In a second sense,\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.5. EXAMPLE: PARALLEL EXPERIMENTS IN EIGHT SCHOOLS 123\n\nSchool Posterior quantiles\n2.5% 25% median 75% 97.5%\n\nA −2 7 10 16 31\nB −5 3 8 12 23\nC −11 2 7 11 19\nD −7 4 8 11 21\nE −9 1 5 10 18\nF −7 2 6 10 28\nG −1 7 10 15 26\nH −6 3 8 13 33\n\nTable 5.3: Summary of 200 simulations of the treatment effects in the eight schools.\n\nFigure 5.8 Histograms of two quantities of interest computed from the 200 simulation draws: (a)\nthe effect in school A, θ1; (b) the largest effect, max{θj}. The jaggedness of the histograms is just\nan artifact caused by sampling variability from using only 200 random draws.\n\nthe results in the table differ from the pooled estimate in a direction toward the eight\nindependent answers: the 95% Bayesian intervals are each almost twice as wide as the one\ncommon interval and suggest substantially greater probabilities of effects larger than 16\npoints, especially in school A, and greater probabilities of negative effects, especially in\nschool C. If greater precision were required in the posterior intervals, one could simulate\nmore simulation draws; we use only 200 draws here to illustrate that a small simulation\ngives adequate inference for many practical purposes.\n\nThe ordering of the effects in the eight schools as suggested by Table 5.3 is essentially the\nsame as would be obtained by the eight separate estimates. However, there are differences\nin the details; for example, the Bayesian probability that the effect in school A is as large\nas 28 points is less than 10%, which is substantially less than the 50% probability based on\nthe separate estimate for school A.\n\nAs an illustration of the simulation-based posterior results, 200 simulations of school\nA’s effect are shown in Figure 5.8a. Having simulated the parameter θ, it is easy to ask\nmore complicated questions of this model. For example, what is the posterior distribution\nof max{θj}, the effect of the most successful of the eight coaching programs? Figure 5.8b\ndisplays a histogram of 200 values from this posterior distribution and shows that only 22\ndraws are larger than 28.4; thus, Pr(max{θj} > 28.4) ≈ 22\n\n200 . Since Figure 5.8a gives the\nmarginal posterior distribution of the effect in school A, and Figure 5.8b gives the marginal\nposterior distribution of the largest effect no matter which school it is in, the latter figure has\nlarger values. For another example, we can estimate Pr(θ1 > θ3|y), the posterior probability\nthat the coaching program is more effective in school A than in school C, by the proportion\nof simulated draws of θ for which θ1 > θ3; the result is 141\n\n200 = 0.705.\nTo sum up, the Bayesian analysis of this example not only allows straightforward infer-\n\nences about many parameters that may be of interest, but the hierarchical model is flexible\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n124 5. HIERARCHICAL MODELS\n\nStudy,\nRaw data\n\n(deaths/total)\nLog-\nodds,\n\nsd,\nPosterior quantiles of effect θj\n\nnormal approx. (on log-odds scale)\nj Control Treated yj σj 2.5% 25% median 75% 97.5%\n\n1 3/39 3/38 0.028 0.850 −0.57 −0.33 −0.24 −0.16 0.12\n2 14/116 7/114 −0.741 0.483 −0.64 −0.37 −0.28 −0.20 −0.00\n3 11/93 5/69 −0.541 0.565 −0.60 −0.35 −0.26 −0.18 0.05\n4 127/1520 102/1533 −0.246 0.138 −0.45 −0.31 −0.25 −0.19 −0.05\n5 27/365 28/355 0.069 0.281 −0.43 −0.28 −0.21 −0.11 0.15\n6 6/52 4/59 −0.584 0.676 −0.62 −0.35 −0.26 −0.18 0.05\n7 152/939 98/945 −0.512 0.139 −0.61 −0.43 −0.36 −0.28 −0.17\n8 48/471 60/632 −0.079 0.204 −0.43 −0.28 −0.21 −0.13 0.08\n9 37/282 25/278 −0.424 0.274 −0.58 −0.36 −0.28 −0.20 −0.02\n\n10 188/1921 138/1916 −0.335 0.117 −0.48 −0.35 −0.29 −0.23 −0.13\n11 52/583 64/873 −0.213 0.195 −0.48 −0.31 −0.24 −0.17 0.01\n12 47/266 45/263 −0.039 0.229 −0.43 −0.28 −0.21 −0.12 0.11\n13 16/293 9/291 −0.593 0.425 −0.63 −0.36 −0.28 −0.20 0.01\n14 45/883 57/858 0.282 0.205 −0.34 −0.22 −0.12 0.00 0.27\n15 31/147 25/154 −0.321 0.298 −0.56 −0.34 −0.26 −0.19 0.01\n16 38/213 33/207 −0.135 0.261 −0.48 −0.30 −0.23 −0.15 0.08\n17 12/122 28/251 0.141 0.364 −0.47 −0.29 −0.21 −0.12 0.17\n18 6/154 8/151 0.322 0.553 −0.51 −0.30 −0.23 −0.13 0.15\n19 3/134 6/174 0.444 0.717 −0.53 −0.31 −0.23 −0.14 0.15\n20 40/218 32/209 −0.218 0.260 −0.50 −0.32 −0.25 −0.17 0.04\n21 43/364 27/391 −0.591 0.257 −0.64 −0.40 −0.31 −0.23 −0.09\n22 39/674 22/680 −0.608 0.272 −0.65 −0.40 −0.31 −0.23 −0.07\n\nTable 5.4 Results of 22 clinical trials of beta-blockers for reducing mortality after myocardial infarc-\ntion, with empirical log-odds and approximate sampling variances. Data from Yusuf et al. (1985).\nPosterior quantiles of treatment effects are based on 5000 draws from a Bayesian hierarchical model\ndescribed here. Negative effects correspond to reduced probability of death under the treatment.\n\nenough to adapt to the data, thereby providing posterior inferences that account for the\npartial pooling as well as the uncertainty in the hyperparameters.\n\n5.6 Hierarchical modeling applied to a meta-analysis\n\nMeta-analysis is an increasingly popular and important process of summarizing and inte-\ngrating the findings of research studies in a particular area. As a method for combining\ninformation from several parallel data sources, meta-analysis is closely connected to hierar-\nchical modeling. In this section we consider a relatively simple application of hierarchical\nmodeling to a meta-analysis in medicine. We consider another meta-analysis problem in\nthe context of a decision problem in Section 9.2.\n\nThe data in our medical example are displayed in the first three columns of Table 5.4,\nwhich summarize mortality after myocardial infarction in 22 clinical trials, each consisting of\ntwo groups of heart attack patients randomly allocated to receive or not receive beta-blockers\n(a family of drugs that affect the central nervous system and can relax the heart muscles).\nMortality varies from 3% to 21% across the studies, most of which show a modest, though\nnot ‘statistically significant,’ benefit from the use of beta-blockers. The aim of a meta-\nanalysis is to provide a combined analysis of the studies that indicates the overall strength\nof the evidence for a beneficial effect of the treatment under study. Before proceeding to a\nformal meta-analysis, it is important to apply rigorous criteria in determining which studies\nare included. (This relates to concerns of ignorability in data collection for observational\nstudies, as discussed in Chapter 8.)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 125\n\nDefining a parameter for each study\n\nIn the beta-blocker example, the meta-analysis involves data in the form of several 2 × 2\ntables. If clinical trial j (in the series to be considered for meta-analysis) involves the use\nof n0j subjects in the control group and n1j in the treatment group, giving rise to y0j and\ny1j deaths in control and treatment groups, respectively, then the usual sampling model\ninvolves two independent binomial distributions with probabilities of death p0j and p1j ,\nrespectively. Estimands of interest include the difference in probabilities, p1j − p0j , the\nprobability or risk ratio, p1j/p0j , and the odds ratio, ρj =\n\np1j\n1−p1j /\n\np0j\n1−p0j . For a number of\n\nreasons, including interpretability in a range of study designs (including case-control studies\nas well as clinical trials and cohort studies), and the fact that its posterior distribution is\nclose to normality even for relatively small sample sizes, we concentrate on inference for the\n(natural) logarithm of the odds ratio, which we label θj = log ρj .\n\nA normal approximation to the likelihood\n\nRelatively simple Bayesian meta-analysis is possible using the normal-theory results of the\nprevious sections if we summarize the results of each experiment j with an approximate\nnormal likelihood for the parameter θj . This is possible with a number of standard analytic\napproaches that produce a point estimate and standard errors, which can be regarded as\napproximating a normal mean and standard deviation. One approach is based on empirical\nlogits: for each study j, one can estimate θj by\n\nyj = log\n\n(\ny1j\n\nn1j − y1j\n\n)\n− log\n\n(\ny0j\n\nn0j − y0j\n\n)\n, (5.23)\n\nwith approximate sampling variance\n\nσ2\nj =\n\n1\n\ny1j\n+\n\n1\n\nn1j − y1j\n+\n\n1\n\ny0j\n+\n\n1\n\nn0j − y0j\n. (5.24)\n\nWe use the notation yj and σ2\nj to be consistent with our earlier expressions for the hier-\n\narchical normal model. There are various refinements of these estimates that improve the\nasymptotic normality of the sampling distributions involved (in particular, it is often rec-\nommended to add a fraction such as 0.5 to each of the four counts in the 2× 2 table), but\nwhenever study-specific sample sizes are moderately large, such details do not concern us.\n\nThe estimated log-odds ratios yj and their estimated standard errors σ2\nj are displayed\n\nas the fourth and fifth columns of Table 5.4. We use a hierarchical Bayesian analysis to\ncombine information from the 22 studies and gain improved estimates of each θj , along with\nestimates of the mean and variance of the effects over all studies.\n\nGoals of inference in meta-analysis\n\nDiscussions of meta-analysis are sometimes imprecise about the estimands of interest in the\nanalysis, especially when the primary focus is on testing the null hypothesis of no effect in\nany of the studies to be combined. Our focus is on estimating meaningful parameters, and\nfor this objective there appear to be three possibilities, accepting the overarching assumption\nthat the studies are comparable in some broad sense. The first possibility is that we view\nthe studies as identical replications of each other, in the sense we regard the individuals in\nall the studies as independent samples from a common population, with the same outcome\nmeasures and so on. A second possibility is that the studies are so different that the results\nof any one study provide no information about the results of any of the others. A third, more\ngeneral, possibility is that we regard the studies as exchangeable but not necessarily either\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n126 5. HIERARCHICAL MODELS\n\nidentical or completely unrelated; in other words we allow differences from study to study,\nbut such that the differences are not expected a priori to have predictable effects favoring\none study over another. As we have discussed in detail in this chapter, this third possibility\nrepresents a continuum between the two extremes, and it is this exchangeable model (with\nunknown hyperparameters characterizing the population distribution) that forms the basis\nof our Bayesian analysis.\n\nExchangeability does not dictate the form of the joint distribution of the study effects.\nIn what follows we adopt the convenient assumption of a normal distribution for the varying\nparameters; in practice it is important to check this assumption using some of the techniques\ndiscussed in Chapter 6.\n\nThe first potential estimand of a meta-analysis, or a hierarchically structured problem\nin general, is the mean of the distribution of effect sizes, since this represents the overall\n‘average’ effect across all studies that could be regarded as exchangeable with the observed\nstudies. Other possible estimands are the effect size in any of the observed studies and the\neffect size in another, comparable (exchangeable) unobserved study.\n\nWhat if exchangeability is inappropriate?\n\nWhen assuming exchangeability we assume there are no important covariates that might\nform the basis of a more complex model, and this assumption (perhaps misguidedly) is\nwidely adopted in meta-analysis. What if other information (in addition to the data (n, y))\nis available to distinguish among the J studies in a meta-analysis, so that an exchangeable\nmodel is inappropriate? In this situation, we can expand the framework of the model to be\nexchangeable in the observed data and covariates, for example using a hierarchical regression\nmodel, as in Chapter 15, so as to estimate how the treatment effect behaves as a function\nof the covariates. The real aim might in general be to estimate a response surface so that\none could predict an effect based on known characteristics of a population and its exposure\nto risk.\n\nA hierarchical normal model\n\nA normal population distribution in conjunction with the approximate normal sampling\ndistribution of the study-specific effect estimates allows an analysis of the same form as\nused for the SAT coaching example in the previous section. Let yj represent generically the\npoint estimate of the effect θj in the jth study, obtained from (5.23), where j = 1, . . . , J .\nThe first stage of the hierarchical normal model assumes that\n\nyj |θj , σj ∼ N(θj , σ\n2\nj ),\n\nwhere σj represents the corresponding estimated standard error from (5.24), which is as-\nsumed known without error. The simplification of known variances has little effect here\nbecause, with the large sample sizes (more than 50 persons in each treatment group in\nnearly all of the studies in the beta-blocker example), the binomial variances in each study\nare precisely estimated. At the second stage of the hierarchy, we again use an exchangeable\nnormal prior distribution, with mean µ and standard deviation τ , which are unknown hy-\nperparameters. Finally, a hyperprior distribution is required for µ and τ . For this problem,\nit is reasonable to assume a noninformative or locally uniform prior density for µ, since\neven with a small number of studies (say 5 or 10), the combined data become relatively\ninformative about the center of the population distribution of effect sizes. As with the\nSAT coaching example, we also assume a locally uniform prior density for τ , essentially for\nconvenience, although it is easy to modify the analysis to include prior information.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.6. HIERARCHICAL MODELING APPLIED TO A META-ANALYSIS 127\n\nPosterior quantiles\nEstimand 2.5% 25% median 75% 97.5%\n\nMean, µ −0.37 −0.29 −0.25 −0.20 −0.11\nStandard deviation, τ 0.02 0.08 0.13 0.18 0.31\n\nPredicted effect, θ̃j −0.58 −0.34 −0.25 −0.17 0.11\n\nTable 5.5 Summary of posterior inference for the overall mean and standard deviation of study\neffects, and for the predicted effect in a hypothetical future study, from the meta-analysis of the\nbeta-blocker trials in Table 5.4. All effects are on the log-odds scale.\n\nResults of the analysis and comparison to simpler methods\n\nThe analysis of our meta-analysis model now follows exactly the same methodology as in\nthe previous sections. First, a plot (not shown here) similar to Figure 5.5 shows that the\nmarginal posterior density of τ peaks at a nonzero value, although values near zero are\nclearly plausible, zero having a posterior density only about 25% lower than that at the\nmode. Posterior quantiles for the effects θj for the 22 studies on the logit scale are displayed\nas the last columns of Table 5.4.\n\nSince the posterior distribution of τ is concentrated around values that are small relative\nto the sampling standard deviations of the data (compare the posterior median of τ , 0.13,\nin Table 5.5 to the values of σj in the fourth column of Table 5.4), considerable shrinkage\nis evident in the Bayes estimates, especially for studies with low internal precision (for\nexample, studies 1, 6, and 18). The substantial degree of homogeneity between the studies\nis further reflected in the large reductions in posterior variance obtained when going from\nthe study-specific estimates to the Bayesian ones, which borrow strength from each other.\nUsing an approximate approach fixing τ would yield standard deviations that would be too\nsmall compared to the fully Bayesian ones.\n\nHistograms (not shown) of the simulated posterior densities for each of the individual\neffects exhibit skewness away from the central value of the overall mean, whereas the distri-\nbution of the overall mean has greater symmetry. The imprecise studies, such as 2 and 18,\nexhibit longer-tailed posterior distributions than the more precise ones, such as 7 and 14.\n\nIn meta-analysis, interest often focuses on the estimate of the overall mean effect, µ.\nSuperimposing the graphs (not shown here) of the conditional posterior mean and standard\ndeviation of µ given τ on the posterior density of τ reveals a small range in the plausible\nvalues of E(µ|τ, y), from about −0.26 to just over −0.24, but sd(µ|τ, y) varies by a factor\nof more than 2 across the plausible range of values of τ . The latter feature indicates\nthe importance of averaging over τ in order to account adequately for uncertainty in its\nestimation. In fact, the conditional posterior standard deviation, sd(µ|τ, y) has the value\n0.060 at τ = 0.13, whereas upon averaging over the posterior distribution for τ we find a\nvalue of sd(µ|y) = 0.071.\n\nTable 5.5 gives a summary of posterior inferences for the hyperparameters µ and τ and\nthe predicted effect, θ̃j , in a hypothetical future study. The approximate 95% highest pos-\nterior density interval for µ is [−0.37,−0.11], or [0.69, 0.90] when converted to the odds\nratio scale (that is, exponentiated). In contrast, the 95% posterior interval that results\nfrom complete pooling—that is, assuming τ = 0—is considerably narrower, [0.70, 0.85]. In\nthe original published discussion of these data, it was remarked that the latter seems an\n‘unusually narrow range of uncertainty.’ The hierarchical Bayesian analysis suggests that\nthis was due to the use of an inappropriate model that had the effect of claiming all the\nstudies were identical. In mathematical terms, complete pooling makes the assumption that\nthe parameter τ is exactly zero, whereas the data supply evidence that τ might be close\nto zero, but might also plausibly be as high as 0.3. A related concern is that commonly\nused analyses tend to place undue emphasis on inference for the overall mean effect. Un-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n128 5. HIERARCHICAL MODELS\n\ncertainty about the probable treatment effect in a particular population where a study has\nnot been performed (or indeed in a previously studied population but with a slightly mod-\nified treatment) might be more reasonably represented by inference for a new study effect,\nexchangeable with those for which studies have been performed, rather than for the overall\nmean. In this case, uncertainty is even greater, as exhibited in the ‘Predicted effect’ row of\nTable 5.5; uncertainty for an individual patient includes yet another component of varia-\ntion. In particular, with the beta-blocker data, there is just over 10% posterior probability\nthat the true effect, θ̃j , in a new study would be positive (corresponding to the treatment\nincreasing the probability of death in that study).\n\n5.7 Weakly informative priors for variance parameters\n\nA key element in the analyses above is the prior distribution for the scale parameter, τ .\nWe have used the uniform, but various other noninformative prior distributions have been\nsuggested in the Bayesian literature. It turns out that the choice of ‘noninformative’ prior\ndistribution can have a big effect on inferences, especially for problems where the number\nof groups J is small or the group-level variation τ is small.\n\nWe discuss the options here in the context of the normal model, but the principles apply\nto inferences for group-level variances more generally.\n\nConcepts relating to the choice of prior distribution\n\nImproper limit of a prior distribution. Improper prior densities can, but do not necessarily,\nlead to proper posterior distributions. To avoid confusion it is useful to define improper\ndistributions as particular limits of proper distributions. For the group-level variance pa-\nrameter, two commonly considered improper densities are uniform(0, A) on τ , as A → ∞,\nand inverse-gamma(ǫ, ǫ) on τ2, as ǫ→ 0.\n\nAs we shall see, the uniform(0, A) model yields a limiting proper posterior density as\nA→ ∞, as long as the number of groups J is at least 3. Thus, for a finite but sufficiently\nlarge A, inferences are not sensitive to the choice of A.\n\nIn contrast, the inverse-gamma(ǫ, ǫ) model does not have any proper limiting poste-\nrior distribution. As a result, posterior inferences are sensitive to ǫ—it cannot simply be\ncomfortably set to a low value such as 0.001.\n\nCalibration. Posterior inferences can be evaluated using the concept of calibration of the\nposterior mean, the Bayesian analogue to the classical notion of bias. For any parameter\nθ, if we label the posterior mean as θ̂ = E(θ|y), we can define the miscalibration of the\n\nposterior mean as E(θ|θ̂) − θ̂. If the prior distribution is true—that is, if the data are\nconstructed by first drawing θ from p(θ), then drawing y from p(y|θ)—then the posterior\n\nmean is automatically calibrated; that is, the miscalibration is 0 for all values of θ̂.\nTo restate: in classical bias analysis, we condition on the true θ and look at the distri-\n\nbution of the data-based estimate, θ̂. In a Bayesian calibration analysis, we condition on\nthe data y (and thus also on the estimate, θ̂) and look at the distribution of parameters θ\nthat could have produced these data.\n\nWhen considering improper models, the theory must be expanded, since it is impossible\nfor θ to be drawn from an unnormalized density. To evaluate calibration in this context,\nit is necessary to posit a ‘true prior distribution’ from which θ is drawn along with the\n‘inferential prior distribution’ that is used in the Bayesian inference.\n\nFor the hierarchical model for the 8 schools, we can consider the improper uniform\ndensity on τ as a limit of uniform prior densities on the range (0, A), with A → ∞. For\nany finite value of A, we can then see that the improper uniform density leads to inferences\nwith a positive miscalibration—that is, overestimates (on average) of τ .\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 129\n\nWe demonstrate this miscalibration in two steps. First, suppose that both the true\nand inferential prior distributions for τ are uniform on (0, A). Then the miscalibration is\ntrivially zero. Now keep the true prior distribution at U(0, A) and let the inferential prior\n\ndistribution go to U(0,∞). This will necessarily increase θ̂ for any data y (since we are now\naveraging over values of θ in the range [A,∞)) without changing the true θ, thus causing\nthe average value of the miscalibration to become positive.\n\nClasses of noninformative and weakly informative prior distributions for hierarchical\nvariance parameters\n\nGeneral considerations. We view any noninformative or weakly informative prior distribu-\ntion as inherently provisional—after the model has been fit, one should look at the posterior\ndistribution and see if it makes sense. If the posterior distribution does not make sense,\nthis implies that additional prior knowledge is available that has not been included in the\nmodel, and that contradicts the assumptions of the prior distribution that has been used.\nIt is then appropriate to go back and alter the prior distribution to be more consistent with\nthis external knowledge.\n\nUniform prior distributions. We first consider uniform priors while recognizing that we\nmust be explicit about the scale on which the distribution is defined. Various choices have\nbeen proposed for modeling variance parameters. A uniform prior distribution on log τ\nwould seem natural—working with the logarithm of a parameter that must be positive—\nbut it results in an improper posterior distribution. An alternative would be to define the\nprior distribution on a compact set (e.g., in the range [−A,A] for some large value of A),\nbut then the posterior distribution would depend strongly on the lower bound −A of the\nprior support.\n\nThe problem arises because the marginal likelihood, p(y|τ)—after integrating over θ and\nµ in (5.16)—approaches a finite nonzero value as τ → 0. Thus, if the prior density for log τ\nis uniform, the posterior will have infinite mass integrating to the limit log τ → −∞. To put\nit another way, in a hierarchical model the data can never rule out a group-level variance\nof zero, and so the prior distribution cannot put an infinite mass in this area.\n\nAnother option is a uniform prior distribution on τ itself, which has a finite integral\nnear τ = 0 and thus avoids the above problem. We have generally used this noninformative\ndensity in our applied work (as illustrated in Section 5.5), but it has a slightly disagreeable\nmiscalibration toward positive values, with its infinite prior mass in the range τ → ∞.\nWith J = 1 or 2 groups, this actually results in an improper posterior density, essentially\nconcluding τ = ∞ and doing no pooling. In a sense this is reasonable behavior, since it\nwould seem difficult from the data alone to decide how much, if any, pooling should be\ndone with data from only one or two groups. However, from a Bayesian perspective it is\nawkward for the decision to be made ahead of time, as it were, with the data having no say\nin the matter. In addition, for small J , such as 4 or 5, we worry that the heavy right tail of\nthe posterior distribution would lead to overestimates of τ and thus result in pooling that\nis less than optimal for estimating the individual θj ’s.\n\nWe can interpret these improper uniform prior densities as limits of weakly informative\nconditionally conjugate priors. The uniform prior distribution on log τ is equivalent to\np(τ) ∝ τ−1 or p(τ2) ∝ τ−2, which has the form of an inverse-χ2 density with 0 degrees of\nfreedom and can be taken as a limit of proper inverse-gamma priors.\n\nThe uniform density on τ is equivalent to p(τ2) ∝ τ−1, an inverse-χ2 density with −1\ndegrees of freedom. This density cannot easily be seen as a limit of proper inverse-χ2\n\ndensities (since these must have positive degrees of freedom), but it can be interpreted as a\nlimit of the half-t family on τ , where the scale approaches ∞ (and any value of ν).\n\nAnother noninformative prior distribution sometimes proposed in the Bayesian literature\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n130 5. HIERARCHICAL MODELS\n\nis uniform on τ2. We do not recommend this, as it seems to have the miscalibration toward\nhigher values as described above, but more so, and also requires J ≥ 4 groups for a proper\nposterior distribution.\n\nInverse-gamma(ǫ, ǫ) prior distributions. The parameter τ in model (5.21) does not have\nany simple family of conjugate prior distributions because its marginal likelihood depends\nin a complex way on the data from all J groups. However, the inverse-gamma family\nis conditionally conjugate given the other parameters in the model: that is, if τ2 has an\ninverse-gamma prior distribution, then the conditional posterior distribution p(τ2 | θ, µ, y)\nis also inverse-gamma. The inverse-gamma(α, β) model for τ2 can also be expressed as an\ninverse-χ2 distribution with scale s2 = β\n\nα and degrees of freedom ν = 2α. The inverse-\nχ2 parameterization can be helpful in understanding the information underlying various\nchoices of proper prior distributions.\n\nThe inverse-gamma(ǫ, ǫ) prior distribution is an attempt at noninformativeness within\nthe conditionally conjugate family, with ǫ set to a low value such as 1 or 0.01 or 0.001.\nA difficulty of this prior distribution is that in the limit of ǫ → 0 it yields an improper\nposterior density, and thus ǫ must be set to a reasonable value. Unfortunately, for datasets\nin which low values of τ are possible, inferences become very sensitive to ǫ in this model,\nand the prior distribution hardly looks noninformative, as we illustrate in Figure 5.9.\n\nHalf-Cauchy prior distributions. We shall also consider the t family of distributions (actu-\nally, the half-t, since the scale parameter τ is constrained to be positive) as an alternative\nclass that includes normal and Cauchy as edge cases. We first considered the t model for\nthis problem because it can be expressed as a conditionally conjugate prior distribution for\nτ using a reparameterization.\n\nFor our purposes here, however, it is enough to recognize that the half-Cauchy can be a\nconvenient weakly informative family; the distribution has a broad peak at zero and a single\nscale parameter, which we shall label A to indicate that it could be set to some large value.\nIn the limit A → ∞ this becomes a uniform prior density on τ . Large but finite values of\nA represent prior distributions which we consider weakly informative because, even in the\ntail, they have a gentle slope (unlike, for example, a half-normal distribution) and can let\nthe data dominate if the likelihood is strong in that region. We shall consider half-Cauchy\nmodels for variance parameters which are estimated from a small number of groups (so that\ninferences are sensitive to the choice of weakly informative prior distribution).\n\nApplication to the 8-schools example\n\nWe demonstrate the properties of some proposed noninformative prior densities on the\neight-schools example of Section 5.5. Here, the parameters θ1, . . . , θ8 represent the relative\neffects of coaching programs in eight different schools, and τ represents the between-school\nstandard deviations of these effects. The effects are measured as points on the test, which\nwas scored from 200 to 800 with an average of about 500; thus the largest possible range of\neffects could be about 300 points, with a realistic upper limit on τ of 100, say.\n\nNoninformative prior distributions for the 8-schools problem. Figure 5.9 displays the pos-\nterior distributions for the 8-schools model resulting from three different choices of prior\ndistributions that are intended to be noninformative.\n\nThe leftmost histogram shows posterior inference for τ for the model with uniform prior\ndensity. The data show support for a range of values below τ = 20, with a slight tail after\nthat, reflecting the possibility of larger values, which are difficult to rule out given that the\nnumber of groups J is only 8—that is, not much more than the J = 3 required to ensure a\nproper posterior density with finite mass in the right tail.\n\nIn contrast, the middle histogram in Figure 5.9 shows the result with an inverse-\ngamma(1, 1) prior distribution for τ2. This new prior distribution leads to changed in-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.7. WEAKLY INFORMATIVE PRIORS FOR VARIANCE PARAMETERS 131\n\nFigure 5.9 Histograms of posterior simulations of the between-school standard deviation, τ ,\nfrom models with three different prior distributions: (a) uniform prior distribution on τ , (b)\ninverse-gamma(1, 1) prior distribution on τ 2, (c) inverse-gamma(0.001, 0.001) prior distribution\non τ 2. Overlain on each is the corresponding prior density function for τ . (For models (b) and\n(c), the density for τ is calculated using the gamma density function multiplied by the Jacobian of\nthe 1/τ 2 transformation.) In models (b) and (c), posterior inferences are strongly constrained by\nthe prior distribution.\n\nferences. In particular, the posterior mean and median of τ are lower, and shrinkage of the\nθj ’s is greater than in the previously fitted model with a uniform prior distribution on τ . To\nunderstand this, it helps to graph the prior distribution in the range for which the posterior\ndistribution is substantial. The graph shows that the prior distribution is concentrated in\nthe range [0.5, 5], a narrow zone in which the likelihood is close to flat compared to this prior\n(as we can see because the distribution of the posterior simulations of τ closely matches the\nprior distribution, p(τ)). By comparison, in the left graph, the uniform prior distribution\non τ seems closer to ‘noninformative’ for this problem, in the sense that it does not appear\nto be constraining the posterior inference.\n\nFinally, the rightmost histogram in Figure 5.9 shows the corresponding result with an\ninverse-gamma(0.001, 0.001) prior distribution for τ2. This prior distribution is even more\nsharply peaked near zero and further distorts posterior inferences, with the problem arising\nbecause the marginal likelihood for τ remains high near zero.\n\nIn this example, we do not consider a uniform prior density on log τ , which would yield\nan improper posterior density with a spike at τ = 0, like the rightmost graph in Figure 5.9\nbut more so. We also do not consider a uniform prior density on τ2, which would yield a\nposterior similar to the leftmost graph in Figure 5.9, but with a slightly higher right tail.\n\nThis example is a gratifying case in which the simplest approach—the uniform prior\ndensity on τ—seems to perform well. As detailed in Appendix C, this model is also straight-\nforward to program directly in R or Stan.\n\nThe appearance of the histograms and density plots in Figure 5.9 is crucially affected by\nthe choice to plot them on the scale of τ . If instead they were plotted on the scale of log τ ,\nthe inverse-gamma(0.001, 0.001) prior density would appear to be the flattest. However, the\ninverse-gamma(ǫ, ǫ) prior is not at all ‘noninformative’ for this problem since the resulting\nposterior distribution remains highly sensitive to the choice of ǫ. The hierarchical model\nlikelihood does not constrain log τ in the limit log τ → −∞, and so a prior distribution that\nis noninformative on the log scale will not work.\n\nWeakly informative prior distribution for the 3-schools problem\n\nThe uniform prior distribution seems fine for the 8-school analysis, but problems arise if the\nnumber of groups J is much smaller, in which case the data supply little information about\nthe group-level variance, and a noninformative prior distribution can lead to a posterior\ndistribution that is improper or is proper but unrealistically broad. We demonstrate by\nreanalyzing the 8-schools example using just the data from the first three of the schools.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n132 5. HIERARCHICAL MODELS\n\nFigure 5.10 Histograms of posterior simulations of the between-school standard deviation, τ , from\nmodels for the 3-schools data with two different prior distributions on τ : (a) uniform (0,∞), (b)\nhalf-Cauchy with scale 25, set as a weakly informative prior distribution given that τ was expected\nto be well below 100. The histograms are not on the same scales. Overlain on each histogram is\nthe corresponding prior density function. With only J = 3 groups, the noninformative uniform\nprior distribution is too weak, and the proper Cauchy distribution works better, without appearing\nto distort inferences in the area of high likelihood.\n\nFigure 5.10 displays the inferences for τ based on two different priors. First we continue\nwith the default uniform distribution that worked well with J = 8 (as seen in Figure 5.9).\nUnfortunately, as the left histogram of Figure 5.10 shows, the resulting posterior distribution\nfor the 3-schools dataset has an extremely long right tail, containing values of τ that are\ntoo high to be reasonable. This heavy tail is expected since J is so low (if J were any lower,\nthe right tail would have an infinite integral), and using this as a posterior distribution will\nhave the effect of underpooling the estimates of the school effects θj .\n\nThe right histogram of Figure 5.10 shows the posterior inference for τ resulting from\na half-Cauchy prior distribution with scale parameter A = 25 (a value chosen to be a bit\nhigher than we expect for the standard deviation of the underlying θj ’s in the context of\nthis educational testing example, so that the model will constrain τ only weakly). As the\nline on the graph shows, this prior distribution is high over the plausible range of τ < 50,\nfalling off gradually beyond this point. This prior distribution appears to perform well in\nthis example, reflecting the marginal likelihood for τ at its low end but removing much of\nthe unrealistic upper tail.\n\nThis half-Cauchy prior distribution would also perform well in the 8-schools problem;\nhowever it was unnecessary because the default uniform prior gave reasonable results. With\nonly 3 schools, we went to the trouble of using a weakly informative prior, a distribution\nthat was not intended to represent our actual prior state of knowledge about τ but rather\nto constrain the posterior distribution, to an extent allowed by the data.\n\n5.8 Bibliographic note\n\nThe early non-Bayesian work on shrinkage estimation of Stein (1955) and James and Stein\n(1960) was influential in the development of hierarchical normal models. Efron and Morris\n(1971, 1972) present subsequent theoretical work on the topic. Robbins (1955, 1964) con-\nstructs and justifies hierarchical methods from a decision-theoretic perspective. De Finetti’s\ntheorem is described by de Finetti (1974); Bernardo and Smith (1994) discuss its role in\nBayesian modeling. An early thorough development of the idea of Bayesian hierarchical\nmodeling is given by Good (1965).\n\nMosteller and Wallace (1964) analyzed a hierarchical Bayesian model using the negative\nbinomial distribution for counts of words in a study of authorship. Restricted to the limited\ncomputing power at the time, they used various approximations and point estimates for\nhyperparameters.\n\nOther historically influential papers on ‘empirical Bayes’ (or, in our terminology, hierar-\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.8. BIBLIOGRAPHIC NOTE 133\n\nchical Bayes) include Hartley and Rao (1967), Laird and Ware (1982) on longitudinal mod-\neling, and Clayton and Kaldor (1987) and Breslow (1990) on epidemiology and biostatistics.\nMorris (1983) and Deely and Lindley (1981) explored the relation between Bayesian and\nnon-Bayesian ideas for these models.\n\nThe problem of estimating several normal means using an exchangeable hierarchical\nmodel was treated in a fully Bayesian framework by Hill (1965), Tiao and Tan (1965,\n1966), and Lindley (1971b). Box and Tiao (1973) present hierarchical normal models using\nslightly different notation from ours. They compare Bayesian and non-Bayesian methods\nand discuss the analysis of variance table in some detail. More references on hierarchical\nnormal models appear in the bibliographic note at the end of Chapter 15.\n\nThe past few decades have seen the publication of applied Bayesian analyses using hierar-\nchical models in a wide variety of application areas. For example, an important application\nof hierarchical models is ‘small-area estimation,’ in which estimates of population charac-\nteristics for local areas are improved by combining the data from each area with information\nfrom neighboring areas (with important early work from Fay and Herriot, 1979, Dempster\nand Raghunathan, 1987, and Mollie and Richardson, 1991). Other applications that have\nmotivated methodological development include measurement error problems in epidemiol-\nogy (for example, Richardson and Gilks, 1993), multiple comparisons in toxicology (Meng\nand Dempster, 1987), and education research (Bock, 1989). We provide references to a\nnumber of other applications in later chapters dealing with specific model types.\n\nHierarchical models can be viewed as a subclass of ‘graphical models,’ and this connec-\ntion has been elegantly exploited for Bayesian inference in the development of the computer\npackage Bugs, using techniques that will be explained in Chapter 11 (see also Appendix C);\nsee Thomas, Spiegelhalter, and Gilks (1992), and Spiegelhalter et al. (1994, 2003). Related\ndiscussion and theoretical work appears in Lauritzen and Spiegelhalter (1988), Pearl (1988),\nWermuth and Lauritzen (1990), and Normand and Tritchler (1992).\n\nThe rat tumor data were analyzed hierarchically by Tarone (1982) and Dempster, Sel-\nwyn, and Weeks (1983); our approach is close in spirit to the latter paper’s. Leonard (1972)\nand Novick, Lewis, and Jackson (1973) are early examples of hierarchical Bayesian analysis\nof binomial data.\n\nMuch of the material in Sections 5.4 and 5.5, along with much of Section 6.5, originally\nappeared in Rubin (1981a), which is an early example of an applied Bayesian analysis using\nsimulation techniques. For later work on the effects of coaching on Scholastic Aptitude Test\nscores, see Hansen (2004).\n\nThe weakly-informative half-Cauchy prior distribution for the 3-schools problem in Sec-\ntion 5.7 comes from Gelman (2006a). Polson and Scott (2012) provide a theoretical justifi-\ncation for this model.\n\nThe material of Section 5.6 is adapted from Carlin (1992), which contains several key\nreferences on meta-analysis; the original data for the example are from Yusuf et al. (1985);\na similar Bayesian analysis of these data under a slightly different model appears as an\nexample in Spiegelhalter et al. (1994, 2003). Thall et al. (2003) discuss hierarchical models\nfor medical treatments that vary across subtypes of a disease. More general treatments\nof meta-analysis from a Bayesian perspective are provided by DuMouchel (1990), Rubin\n(1989), Skene and Wakefield (1990), and Smith, Spiegelhalter, and Thomas (1995). An ex-\nample of a Bayesian meta-analysis appears in Dominici et al. (1999). DuMouchel and Harris\n(1983) present what is essentially a meta-analysis with covariates on the studies; this article\nis accompanied by some interesting discussion by prominent Bayesian and non-Bayesian\nstatisticians. Higgins and Whitehead (1996) discuss how to construct a prior distribution\nfor the group-level variance in a meta-analysis by considering it as an example from larger\npopulation of meta-analyses. Lau, Ioannidis, and Schmid (1997) provide practical advice\non meta-analysis.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n134 5. HIERARCHICAL MODELS\n\n5.9 Exercises\n\n1. Exchangeability with known model parameters: For each of the following three examples,\nanswer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and y2\nindependent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has one black ball and one white ball. We pick a ball y1 at random, put it back,\nand pick another ball y2 at random.\n\n(b) A box has one black ball and one white ball. We pick a ball y1 at random, we do not\nput it back, then we pick ball y2.\n\n(c) A box has a million black balls and a million white balls. We pick a ball y1 at random,\nwe do not put it back, then we pick ball y2 at random.\n\n2. Exchangeability with unknown model parameters: For each of the following three exam-\nples, answer: (i) Are observations y1 and y2 exchangeable? (ii) Are observations y1 and\ny2 independent? (iii) Can we act as if the two observations are independent?\n\n(a) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, put it back, and pick another ball y2 at random.\n\n(b) A box has n black and white balls but we do not know how many of each color. We\npick a ball y1 at random, we do not put it back, then we pick ball y2 at random.\n\n(c) Same as (b) but we know that there are many balls of each color in the box.\n\n3. Hierarchical models and multiple comparisons:\n\n(a) Reproduce the computations in Section 5.5 for the educational testing example. Use\nthe posterior simulations to estimate (i) for each school j, the probability that its\ncoaching program is the best of the eight; and (ii) for each pair of schools, j and k,\nthe probability that the coaching program in school j is better than that in school k.\n\n(b) Repeat (a), but for the simpler model with τ set to ∞ (that is, separate estimation\nfor the eight schools). In this case, the probabilities (ii) can be computed analytically.\n\n(c) Discuss how the answers in (a) and (b) differ.\n\n(d) In the model with τ set to 0, the probabilities (i) and (ii) have degenerate values; what\nare they?\n\n4. Exchangeable prior distributions: suppose it is known a priori that the 2J parameters\nθ1, . . . , θ2J are clustered into two groups, with exactly half being drawn from a N(1, 1)\ndistribution, and the other half being drawn from a N(−1, 1) distribution, but we have\nnot observed which parameters come from which distribution.\n\n(a) Are θ1, . . . , θ2J exchangeable under this prior distribution?\n\n(b) Show that this distribution cannot be written as a mixture of independent and iden-\ntically distributed components.\n\n(c) Why can we not simply take the limit as J → ∞ and get a counterexample to de\nFinetti’s theorem?\n\nSee Exercise 8.10 for a related problem.\n\n5. Mixtures of independent distributions: suppose the distribution of θ = (θ1, . . . , θJ) can\nbe written as a mixture of independent and identically distributed components:\n\np(θ) =\n\n∫ J∏\n\nj=1\n\np(θj |φ)p(φ)dφ.\n\nProve that the covariances cov(θi, θj) are all nonnegative.\n\n6. Exchangeable models:\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 135\n\n(a) In the divorce rate example of Section 5.2, set up a prior distribution for the values\ny1, . . . , y8 that allows for one low value (Utah) and one high value (Nevada), with\nindependent and identical distributions for the other six values. This prior distribution\nshould be exchangeable, because it is not known which of the eight states correspond\nto Utah and Nevada.\n\n(b) Determine the posterior distribution for y8 under this model given the observed values\nof y1, . . . , y7 given in the example. This posterior distribution should probably have\ntwo or three modes, corresponding to the possibilities that the missing state is Utah,\nNevada, or one of the other six.\n\n(c) Now consider the entire set of eight data points, including the value for y8 given at\nthe end of the example. Are these data consistent with the prior distribution you gave\nin part (a) above? In particular, did your prior distribution allow for the possibility\nthat the actual data have an outlier (Nevada) at the high end, but no outlier at the\nlow end?\n\n7. Continuous mixture models:\n\n(a) If y|θ ∼ Poisson(θ), and θ ∼ Gamma(α, β), then the marginal (prior predictive)\ndistribution of y is negative binomial with parameters α and β (or p = β/(1 + β)).\nUse the formulas (2.7) and (2.8) to derive the mean and variance of the negative\nbinomial.\n\n(b) In the normal model with unknown location and scale (µ, σ2), the noninformative\nprior density, p(µ, σ2) ∝ 1/σ2, results in a normal-inverse-χ2 posterior distribution for\n(µ, σ2). Marginally then\n\n√\nn(µ − y)/s has a posterior distribution that is tn−1. Use\n\n(2.7) and (2.8) to derive the first two moments of the latter distribution, stating the\nappropriate condition on n for existence of both moments.\n\n8. Discrete mixture models: if pm(θ), for m = 1, . . . ,M , are conjugate prior densities for\nthe sampling model y|θ, show that the class of finite mixture prior densities given by\n\np(θ) =\n\nM∑\n\nm=1\n\nλmpm(θ)\n\nis also a conjugate class, where the λm’s are nonnegative weights that sum to 1. This\ncan provide a useful extension of the natural conjugate prior family to more flexible\ndistributional forms. As an example, use the mixture form to create a bimodal prior\ndensity for a normal mean, that is thought to be near 1, with a standard deviation of\n0.5, but has a small probability of being near −1, with the same standard deviation. If\nthe variance of each observation y1, . . . , y10 is known to be 1, and their observed mean\nis y = −0.25, derive your posterior distribution for the mean, making a sketch of both\nprior and posterior densities. Be careful: the prior and posterior mixture proportions are\ndifferent.\n\n9. Noninformative hyperprior distributions: consider the hierarchical binomial model in\nSection 5.3. Improper posterior distributions are, in fact, a general problem with hier-\narchical models when a uniform prior distribution is specified for the logarithm of the\npopulation standard deviation of the exchangeable parameters. In the case of the beta\npopulation distribution, the prior variance is approximately (α+β)−1 (see Appendix A),\nand so a uniform distribution on log(α+β) is approximately uniform on the log standard\ndeviation. The resulting unnormalized posterior density (5.8) has an infinite integral\nin the limit as the population standard deviation approaches 0. We encountered the\nproblem again in Section 5.4 for the hierarchical normal model.\n\n(a) Show that, with a uniform prior density on (log(αβ ), log(α+β)), the unnormalized\nposterior density has an infinite integral.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n136 5. HIERARCHICAL MODELS\n\n(b) A simple way to avoid the impropriety is to assign a uniform prior distribution to the\nstandard deviation parameter itself, rather than its logarithm. For the beta population\ndistribution we are considering here, this is achieved approximately by assigning a uni-\nform prior distribution to (α+β)−1/2. Show that combining this with an independent\nuniform prior distribution on α\n\nα+β yields the prior density (5.10).\n\n(c) Show that the resulting posterior density (5.8) is proper as long as 0 < yj < nj for at\nleast one experiment j.\n\n10. Checking the integrability of the posterior distribution: consider the hierarchical normal\nmodel in Section 5.4.\n\n(a) If the hyperprior distribution is p(µ, τ) ∝ τ−1 (that is, p(µ, log τ) ∝ 1), show that the\nposterior density is improper.\n\n(b) If the hyperprior distribution is p(µ, τ) ∝ 1, show that the posterior density is proper\nif J > 2.\n\n(c) How would you analyze SAT coaching data if J = 2 (that is, data from only two\nschools)?\n\n11. Nonconjugate hierarchical models: suppose that in the rat tumor example, we wish to\nuse a normal population distribution on the log-odds scale: logit(θj) ∼ N(µ, τ2), for\nj = 1, . . . , J . As in Section 5.3, you will assign a noninformative prior distribution to the\nhyperparameters and perform a full Bayesian analysis.\n\n(a) Write the joint posterior density, p(θ, µ, τ |y).\n(b) Show that the integral (5.4) has no closed-form expression.\n\n(c) Why is expression (5.5) no help for this problem?\n\nIn practice, we can solve this problem by normal approximation, importance sampling,\nand Markov chain simulation, as described in Part III.\n\n12. Conditional posterior means and variances: derive analytic expressions for E(θj |τ, y) and\nvar(θj |τ, y) in the hierarchical normal model (and used in Figures 5.6 and 5.7). (Hint:\nuse (2.7) and (2.8), averaging over µ.)\n\n13. Hierarchical binomial model: Exercise 3.8 described a survey of bicycle traffic in Berkeley,\nCalifornia, with data displayed in Table 3.3. For this problem, restrict your attention to\nthe first two rows of the table: residential streets labeled as ‘bike routes,’ which we will\nuse to illustrate this computational exercise.\n\n(a) Set up a model for the data in Table 3.3 so that, for j = 1, . . . , 10, the observed number\nof bicycles at location j is binomial with unknown probability θj and sample size equal\nto the total number of vehicles (bicycles included) in that block. The parameter θj\ncan be interpreted as the underlying or ‘true’ proportion of traffic at location j that is\nbicycles. (See Exercise 3.8.) Assign a beta population distribution for the parameters\nθj and a noninformative hyperprior distribution as in the rat tumor example of Section\n5.3. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and draw simulations\nfrom the joint posterior distribution of the parameters and hyperparameters, as in\nSection 5.3.\n\n(c) Compare the posterior distributions of the parameters θj to the raw proportions,\n(number of bicycles / total number of vehicles) in location j. How do the inferences\nfrom the posterior distribution differ from the raw proportions?\n\n(d) Give a 95% posterior interval for the average underlying proportion of traffic that is\nbicycles.\n\n(e) A new city block is sampled at random and is a residential street with a bike route. In\nan hour of observation, 100 vehicles of all kinds go by. Give a 95% posterior interval\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n5.9. EXERCISES 137\n\nfor the number of those vehicles that are bicycles. Discuss how much you trust this\ninterval in application.\n\n(f) Was the beta distribution for the θj ’s reasonable?\n\n14. Hierarchical Poisson model: consider the dataset in the previous problem, but suppose\nonly the total amount of traffic at each location is observed.\n\n(a) Set up a model in which the total number of vehicles observed at each location j\nfollows a Poisson distribution with parameter θj , the ‘true’ rate of traffic per hour at\nthat location. Assign a gamma population distribution for the parameters θj and a\nnoninformative hyperprior distribution. Write down the joint posterior distribution.\n\n(b) Compute the marginal posterior density of the hyperparameters and plot its contours.\nSimulate random draws from the posterior distribution of the hyperparameters and\nmake a scatterplot of the simulation draws.\n\n(c) Is the posterior density integrable? Answer analytically by examining the joint pos-\nterior density at the limits or empirically by examining the plots of the marginal\nposterior density above.\n\n(d) If the posterior density is not integrable, alter it and repeat the previous two steps.\n\n(e) Draw samples from the joint posterior distribution of the parameters and hyperpa-\nrameters, by analogy to the method used in the hierarchical binomial model.\n\n15. Meta-analysis: perform the computations for the meta-analysis data of Table 5.4.\n\n(a) Plot the posterior density of τ over an appropriate range that includes essentially all\nof the posterior density, analogous to Figure 5.5.\n\n(b) Produce graphs analogous to Figures 5.6 and 5.7 to display how the posterior means\nand standard deviations of the θj ’s depend on τ .\n\n(c) Produce a scatterplot of the crude effect estimates vs. the posterior median effect\nestimates of the 22 studies. Verify that the studies with smallest sample sizes are\npartially pooled the most toward the mean.\n\n(d) Draw simulations from the posterior distribution of a new treatment effect, θ̃j . Plot\na histogram of the simulations.\n\n(e) Given the simulations just obtained, draw simulated outcomes from replications of\na hypothetical new experiment with 100 persons in each of the treated and control\ngroups. Plot a histogram of the simulations of the crude estimated treatment effect\n(5.23) in the new experiment.\n\n16. Equivalent data: Suppose we wish to apply the inferences from the meta-analysis example\nin Section 5.6 to data on a new study with equal numbers of people in the control and\ntreatment groups. How large would the study have to be so that the prior and data were\nweighted equally in the posterior inference for that study?\n\n17. Informative prior distributions: Continuing the example from Exercise 2.22, consider a\n(hypothetical) study of a simple training program for basketball free-throw shooting. A\nrandom sample of 100 college students is recruited into the study. Each student first\nshoots 100 free-throws to establish a baseline success probability. Each student then\ntakes 50 practice shots each day for a month. At the end of that time, he or she takes\n100 shots for a final measurement.\nLet θi be the improvement in success probability for person i. For simplicity, assume the\nθi’s are normally distributed with mean µ and standard deviation σ.\nGive three joint prior distributions for µ, σ:\n\n(a) A noninformative prior distribution,\n\n(b) A subjective prior distribution based on your best knowledge, and\n\n(c) A weakly informative prior distribution.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nPart II: Fundamentals of Bayesian Data\n\nAnalysis\n\nFor most problems of applied Bayesian statistics, the data analyst must go beyond the\nsimple structure of prior distribution, likelihood, and posterior distribution. In Chapter 6,\nwe discuss methods of assessing the sensitivity of posterior inferences to model assumptions\nand checking the fit of a probability model to data and substantive information. Model\nchecking allows an escape from the tautological aspect of formal approaches to Bayesian\ninference, under which all conclusions are conditional on the truth of the posited model.\nChapter 7 considers evaluating and comparing models using predictive accuracy, adjusting\nfor the parameters being fit to the data. Chapter 8 outlines the role of study design and\nmethods of data collection in probability modeling, focusing on how to set up Bayesian\ninference for sample surveys, designed experiments, and observational studies; this chapter\ncontains some of the most conceptually distinctive and potentially difficult material in\nthe book. Chapter 9 discusses the use of Bayesian inference in applied decision analysis,\nillustrating with examples from social science, medicine, and public health. These four\nchapters explore the creative choices that are required, first to set up a Bayesian model in\na complex problem, then to perform the model checking and confidence building that is\ntypically necessary to make posterior inferences scientifically defensible, and finally to use\nthe inferences in decision making.\n\n139\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\nChapter 6\n\nModel checking\n\n6.1 The place of model checking in applied Bayesian statistics\n\nOnce we have accomplished the first two steps of a Bayesian analysis—constructing a prob-\nability model and computing the posterior distribution of all estimands—we should not\nignore the relatively easy step of assessing the fit of the model to the data and to our\nsubstantive knowledge. It is difficult to include in a probability distribution all of one’s\nknowledge about a problem, and so it is wise to investigate what aspects of reality are not\ncaptured by the model.\n\nChecking the model is crucial to statistical analysis. Bayesian prior-to-posterior infer-\nences assume the whole structure of a probability model and can yield misleading inferences\nwhen the model is poor. A good Bayesian analysis, therefore, should include at least some\ncheck of the adequacy of the fit of the model to the data and the plausibility of the model\nfor the purposes for which the model will be used. This is sometimes discussed as a problem\nof sensitivity to the prior distribution, but in practice the likelihood model is typically just\nas suspect; throughout, we use ‘model’ to encompass the sampling distribution, the prior\ndistribution, any hierarchical structure, and issues such as which explanatory variables have\nbeen included in a regression.\n\nSensitivity analysis and model improvement\n\nIt is typically the case that more than one reasonable probability model can provide an\nadequate fit to the data in a scientific problem. The basic question of a sensitivity analysis\nis: how much do posterior inferences change when other reasonable probability models\nare used in place of the present model? Other reasonable models may differ substantially\nfrom the present model in the prior specification, the sampling distribution, or in what\ninformation is included (for example, predictor variables in a regression). It is possible that\nthe present model provides an adequate fit to the data, but that posterior inferences differ\nunder plausible alternative models.\n\nIn theory, both model checking and sensitivity analysis can be incorporated into the\nusual prior-to-posterior analysis. Under this perspective, model checking is done by set-\nting up a comprehensive joint distribution, such that any data that might be observed are\nplausible outcomes under the joint distribution. That is, this joint distribution is a mixture\nof all possible ‘true’ models or realities, incorporating all known substantive information.\nThe prior distribution in such a case incorporates prior beliefs about the likelihood of the\ncompeting realities and about the parameters of the constituent models. The posterior dis-\ntribution of such an exhaustive probability model automatically incorporates all ‘sensitivity\nanalysis’ but is still predicated on the truth of some member of the larger class of models.\n\nIn practice, however, setting up such a super-model to include all possibilities and all\nsubstantive knowledge is both conceptually impossible and computationally infeasible in all\nbut the simplest problems. It is thus necessary for us to examine our models in other ways\n\n141\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n142 6. MODEL CHECKING\n\nto see how they fail to fit reality and how sensitive the resulting posterior distributions are\nto arbitrary specifications.\n\nJudging model flaws by their practical implications\n\nWe do not like to ask, ‘Is our model true or false?’, since probability models in most\ndata analyses will not be perfectly true. Even the coin tosses and die rolls ubiquitous in\nprobability theory texts are not truly exchangeable. The more relevant question is, ‘Do the\nmodel’s deficiencies have a noticeable effect on the substantive inferences?’\n\nIn the examples of Chapter 5, the beta population distribution for the tumor rates and\nthe normal distribution for the eight school effects are both chosen partly for convenience.\nIn these examples, making convenient distributional assumptions turns out not to matter,\nin terms of the impact on the inferences of most interest. How to judge when assumptions\nof convenience can be made safely is a central task of Bayesian sensitivity analysis. Failures\nin the model lead to practical problems by creating clearly false inferences about estimands\nof interest.\n\n6.2 Do the inferences from the model make sense?\n\nIn any applied problem, there will be knowledge that is not included formally in either\nthe prior distribution or the likelihood, for reasons of convenience or objectivity. If the\nadditional information suggests that posterior inferences of interest are false, then this\nsuggests a potential for creating a more accurate probability model for the parameters and\ndata collection process. We illustrate with an example of a hierarchical regression model.\n\nExample. Evaluating election predictions by comparing to substantive po-\nlitical knowledge\nFigure 6.1 displays a forecast, made in early October, 1992, of the probability that Bill\nClinton would win each state in the U.S. presidential election that November. The\nestimates are posterior probabilities based on a hierarchical linear regression model.\nFor each state, the height of the shaded part of the box represents the estimated\nprobability that Clinton would win the state. Even before the election occurred, the\nforecasts for some of the states looked wrong; for example, from state polls, Clinton\nwas known in October to be much weaker in Texas and Florida than shown in the\nmap. This does not mean that the forecast is useless, but it is good to know where\nthe weak points are. Certainly, after the election, we can do an even better job of\ncriticizing the model and understanding its weaknesses. We return to this election\nforecasting example in Section 15.2 as an example of a hierarchical linear model.\n\nExternal validation\n\nMore formally, we can check a model by external validation using the model to make predic-\ntions about future data, and then collecting those data and comparing to their predictions.\nPosterior means should be correct on average, 50% intervals should contain the true values\nhalf the time, and so forth. We used external validation to check the empirical probability\nestimates in the record-linkage example in Section 1.7, and we apply the idea again to check\na toxicology model in Section 19.2. In the latter example, the external validation (see Figure\n19.10 on page 484) reveals a generally reasonable fit but with some notable discrepancies\nbetween predictions and external data. Often we need to check the model before obtaining\nnew data or waiting for the future to happen. In this chapter and the next, we discuss\nmethods which can approximate external validation using the data we already have.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 143\n\nFigure 6.1 Summary of a forecast of the 1992 U.S. presidential election performed one month\nbefore the election. For each state, the proportion of the box that is shaded represents the estimated\nprobability of Clinton winning the state; the width of the box is proportional to the number of\nelectoral votes for the state.\n\nChoices in defining the predictive quantities\n\nA single model can be used to make different predictions. For example, in the SAT example\nwe could consider a joint prediction for future data from the 8 schools in the study, p(ỹ|y),\na joint prediction for 8 new schools p(ỹi|y), i = 9, . . . , 16, or any other combination of new\nand existing schools. Other scenarios may have even more different choices in defining the\nfocus of predictions. For example, in analyses of sample surveys and designed experiments,\nit often makes sense to consider hypothetical replications of the experiment with a new\nrandomization of selection or treatment assignment, by analogy to classical randomization\ntests.\n\nSections 6.3 and 6.4 discuss posterior predictive checking, which use global summaries\nto check the joint posterior predictive distribution p(ỹ|y). At the end of Section 6.3 we\nbriefly discuss methods that combine inferences for local quantities to check marginal pre-\ndictive distributions p(ỹi|y), an idea that is related to cross-validation methods considered\nin Chapter 7.\n\n6.3 Posterior predictive checking\n\nIf the model fits, then replicated data generated under the model should look similar to\nobserved data. To put it another way, the observed data should look plausible under\nthe posterior predictive distribution. This is really a self-consistency check: an observed\ndiscrepancy can be due to model misfit or chance.\n\nOur basic technique for checking the fit of a model to data is to draw simulated values\nfrom the joint posterior predictive distribution of replicated data and compare these samples\nto the observed data. Any systematic differences between the simulations and the data\nindicate potential failings of the model.\n\nWe introduce posterior predictive checking with a simple example of an obviously poorly\nfitting model, and then in the rest of this section we lay out the key choices involved in pos-\nterior predictive checking. Sections 6.3 and 6.4 discuss numerical and graphical predictive\nchecks in more detail.\n\nExample. Comparing Newcomb’s speed of light measurements to the pos-\nterior predictive distribution\nSimon Newcomb’s 66 measurements on the speed of light are presented in Section 3.2.\nIn the absence of other information, in Section 3.2 we modeled the measurements as\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n144 6. MODEL CHECKING\n\nFigure 6.2 Twenty replications, yrep, of the speed of light data from the posterior predictive distri-\nbution, p(yrep|y); compare to observed data, y, in Figure 3.1. Each histogram displays the result\nof drawing 66 independent values ỹi from a common normal distribution with mean and variance\n(µ, σ2) drawn from the posterior distribution, p(µ, σ2|y), under the normal model.\n\nFigure 6.3 Smallest observation of Newcomb’s speed of light data (the vertical line at the left of the\ngraph), compared to the smallest observations from each of the 20 posterior predictive simulated\ndatasets displayed in Figure 6.2.\n\nN(µ, σ2), with a noninformative uniform prior distribution on (µ, log σ). However, the\nlowest of Newcomb’s measurements look like outliers compared to the rest of the data.\nCould the extreme measurements have reasonably come from a normal distribution?\nWe address this question by comparing the observed data to what we expect to be\nobserved under our posterior distribution. Figure 6.2 displays twenty histograms,\neach of which represents a single draw from the posterior predictive distribution of\nthe values in Newcomb’s experiment, obtained by first drawing (µ, σ2) from their\njoint posterior distribution, then drawing 66 values from a normal distribution with\nthis mean and variance. All these histograms look different from the histogram of\nactual data in Figure 3.1 on page 67. One way to measure the discrepancy is to\ncompare the smallest value in each hypothetical replicated dataset to Newcomb’s\nsmallest observation, −44. The histogram in Figure 6.3 shows the smallest observation\nin each of the 20 hypothetical replications; all are much larger than Newcomb’s smallest\nobservation, which is indicated by a vertical line on the graph. The normal model\nclearly does not capture the variation that Newcomb observed. A revised model\nmight use an asymmetric contaminated normal distribution or a symmetric long-tailed\ndistribution in place of the normal measurement model.\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n6.3. POSTERIOR PREDICTIVE CHECKING 145\n\nMany other examples of posterior predictive checks appear throughout the book, includ-\ning the educational testing example in Section 6.5, linear regressions examples in Sections\n14.3 and 15.2, and a hierarchical mixture model in Section 22.2.\n\nFor many problems, it is useful to examine graphical comparisons of summaries of the\ndata to summaries from posterior predictive simulations, as in Figure 6.3. In cases with\nless blatant discrepancies than the outliers in the speed of light data, it is often also useful\nto measure the ‘statistical significance’ of the lack of fit, a notion we formalize here.\n\nNotation for replications\n\nLet y be the observed data and θ be the vector of parameters (including all the hyperpa-\nrameters if the model is hierarchical). To avoid confusion with the observed data, y, we\ndefine yrep as the replicated data that could have been observed, or, to think predictively, as\nthe data we would see tomorrow if the experiment that produced y today were replicated\nwith the same model and the same value of θ that produced the observed data.\n\nWe distinguish between yrep and ỹ, our general notation for predictive outcomes: ỹ is\nany future observable value or vector of observable quantities, whereas yrep is specifically a\nreplication just like y. For example, if the model has explanatory variables, x, they will be\nidentical for y and yrep, but ỹ may have its own explanatory variables, x̃.\n\nWe will work with the distribution of yrep given the current state of knowledge, that is,\nwith the posterior predictive distribution\n\np(yrep|y) =\n∫\np(yrep|θ)p(θ|y)dθ. (6.1)\n\nTest quantities\n\nWe measure the discrepancy between model and data by defining test quantities, the aspects\nof the data we wish to check. A test quantity, or discrepancy measure, T (y, θ), is a scalar\nsummary of parameters and data that is used as a standard when comparing data to\npredictive simulations. Test quantities play the role in Bayesian model checking that test\nstatistics play in classical testing. We use the notation T (y) for a test statistic, which is a\ntest quantity that depends only on data; in the Bayesian context, we can generalize test\nstatistics to allow dependence on the model parameters under their posterior distribution.\nThis can be useful in directly summarizing discrepancies between model and data. We\ndiscuss options for graphical test quantities in Section 6.4. The test quantities in this\nsection are usually functions of y or replicated data yrep. In the end of this section we\nbriefly discuss a different sort of test quantities used for calibration that are functions of\nboth yi and y\n\nrep\ni (or ỹi). In Chapter 7 we discuss measures of discrepancy between model\n\nand data, that is, measures of predictive accuracy that are also functions of both yi and\nyrepi (or ỹi).\n\nTail-area probabilities\n\nLack of fit of the data with respect to the posterior predictive distribution can be measured\nby the tail-area probability, or p-value, of the test quantity, and computed using posterior\nsimulations of (θ, yrep). We define the p-value mathematically, first for the familiar classical\ntest and then in the Bayesian context.\n\nClassical p-values. The classical p-value for the test statistic T (y) is\n\npC = Pr(T (yrep)≥T (y)|θ), (6.2)\n\n\n\nThis electronic edition is for non-commercial purposes only.\n\n146 6. MODEL CHECKING\n\nwhere the probability is taken over the distribution of yrep with θ fixed. (The distribution of\nyrep given y and θ is the same as its distribution given θ alone","extracted_metadata":{"access_permission:can_print":["true"],"pdf:charsPerPage":["396","53","1436","1728","1697","1636","1492","528","3071","3140","1234","53","2742","3330","3069","2988","2256","2803","2648","2740","3465","3348","2958","2470","2340","3049","1963","2161","2318","3056","2205","2627","2963","3087","3225","3644","2929","2677","2494","1927","2281","2873","3093","3116","2331","2646","3530","2407","1793","1903","1918","2860","1541","1822","3183","1972","1401","2495","2382","1813","2158","2677","2369","2617","3359","3595","2953","2521","2429","2675","3013","177","2211","2155","2102","2430","2259","1865","2320","2463","1936","2317","2074","2392","2285","1981","2439","3093","2762","3198","2770","2935","2362","1684","3302","2463","2576","3133","3078","3559","2965","3399","3413","3082","3222","3652","3361","3382","2915","3107","3106","2884","2549","3091","3033","3268","2839","3066","2445","2758","2420","2181","1850","2903","2149","2374","2207","2834","3400","2905","2465","1749","2319","3019","2985","3110","3392","3275","3498","3496","2971","2867","3626","2513","2989","2931","2971","53","1343","53","2696","3022","2284","1753","2796"],"pdf:docinfo:title":["book.dvi"],"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"access_permission:extract_content":["true"],"pdf:docinfo:creator_tool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"xmp:CreateDate":["2020-04-27T08:01:52Z"],"pdf:docinfo:created":["2020-04-27T08:01:52Z"],"xmp:ModifyDate":["2020-04-27T08:01:52Z"],"access_permission:assemble_document":["true"],"xmpMM:DocumentID":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"access_permission:fill_in_form":["true"],"access_permission:extract_for_accessibility":["true"],"pdf:PDFVersion":["1.4"],"resourceName":["Bayesian Data Analysis - Third Edition (13th Feb 2020).pdf"],"access_permission:can_modify":["true"],"pdf:hasMarkedContent":["false"],"pdf:producer":["GPL Ghostscript 9.26"],"pdf:unmappedUnicodeCharsPerPage":["1","0","0","0","0","0","0","0","0","0","0","0","0","0","0","1","7","0","1","0","0","0","0","0","0","2","0","0","0","4","13","2","3","0","0","0","0","0","2","2","9","2","0","0","8","7","0","0","0","8","18","3","15","9","4","0","0","0","2","5","4","24","32","8","0","0","0","1","0","1","0","0","1","12","11","14","2","6","9","0","12","6","3","0","1","0","2","0","0","0","0","0","2","19","0","0","0","7","8","0","0","0","0","0","1","0","0","0","2","1","0","0","7","0","5","0","2","0","2","4","2","1","1","8","7","1","8","0","0","2","0","0","0","0","4","0","0","0","0","0","0","0","0","2","1","0","0","0","0","0","0","0","0","0","1"],"pdf:docinfo:producer":["GPL Ghostscript 9.26"],"pdf:docinfo:modified":["2020-04-27T08:01:52Z"],"xmpTPg:NPages":["677"],"access_permission:modify_annotations":["true"],"dc:title":["book.dvi"],"xmp:About":["uuid:e7482641-c079-11f5-0000-29f70e04097b"],"pdf:hasXMP":["true"],"pdf:encrypted":["false"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"xmp:CreatorTool":["dvips(k) 5.995 Copyright 2015 Radical Eye Software"],"pdf:hasCollection":["false"],"dcterms:modified":["2020-04-27T08:01:52Z"],"dc:format":["application/pdf; version=1.4"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"pdf:hasXFA":["false"],"Content-Length":["35493143"],"dcterms:created":["2020-04-27T08:01:52Z"],"access_permission:can_print_degraded":["true"],"Content-Type":["application/pdf"]},"metadata_field_count":38,"attempts":1,"timestamp":1754065150.2395806,"platform":"Linux","python_version":"3.13.5"},{"file_path":"test_documents/pdfs/Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf","file_size":20812100,"file_type":"pdf","category":"large","framework":"extractous","iteration":2,"extraction_time":132.45182609558105,"startup_time":null,"peak_memory_mb":448.421875,"avg_memory_mb":454.271875,"peak_cpu_percent":1285.1,"avg_cpu_percent":263.02,"total_io_mb":null,"status":"success","character_count":500000,"word_count":112307,"error_type":null,"error_message":null,"quality_metrics":{"char_count":500000,"word_count":112307,"sentence_count":7563,"paragraph_count":6926,"avg_word_length":3.38541675941838,"avg_sentence_length":13.423773634801005,"extraction_completeness":1.0,"text_coherence":0.30375980524749796,"noise_ratio":0.5307600000000001,"gibberish_ratio":0.01694915254237288,"flesch_reading_ease":70.7747961398366,"gunning_fog_index":10.59532347580288,"has_proper_formatting":true,"maintains_line_breaks":true,"preserves_whitespace":true,"table_structure_preserved":true,"format_specific_score":0.49999999999999994,"expected_content_preserved":false,"has_encoding_issues":true,"has_ocr_artifacts":true,"preserves_pdf_formatting":true},"overall_quality_score":0.5120034464866704,"extracted_text":"\nAlgebra, Topology, Differential Calculus, and\nOptimization Theory\n\nFor Computer Science and Machine Learning\n\nJean Gallier and Jocelyn Quaintance\nDepartment of Computer and Information Science\n\nUniversity of Pennsylvania\nPhiladelphia, PA 19104, USA\ne-mail: jean@cis.upenn.edu\n\nc© Jean Gallier\n\nAugust 2, 2019\n\n\n\n2\n\n\n\n\n\nContents\n\nContents 3\n\n1 Introduction 17\n\n2 Groups, Rings, and Fields 19\n2.1 Groups, Subgroups, Cosets . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.2 Cyclic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n2.3 Rings and Fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n\nI Linear Algebra 43\n\n3 Vector Spaces, Bases, Linear Maps 45\n3.1 Motivations: Linear Combinations, Linear Independence, Rank . . . . . . . 45\n3.2 Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\n3.3 Indexed Families; the Sum Notation\n\n∑\ni∈I ai . . . . . . . . . . . . . . . . . . 60\n\n3.4 Linear Independence, Subspaces . . . . . . . . . . . . . . . . . . . . . . . . 66\n3.5 Bases of a Vector Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n3.6 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n3.7 Linear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\n3.8 Quotient Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93\n3.9 Linear Forms and the Dual Space . . . . . . . . . . . . . . . . . . . . . . . . 94\n3.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n3.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n\n4 Matrices and Linear Maps 107\n4.1 Representation of Linear Maps by Matrices . . . . . . . . . . . . . . . . . . 107\n4.2 Composition of Linear Maps and Matrix Multiplication . . . . . . . . . . . 112\n4.3 Change of Basis Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n4.4 The Effect of a Change of Bases on Matrices . . . . . . . . . . . . . . . . . 120\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\n\n5 Haar Bases, Haar Wavelets, Hadamard Matrices 131\n\n3\n\n\n\n4 CONTENTS\n\n5.1 Introduction to Signal Compression Using Haar Wavelets . . . . . . . . . . 131\n5.2 Haar Matrices, Scaling Properties of Haar Wavelets . . . . . . . . . . . . . . 133\n5.3 Kronecker Product Construction of Haar Matrices . . . . . . . . . . . . . . 138\n5.4 Multiresolution Signal Analysis with Haar Bases . . . . . . . . . . . . . . . 140\n5.5 Haar Transform for Digital Images . . . . . . . . . . . . . . . . . . . . . . . 142\n5.6 Hadamard Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n5.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n5.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\n\n6 Direct Sums 155\n6.1 Sums, Direct Sums, Direct Products . . . . . . . . . . . . . . . . . . . . . . 155\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation . . . . . . . . . . . . . . 165\n6.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\n6.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\n\n7 Determinants 181\n7.1 Permutations, Signature of a Permutation . . . . . . . . . . . . . . . . . . . 181\n7.2 Alternating Multilinear Maps . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n7.3 Definition of a Determinant . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\n7.4 Inverse Matrices and Determinants . . . . . . . . . . . . . . . . . . . . . . . 197\n7.5 Systems of Linear Equations and Determinants . . . . . . . . . . . . . . . . 200\n7.6 Determinant of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.7 The Cayley–Hamilton Theorem . . . . . . . . . . . . . . . . . . . . . . . . . 203\n7.8 Permanents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209\n7.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211\n7.10 Further Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n7.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n\n8 Gaussian Elimination, LU, Cholesky, Echelon Form 219\n8.1 Motivating Example: Curve Interpolation . . . . . . . . . . . . . . . . . . . 219\n8.2 Gaussian Elimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223\n8.3 Elementary Matrices and Row Operations . . . . . . . . . . . . . . . . . . . 228\n8.4 LU -Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231\n8.5 PA = LU Factorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n8.6 Proof of Theorem 8.5 ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n8.7 Dealing with Roundoff Errors; Pivoting Strategies . . . . . . . . . . . . . . . 251\n8.8 Gaussian Elimination of Tridiagonal Matrices . . . . . . . . . . . . . . . . . 252\n8.9 SPD Matrices and the Cholesky Decomposition . . . . . . . . . . . . . . . . 254\n8.10 Reduced Row Echelon Form . . . . . . . . . . . . . . . . . . . . . . . . . . . 263\n8.11 RREF, Free Variables, Homogeneous Systems . . . . . . . . . . . . . . . . . 269\n8.12 Uniqueness of RREF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n8.13 Solving Linear Systems Using RREF . . . . . . . . . . . . . . . . . . . . . . 274\n8.14 Elementary Matrices and Columns Operations . . . . . . . . . . . . . . . . 281\n\n\n\nCONTENTS 5\n\n8.15 Transvections and Dilatations ~ . . . . . . . . . . . . . . . . . . . . . . . . 282\n8.16 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287\n8.17 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289\n\n9 Vector Norms and Matrix Norms 301\n9.1 Normed Vector Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301\n9.2 Matrix Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n9.3 Subordinate Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n9.4 Inequalities Involving Subordinate Norms . . . . . . . . . . . . . . . . . . . 324\n9.5 Condition Numbers of Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 326\n9.6 An Application of Norms: Inconsistent Linear Systems . . . . . . . . . . . . 335\n9.7 Limits of Sequences and Series . . . . . . . . . . . . . . . . . . . . . . . . . 336\n9.8 The Matrix Exponential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339\n9.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342\n9.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344\n\n10 Iterative Methods for Solving Linear Systems 351\n10.1 Convergence of Sequences of Vectors and Matrices . . . . . . . . . . . . . . 351\n10.2 Convergence of Iterative Methods . . . . . . . . . . . . . . . . . . . . . . . . 354\n10.3 Methods of Jacobi, Gauss–Seidel, and Relaxation . . . . . . . . . . . . . . . 356\n10.4 Convergence of the Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 364\n10.5 Convergence Methods for Tridiagonal Matrices . . . . . . . . . . . . . . . . 367\n10.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n10.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373\n\n11 The Dual Space and Duality 375\n11.1 The Dual Space E∗ and Linear Forms . . . . . . . . . . . . . . . . . . . . . 375\n11.2 Pairing and Duality Between E and E∗ . . . . . . . . . . . . . . . . . . . . 382\n11.3 The Duality Theorem and Some Consequences . . . . . . . . . . . . . . . . 387\n11.4 The Bidual and Canonical Pairings . . . . . . . . . . . . . . . . . . . . . . . 393\n11.5 Hyperplanes and Linear Forms . . . . . . . . . . . . . . . . . . . . . . . . . 395\n11.6 Transpose of a Linear Map and of a Matrix . . . . . . . . . . . . . . . . . . 396\n11.7 Properties of the Double Transpose . . . . . . . . . . . . . . . . . . . . . . . 403\n11.8 The Four Fundamental Subspaces . . . . . . . . . . . . . . . . . . . . . . . 405\n11.9 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408\n11.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 409\n\n12 Euclidean Spaces 413\n12.1 Inner Products, Euclidean Spaces . . . . . . . . . . . . . . . . . . . . . . . . 413\n12.2 Orthogonality and Duality in Euclidean Spaces . . . . . . . . . . . . . . . . 422\n12.3 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 429\n12.4 Existence and Construction of Orthonormal Bases . . . . . . . . . . . . . . 432\n12.5 Linear Isometries (Orthogonal Transformations) . . . . . . . . . . . . . . . . 439\n\n\n\n6 CONTENTS\n\n12.6 The Orthogonal Group, Orthogonal Matrices . . . . . . . . . . . . . . . . . 442\n12.7 The Rodrigues Formula . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444\n12.8 QR-Decomposition for Invertible Matrices . . . . . . . . . . . . . . . . . . . 447\n12.9 Some Applications of Euclidean Geometry . . . . . . . . . . . . . . . . . . . 452\n12.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 453\n12.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455\n\n13 QR-Decomposition for Arbitrary Matrices 467\n13.1 Orthogonal Reflections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 467\n13.2 QR-Decomposition Using Householder Matrices . . . . . . . . . . . . . . . . 472\n13.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n13.4 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 482\n\n14 Hermitian Spaces 489\n14.1 Hermitian Spaces, Pre-Hilbert Spaces . . . . . . . . . . . . . . . . . . . . . 489\n14.2 Orthogonality, Duality, Adjoint of a Linear Map . . . . . . . . . . . . . . . 498\n14.3 Linear Isometries (Also Called Unitary Transformations) . . . . . . . . . . . 503\n14.4 The Unitary Group, Unitary Matrices . . . . . . . . . . . . . . . . . . . . . 505\n14.5 Hermitian Reflections and QR-Decomposition . . . . . . . . . . . . . . . . . 508\n14.6 Orthogonal Projections and Involutions . . . . . . . . . . . . . . . . . . . . 513\n14.7 Dual Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n14.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 523\n14.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n\n15 Eigenvectors and Eigenvalues 529\n15.1 Eigenvectors and Eigenvalues of a Linear Map . . . . . . . . . . . . . . . . . 529\n15.2 Reduction to Upper Triangular Form . . . . . . . . . . . . . . . . . . . . . . 537\n15.3 Location of Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 541\n15.4 Conditioning of Eigenvalue Problems . . . . . . . . . . . . . . . . . . . . . . 544\n15.5 Eigenvalues of the Matrix Exponential . . . . . . . . . . . . . . . . . . . . . 547\n15.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n15.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 550\n\n16 Unit Quaternions and Rotations in SO(3) 561\n16.1 The Group SU(2) and the Skew Field H of Quaternions . . . . . . . . . . . 561\n16.2 Representation of Rotation in SO(3) By Quaternions in SU(2) . . . . . . . 563\n16.3 Matrix Representation of the Rotation rq . . . . . . . . . . . . . . . . . . . 568\n16.4 An Algorithm to Find a Quaternion Representing a Rotation . . . . . . . . 570\n16.5 The Exponential Map exp: su(2)→ SU(2) . . . . . . . . . . . . . . . . . . 573\n16.6 Quaternion Interpolation ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 575\n16.7 Nonexistence of a “Nice” Section from SO(3) to SU(2) . . . . . . . . . . . . 577\n16.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 579\n16.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 580\n\n\n\nCONTENTS 7\n\n17 Spectral Theorems 583\n17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583\n17.2 Normal Linear Maps: Eigenvalues and Eigenvectors . . . . . . . . . . . . . . 583\n17.3 Spectral Theorem for Normal Linear Maps . . . . . . . . . . . . . . . . . . . 589\n17.4 Self-Adjoint and Other Special Linear Maps . . . . . . . . . . . . . . . . . . 594\n17.5 Normal and Other Special Matrices . . . . . . . . . . . . . . . . . . . . . . . 600\n17.6 Rayleigh–Ritz Theorems and Eigenvalue Interlacing . . . . . . . . . . . . . 603\n17.7 The Courant–Fischer Theorem; Perturbation Results . . . . . . . . . . . . . 608\n17.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611\n17.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n\n18 Computing Eigenvalues and Eigenvectors 619\n18.1 The Basic QR Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . 621\n18.2 Hessenberg Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627\n18.3 Making the QR Method More Efficient Using Shifts . . . . . . . . . . . . . 633\n18.4 Krylov Subspaces; Arnoldi Iteration . . . . . . . . . . . . . . . . . . . . . . 638\n18.5 GMRES . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642\n18.6 The Hermitian Case; Lanczos Iteration . . . . . . . . . . . . . . . . . . . . . 643\n18.7 Power Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 644\n18.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 646\n18.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647\n\n19 Introduction to The Finite Elements Method 649\n19.1 A One-Dimensional Problem: Bending of a Beam . . . . . . . . . . . . . . . 649\n19.2 A Two-Dimensional Problem: An Elastic Membrane . . . . . . . . . . . . . 660\n19.3 Time-Dependent Boundary Problems . . . . . . . . . . . . . . . . . . . . . . 663\n\n20 Graphs and Graph Laplacians; Basic Facts 671\n20.1 Directed Graphs, Undirected Graphs, Weighted Graphs . . . . . . . . . . . 674\n20.2 Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 681\n20.3 Normalized Laplacian Matrices of Graphs . . . . . . . . . . . . . . . . . . . 685\n20.4 Graph Clustering Using Normalized Cuts . . . . . . . . . . . . . . . . . . . 689\n20.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 691\n20.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 692\n\n21 Spectral Graph Drawing 695\n21.1 Graph Drawing and Energy Minimization . . . . . . . . . . . . . . . . . . . 695\n21.2 Examples of Graph Drawings . . . . . . . . . . . . . . . . . . . . . . . . . . 698\n21.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 702\n\n22 Singular Value Decomposition and Polar Form 705\n22.1 Properties of f ∗ ◦ f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 705\n22.2 Singular Value Decomposition for Square Matrices . . . . . . . . . . . . . . 709\n\n\n\n8 CONTENTS\n\n22.3 Polar Form for Square Matrices . . . . . . . . . . . . . . . . . . . . . . . . . 712\n22.4 Singular Value Decomposition for Rectangular Matrices . . . . . . . . . . . 715\n22.5 Ky Fan Norms and Schatten Norms . . . . . . . . . . . . . . . . . . . . . . 718\n22.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n22.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\n\n23 Applications of SVD and Pseudo-Inverses 723\n23.1 Least Squares Problems and the Pseudo-Inverse . . . . . . . . . . . . . . . . 723\n23.2 Properties of the Pseudo-Inverse . . . . . . . . . . . . . . . . . . . . . . . . 730\n23.3 Data Compression and SVD . . . . . . . . . . . . . . . . . . . . . . . . . . . 735\n23.4 Principal Components Analysis (PCA) . . . . . . . . . . . . . . . . . . . . . 737\n23.5 Best Affine Approximation . . . . . . . . . . . . . . . . . . . . . . . . . . . 748\n23.6 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 752\n23.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 753\n\nII Affine and Projective Geometry 757\n\n24 Basics of Affine Geometry 759\n24.1 Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 759\n24.2 Examples of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . 768\n24.3 Chasles’s Identity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 769\n24.4 Affine Combinations, Barycenters . . . . . . . . . . . . . . . . . . . . . . . . 770\n24.5 Affine Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 775\n24.6 Affine Independence and Affine Frames . . . . . . . . . . . . . . . . . . . . . 781\n24.7 Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 787\n24.8 Affine Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 794\n24.9 Affine Geometry: A Glimpse . . . . . . . . . . . . . . . . . . . . . . . . . . 796\n24.10 Affine Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 800\n24.11 Intersection of Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 802\n\n25 Embedding an Affine Space in a Vector Space 805\n25.1 The “Hat Construction,” or Homogenizing . . . . . . . . . . . . . . . . . . . 805\n25.2 Affine Frames of E and Bases of Ê . . . . . . . . . . . . . . . . . . . . . . . 812\n25.3 Another Construction of Ê . . . . . . . . . . . . . . . . . . . . . . . . . . . 815\n25.4 Extending Affine Maps to Linear Maps . . . . . . . . . . . . . . . . . . . . . 818\n\n26 Basics of Projective Geometry 823\n26.1 Why Projective Spaces? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 823\n26.2 Projective Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 828\n26.3 Projective Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 833\n26.4 Projective Frames . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 836\n26.5 Projective Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 850\n\n\n\nCONTENTS 9\n\n26.6 Finding a Homography Between Two Projective Frames . . . . . . . . . . . 856\n26.7 Affine Patches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 869\n26.8 Projective Completion of an Affine Space . . . . . . . . . . . . . . . . . . . 872\n26.9 Making Good Use of Hyperplanes at Infinity . . . . . . . . . . . . . . . . . 877\n26.10 The Cross-Ratio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 880\n26.11 Fixed Points of Homographies and Homologies . . . . . . . . . . . . . . . . 884\n26.12 Duality in Projective Geometry . . . . . . . . . . . . . . . . . . . . . . . . . 898\n26.13 Cross-Ratios of Hyperplanes . . . . . . . . . . . . . . . . . . . . . . . . . . . 902\n26.14 Complexification of a Real Projective Space . . . . . . . . . . . . . . . . . . 904\n26.15 Similarity Structures on a Projective Space . . . . . . . . . . . . . . . . . . 906\n26.16 Some Applications of Projective Geometry . . . . . . . . . . . . . . . . . . . 915\n\nIII The Geometry of Bilinear Forms 921\n\n27 The Cartan–Dieudonné Theorem 923\n27.1 The Cartan–Dieudonné Theorem for Linear Isometries . . . . . . . . . . . . 923\n27.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 935\n27.3 Fixed Points of Affine Maps . . . . . . . . . . . . . . . . . . . . . . . . . . . 937\n27.4 Affine Isometries and Fixed Points . . . . . . . . . . . . . . . . . . . . . . . 939\n27.5 The Cartan–Dieudonné Theorem for Affine Isometries . . . . . . . . . . . . 945\n\n28 Isometries of Hermitian Spaces 949\n28.1 The Cartan–Dieudonné Theorem, Hermitian Case . . . . . . . . . . . . . . . 949\n28.2 Affine Isometries (Rigid Motions) . . . . . . . . . . . . . . . . . . . . . . . . 958\n\n29 The Geometry of Bilinear Forms; Witt’s Theorem 963\n29.1 Bilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 963\n29.2 Sesquilinear Forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 971\n29.3 Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 975\n29.4 Adjoint of a Linear Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 980\n29.5 Isometries Associated with Sesquilinear Forms . . . . . . . . . . . . . . . . . 982\n29.6 Totally Isotropic Subspaces . . . . . . . . . . . . . . . . . . . . . . . . . . . 986\n29.7 Witt Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 992\n29.8 Symplectic Groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1000\n29.9 Orthogonal Groups and the Cartan–Dieudonné Theorem . . . . . . . . . . . 1004\n29.10 Witt’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1011\n\nIV Algebra: PID’s, UFD’s, Noetherian Rings, Tensors,\nModules over a PID, Normal Forms 1017\n\n30 Polynomials, Ideals and PID’s 1019\n\n\n\n10 CONTENTS\n\n30.1 Multisets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1019\n30.2 Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1020\n30.3 Euclidean Division of Polynomials . . . . . . . . . . . . . . . . . . . . . . . 1026\n30.4 Ideals, PID’s, and Greatest Common Divisors . . . . . . . . . . . . . . . . . 1028\n30.5 Factorization and Irreducible Factors in K[X] . . . . . . . . . . . . . . . . . 1036\n30.6 Roots of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1040\n30.7 Polynomial Interpolation (Lagrange, Newton, Hermite) . . . . . . . . . . . . 1047\n\n31 Annihilating Polynomials; Primary Decomposition 1055\n31.1 Annihilating Polynomials and the Minimal Polynomial . . . . . . . . . . . . 1057\n31.2 Minimal Polynomials of Diagonalizable Linear Maps . . . . . . . . . . . . . 1059\n31.3 Commuting Families of Linear Maps . . . . . . . . . . . . . . . . . . . . . . 1062\n31.4 The Primary Decomposition Theorem . . . . . . . . . . . . . . . . . . . . . 1065\n31.5 Jordan Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1071\n31.6 Nilpotent Linear Maps and Jordan Form . . . . . . . . . . . . . . . . . . . . 1074\n31.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1080\n31.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1081\n\n32 UFD’s, Noetherian Rings, Hilbert’s Basis Theorem 1085\n32.1 Unique Factorization Domains (Factorial Rings) . . . . . . . . . . . . . . . . 1085\n32.2 The Chinese Remainder Theorem . . . . . . . . . . . . . . . . . . . . . . . . 1099\n32.3 Noetherian Rings and Hilbert’s Basis Theorem . . . . . . . . . . . . . . . . 1105\n32.4 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1109\n\n33 Tensor Algebras 1111\n33.1 Linear Algebra Preliminaries: Dual Spaces and Pairings . . . . . . . . . . . 1113\n33.2 Tensors Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1118\n33.3 Bases of Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1130\n33.4 Some Useful Isomorphisms for Tensor Products . . . . . . . . . . . . . . . . 1131\n33.5 Duality for Tensor Products . . . . . . . . . . . . . . . . . . . . . . . . . . . 1135\n33.6 Tensor Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1141\n33.7 Symmetric Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1148\n33.8 Bases of Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1152\n33.9 Some Useful Isomorphisms for Symmetric Powers . . . . . . . . . . . . . . . 1155\n33.10 Duality for Symmetric Powers . . . . . . . . . . . . . . . . . . . . . . . . . . 1155\n33.11 Symmetric Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1159\n33.12 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1162\n\n34 Exterior Tensor Powers and Exterior Algebras 1165\n34.1 Exterior Tensor Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1165\n34.2 Bases of Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1170\n34.3 Some Useful Isomorphisms for Exterior Powers . . . . . . . . . . . . . . . . 1173\n34.4 Duality for Exterior Powers . . . . . . . . . . . . . . . . . . . . . . . . . . . 1173\n\n\n\nCONTENTS 11\n\n34.5 Exterior Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1177\n34.6 The Hodge ∗-Operator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1181\n34.7 Left and Right Hooks ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1185\n34.8 Testing Decomposability ~ . . . . . . . . . . . . . . . . . . . . . . . . . . . 1195\n34.9 The Grassmann-Plücker’s Equations and Grassmannians ~ . . . . . . . . . 1198\n34.10 Vector-Valued Alternating Forms . . . . . . . . . . . . . . . . . . . . . . . . 1201\n34.11 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1205\n\n35 Introduction to Modules; Modules over a PID 1207\n35.1 Modules over a Commutative Ring . . . . . . . . . . . . . . . . . . . . . . . 1207\n35.2 Finite Presentations of Modules . . . . . . . . . . . . . . . . . . . . . . . . . 1216\n35.3 Tensor Products of Modules over a Commutative Ring . . . . . . . . . . . . 1222\n35.4 Torsion Modules over a PID; Primary Decomposition . . . . . . . . . . . . . 1225\n35.5 Finitely Generated Modules over a PID . . . . . . . . . . . . . . . . . . . . 1231\n35.6 Extension of the Ring of Scalars . . . . . . . . . . . . . . . . . . . . . . . . 1247\n\n36 Normal Forms; The Rational Canonical Form 1253\n36.1 The Torsion Module Associated With An Endomorphism . . . . . . . . . . 1253\n36.2 The Rational Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . 1261\n36.3 The Rational Canonical Form, Second Version . . . . . . . . . . . . . . . . . 1268\n36.4 The Jordan Form Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . 1269\n36.5 The Smith Normal Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1272\n\nV Topology, Differential Calculus 1285\n\n37 Topology 1287\n37.1 Metric Spaces and Normed Vector Spaces . . . . . . . . . . . . . . . . . . . 1287\n37.2 Topological Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1294\n37.3 Continuous Functions, Limits . . . . . . . . . . . . . . . . . . . . . . . . . . 1303\n37.4 Connected Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1311\n37.5 Compact Sets and Locally Compact Spaces . . . . . . . . . . . . . . . . . . 1320\n37.6 Second-Countable and Separable Spaces . . . . . . . . . . . . . . . . . . . . 1331\n37.7 Sequential Compactness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1335\n37.8 Complete Metric Spaces and Compactness . . . . . . . . . . . . . . . . . . . 1341\n37.9 Completion of a Metric Space . . . . . . . . . . . . . . . . . . . . . . . . . . 1344\n37.10 The Contraction Mapping Theorem . . . . . . . . . . . . . . . . . . . . . . 1351\n37.11 Continuous Linear and Multilinear Maps . . . . . . . . . . . . . . . . . . . . 1355\n37.12 Completion of a Normed Vector Space . . . . . . . . . . . . . . . . . . . . . 1362\n37.13 Normed Affine Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n37.14 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365\n\n38 A Detour On Fractals 1367\n\n\n\n12 CONTENTS\n\n38.1 Iterated Function Systems and Fractals . . . . . . . . . . . . . . . . . . . . 1367\n\n39 Differential Calculus 1375\n39.1 Directional Derivatives, Total Derivatives . . . . . . . . . . . . . . . . . . . 1375\n39.2 Jacobian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1389\n39.3 The Implicit and The Inverse Function Theorems . . . . . . . . . . . . . . . 1397\n39.4 Tangent Spaces and Differentials . . . . . . . . . . . . . . . . . . . . . . . . 1401\n39.5 Second-Order and Higher-Order Derivatives . . . . . . . . . . . . . . . . . . 1402\n39.6 Taylor’s formula, Faà di Bruno’s formula . . . . . . . . . . . . . . . . . . . . 1407\n39.7 Vector Fields, Covariant Derivatives, Lie Brackets . . . . . . . . . . . . . . . 1411\n39.8 Futher Readings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1413\n\nVI Preliminaries for Optimization Theory 1415\n\n40 Extrema of Real-Valued Functions 1417\n40.1 Local Extrema and Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . 1417\n40.2 Using Second Derivatives to Find Extrema . . . . . . . . . . . . . . . . . . . 1427\n40.3 Using Convexity to Find Extrema . . . . . . . . . . . . . . . . . . . . . . . 1430\n40.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1440\n\n41 Newton’s Method and Its Generalizations 1441\n41.1 Newton’s Method for Real Functions of a Real Argument . . . . . . . . . . 1441\n41.2 Generalizations of Newton’s Method . . . . . . . . . . . . . . . . . . . . . . 1442\n41.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1448\n\n42 Quadratic Optimization Problems 1449\n42.1 Quadratic Optimization: The Positive Definite Case . . . . . . . . . . . . . 1449\n42.2 Quadratic Optimization: The General Case . . . . . . . . . . . . . . . . . . 1458\n42.3 Maximizing a Quadratic Function on the Unit Sphere . . . . . . . . . . . . 1463\n42.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1468\n\n43 Schur Complements and Applications 1469\n43.1 Schur Complements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1469\n43.2 SPD Matrices and Schur Complements . . . . . . . . . . . . . . . . . . . . . 1472\n43.3 SP Semidefinite Matrices and Schur Complements . . . . . . . . . . . . . . 1473\n\nVII Linear Optimization 1475\n\n44 Convex Sets, Cones, H-Polyhedra 1477\n44.1 What is Linear Programming? . . . . . . . . . . . . . . . . . . . . . . . . . 1477\n44.2 Affine Subsets, Convex Sets, Hyperplanes, Half-Spaces . . . . . . . . . . . . 1479\n44.3 Cones, Polyhedral Cones, and H-Polyhedra . . . . . . . . . . . . . . . . . . 1482\n\n\n\nCONTENTS 13\n\n45 Linear Programs 1489\n45.1 Linear Programs, Feasible Solutions, Optimal Solutions . . . . . . . . . . . 1489\n45.2 Basic Feasible Solutions and Vertices . . . . . . . . . . . . . . . . . . . . . . 1495\n\n46 The Simplex Algorithm 1503\n46.1 The Idea Behind the Simplex Algorithm . . . . . . . . . . . . . . . . . . . . 1503\n46.2 The Simplex Algorithm in General . . . . . . . . . . . . . . . . . . . . . . . 1512\n46.3 How to Perform a Pivoting Step Efficiently . . . . . . . . . . . . . . . . . . 1519\n46.4 The Simplex Algorithm Using Tableaux . . . . . . . . . . . . . . . . . . . . 1523\n46.5 Computational Efficiency of the Simplex Method . . . . . . . . . . . . . . . 1532\n\n47 Linear Programming and Duality 1535\n47.1 Variants of the Farkas Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 1535\n47.2 The Duality Theorem in Linear Programming . . . . . . . . . . . . . . . . . 1540\n47.3 Complementary Slackness Conditions . . . . . . . . . . . . . . . . . . . . . 1548\n47.4 Duality for Linear Programs in Standard Form . . . . . . . . . . . . . . . . 1550\n47.5 The Dual Simplex Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 1553\n47.6 The Primal-Dual Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 1558\n\nVIII NonLinear Optimization 1569\n\n48 Basics of Hilbert Spaces 1571\n48.1 The Projection Lemma, Duality . . . . . . . . . . . . . . . . . . . . . . . . 1571\n48.2 Farkas–Minkowski Lemma in Hilbert Spaces . . . . . . . . . . . . . . . . . . 1588\n\n49 General Results of Optimization Theory 1591\n49.1 Optimization Problems; Basic Terminology . . . . . . . . . . . . . . . . . . 1591\n49.2 Existence of Solutions of an Optimization Problem . . . . . . . . . . . . . . 1594\n49.3 Minima of Quadratic Functionals . . . . . . . . . . . . . . . . . . . . . . . . 1599\n49.4 Elliptic Functionals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1605\n49.5 Iterative Methods for Unconstrained Problems . . . . . . . . . . . . . . . . 1608\n49.6 Gradient Descent Methods for Unconstrained Problems . . . . . . . . . . . 1612\n49.7 Convergence of Gradient Descent with Variable Stepsize . . . . . . . . . . . 1617\n49.8 Steepest Descent for an Arbitrary Norm . . . . . . . . . . . . . . . . . . . . 1622\n49.9 Newton’s Method For Finding a Minimum . . . . . . . . . . . . . . . . . . . 1624\n49.10 Conjugate Gradient Methods; Unconstrained Problems . . . . . . . . . . . . 1628\n49.11 Gradient Projection for Constrained Optimization . . . . . . . . . . . . . . 1640\n49.12 Penalty Methods for Constrained Optimization . . . . . . . . . . . . . . . . 1642\n49.13 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1644\n\n50 Introduction to Nonlinear Optimization 1647\n50.1 The Cone of Feasible Directions . . . . . . . . . . . . . . . . . . . . . . . . . 1647\n\n\n\n14 CONTENTS\n\n50.2 Active Constraints and Qualified Constraints . . . . . . . . . . . . . . . . . 1654\n50.3 The Karush–Kuhn–Tucker Conditions . . . . . . . . . . . . . . . . . . . . . 1660\n50.4 Equality Constrained Minimization . . . . . . . . . . . . . . . . . . . . . . . 1672\n50.5 Hard Margin Support Vector Machine; Version I . . . . . . . . . . . . . . . 1677\n50.6 Hard Margin Support Vector Machine; Version II . . . . . . . . . . . . . . . 1681\n50.7 Lagrangian Duality and Saddle Points . . . . . . . . . . . . . . . . . . . . . 1690\n50.8 Weak and Strong Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1699\n50.9 Handling Equality Constraints Explicitly . . . . . . . . . . . . . . . . . . . . 1707\n50.10 Dual of the Hard Margin Support Vector Machine . . . . . . . . . . . . . . 1710\n50.11 Conjugate Function and Legendre Dual Function . . . . . . . . . . . . . . . 1715\n50.12 Some Techniques to Obtain a More Useful Dual Program . . . . . . . . . . 1725\n50.13 Uzawa’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1729\n50.14 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1735\n\n51 Subgradients and Subdifferentials 1737\n51.1 Extended Real-Valued Convex Functions . . . . . . . . . . . . . . . . . . . . 1739\n51.2 Subgradients and Subdifferentials . . . . . . . . . . . . . . . . . . . . . . . . 1748\n51.3 Basic Properties of Subgradients and Subdifferentials . . . . . . . . . . . . . 1760\n51.4 Additional Properties of Subdifferentials . . . . . . . . . . . . . . . . . . . . 1766\n51.5 The Minimum of a Proper Convex Function . . . . . . . . . . . . . . . . . . 1770\n51.6 Generalization of the Lagrangian Framework . . . . . . . . . . . . . . . . . 1776\n51.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1780\n\n52 Dual Ascent Methods; ADMM 1783\n52.1 Dual Ascent . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1785\n52.2 Augmented Lagrangians and the Method of Multipliers . . . . . . . . . . . . 1789\n52.3 ADMM: Alternating Direction Method of Multipliers . . . . . . . . . . . . . 1794\n52.4 Convergence of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1797\n52.5 Stopping Criteria . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1806\n52.6 Some Applications of ADMM . . . . . . . . . . . . . . . . . . . . . . . . . . 1807\n52.7 Applications of ADMM to `1-Norm Problems . . . . . . . . . . . . . . . . . 1810\n52.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1815\n\nIX Applications to Machine Learning 1817\n\n53 Ridge Regression and Lasso Regression 1819\n53.1 Ridge Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1819\n53.2 Lasso Regression (`1-Regularized Regression) . . . . . . . . . . . . . . . . . 1829\n53.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1835\n\n54 Positive Definite Kernels 1837\n54.1 Basic Properties of Positive Definite Kernels . . . . . . . . . . . . . . . . . . 1837\n\n\n\nCONTENTS 15\n\n54.2 Hilbert Space Representation of a Positive Kernel . . . . . . . . . . . . . . . 1848\n54.3 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1852\n54.4 ν-SV Regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1855\n\n55 Soft Margin Support Vector Machines 1865\n55.1 Soft Margin Support Vector Machines; (SVMs1) . . . . . . . . . . . . . . . . 1868\n55.2 Soft Margin Support Vector Machines; (SVMs2) . . . . . . . . . . . . . . . . 1878\n55.3 Soft Margin Support Vector Machines; (SVMs2′) . . . . . . . . . . . . . . . 1885\n55.4 Soft Margin SVM; (SVMs3) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1900\n55.5 Soft Margin Support Vector Machines; (SVMs4) . . . . . . . . . . . . . . . . 1903\n55.6 Soft Margin SVM; (SVMs5) . . . . . . . . . . . . . . . . . . . . . . . . . . . 1911\n55.7 Summary and Comparison of the SVM Methods . . . . . . . . . . . . . . . 1914\n\nX Appendices 1927\n\nA Total Orthogonal Families in Hilbert Spaces 1929\nA.1 Total Orthogonal Families, Fourier Coefficients . . . . . . . . . . . . . . . . 1929\nA.2 The Hilbert Space `2(K) and the Riesz-Fischer Theorem . . . . . . . . . . . 1937\n\nB Zorn’s Lemma; Some Applications 1947\nB.1 Statement of Zorn’s Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . 1947\nB.2 Proof of the Existence of a Basis in a Vector Space . . . . . . . . . . . . . . 1948\nB.3 Existence of Maximal Proper Ideals . . . . . . . . . . . . . . . . . . . . . . 1949\n\nBibliography 1951\n\n\n\n16 CONTENTS\n\n16\n\nCONTENTS\n\n\n\n\nChapter 1\n\nIntroduction\n\n17\n\n\n\n18 CHAPTER 1. INTRODUCTION\n\n\n\nChapter 2\n\nGroups, Rings, and Fields\n\nIn the following four chapters, the basic algebraic structures (groups, rings, fields, vector\nspaces) are reviewed, with a major emphasis on vector spaces. Basic notions of linear alge-\nbra such as vector spaces, subspaces, linear combinations, linear independence, bases, quo-\ntient spaces, linear maps, matrices, change of bases, direct sums, linear forms, dual spaces,\nhyperplanes, transpose of a linear maps, are reviewed.\n\n2.1 Groups, Subgroups, Cosets\n\nThe set R of real numbers has two operations +: R × R → R (addition) and ∗ : R × R →\nR (multiplication) satisfying properties that make R into an abelian group under +, and\nR− {0} = R∗ into an abelian group under ∗. Recall the definition of a group.\n\nDefinition 2.1. A group is a set G equipped with a binary operation · : G × G → G that\nassociates an element a · b ∈ G to every pair of elements a, b ∈ G, and having the following\nproperties: · is associative, has an identity element e ∈ G, and every element in G is invertible\n(w.r.t. ·). More explicitly, this means that the following equations hold for all a, b, c ∈ G:\n\n(G1) a · (b · c) = (a · b) · c. (associativity);\n\n(G2) a · e = e · a = a. (identity);\n\n(G3) For every a ∈ G, there is some a−1 ∈ G such that a · a−1 = a−1 · a = e. (inverse).\n\nA group G is abelian (or commutative) if\n\na · b = b · a for all a, b ∈ G.\n\nA set M together with an operation · : M ×M → M and an element e satisfying only\nConditions (G1) and (G2) is called a monoid . For example, the set N = {0, 1, . . . , n, . . .} of\nnatural numbers is a (commutative) monoid under addition. However, it is not a group.\n\nSome examples of groups are given below.\n\n19\n\n\n\n20 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.1.\n\n1. The set Z = {. . . ,−n, . . . ,−1, 0, 1, . . . , n, . . .} of integers is an abelian group under\naddition, with identity element 0. However, Z∗ = Z − {0} is not a group under\nmultiplication.\n\n2. The set Q of rational numbers (fractions p/q with p, q ∈ Z and q 6= 0) is an abelian\ngroup under addition, with identity element 0. The set Q∗ = Q−{0} is also an abelian\ngroup under multiplication, with identity element 1.\n\n3. Given any nonempty set S, the set of bijections f : S → S, also called permutations\nof S, is a group under function composition (i.e., the multiplication of f and g is the\ncomposition g ◦ f), with identity element the identity function idS. This group is not\nabelian as soon as S has more than two elements. The permutation group of the set\nS = {1, . . . , n} is often denoted Sn and called the symmetric group on n elements.\n\n4. For any positive integer p ∈ N, define a relation on Z, denoted m ≡ n (mod p), as\nfollows:\n\nm ≡ n (mod p) iff m− n = kp for some k ∈ Z.\n\nThe reader will easily check that this is an equivalence relation, and, moreover, it is\ncompatible with respect to addition and multiplication, which means that if m1 ≡ n1\n\n(mod p) and m2 ≡ n2 (mod p), then m1 + m2 ≡ n1 + n2 (mod p) and m1m2 ≡ n1n2\n\n(mod p). Consequently, we can define an addition operation and a multiplication\noperation of the set of equivalence classes (mod p):\n\n[m] + [n] = [m+ n]\n\nand\n[m] · [n] = [mn].\n\nThe reader will easily check that addition of residue classes (mod p) induces an abelian\ngroup structure with [0] as zero. This group is denoted Z/pZ.\n\n5. The set of n×n invertible matrices with real (or complex) coefficients is a group under\nmatrix multiplication, with identity element the identity matrix In. This group is\ncalled the general linear group and is usually denoted by GL(n,R) (or GL(n,C)).\n\n6. The set of n × n invertible matrices A with real (or complex) coefficients such that\ndet(A) = 1 is a group under matrix multiplication, with identity element the identity\nmatrix In. This group is called the special linear group and is usually denoted by\nSL(n,R) (or SL(n,C)).\n\n7. The set of n× n matrices Q with real coefficients such that\n\nQQ> = Q>Q = In\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nwe have Q−1 = Q>. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n× n invertible matrices Q with real coefficients such that\n\nQQ> = Q>Q = In and det(Q) = 1\n\nis a group under matrix multiplication, with identity element the identity matrix In;\nas in (6), we have Q−1 = Q>. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)–(8) are nonabelian for n ≥ 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a−1 of an element a ∈ G is denoted by −a.\n\nThe identity element of a group is unique. In fact, we can prove a more general fact:\n\nProposition 2.1. If a binary operation · : M ×M → M is associative and if e′ ∈ M is a\nleft identity and e′′ ∈M is a right identity, which means that\n\ne′ · a = a for all a ∈M (G2l)\n\nand\na · e′′ = a for all a ∈M, (G2r)\n\nthen e′ = e′′.\n\nProof. If we let a = e′′ in equation (G2l), we get\n\ne′ · e′′ = e′′,\n\nand if we let a = e′ in equation (G2r), we get\n\ne′ · e′′ = e′,\n\nand thus\ne′ = e′ · e′′ = e′′,\n\nas claimed.\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n2.1. GROUPS, SUBGROUPS, COSETS 21\n\nis a group under matrix multiplication, with identity element the identity matrix /,,;\nwe have Q-! = Q'. This group is called the orthogonal group and is usually denoted\nby O(n).\n\n8. The set of n x n invertible matrices Q with real coefficients such that\n\nQQ'=Q'Q=I, and det(Q)=1\n\nis a group under matrix multiplication, with identity element the identity matrix [,,;\nas in (6), we have Q-! = Q'. This group is called the special orthogonal group or\nrotation group and is usually denoted by SO(n).\n\nThe groups in (5)—(8) are nonabelian for n > 2, except for SO(2) which is abelian (but O(2)\nis not abelian).\n\nIt is customary to denote the operation of an abelian group G by +, in which case the\ninverse a~! of an element a € G is denoted by —a.\n\nThe identity element of a group is unzque. In fact, we can prove a more general fact:\n\nProposition 2.1. /f a binary operation -: M x M + M is associative and if e’ € M is a\nleft identity and e” € M is a right identity, which means that\n\ne-a=a forall ae M (G21)\n\nand\na-e’=a forall ae M, (G2r)\n\nthen e' = e”.\nProof. If we let a = e” in equation (G21), we get\n/ \" 1\n\ne-e =e,\n\nand if we let a = e’ in equation (G2r), we get\n\nand thus\n\nas claimed. Oo\n\nProposition 2.1 implies that the identity element of a monoid is unique, and since every\ngroup is a monoid, the identity element of a group is unique. Furthermore, every element in\na group has a unique inverse. This is a consequence of a slightly more general fact:\n\n\n\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a ∈M has some\nleft inverse a′ ∈M and some right inverse a′′ ∈M , which means that\n\na′ · a = e (G3l)\n\nand\na · a′′ = e, (G3r)\n\nthen a′ = a′′.\n\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a′ · a) · a′′ = e · a′′ = a′′.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na′ · (a · a′′) = a′ · e = a′.\n\nHowever, since M is monoid, the operation · is associative, so\n\na′ = a′ · (a · a′′) = (a′ · a) · a′′ = a′′,\n\nas claimed.\n\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n(G2l) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G is a group\nof order n. If G is infinite, we say that G has infinite order . The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R, S ⊆ G, we let\n\nRS = {r · s | r ∈ R, s ∈ S}.\n\nIn particular, for any g ∈ G, if R = {g}, we write\n\ngS = {g · s | s ∈ S},\n\nand similarly, if S = {g}, we write\n\nRg = {r · g | r ∈ R}.\n\nFrom now on, we will drop the multiplication sign and write g1g2 for g1 · g2.\n\n22 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.2. In a monoid M with identity element e, if some element a € M has some\nleft inverse a’ € M and some right inverse a\" € M, which means that\n\na-a=e (G31)\n\nand\na-a\" =e, (G3r)\n\nthen a’ =a\".\nProof. Using (G3l) and the fact that e is an identity element, we have\n\n(a’-a) ql — e-q’ — a’.\n\nSimilarly, Using (G3r) and the fact that e is an identity element, we have\n\na’-(a-a\")=d'-e=d’.\nHowever, since M is monoid, the operation - is associative, so\na’ =a'-(a-a\") =(a'-a)-a\" =a\",\nas claimed. O\nRemark: Axioms (G2) and (G3) can be weakened a bit by requiring only (G2r) (the exis-\ntence of a right identity) and (G3r) (the existence of a right inverse for every element) (or\n\n(G21) and (G3l)). It is a good exercise to prove that the group axioms (G2) and (G3) follow\nfrom (G2r) and (G3r).\n\nDefinition 2.2. If a group G has a finite number n of elements, we say that G' is a group\nof order n. If G is infinite, we say that G has infinite order. The order of a group is usually\ndenoted by |G| (if G is finite).\n\nGiven a group G, for any two subsets R,S' C G, we let\nRS ={r-s|reR,seS}.\nIn particular, for any g € G, if R = {g}, we write\ngS = {g-s|s € S},\nand similarly, if S = {g}, we write\n\nRg={r-g|reR}.\n\nFrom now on, we will drop the multiplication sign and write gg for gi - go.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 23\n\nDefinition 2.3. Let G be a group. For any g ∈ G, define Lg, the left translation by g, by\nLg(a) = ga, for all a ∈ G, and Rg, the right translation by g, by Rg(a) = ag, for all a ∈ G.\n\nThe following simple fact is often used.\n\nProposition 2.3. Given a group G, the translations Lg and Rg are bijections.\n\nProof. We show this for Lg, the proof for Rg being similar.\n\nIf Lg(a) = Lg(b), then ga = gb, and multiplying on the left by g−1, we get a = b, so Lg\ninjective. For any b ∈ G, we have Lg(g\n\n−1b) = gg−1b = b, so Lg is surjective. Therefore, Lg\nis bijective.\n\nDefinition 2.4. Given a group G, a subset H of G is a subgroup of G iff\n\n(1) The identity element e of G also belongs to H (e ∈ H);\n\n(2) For all h1, h2 ∈ H, we have h1h2 ∈ H;\n\n(3) For all h ∈ H, we have h−1 ∈ H.\n\nThe proof of the following proposition is left as an exercise.\n\nProposition 2.4. Given a group G, a subset H ⊆ G is a subgroup of G iff H is nonempty\nand whenever h1, h2 ∈ H, then h1h\n\n−1\n2 ∈ H.\n\nIf the group G is finite, then the following criterion can be used.\n\nProposition 2.5. Given a finite group G, a subset H ⊆ G is a subgroup of G iff\n\n(1) e ∈ H;\n\n(2) H is closed under multiplication.\n\nProof. We just have to prove that Condition (3) of Definition 2.4 holds. For any a ∈ H,\nsince the left translation La is bijective, its restriction to H is injective, and since H is finite,\nit is also bijective. Since e ∈ H, there is a unique b ∈ H such that La(b) = ab = e. However,\nif a−1 is the inverse of a in G, we also have La(a\n\n−1) = aa−1 = e, and by injectivity of La, we\nhave a−1 = b ∈ H.\n\nExample 2.2.\n\n1. For any integer n ∈ Z, the set\n\nnZ = {nk | k ∈ Z}\n\nis a subgroup of the group Z.\n\n\n\n24 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The set of matrices\n\nGL+(n,R) = {A ∈ GL(n,R) | det(A) > 0}\n\nis a subgroup of the group GL(n,R).\n\n3. The group SL(n,R) is a subgroup of the group GL(n,R).\n\n4. The group O(n) is a subgroup of the group GL(n,R).\n\n5. The group SO(n) is a subgroup of the group O(n), and a subgroup of the group\nSL(n,R).\n\n6. It is not hard to show that every 2× 2 rotation matrix R ∈ SO(2) can be written as\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n, with 0 ≤ θ < 2π.\n\nThen SO(2) can be considered as a subgroup of SO(3) by viewing the matrix\n\nR =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\nas the matrix\n\nQ =\n\ncos θ − sin θ 0\nsin θ cos θ 0\n\n0 0 1\n\n .\n\n7. The set of 2× 2 upper-triangular matrices of the form(\na b\n0 c\n\n)\na, b, c ∈ R, a, c 6= 0\n\nis a subgroup of the group GL(2,R).\n\n8. The set V consisting of the four matrices(\n±1 0\n0 ±1\n\n)\nis a subgroup of the group GL(2,R) called the Klein four-group.\n\nDefinition 2.5. If H is a subgroup of G and g ∈ G is any element, the sets of the form\ngH are called left cosets of H in G and the sets of the form Hg are called right cosets of H\nin G. The left cosets (resp. right cosets) of H induce an equivalence relation ∼ defined as\nfollows: For all g1, g2 ∈ G,\n\ng1 ∼ g2 iff g1H = g2H\n\n(resp. g1 ∼ g2 iff Hg1 = Hg2). Obviously, ∼ is an equivalence relation.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 25\n\nNow, we claim the following fact:\n\nProposition 2.6. Given a group G and any subgroup H of G, we have g1H = g2H iff\ng−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H, for all g1, g2 ∈ G.\n\nProof. If we apply the bijection Lg−1\n2\n\nto both g1H and g2H we get Lg−1\n2\n\n(g1H) = g−1\n2 g1H\n\nand Lg−1\n2\n\n(g2H) = H, so g1H = g2H iff g−1\n2 g1H = H. If g−1\n\n2 g1H = H, since 1 ∈ H, we get\n\ng−1\n2 g1 ∈ H. Conversely, if g−1\n\n2 g1 ∈ H, since H is a group, the left translation Lg−1\n2 g1\n\nis a\n\nbijection of H, so g−1\n2 g1H = H. Thus, g−1\n\n2 g1H = H iff g−1\n2 g1 ∈ H.\n\nIt follows that the equivalence class of an element g ∈ G is the coset gH (resp. Hg).\nSince Lg is a bijection between H and gH, the cosets gH all have the same cardinality. The\nmap Lg−1 ◦ Rg is a bijection between the left coset gH and the right coset Hg, so they also\nhave the same cardinality. Since the distinct cosets gH form a partition of G, we obtain the\nfollowing fact:\n\nProposition 2.7. (Lagrange) For any finite group G and any subgroup H of G, the order\nh of H divides the order n of G.\n\nDefinition 2.6. Given a finite group G and a subgroup H of G, if n = |G| and h = |H|,\nthen the ratio n/h is denoted by (G : H) and is called the index of H in G.\n\nThe index (G : H) is the number of left (and right) cosets of H in G. Proposition 2.7\ncan be stated as\n\n|G| = (G : H)|H|.\n\nThe set of left cosets of H in G (which, in general, is not a group) is denoted G/H.\nThe “points” of G/H are obtained by “collapsing” all the elements in a coset into a single\nelement.\n\nExample 2.3.\n\n1. Let n be any positive integer, and consider the subgroup nZ of Z (under addition).\nThe coset of 0 is the set {0}, and the coset of any nonzero integer m ∈ Z is\n\nm+ nZ = {m+ nk | k ∈ Z}.\n\nBy dividing m by n, we have m = nq + r for some unique r such that 0 ≤ r ≤ n− 1.\nBut then we see that r is the smallest positive element of the coset m + nZ. This\nimplies that there is a bijection betwen the cosets of the subgroup nZ of Z and the set\nof residues {0, 1, . . . , n− 1} modulo n, or equivalently a bijection with Z/nZ.\n\n\n\n26 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2. The cosets of SL(n,R) in GL(n,R) are the sets of matrices\n\nASL(n,R) = {AB | B ∈ SL(n,R)}, A ∈ GL(n,R).\n\nSince A is invertible, det(A) 6= 0, and we can write A = (det(A))1/n((det(A))−1/nA)\nif det(A) > 0 and A = (− det(A))1/n((− det(A))−1/nA) if det(A) < 0. But we have\n(det(A))−1/nA ∈ SL(n,R) if det(A) > 0 and −(− det(A))−1/nA ∈ SL(n,R) if det(A) <\n0, so the coset ASL(n,R) contains the matrix\n\n(det(A))1/nIn if det(A) > 0, −(− det(A))1/nIn if det(A) < 0.\n\nIt follows that there is a bijection between the cosets of SL(n,R) in GL(n,R) and R.\n\n3. The cosets of SO(n) in GL+(n,R) are the sets of matrices\n\nASO(n) = {AQ | Q ∈ SO(n)}, A ∈ GL+(n,R).\n\nIt can be shown (using the polar form for matrices) that there is a bijection between\nthe cosets of SO(n) in GL+(n,R) and the set of n × n symmetric, positive, definite\nmatrices; these are the symmetric matrices whose eigenvalues are strictly positive.\n\n4. The cosets of SO(2) in SO(3) are the sets of matrices\n\nQSO(2) = {QR | R ∈ SO(2)}, Q ∈ SO(3).\n\nThe group SO(3) moves the points on the sphere S2 in R3, namely for any x ∈ S2,\n\nx 7→ Qx for any rotation Q ∈ SO(3).\n\nHere,\nS2 = {(x, y, z) ∈ R3 | x2 + y2 + z2 = 1}.\n\nLet N = (0, 0, 1) be the north pole on the sphere S2. Then it is not hard to show that\nSO(2) is precisely the subgroup of SO(3) that leaves N fixed. As a consequence, all\nrotations QR in the coset QSO(2) map N to the same point QN ∈ S2, and it can be\nshown that there is a bijection between the cosets of SO(2) in SO(3) and the points\non S2. The surjectivity of this map has to do with the fact that the action of SO(3)\non S2 is transitive, which means that for any point x ∈ S2, there is some rotation\nQ ∈ SO(3) such that QN = x.\n\nIt is tempting to define a multiplication operation on left cosets (or right cosets) by\nsetting\n\n(g1H)(g2H) = (g1g2)H,\n\nbut this operation is not well defined in general, unless the subgroup H possesses a special\nproperty. In Example 2.3, it is possible to define multiplication of cosets in (1), but it is not\npossible in (2) and (3).\n\nThe property of the subgroup H that allows defining a multiplication operation on left\ncosets is typical of the kernels of group homomorphisms, so we are led to the following\ndefinition.\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G′, a function ϕ : G→ G′ is a homomorphism\niff\n\nϕ(g1g2) = ϕ(g1)ϕ(g2), for all g1, g2 ∈ G.\n\nTaking g1 = g2 = e (in G), we see that\n\nϕ(e) = e′,\n\nand taking g1 = g and g2 = g−1, we see that\n\nϕ(g−1) = (ϕ(g))−1.\n\nExample 2.4.\n\n1. The map ϕ : Z→ Z/nZ given by ϕ(m) = m mod n for all m ∈ Z is a homomorphism.\n\n2. The map det : GL(n,R) → R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A,B. Similarly, the map det : O(n)→ R is a homomorphism.\n\nIf ϕ : G → G′ and ψ : G′ → G′′ are group homomorphisms, then ψ ◦ ϕ : G → G′′ is also\na homomorphism. If ϕ : G→ G′ is a homomorphism of groups, and if H ⊆ G, H ′ ⊆ G′ are\ntwo subgroups, then it is easily checked that\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H}\n\nis a subgroup of G′ and\nϕ−1(H ′) = {g ∈ G | ϕ(g) ∈ H ′}\n\nis a subgroup of G. In particular, when H ′ = {e′}, we obtain the kernel , Ker ϕ, of ϕ.\n\nDefinition 2.8. If ϕ : G → G′ is a homomorphism of groups, and if H ⊆ G is a subgroup\nof G, then the subgroup of G′,\n\nIm H = ϕ(H) = {ϕ(g) | g ∈ H},\n\nis called the image of H by ϕ, and the subgroup of G,\n\nKer ϕ = {g ∈ G | ϕ(g) = e′},\n\nis called the kernel of ϕ.\n\nExample 2.5.\n\n1. The kernel of the homomorphism ϕ : Z→ Z/nZ is nZ.\n\n2. The kernel of the homomorphism det : GL(n,R)→ R is SL(n,R). Similarly, the kernel\nof the homomorphism det : O(n)→ R is SO(n).\n\n2.1. GROUPS, SUBGROUPS, COSETS 27\n\nDefinition 2.7. Given any two groups G and G’, a function y: G > G’ is a homomorphism\niff\n(9192) = P(m)e(G2), for all gr, g2 € G.\n\nTaking g, = go =e (in G), we see that\n\n! we see that\n\nog\") = (v(g))*.\n\nand taking g; = g and g2 = g~\n\nExample 2.4.\n1. The map y: Z > Z/nZ given by y(m) = m mod n for all m € Z is a homomorphism.\n\n2. The map det: GL(n, R) — R is a homomorphism because det(AB) = det(A) det(B)\nfor any two matrices A, B. Similarly, the map det: O(n) > R is a homomorphism.\n\nIf y: G > G' and wy: G’ > G\"” are group homomorphisms, then yo y: G > G\" is also\na homomorphism. If y: G + G’ is a homomorphism of groups, and if H C G, H’ C G’ are\ntwo subgroups, then it is easily checked that\n\nIm H = 9(H) = {y(9) |g © A}\n\nis a subgroup of G’ and\neg (H')= {9 €G| lg) € H’}\nis a subgroup of G. In particular, when H’ = {e’}, we obtain the kernel, Ker y, of y.\n\nDefinition 2.8. If ¢: G > G’ is a homomorphism of groups, and if H C G is a subgroup\nof G, then the subgroup of G’,\n\nIm H = 9(H) = {y(9) |g € FH},\n\nis called the image of H by vy, and the subgroup of G,\n\nKer p= {9 €G | p(y) =e},\nis called the kernel of y.\nExample 2.5.\n1. The kernel of the homomorphism y: Z > Z/nZ is nZ.\n\n2. The kernel of the homomorphism det: GL(n, R) > Ris SL(n, R). Similarly, the kernel\nof the homomorphism det: O(n) > R is SO(n).\n\n\n\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. If ϕ : G→ G′ is a homomorphism of groups, then ϕ : G→ G′ is injective\niff Ker ϕ = {e}. (We also write Ker ϕ = (0).)\n\nProof. Assume ϕ is injective. Since ϕ(e) = e′, if ϕ(g) = e′, then ϕ(g) = ϕ(e), and by\ninjectivity of ϕ we must have g = e, so Ker ϕ = {e}.\n\nConversely, assume that Ker ϕ = {e}. If ϕ(g1) = ϕ(g2), then by multiplication on the\nleft by (ϕ(g1))−1 we get\n\ne′ = (ϕ(g1))−1ϕ(g1) = (ϕ(g1))−1ϕ(g2),\n\nand since ϕ is a homomorphism (ϕ(g1))−1 = ϕ(g−1\n1 ), so\n\ne′ = (ϕ(g1))−1ϕ(g2) = ϕ(g−1\n1 )ϕ(g2) = ϕ(g−1\n\n1 g2).\n\nThis shows that g−1\n1 g2 ∈ Ker ϕ, but since Ker ϕ = {e} we have g−1\n\n1 g2 = e, and thus g2 = g1,\nproving that ϕ is injective.\n\nDefinition 2.9. We say that a group homomorphism ϕ : G→ G′ is an isomorphism if there\nis a homomorphism ψ : G′ → G, so that\n\nψ ◦ ϕ = idG and ϕ ◦ ψ = idG′ . (†)\n\nIf ϕ is an isomorphism we say that the groups G and G′ are isomorphic. When G′ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nϕ : G→ G′ is an isomorphism, then the homomorphism ψ : G′ → G satisfying Condition (†)\nis unique. This homomorphism is denoted ϕ−1.\n\nThe left translations Lg and the right translations Rg are automorphisms of G.\n\nSuppose ϕ : G → G′ is a bijective homomorphism, and let ϕ−1 be the inverse of ϕ (as a\nfunction). Then for all a, b ∈ G, we have\n\nϕ(ϕ−1(a)ϕ−1(b)) = ϕ(ϕ−1(a))ϕ(ϕ−1(b)) = ab,\n\nand so\n\nϕ−1(ab) = ϕ−1(a)ϕ−1(b),\n\nwhich proves that ϕ−1 is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism ϕ : G→ G′ is an isomorphism.\n\n28 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nThe following characterization of the injectivity of a group homomorphism is used all the\ntime.\n\nProposition 2.8. Ifo: G > G’' is a homomorphism of groups, then py: G — G\" is injective\niff Ker y = {e}. (We also write Ker y = (0).)\n\nProof. Assume ¢p is injective. Since y(e) = e’, if y(g) = e’, then y(g) = y(e), and by\ninjectivity of ~ we must have g = e, so Ker y = {e}.\n\nConversely, assume that Ker y = {e}. If y(g1) = v(ge), then by multiplication on the\nleft by (y(g1))~* we get\n\ne’ = (9(m)) yg) = (¢(91)) *9(92),\n\nand since y is a homomorphism (y(g))~! = y(g,\"), so\n\ne' = (y(m1)) *v(g2) = v(97')9(92) = v(9r 92):\n\nThis shows that g>'g2 € Ker y, but since Ker y = {e} we have gj ‘go = e, and thus g = gu,\nproving that y is injective. L\n\nDefinition 2.9. We say that a group homomorphism y: G — G’ is an isomorphism if there\nis a homomorphism w: G’ + G, so that\n\nwop=idg and pow=idq@. (Tt)\n\nIf y is an isomorphism we say that the groups G and G’ are isomorphic. When G’ = G, a\ngroup isomorphism is called an automorphism.\n\nThe reasoning used in the proof of Proposition 2.2 shows that if a a group homomorphism\nyp: G > G’ is an isomorphism, then the homomorphism ~: G’ + G satisfying Condition (T)\nis unique. This homomorphism is denoted y!.\n\nThe left translations L, and the right translations R, are automorphisms of G.\n\nSuppose vy: G > G’ is a bijective homomorphism, and let y~' be the inverse of y (as a\nfunction). Then for all a,b € G, we have\n\no(y ‘(a)y *(b)) = v(e*(a))e(y *(b)) = ab,\n\nand so\nyp *(ab) =p ‘(a)y (0),\n\nwhich proves that y~! is a homomorphism. Therefore, we proved the following fact.\n\nProposition 2.9. A bijective group homomorphism yp: G > G\" is an isomorphism.\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\n\ngH = Hg, for all g ∈ G. (∗)\n\nis equivalent by multiplication on the right by g−1 to\n\ngHg−1 = H, for all g ∈ G,\n\nand the above is equivalent to\n\ngHg−1 ⊆ H, for all g ∈ G. (∗∗)\n\nThis is because gHg−1 ⊆ H implies H ⊆ g−1Hg, and this for all g ∈ G.\n\nProposition 2.10. Let ϕ : G → G′ be a group homomorphism. Then H = Ker ϕ satisfies\nProperty (∗∗), and thus Property (∗).\n\nProof. We have\n\nϕ(ghg−1) = ϕ(g)ϕ(h)ϕ(g−1) = ϕ(g)e′ϕ(g)−1 = ϕ(g)ϕ(g)−1 = e′,\n\nfor all h ∈ H = Ker ϕ and all g ∈ G. Thus, by definition of H = Ker ϕ, we have gHg−1 ⊆\nH.\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\n\ngNg−1 = N, for all g ∈ G.\n\nThis is denoted by N CG.\n\nProposition 2.10 shows that the kernel Ker ϕ of a homomorphism ϕ : G→ G′ is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R ∈ SO(2) and A ∈ SL(2,R) be the matrices\n\nR =\n\n(\n0 −1\n1 0\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nThen\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\nand we have\n\nARA−1 =\n\n(\n1 1\n0 1\n\n)(\n0 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n1 0\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n1 −1\n\n)\n,\n\n2.1. GROUPS, SUBGROUPS, COSETS 29\n\nObserve that the property\ngH =Hg, forallg €G. (*)\nis equivalent by multiplication on the right by g~! to\ngHg ‘=H, forallg€G,\nand the above is equivalent to\ngHg' CH, forallg€G. (x)\nThis is because gHg~' C H implies H C g7!Hq, and this for all g € G.\n\nProposition 2.10. Let y: G — G’ be a group homomorphism. Then H = Ker y satisfies\nProperty (**), and thus Property (x).\n\nProof. We have\n\n/\n\ne(ghg*) = v(gelh)ye(g\") = v(ae'e(9)* = vla)e(g)* =e,\n\nfor all h € H = Ker y and all g € G. Thus, by definition of H = Ker y, we have gHg™! C\nH. O\n\nDefinition 2.10. For any group G, a subgroup N of G is a normal subgroup of G iff\ngNg |=N, forallg €G.\n\nThis is denoted by N dG.\n\nProposition 2.10 shows that the kernel Ker y of a homomorphism y: G > G\" is a normal\nsubgroup of G.\n\nObserve that if G is abelian, then every subgroup of G is normal.\n\nConsider Example 2.2. Let R € SO(2) and A € SL(2,R) be the matrices\n\ne=(1o)) a= i)\n\nThen\n\nand we have\n\nae (00 G6 TG ov) Z)-G 5)\n\n\n\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA−1 /∈ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R ∈ SO(2) and Q ∈ SO(3) be the matrices\n\nR =\n\n0 −1 0\n1 0 0\n0 0 1\n\n , Q =\n\n1 0 0\n0 0 −1\n0 1 0\n\n .\n\nThen\n\nQ−1 = Q> =\n\n1 0 0\n0 0 1\n0 −1 0\n\n\nand we have\n\nQRQ−1 =\n\n1 0 0\n0 0 −1\n0 1 0\n\n0 −1 0\n1 0 0\n0 0 1\n\n1 0 0\n0 0 1\n0 −1 0\n\n =\n\n0 −1 0\n0 0 −1\n1 0 0\n\n1 0 0\n0 0 1\n0 −1 0\n\n\n=\n\n0 0 −1\n0 1 0\n1 0 0\n\n .\n\nObserve that QRQ−1 /∈ SO(2), so SO(2) is not a normal subgroup of SO(3).\n\nLet T and A ∈ GL(2,R) be the following matrices\n\nT =\n\n(\n1 1\n0 1\n\n)\n, A =\n\n(\n0 1\n1 0\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n0 1\n1 0\n\n)\n= A,\n\nand\n\nATA−1 =\n\n(\n0 1\n1 0\n\n)(\n1 1\n0 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n0 1\n1 1\n\n)(\n0 1\n1 0\n\n)\n=\n\n(\n1 0\n1 1\n\n)\n.\n\nThe matrix T is upper triangular, but ATA−1 is not, so the group of 2× 2 upper triangular\nmatrices is not a normal subgroup of GL(2,R).\n\nLet Q ∈ V and A ∈ GL(2,R) be the following matrices\n\nQ =\n\n(\n1 0\n0 −1\n\n)\n, A =\n\n(\n1 1\n0 1\n\n)\n.\n\nWe have\n\nA−1 =\n\n(\n1 −1\n0 1\n\n)\n\n30 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nand clearly ARA~! ¢ SO(2). Therefore SO(2) is not a normal subgroup of SL(2,R). The\nsame counter-example shows that O(2) is not a normal subgroup of GL(2,R).\n\nLet R € SO(2) and Q € SO(3) be the matrices\n\n0 -1 O 10 0\nR={1 0 0], Q={0 0 -1\n0 0 1 01 0\nThen\n1 O 0\nQ't=Q'={0 0 1\n0 —l1 O\nand we have\n10 0 0 -1 0 1 O 0 0 -1 O 1 O 0\nQRQ‘'*={0 0 -1 1 0 O 0 0 1y}y=]0 0 -I1 0 0 1\n01 0 0 0 1 0 —-l1 O 1 0 0O 0 —-1 O\n00 -l\n=|{|0 1 O\n10 0\n\nObserve that QRQ7! ¢ SO(2), so SO(2) is not a normal subgroup of SO(3).\nLet T and A € GL(2,R) be the following matrices\n\n(0)\nAt= (9 )) =4\naera 0 CO a) =0 9):\n\nThe matrix T is upper triangular, but ATA! is not, so the group of 2 x 2 upper triangular\nmatrices is not a normal subgroup of GL(2, R).\n\nLet Q € V and A € GL(2,R) be the following matrices\n\na=(5 4). 4-0)\n\nWe have\n\nWe have\n\n\n\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\nand\n\nAQA−1 =\n\n(\n1 1\n0 1\n\n)(\n1 0\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −1\n0 −1\n\n)(\n1 −1\n0 1\n\n)\n=\n\n(\n1 −2\n0 −1\n\n)\n.\n\nClearly AQA−1 /∈ V , which shows that the Klein four group is not a normal subgroup of\nGL(2,R).\n\nThe reader should check that the subgroups nZ, GL+(n,R), SL(n,R), and SO(n,R) as\na subgroup of O(n,R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ∼ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all g1, g2, g\n\n′\n1, g\n′\n2 ∈ G,\n\n(1) If g1N = g′1N and g2N = g′2N , then g1g2N = g′1g\n′\n2N , and\n\n(2) If g1N = g2N , then g−1\n1 N = g−1\n\n2 N .\n\nAs a consequence, we can define a group structure on the set G/ ∼ of equivalence classes\nmodulo ∼, by setting\n\n(g1N)(g2N) = (g1g2)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(g1N)(g2N) = (g1g2)N, g1, g2 ∈ G\n\nis denoted G/N , and called the quotient of G by N . The equivalence class gN of an element\ng ∈ G is also denoted g (or [g]). The map π : G→ G/N given by\n\nπ(g) = g = gN\n\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups ϕ : G→ G′, the groups G/Ker ϕ and\nIm ϕ = ϕ(G) are isomorphic.\n\nProof. Since ϕ is surjective onto its image, we may assume that ϕ is surjective, so that\nG′ = Im ϕ. We define a map ϕ : G/Ker ϕ→ G′ as follows:\n\nϕ(g) = ϕ(g), g ∈ G.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = gKer ϕ, and that it is a homomorphism. If g′ is another element in\nthe coset gKer ϕ, which means that g′ = gh for some h ∈ Kerϕ, then\n\nϕ(g′) = ϕ(gh) = ϕ(g)ϕ(h) = ϕ(g)e′ = ϕ(g),\n\n2.1. GROUPS, SUBGROUPS, COSETS 31\n\n1 fl i\\fa 0\\fa 71). fa -1\\ fa -1\\_ 1 -2\n4os*=(5 i)(0 S)(o a)=(0 a) (0 a )=(0 =):\nClearly AQA~' ¢ V, which shows that the Klein four group is not a normal subgroup of\n\nGL(2, R).\n\nThe reader should check that the subgroups nZ, GL*(n,R), SL(n, R), and SO(n, R) as\na subgroup of O(n, R), are normal subgroups.\n\nIf N is a normal subgroup of G, the equivalence relation ~ induced by left cosets (see\nDefinition 2.5) is the same as the equivalence induced by right cosets. Furthermore, this\nequivalence relation is a congruence, which means that: For all gi, 92, 95,95 € G,\n\n(1) IfgN =g,N and gN = 95N, then gigoN = gig5N, and\n(2) If g.N = goN, then g7'N = go N.\n\nAs a consequence, we can define a group structure on the set G/ ~ of equivalence classes\nmodulo ~, by setting\n\n(mN)(g2N) = (giga)N.\n\nDefinition 2.11. Let G be a group and N be a normal subgroup of G. The group obtained\nby defining the multiplication of (left) cosets by\n\n(mN)(gN) =(ng)N, 1,92€G\n\nis denoted G/N, and called the quotient of G by N. The equivalence class gN of an element\ng € G is also denoted g (or [g]). The map 7: G > G/N given by\n\n™(9)=9=9N\nis a group homomorphism called the canonical projection.\n\nSince the kernel of a homomorphism is a normal subgroup, we obtain the following very\nuseful result.\n\nProposition 2.11. Given a homomorphism of groups yp: G > G\", the groups G/Ker vy and\nIm y = ¢(G) are isomorphic.\n\nProof. Since y is surjective onto its image, we may assume that y is surjective, so that\nG’ = Im y. We define a map G: G/Ker y > G’ as follows:\n\nAM=v(9), GEG.\n\nWe need to check that the definition of this map does not depend on the representative\nchosen in the coset g = g Ker y, and that it is a homomorphism. If g’ is another element in\nthe coset g Ker y, which means that g’ = gh for some h € Ker y, then\n\n9(9') = v(gh) = v(g)y(h) = vlg)e’ = ¥(9),\n\n\n\n\n32 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nsince ϕ(h) = e′ as h ∈ Ker ϕ. This shows that\n\nϕ(g′) = ϕ(g′) = ϕ(g) = ϕ(g),\n\nso the map ϕ is well defined. It is a homomorphism because\n\nϕ(gg′) = ϕ(gg′)\n\n= ϕ(gg′)\n\n= ϕ(g)ϕ(g′)\n\n= ϕ(g)ϕ(g′).\n\nThe map ϕ is injective because ϕ(g) = e′ iff ϕ(g) = e′ iff g ∈ Ker ϕ, iff g = e. The map ϕ\nis surjective because ϕ is surjective. Therefore ϕ is a bijective homomorphism, and thus an\nisomorphism, as claimed.\n\nProposition 2.11 is called the first isomorphism theorem.\n\nA useful way to construct groups is the direct product construction.\n\nDefinition 2.12. Given two groups G an H, we let G×H be the Cartestian product of the\nsets G and H with the multiplication operation · given by\n\n(g1, h1) · (g2, h2) = (g1g2, h1h2).\n\nIt is immediately verified that G×H is a group called the direct product of G and H.\n\nSimilarly, given any n groups G1, . . . , Gn, we can define the direct product G1× · · ·×Gn\n\nis a similar way.\n\nIf G is an abelian group and H1, . . . , Hn are subgroups of G, the situation is simpler.\nConsider the map\n\na : H1 × · · · ×Hn → G\n\ngiven by\na(h1, . . . , hn) = h1 + · · ·+ hn,\n\nusing + for the operation of the group G. It is easy to verify that a is a group homomorphism,\nso its image is a subgroup of G denoted by H1 + · · ·+Hn, and called the sum of the groups\nHi. The following proposition will be needed.\n\nProposition 2.12. Given an abelian group G, if H1 and H2 are any subgroups of G such\nthat H1 ∩H2 = {0}, then the map a is an isomorphism\n\na : H1 ×H2 → H1 +H2.\n\nProof. The map is surjective by definition, so we just have to check that it is injective. For\nthis, we show that Ker a = {(0, 0)}. We have a(a1, a2) = 0 iff a1 + a2 = 0 iff a1 = −a2. Since\na1 ∈ H1 and a2 ∈ H2, we see that a1, a2 ∈ H1 ∩H2 = {0}, so a1 = a2 = 0, which proves that\nKer a = {(0, 0)}.\n\n\n\n2.2. CYCLIC GROUPS 33\n\nUnder the conditions of Proposition 2.12, namely H1 ∩H2 = {0}, the group H1 + H2 is\ncalled the direct sum of H1 and H2; it is denoted by H1 ⊕H2, and we have an isomorphism\nH1 ×H2\n\n∼= H1 ⊕H2.\n\n2.2 Cyclic Groups\n\nGiven a group G with unit element 1, for any element g ∈ G and for any natural number\nn ∈ N, define gn as follows:\n\ng0 = 1\n\ngn+1 = g · gn.\n\nFor any integer n ∈ Z, we define gn by\n\ngn =\n\n{\ngn if n ≥ 0\n\n(g−1)(−n) if n < 0.\n\nThe following properties are easily verified:\n\ngi · gj = gi+j\n\n(gi)−1 = g−i\n\ngi · gj = gj · gi,\n\nfor all i, j ∈ Z.\n\nDefine the subset 〈g〉 of G by\n\n〈g〉 = {gn | n ∈ Z}.\n\nThe following proposition is left as an exercise.\n\nProposition 2.13. Given a group G, for any element g ∈ G, the set 〈g〉 is the smallest\nabelian subgroup of G containing g.\n\nDefinition 2.13. A group G is cyclic iff there is some element g ∈ G such that G = 〈g〉.\nAn element g ∈ G with this property is called a generator of G.\n\nThe Klein four group V of Example 2.2 is abelian, but not cyclic. This is because V has\nfour elements, but all the elements different from the identity have order 2.\n\nCyclic groups are quotients of Z. For this, we use a basic property of Z. Recall that for\nany n ∈ Z, we let nZ denote the set of multiples of n,\n\nnZ = {nk | k ∈ Z}.\n\n\n\n34 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nProposition 2.14. Every subgroup H of Z is of the form H = nZ for some n ∈ N.\n\nProof. If H is the trivial group {0}, then let n = 0. If H is nontrivial, for any nonzero element\nm ∈ H, we also have −m ∈ H and either m or −m is positive, so let n be the smallest\npositive integer in H. By Proposition 2.13, nZ is the smallest subgroup of H containing n.\nFor any m ∈ H with m 6= 0, we can write\n\nm = nq + r, with 0 ≤ r < n.\n\nNow, since nZ ⊆ H, we have nq ∈ H, and since m ∈ H, we get r = m− nq ∈ H. However,\n0 ≤ r < n, contradicting the minimality of n, so r = 0, and H = nZ.\n\nGiven any cyclic group G, for any generator g of G, we can define a mapping ϕ : Z→ G\nby ϕ(m) = gm. Since g generates G, this mapping is surjective. The mapping ϕ is clearly a\ngroup homomorphism, so let H = Kerϕ be its kernel. By a previous observation, H = nZ\nfor some n ∈ Z, so by the first homomorphism theorem, we obtain an isomorphism\n\nϕ : Z/nZ −→ G\n\nfrom the quotient group Z/nZ onto G. Obviously, if G has finite order, then |G| = n. In\nsummary, we have the following result.\n\nProposition 2.15. Every cyclic group G is either isomorphic to Z, or to Z/nZ, for some\nnatural number n > 0. In the first case, we say that G is an infinite cyclic group, and in the\nsecond case, we say that G is a cyclic group of order n.\n\nThe quotient group Z/nZ consists of the cosets m+nZ = {m+nk | k ∈ Z}, with m ∈ Z,\nthat is, of the equivalence classes of Z under the equivalence relation ≡ defined such that\n\nx ≡ y iff x− y ∈ nZ iff x ≡ y (mod n).\n\nWe also denote the equivalence class x + nZ of x by x, or if we want to be more precise by\n[x]n. The group operation is given by\n\nx+ y = x+ y.\n\nFor every x ∈ Z, there is a unique representative, x mod n (the nonnegative remainder of\nthe division of x by n) in the class x of x, such that 0 ≤ x mod n ≤ n − 1. For this\nreason, we often identity Z/nZ with the set {0, . . . , n−1}. To be more rigorous, we can give\n{0, . . . , n− 1} a group structure by defining +n such that\n\nx+n y = (x+ y) mod n.\n\nThen, it is easy to see that {0, . . . , n − 1} with the operation +n is a group with identity\nelement 0 isomorphic to Z/nZ.\n\n\n\n2.2. CYCLIC GROUPS 35\n\nWe can also define a multiplication operation · on Z/nZ as follows:\n\na · b = ab = ab mod n.\n\nThen, it is easy to check that · is abelian, associative, that 1 is an identity element for ·, and\nthat · is distributive on the left and on the right with respect to addition. This makes Z/nZ\ninto a commutative ring . We usually suppress the dot and write a b instead of a · b.\n\nProposition 2.16. Given any integer n ≥ 1, for any a ∈ Z, the residue class a ∈ Z/nZ is\ninvertible with respect to multiplication iff gcd(a, n) = 1.\n\nProof. If a has inverse b in Z/nZ, then a b = 1, which means that\n\nab ≡ 1 (mod n),\n\nthat is ab = 1 + nk for some k ∈ Z, which is the Bezout identity\n\nab− nk = 1\n\nand implies that gcd(a, n) = 1. Conversely, if gcd(a, n) = 1, then by Bezout’s identity there\nexist u, v ∈ Z such that\n\nau+ nv = 1,\n\nso au = 1− nv, that is,\n\nau ≡ 1 (mod n),\n\nwhich means that a u = 1, so a is invertible in Z/nZ.\n\nDefinition 2.14. The group (under multiplication) of invertible elements of the ring Z/nZ\nis denoted by (Z/nZ)∗. Note that this group is abelian and only defined if n ≥ 2.\n\nThe Euler ϕ-function plays an important role in the theory of the groups (Z/nZ)∗.\n\nDefinition 2.15. Given any positive integer n ≥ 1, the Euler ϕ-function (or Euler totient\nfunction) is defined such that ϕ(n) is the number of integers a, with 1 ≤ a ≤ n, which are\nrelatively prime to n; that is, with gcd(a, n) = 1.1\n\nThen, by Proposition 2.16, we see that the group (Z/nZ)∗ has order ϕ(n).\n\nFor n = 2, (Z/2Z)∗ = {1}, the trivial group. For n = 3, (Z/3Z)∗ = {1, 2}, and for\nn = 4, we have (Z/4Z)∗ = {1, 3}. Both groups are isomorphic to the group {−1, 1}. Since\ngcd(a, n) = 1 for every a ∈ {1, . . . , n − 1} iff n is prime, by Proposition 2.16 we see that\n(Z/nZ)∗ = Z/nZ− {0} iff n is prime.\n\n1We allow a = n to accomodate the special case n = 1.\n\n\n\n36 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\n2.3 Rings and Fields\n\nThe groups Z,Q,R, C, Z/nZ, and Mn(R) are more than abelian groups, they are also\ncommutative rings. Furthermore, Q, R, and C are fields. We now introduce rings and fields.\n\nDefinition 2.16. A ring is a set A equipped with two operations +: A × A → A (called\naddition) and ∗ : A× A→ A (called multiplication) having the following properties:\n\n(R1) A is an abelian group w.r.t. +;\n\n(R2) ∗ is associative and has an identity element 1 ∈ A;\n\n(R3) ∗ is distributive w.r.t. +.\n\nThe identity element for addition is denoted 0, and the additive inverse of a ∈ A is\ndenoted by −a. More explicitly, the axioms of a ring are the following equations which hold\nfor all a, b, c ∈ A:\n\na+ (b+ c) = (a+ b) + c (associativity of +) (2.1)\n\na+ b = b+ a (commutativity of +) (2.2)\n\na+ 0 = 0 + a = a (zero) (2.3)\n\na+ (−a) = (−a) + a = 0 (additive inverse) (2.4)\n\na ∗ (b ∗ c) = (a ∗ b) ∗ c (associativity of ∗) (2.5)\n\na ∗ 1 = 1 ∗ a = a (identity for ∗) (2.6)\n\n(a+ b) ∗ c = (a ∗ c) + (b ∗ c) (distributivity) (2.7)\n\na ∗ (b+ c) = (a ∗ b) + (a ∗ c) (distributivity) (2.8)\n\nThe ring A is commutative if\n\na ∗ b = b ∗ a for all a, b ∈ A.\n\nFrom (2.7) and (2.8), we easily obtain\n\na ∗ 0 = 0 ∗ a = 0 (2.9)\n\na ∗ (−b) = (−a) ∗ b = −(a ∗ b). (2.10)\n\nNote that (2.9) implies that if 1 = 0, then a = 0 for all a ∈ A, and thus, A = {0}. The\nring A = {0} is called the trivial ring . A ring for which 1 6= 0 is called nontrivial . The\nmultiplication a ∗ b of two elements a, b ∈ A is often denoted by ab.\n\nExample 2.6.\n\n1. The additive groups Z,Q,R,C, are commutative rings.\n\n\n\n2.3. RINGS AND FIELDS 37\n\n2. For any positive integer n ∈ N, the group Z/nZ is a group under addition. We can\nalso define a multiplication operation by\n\na · b = ab = ab mod n,\n\nfor all a, b ∈ Z. The reader will easily check that the ring axioms are satisfied, with 0\nas zero and 1 as multiplicative unit. The resulting ring is denoted by Z/nZ.2\n\n3. The group R[X] of polynomials in one variable with real coefficients is a ring under\nmultiplication of polynomials. It is a commutative ring.\n\n4. Let d be any positive integer. If d is not divisible by any integer of the form m2, with\nm ∈ N and m ≥ 2, then we say that d is square-free. For example, d = 1, 2, 3, 5, 6, 7, 10\nare square-free, but 4, 8, 9, 12 are not square-free. If d is any square-free integer and if\nd ≥ 2, then the set of real numbers\n\nZ[\n√\nd] = {a+ b\n\n√\nd ∈ R | a, b ∈ Z}\n\nis a commutative a ring. If z = a + b\n√\nd ∈ Z[\n\n√\nd], we write z = a − b\n\n√\nd. Note that\n\nzz = a2 − db2.\n\n5. Similarly, if d ≥ 1 is a positive square-free integer, then the set of complex numbers\n\nZ[\n√\n−d] = {a+ ib\n\n√\nd ∈ C | a, b ∈ Z}\n\nis a commutative ring. If z = a + ib\n√\nd ∈ Z[\n\n√\n−d], we write z = a− ib\n\n√\nd. Note that\n\nzz = a2 + db2. The case where d = 1 is a famous example that was investigated by\nGauss, and Z[\n\n√\n−1], also denoted Z[i], is called the ring of Gaussian integers .\n\n6. The group of n× n matrices Mn(R) is a ring under matrix multiplication. However, it\nis not a commutative ring.\n\n7. The group C(a, b) of continuous functions f : (a, b) → R is a ring under the operation\nf · g defined such that\n\n(f · g)(x) = f(x)g(x)\n\nfor all x ∈ (a, b).\n\nDefinition 2.17. Given a ring A, for any element a ∈ A, if there is some element b ∈ A\nsuch that b 6= 0 and ab = 0, then we say that a is a zero divisor . A ring A is an integral\ndomain (or an entire ring) if 0 6= 1, A is commutative, and ab = 0 implies that a = 0 or\nb = 0, for all a, b ∈ A. In other words, an integral domain is a nontrivial commutative ring\nwith no zero divisors besides 0.\n\n2The notation Zn is sometimes used instead of Z/nZ but it clashes with the notation for the n-adic\nintegers so we prefer not to use it.\n\n\n\n38 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nExample 2.7.\n\n1. The rings Z,Q,R,C, are integral domains.\n\n2. The ring R[X] of polynomials in one variable with real coefficients is an integral domain.\n\n3. For any positive integer, n ∈ N, we have the ring Z/nZ. Observe that if n is composite,\nthen this ring has zero-divisors. For example, if n = 4, then we have\n\n2 · 2 ≡ 0 (mod 4).\n\nThe reader should prove that Z/nZ is an integral domain iff n is prime (use Proposition\n2.16).\n\n4. If d is a square-free positive integer and if d ≥ 2, the ring Z[\n√\nd] is an integral domain.\n\nSimilarly, if d ≥ 1 is a square-free positive integer, the ring Z[\n√\n−d] is an integral\n\ndomain. Finding the invertible elements of these rings is a very interesting problem.\n\n5. The ring of n× n matrices Mn(R) has zero divisors.\n\nA homomorphism between rings is a mapping preserving addition and multiplication\n(and 0 and 1).\n\nDefinition 2.18. Given two rings A and B, a homomorphism between A and B is a function\nh : A→ B satisfying the following conditions for all x, y ∈ A:\n\nh(x+ y) = h(x) + h(y)\n\nh(xy) = h(x)h(y)\n\nh(0) = 0\n\nh(1) = 1.\n\nActually, because B is a group under addition, h(0) = 0 follows from\n\nh(x+ y) = h(x) + h(y).\n\nExample 2.8.\n\n1. If A is a ring, for any integer n ∈ Z, for any a ∈ A, we define n · a by\n\nn · a = a+ · · ·+ a︸ ︷︷ ︸\nn\n\nif n ≥ 0 (with 0 · a = 0) and\nn · a = −(−n) · a\n\nif n < 0. Then, the map h : Z→ A given by\n\nh(n) = n · 1A\nis a ring homomorphism (where 1A is the multiplicative identity of A).\n\n\n\n2.3. RINGS AND FIELDS 39\n\n2. Given any real λ ∈ R, the evaluation map ηλ : R[X]→ R defined by\n\nηλ(f(X)) = f(λ)\n\nfor every polynomial f(X) ∈ R[X] is a ring homomorphism.\n\nDefinition 2.19. A ring homomorphism h : A → B is an isomorphism iff there is a ring\nhomomorphism g : B → A such that g ◦ f = idA and f ◦ g = idB. An isomorphism from a\nring to itself is called an automorphism.\n\nAs in the case of a group isomorphism, the homomorphism g is unique and denoted by\nh−1, and it is easy to show that a bijective ring homomorphism h : A→ B is an isomorphism.\n\nDefinition 2.20. Given a ring A, a subset A′ of A is a subring of A if A′ is a subgroup of\nA (under addition), is closed under multiplication, and contains 1.\n\nFor example, we have the following sequence in which every ring on the left of an inlcusion\nsign is a subring of the ring on the right of the inclusion sign:\n\nZ ⊆ Q ⊆ R ⊆ C.\n\nThe ring Z is a subring of both Z[\n√\nd] and Z[\n\n√\n−d], the ring Z[\n\n√\nd] is a subring of R and the\n\nring Z[\n√\n−d] is a subring of C.\n\nIf h : A→ B is a homomorphism of rings, then it is easy to show for any subring A′, the\nimage h(A′) is a subring of B, and for any subring B′ of B, the inverse image h−1(B′) is a\nsubring of A.\n\nAs for groups, the kernel of a ring homomorphism h : A→ B is defined by\n\nKer h = {a ∈ A | h(a) = 0}.\n\nJust as in the case of groups, we have the following criterion for the injectivity of a ring\nhomomorphism. The proof is identical to the proof for groups.\n\nProposition 2.17. If h : A → B is a homomorphism of rings, then h : A → B is injective\niff Ker h = {0}. (We also write Ker h = (0).)\n\nThe kernel of a ring homomorphism is an abelian subgroup of the additive group A, but\nin general it is not a subring of A, because it may not contain the multiplicative identity\nelement 1. However, it satisfies the following closure property under multiplication:\n\nab ∈ Ker h and ba ∈ Ker h for all a ∈ Ker h and all b ∈ A.\n\nThis is because if h(a) = 0, then for all b ∈ A we have\n\nh(ab) = h(a)h(b) = 0h(b) = 0 and h(ba) = h(b)h(a) = h(b)0 = 0.\n\n\n\n40 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.21. Given a ring A, an additive subgroup I of A satisfying the property below\n\nab ∈ I and ba ∈ I for all a ∈ I and all b ∈ A (∗ideal)\n\nis called a two-sided ideal . If A is a commutative ring, we simply say an ideal .\n\nIt turns out that for any ring A and any two-sided ideal I, the set A/I of additive cosets\na + I (with a ∈ A) is a ring called a quotient ring . Then we have the following analog of\nProposition 2.11, also called the first isomorphism theorem.\n\nProposition 2.18. Given a homomorphism of rings h : A → B, the rings A/Ker h and\nIm h = h(A) are isomorphic.\n\nA field is a commutative ring K for which K − {0} is a group under multiplication.\n\nDefinition 2.22. A set K is a field if it is a ring and the following properties hold:\n\n(F1) 0 6= 1;\n\n(F2) K∗ = K − {0} is a group w.r.t. ∗ (i.e., every a 6= 0 has an inverse w.r.t. ∗);\n\n(F3) ∗ is commutative.\n\nIf ∗ is not commutative but (F1) and (F2) hold, we say that we have a skew field (or\nnoncommutative field).\n\nNote that we are assuming that the operation ∗ of a field is commutative. This convention\nis not universally adopted, but since ∗ will be commutative for most fields we will encounter,\nwe may as well include this condition in the definition.\n\nExample 2.9.\n\n1. The rings Q, R, and C are fields.\n\n2. The set of (formal) fractions f(X)/g(X) of polynomials f(X), g(X) ∈ R[X], where\ng(X) is not the null polynomial, is a field.\n\n3. The ring C(a, b) of continuous functions f : (a, b) → R such that f(x) 6= 0 for all\nx ∈ (a, b) is a field.\n\n4. Using Proposition 2.16, it is easy to see that the ring Z/pZ is a field iff p is prime.\n\n5. If d is a square-free positive integer and if d ≥ 2, the set\n\nQ(\n√\nd) = {a+ b\n\n√\nd ∈ R | a, b ∈ Q}\n\nis a field. If z = a + b\n√\nd ∈ Q(\n\n√\nd) and z = a − b\n\n√\nd, then it is easy to check that if\n\nz 6= 0, then z−1 = z/(zz).\n\n\n\n2.3. RINGS AND FIELDS 41\n\n6. Similarly, If d ≥ 1 is a square-free positive integer, the set of complex numbers\n\nQ(\n√\n−d) = {a+ ib\n\n√\nd ∈ C | a, b ∈ Q}\n\nis a field. If z = a + ib\n√\nd ∈ Q(\n\n√\n−d) and z = a− ib\n\n√\nd, then it is easy to check that\n\nif z 6= 0, then z−1 = z/(zz).\n\nDefinition 2.23. A homomorphism h : K1 → K2 between two fields K1 and K2 is just a\nhomomorphism between the rings K1 and K2.\n\nHowever, because K∗1 and K∗2 are groups under multiplication, a homomorphism of fields\nmust be injective.\n\nProof. First, observe that for any x 6= 0,\n\n1 = h(1) = h(xx−1) = h(x)h(x−1)\n\nand\n1 = h(1) = h(x−1x) = h(x−1)h(x),\n\nso h(x) 6= 0 and\nh(x−1) = h(x)−1.\n\nBut then, if h(x) = 0, we must have x = 0. Consequently, h is injective.\n\nDefinition 2.24. A field homomorphism h : K1 → K2 is an isomorphism iff there is a\nhomomorphism g : K2 → K1 such that g ◦ f = idK1 and f ◦ g = idK2 . An isomorphism from\na field to itself is called an automorphism.\n\nThen, just as in the case of rings, g is unique and denoted by h−1, and a bijective field\nhomomorphism h : K1 → K2 is an isomorphism.\n\nDefinition 2.25. Since every homomorphism h : K1 → K2 between two fields is injective,\nthe image f(K1) of K1 is a subfield of K2. We say that K2 is an extension of K1.\n\nFor example, R is an extension of Q and C is an extension of R. The fields Q(\n√\nd) and\n\nQ(\n√\n−d) are extensions of Q, the field R is an extension of Q(\n\n√\nd) and the field C is an\n\nextension of Q(\n√\n−d).\n\nDefinition 2.26. A field K is said to be algebraically closed if every polynomial p(X) with\ncoefficients in K has some root in K; that is, there is some a ∈ K such that p(a) = 0.\n\nIt can be shown that every field K has some minimal extension Ω which is algebraically\nclosed, called an algebraic closure of K. For example, C is the algebraic closure of R. The\nalgebraic closure of Q is called the field of algebraic numbers . This field consists of all\ncomplex numbers that are zeros of a polynomial with coefficients in Q.\n\n\n\n42 CHAPTER 2. GROUPS, RINGS, AND FIELDS\n\nDefinition 2.27. Given a field K and an automorphism h : K → K of K, it is easy to check\nthat the set\n\nFix(h) = {a ∈ K | h(a) = a}\nof elements of K fixed by h is a subfield of K called the field fixed by h.\n\nFor example, if d ≥ 2 is square-free, then the map c : Q(\n√\nd)→ Q(\n\n√\nd) given by\n\nc(a+ b\n√\nd) = a− b\n\n√\nd\n\nis an automorphism of Q(\n√\nd), and Fix(c) = Q.\n\nIf K is a field, we have the ring homomorphism h : Z → K given by h(n) = n · 1. If h\nis injective, then K contains a copy of Z, and since it is a field, it contains a copy of Q. In\nthis case, we say that K has characteristic 0. If h is not injective, then h(Z) is a subring of\nK, and thus an integral domain, the kernel of h is a subgroup of Z, which by Proposition\n2.14 must be of the form pZ for some p ≥ 1. By the first isomorphism theorem, h(Z) is\nisomorphic to Z/pZ for some p ≥ 1. But then, p must be prime since Z/pZ is an integral\ndomain iff it is a field iff p is prime. The prime p is called the characteristic of K, and we\nalso says that K is of finite characteristic.\n\nDefinition 2.28. If K is a field, then either\n\n(1) n · 1 6= 0 for all integer n ≥ 1, in which case we say that K has characteristic 0, or\n\n(2) There is some smallest prime number p such that p · 1 = 0 called the characteristic of\nK, and we say K is of finite characteristic.\n\nA field K of characteristic 0 contains a copy of Q, thus is infinite. As we will see in\nSection 8.10, a finite field has nonzero characteristic p. However, there are infinite fields of\nnonzero characteristic.\n\n\n\nPart I\n\nLinear Algebra\n\n43\n\n\n\n\n\n\n\nChapter 3\n\nVector Spaces, Bases, Linear Maps\n\n3.1 Motivations: Linear Combinations, Linear Inde-\n\npendence and Rank\n\nIn linear optimization problems, we often encounter systems of linear equations. For example,\nconsider the problem of solving the following system of three linear equations in the three\nvariables x1, x2, x3 ∈ R:\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3.\n\nOne way to approach this problem is introduce the “vectors” u, v, w, and b, given by\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−2\n\n w =\n\n−1\n1\n−2\n\n b =\n\n1\n2\n3\n\n\nand to write our linear system as\n\nx1u+ x2v + x3w = b.\n\nIn the above equation, we used implicitly the fact that a vector z can be multiplied by a\nscalar λ ∈ R, where\n\nλz = λ\n\nz1\n\nz2\n\nz3\n\n =\n\nλz1\n\nλz2\n\nλz3\n\n ,\n\nand two vectors y and and z can be added, where\n\ny + z =\n\ny1\n\ny2\n\ny3\n\n+\n\nz1\n\nz2\n\nz3\n\n =\n\ny1 + z1\n\ny2 + z2\n\ny3 + z3\n\n .\n\n45\n\n\n\n46 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAlso, given a vector\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\nwe define the additive inverse −x of x (pronounced minus x) as\n\n−x =\n\n−x1\n\n−x2\n\n−x3\n\n .\n\nObserve that −x = (−1)x, the scalar multiplication of x by −1.\n\nThe set of all vectors with three components is denoted by R3×1. The reason for using\nthe notation R3×1 rather than the more conventional notation R3 is that the elements of\nR3×1 are column vectors ; they consist of three rows and a single column, which explains the\nsuperscript 3 × 1. On the other hand, R3 = R × R × R consists of all triples of the form\n(x1, x2, x3), with x1, x2, x3 ∈ R, and these are row vectors . However, there is an obvious\nbijection between R3×1 and R3 and they are usually identified. For the sake of clarity, in\nthis introduction, we will denote the set of column vectors with n components by Rn×1.\n\nAn expression such as\nx1u+ x2v + x3w\n\nwhere u, v, w are vectors and the xis are scalars (in R) is called a linear combination. Using\nthis notion, the problem of solving our linear system\n\nx1u+ x2v + x3w = b.\n\nis equivalent to determining whether b can be expressed as a linear combination of u, v, w.\n\nNow if the vectors u, v, w are linearly independent , which means that there is no triple\n(x1, x2, x3) 6= (0, 0, 0) such that\n\nx1u+ x2v + x3w = 03,\n\nit can be shown that every vector in R3×1 can be written as a linear combination of u, v, w.\nHere, 03 is the zero vector\n\n03 =\n\n0\n0\n0\n\n .\n\nIt is customary to abuse notation and to write 0 instead of 03. This rarely causes a problem\nbecause in most cases, whether 0 denotes the scalar zero or the zero vector can be inferred\nfrom the context.\n\nIn fact, every vector z ∈ R3×1 can be written in a unique way as a linear combination\n\nz = x1u+ x2v + x3w.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nz = x1u+ x2v + x3w = y1u+ y2v + y3w,\n\nthen by using our (linear!) operations on vectors, we get\n\n(y1 − x1)u+ (y2 − x2)v + (y3 − x3)w = 0,\n\nwhich implies that\ny1 − x1 = y2 − x2 = y3 − x3 = 0,\n\nby linear independence. Thus,\n\ny1 = x1, y2 = x2, y3 = x3,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\n\nx1u+ x2v + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nx1 = 1.4\n\nx2 = −0.4\n\nx3 = −0.4\n\nis the solution.\n\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u, v, w), called the determinant of\n(u, v, w), and to check that it is nonzero. In our case, it turns out that\n\ndet(u, v, w) =\n\n∣∣∣∣∣∣\n1 2 −1\n2 1 1\n1 −2 −2\n\n∣∣∣∣∣∣ = 15,\n\nwhich confirms that u, v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\nA =\n(\nu v w\n\n)\n=\n\n1 2 −1\n2 1 1\n1 −2 −2\n\n .\n\nIf we form the vector of unknowns\n\nx =\n\nx1\n\nx2\n\nx3\n\n ,\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK47\n\nThis is because if\nZz XU LQV + U3W = YU + You + Y3U,\n\nthen by using our (linear!) operations on vectors, we get\n(yi — t1)u + (yo — t2)u + (y3 — Z3)w = 0,\n\nwhich implies that\nYi — Ly = Yo — Lo = y3 — ®3 = O,\n\nby linear independence. Thus,\n\nYF, Yor, Y3>= %,\n\nwhich shows that z has a unique expression as a linear combination, as claimed. Then our\nequation\nrut rv + x3w = b\n\nhas a unique solution, and indeed, we can check that\n\nLy = 1.4\nt= —0.4\nL3 = —0.4\n\nis the solution.\nBut then, how do we determine that some vectors are linearly independent?\n\nOne answer is to compute a numerical quantity det(u,v,w), called the determinant of\n(u,v, w), and to check that it is nonzero. In our case, it turns out that\n\n1 2 -!1\ndet(u,v,w) =|2 1 1)/=15,\n1 -2 -2\n\nwhich confirms that u,v, w are linearly independent.\n\nOther methods, which are much better for systems with a large number of variables,\nconsist of computing an LU-decomposition or a QR-decomposition, or an SVD of the matrix\nconsisting of the three columns u, v, w,\n\n1 2 -1\nA=(u v w) = 2 1 1\n1 —2 —2\n\nIf we form the vector of unknowns\n\n\n\n\n48 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nthen our linear combination x1u+ x2v + x3w can be written in matrix form as\n\nx1u+ x2v + x3w =\n\n1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n ,\n\nso our linear system is expressed by1 2 −1\n2 1 1\n1 −2 −2\n\nx1\n\nx2\n\nx3\n\n =\n\n1\n2\n3\n\n ,\n\nor more concisely as\nAx = b.\n\nNow what if the vectors u, v, w are linearly dependent? For example, if we consider the\nvectors\n\nu =\n\n1\n2\n1\n\n v =\n\n 2\n1\n−1\n\n w =\n\n−1\n1\n2\n\n ,\n\nwe see that\nu− v = w,\n\na nontrivial linear dependence. It can be verified that u and v are still linearly independent.\nNow for our problem\n\nx1u+ x2v + x3w = b\n\nit must be the case that b can be expressed as linear combination of u and v. However,\nit turns out that u, v, b are linearly independent (one way to see this is to compute the\ndeterminant det(u, v, b) = −6), so b cannot be expressed as a linear combination of u and v\nand thus, our system has no solution.\n\nIf we change the vector b to\n\nb =\n\n3\n3\n0\n\n ,\n\nthen\nb = u+ v,\n\nand so the system\nx1u+ x2v + x3w = b\n\nhas the solution\nx1 = 1, x2 = 1, x3 = 0.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK49\n\nActually, since w = u− v, the above system is equivalent to\n\n(x1 + x3)u+ (x2 − x3)v = b,\n\nand because u and v are linearly independent, the unique solution in x1 + x3 and x2 − x3 is\n\nx1 + x3 = 1\n\nx2 − x3 = 1,\n\nwhich yields an infinite number of solutions parameterized by x3, namely\n\nx1 = 1− x3\n\nx2 = 1 + x3.\n\nIn summary, a 3× 3 linear system may have a unique solution, no solution, or an infinite\nnumber of solutions, depending on the linear independence (and dependence) or the vectors\nu, v, w, b. This situation can be generalized to any n × n system, and even to any n × m\nsystem (n equations in m variables), as we will see later.\n\nThe point of view where our linear system is expressed in matrix form as Ax = b stresses\nthe fact that the map x 7→ Ax is a linear transformation. This means that\n\nA(λx) = λ(Ax)\n\nfor all x ∈ R3×1 and all λ ∈ R and that\n\nA(u+ v) = Au+ Av,\n\nfor all u, v ∈ R3×1. We can view the matrix A as a way of expressing a linear map from R3×1\n\nto R3×1 and solving the system Ax = b amounts to determining whether b belongs to the\nimage of this linear map.\n\nGiven a 3× 3 matrix\n\nA =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n ,\n\nwhose columns are three vectors denoted A1, A2, A3, and given any vector x = (x1, x2, x3),\nwe defined the product Ax as the linear combination\n\nAx = x1A\n1 + x2A\n\n2 + x3A\n3 =\n\na11x1 + a12x2 + a13x3\n\na21x1 + a22x2 + a23x3\n\na31x1 + a32x2 + a33x3\n\n .\n\nThe common pattern is that the ith coordinate of Ax is given by a certain kind of product\ncalled an inner product , of a row vector , the ith row of A, times the column vector x:\n\n(\nai1 ai2 ai3\n\n)\n·\n\nx1\n\nx2\n\nx3\n\n = ai1x1 + ai2x2 + ai3x3.\n\n\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn) ∈ Rn, their\ninner product denoted x · y, or 〈x, y〉, is the number\n\nx · y =\n(\nx1 x2 · · · xn\n\n)\n·\n\n\ny1\n\ny2\n...\nyn\n\n =\nn∑\ni=1\n\nxiyi.\n\nInner products play a very important role. First, we quantity\n\n‖x‖2 =\n√\nx · x = (x2\n\n1 + · · ·+ x2\nn)1/2\n\nis a generalization of the length of a vector, called the Euclidean norm, or `2-norm. Second,\nit can be shown that we have the inequality\n\n|x · y| ≤ ‖x‖ ‖y‖ ,\n\nso if x, y 6= 0, the ratio (x · y)/(‖x‖ ‖y‖) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if x · y = 0 then the vectors x and y make the angle π/2,\nthat is, they are orthogonal . The (square) matrices Q that preserve the inner product, in\nthe sense that 〈Qx,Qy〉 = 〈x, y〉 for all x, y ∈ Rn, also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m × n matrix consisting of n columns A1, . . . , An (in\nRm), and B is a n× p matrix consisting of p columns B1, . . . , Bp (in Rn) we can form the p\nvectors (in Rm)\n\nAB1, . . . , ABp.\n\nThese p vectors constitute the m × p matrix denoted AB, whose jth column is ABj. But\nwe know that the ith coordinate of ABj is the inner product of the ith row of A by the jth\ncolumn of B,\n\n(\nai1 ai2 · · · ain\n\n)\n·\n\n\nb1j\n\nb2j\n...\nbnj\n\n =\nn∑\nk=1\n\naikbkj.\n\nThus we have defined a multiplication operation on matrices, namely if A = (aik) is a m×n\nmatrix and if B = (bjk) if n× p matrix, then their product AB is the m× n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\n(AB)ij =\nn∑\nk=1\n\naikbkj.\n\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are two n×n\nmatrices, in general, AB 6= BA.\n\n50 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nMore generally, given any two vectors 7 = (#1,...,%n,) and y = (y1,.--,Yn) € R”, their\ninner product denoted x - y, or (x,y), is the number\n\nY1\ny n\nvey = (a TQ 7° In) * ° = So xiyi.\n. i=l\nYn\n\nInner products play a very important role. First, we quantity\nvp = Vee = (ap +--+ 27)'?\n\nis a generalization of the length of a vector, called the Euclidean norm, or (?-norm. Second,\nit can be shown that we have the inequality\n\nIz-y| < fall ly,\n\nso if x,y £0, the ratio (x - y)/(||2|| ||y||) can be viewed as the cosine of an angle, the angle\nbetween x and y. In particular, if «-y = 0 then the vectors x and y make the angle 7/2,\nthat is, they are orthogonal. The (square) matrices Q that preserve the inner product, in\nthe sense that (Qz, Qy) = (x,y) for all x,y € R\", also play a very important role. They can\nbe thought of as generalized rotations.\n\nReturning to matrices, if A is an m X n matrix consisting of n columns A!,...,A” (in\nR™), and B is an Xx p matrix consisting of p columns B',...,B? (in R\") we can form the p\nvectors (in R”)\n\nAB',..., ABP.\n\nThese p vectors constitute the m x p matrix denoted AB, whose jth column is AB’. But\nwe know that the ith coordinate of AB is the inner product of the ith row of A by the jth\ncolumn of B,\n\nbi;\nn\nbo; _\n(ait aig \"7° Gin) * . =) i¢dp;-\n, k=1\nDnj\n\nThus we have defined a multiplication operation on matrices, namely if A = (a;,) isamxn\nmatrix and if B = (b;,) if n x p matrix, then their product AB is the m x n matrix whose\nentry on the ith row and the jth column is given by the inner product of the ith row of A\nby the jth column of B,\n\nn\n\n(AB);; = Ss\" indy; -\n\nk=1\nBeware that unlike the multiplication of real (or complex) numbers, if A and B are twon xn\nmatrices, in general, AB 4 BA.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK51\n\nSuppose that A is an n× n matrix and that we are trying to solve the linear system\n\nAx = b,\n\nwith b ∈ Rn. Suppose we can find an n× n matrix B such that\n\nBAi = ei, i = 1, . . . , n,\n\nwith ei = (0, . . . , 0, 1, 0 . . . , 0), where the only nonzero entry is 1 in the ith slot. If we form\nthe n× n matrix\n\nIn =\n\n\n\n1 0 0 · · · 0 0\n0 1 0 · · · 0 0\n0 0 1 · · · 0 0\n...\n\n...\n...\n\n. . .\n...\n\n...\n0 0 0 · · · 1 0\n0 0 0 · · · 0 1\n\n\n,\n\ncalled the identity matrix , whose ith column is ei, then the above is equivalent to\n\nBA = In.\n\nIf Ax = b, then multiplying both sides on the left by B, we get\n\nB(Ax) = Bb.\n\nBut is is easy to see that B(Ax) = (BA)x = Inx = x, so we must have\n\nx = Bb.\n\nWe can verify that x = Bb is indeed a solution, because it can be shown that\n\nA(Bb) = (AB)b = Inb = b.\n\nWhat is not obvious is that BA = In implies AB = In, but this is indeed provable. The\nmatrix B is usually denoted A−1 and called the inverse of A. It can be shown that it is the\nunique matrix such that\n\nAA−1 = A−1A = In.\n\nIf a square matrix A has an inverse, then we say that it is invertible or nonsingular , otherwise\nwe say that it is singular . We will show later that a square matrix is invertible iff its columns\nare linearly independent iff its determinant is nonzero.\n\nIn summary, if A is a square invertible matrix, then the linear system Ax = b has the\nunique solution x = A−1b. In practice, this is not a good way to solve a linear system because\ncomputing A−1 is too expensive. A practical method for solving a linear system is Gaussian\nelimination, discussed in Chapter 8. Other practical methods for solving a linear system\n\n\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m × n matrix A = (akl), the n × m matrix A> = (a>ij) whose ith row is the\nith column of A, which means that a>ij = aji for i = 1, . . . , n and j = 1, . . . ,m, is called the\ntranspose of A. An n× n matrix Q such that\n\nQQ> = Q>Q = In\n\nis called an orthogonal matrix . Equivalently, the inverse Q−1 of an orthogonal matrix Q is\nequal to its transpose Q>. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m× n matrix A can be written as\n\nA = V ΣU>,\n\nwhere V is an m×m orthogonal matrix, U is an n×n orthogonal matrix, and Σ is an m×n\nmatrix whose only nonzero entries are nonnegative diagonal entries σ1 ≥ σ2 ≥ · · · ≥ σp,\nwhere p = min(m,n), called the singular values of A. The factorization A = V ΣU> is called\na singular decomposition of A, or SVD .\n\nThe SVD can be used to “solve” a linear system Ax = b where A is an m × n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m > n) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax − b. Legendre and Gauss used ‖Ax− b‖2\n\n2, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\nx+ of minimum Euclidean norm that minimizes ‖Ax− b‖2\n\n2. Furthermore, x+ is given by the\nexpression x+ = A+b, where A+ is the pseudo-inverse of A, and A+ can be computed from\nan SVD A = V ΣU> of A. Indeed, A+ = UΣ+V >, where Σ+ is the matrix obtained from Σ\nby replacing every positive singular value σi by its inverse σ−1\n\ni , leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ‖Ax− b‖2\n2, we\n\ncan add the penalty term K ‖x‖2\n2 (for some positive K > 0) to ‖Ax− b‖2\n\n2 and minimize the\nquantity ‖Ax− b‖2\n\n2 + K ‖x‖2\n2. This approach is called ridge regression. It turns out that\n\nthere is a unique minimizer x+ given by x+ = (A>A + KIn)−1A>b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K ‖x‖2\n2 by K ‖x‖1, where ‖x‖1 = |x1|+\n\n· · · + |xn| (the `1-norm of x). The remarkable fact is that the minimizers x of ‖Ax− b‖2\n2 +\n\nK ‖x‖1 tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n52 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nAx = b make use of a factorization of A (QR decomposition, SVD decomposition), using\northogonal matrices defined next.\n\nGiven an m x n matrix A = (aq), the n x m matrix A‘ = (a,;) whose ith row is the\nith column of A, which means that aj, =a; fori =1,...,n and j =1,...,m, is called the\ntranspose of A. An n X n matrix Q such that\n\nQQ'=Q'Q=In\n\nis called an orthogonal matrix. Equivalently, the inverse Q~! of an orthogonal matrix Q is\nequal to its transpose Q'. Orthogonal matrices play an important role. Geometrically, they\ncorrespond to linear transformation that preserve length. A major result of linear algebra\nstates that every m x n matrix A can be written as\n\nA=VSUT,\n\nwhere V is an m xX m orthogonal matrix, U is an n x n orthogonal matrix, and © is anm xn\nmatrix whose only nonzero entries are nonnegative diagonal entries 0) > 02 > ++: > Op,\nwhere p = min(m, n), called the singular values of A. The factorization A = VXU' is called\na singular decomposition of A, or SVD.\n\nThe SVD can be used to “solve” a linear system Ax = 6 where A is an m x n matrix,\neven when this system has no solution. This may happen when there are more equations\nthat variables (m >) , in which case the system is overdetermined.\n\nOf course, there is no miracle, an unsolvable system has no solution. But we can look\nfor a good approximate solution, namely a vector x that minimizes some measure of the\nerror Ax — b. Legendre and Gauss used ||Aa — ||}, which is the squared Euclidean norm\nof the error. This quantity is differentiable, and it turns out that there is a unique vector\na+ of minimum Euclidean norm that minimizes || Ax — ||}. Furthermore, «+ is given by the\nexpression xt = Atb, where At is the pseudo-inverse of A, and At can be computed from\nan SVD A=VU' of A. Indeed, At = UXNt+V', where ¥*+ is the matrix obtained from Y\nby replacing every positive singular value o; by its inverse a; ', leaving all zero entries intact,\nand transposing.\n\nInstead of searching for the vector of least Euclidean norm minimizing ||Ax — 6||5, we\ncan add the penalty term K ||z||3 (for some positive K > 0) to || Ax — ||} and minimize the\nquantity || Ax — b||} + K ||a||3. This approach is called ridge regression. It turns out that\nthere is a unique minimizer 2+ given by + = (A'A+ KI,)~1A'b, as shown in the second\nvolume.\n\nAnother approach is to replace the penalty term K |||]. by K ||a||,, where |Ja'||, = |a1| +\n--»4+|x,| (the ¢'-norm of «). The remarkable fact is that the minimizers « of || Ax — b||3 +\nK |x|, tend to be sparse, which means that many components of x are equal to zero. This\napproach known as lasso is popular in machine learning and will be discussed in the second\nvolume.\n\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK53\n\nAnother important application of the SVD is principal component analysis (or PCA), an\nimportant tool in data analysis.\n\nYet another fruitful way of interpreting the resolution of the system Ax = b is to view\nthis problem as an intersection problem. Indeed, each of the equations\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − 2x2 − 2x3 = 3\n\ndefines a subset of R3 which is actually a plane. The first equation\n\nx1 + 2x2 − x3 = 1\n\ndefines the plane H1 passing through the three points (1, 0, 0), (0, 1/2, 0), (0, 0,−1), on the\ncoordinate axes, the second equation\n\n2x1 + x2 + x3 = 2\n\ndefines the plane H2 passing through the three points (1, 0, 0), (0, 2, 0), (0, 0, 2), on the coor-\ndinate axes, and the third equation\n\nx1 − 2x2 − 2x3 = 3\n\ndefines the plane H3 passing through the three points (3, 0, 0), (0,−3/2, 0), (0, 0,−3/2), on\nthe coordinate axes. See Figure 3.1.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\nx -2x -2x = 31 2 3\n\nFigure 3.1: The planes defined by the preceding linear equations.\n\n\n\n54 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nx -2x -2x = 31 2 3\n\n2x + x + x = 21 2 3\n\n2x + 2x - x = 11 2 3\n\n(1.4, -0.4, -0.4)\n\nFigure 3.2: The solution of the system is the point in common with each of the three planes.\n\nThe intersection Hi∩Hj of any two distinct planes Hi and Hj is a line, and the intersection\nH1 ∩H2 ∩H3 of the three planes consists of the single point (1.4,−0.4,−0.4), as illustrated\nin Figure 3.2.\n\nThe planes corresponding to the system\n\nx1 + 2x2 − x3 = 1\n\n2x1 + x2 + x3 = 2\n\nx1 − x2 + 2x3 = 3,\n\nare illustrated in Figure 3.3.\n\n2x + 2x - x = 11 2 3\n\n2x + x + x = 21 2 3\n\n1 2 3\n\nx - x +2x = 31 2 3\n\nFigure 3.3: The planes defined by the equations x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, and\nx1 − x2 + 2x3 = 3.\n\n\n\n3.1. MOTIVATIONS: LINEAR COMBINATIONS, LINEAR INDEPENDENCE, RANK55\n\nThis system has no solution since there is no point simultaneously contained in all three\nplanes; see Figure 3.4.\n\n2x + 2x - x = 11 2 3\n\nx - x +2x = 31 2 3\n\n2x + x + x = 21 2 32x + x + x = 21 2 3\n\nFigure 3.4: The linear system x1 + 2x2 − x3 = 1, 2x1 + x2 + x3 = 2, x1 − x2 + 2x3 = 3 has\nno solution.\n\nFinally, the planes corresponding to the system\n\nx1 + 2x2 − x3 = 3\n\n2x1 + x2 + x3 = 3\n\nx1 − x2 + 2x3 = 0,\n\nare illustrated in Figure 3.5.\n\n2x + 2x -  x = 3\n1\n\n1\n\n1\n\n2\n\n2 3\n\n3\n\n2x + x + x = 32 3\n\nx - x + 2x = 01 2 3\n\n1\n\nFigure 3.5: The planes defined by the equations x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, and\nx1 − x2 + 2x3 = 0.\n\n\n\n56 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis system has infinitely many solutions, given parametrically by (1 − x3, 1 + x3, x3).\nGeometrically, this is a line common to all three planes; see Figure 3.6.\n\n2x + 2x -  x = 3\n1 2 3\n\nx - x + 2x = 01 2 3\n\n12x + x + x = 32 3\n\nFigure 3.6: The linear system x1 + 2x2 − x3 = 3, 2x1 + x2 + x3 = 3, x1 − x2 + 2x3 = 0 has\nthe red line common to all three planes.\n\nUnder the above interpretation, observe that we are focusing on the rows of the matrix\nA, rather than on its columns , as in the previous interpretations.\n\nAnother great example of a real-world problem where linear algebra proves to be very\neffective is the problem of data compression, that is, of representing a very large data set\nusing a much smaller amount of storage.\n\nTypically the data set is represented as an m× n matrix A where each row corresponds\nto an n-dimensional data point and typically, m ≥ n. In most applications, the data are not\nindependent so the rank of A is a lot smaller than min{m,n}, and the the goal of low-rank\ndecomposition is to factor A as the product of two matrices B and C, where B is a m × k\nmatrix and C is a k×n matrix, with k � min{m,n} (here,� means “much smaller than”):\n\nA\nm× n\n\n\n=\n\n\nB\n\nm× k\n\n\n C\n\nk × n\n\n\n\nNow it is generally too costly to find an exact factorization as above, so we look for a\nlow-rank matrix A′ which is a “good” approximation of A. In order to make this statement\nprecise, we need to define a mechanism to determine how close two matrices are. This can\nbe done using matrix norms , a notion discussed in Chapter 9. The norm of a matrix A is a\n\n\n\n3.2. VECTOR SPACES 57\n\nnonnegative real number ‖A‖ which behaves a lot like the absolute value |x| of a real number\nx. Then our goal is to find some low-rank matrix A′ that minimizes the norm\n\n‖A− A′‖2\n,\n\nover all matrices A′ of rank at most k, for some given k � min{m,n}.\nSome advantages of a low-rank approximation are:\n\n1. Fewer elements are required to represent A; namely, k(m + n) instead of mn. Thus\nless storage and fewer operations are needed to reconstruct A.\n\n2. Often, the process for obtaining the decomposition exposes the underlying structure of\nthe data. Thus, it may turn out that “most” of the significant data are concentrated\nalong some directions called principal directions .\n\nLow-rank decompositions of a set of data have a multitude of applications in engineering,\nincluding computer science (especially computer vision), statistics, and machine learning.\nAs we will see later in Chapter 23, the singular value decomposition (SVD) provides a very\nsatisfactory solution to the low-rank approximation problem. Still, in many cases, the data\nsets are so large that another ingredient is needed: randomization. However, as a first step,\nlinear algebra often yields a good initial solution.\n\nWe will now be more precise as to what kinds of operations are allowed on vectors. In\nthe early 1900, the notion of a vector space emerged as a convenient and unifying framework\nfor working with “linear” objects and we will discuss this notion in the next few sections.\n\n3.2 Vector Spaces\n\nFor every n ≥ 1, let Rn be the set of n-tuples x = (x1, . . . , xn). Addition can be extended to\nRn as follows:\n\n(x1, . . . , xn) + (y1, . . . , yn) = (x1 + y1, . . . , xn + yn).\n\nWe can also define an operation · : R× Rn → Rn as follows:\n\nλ · (x1, . . . , xn) = (λx1, . . . , λxn).\n\nThe resulting algebraic structure has some interesting properties, those of a vector space.\n\nHowever, keep in mind that vector spaces are not just algebraic\nobjects; they are also geometric objects.\n\nVector spaces are defined as follows.\n\n\n\n58 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nDefinition 3.1. Given a field K (with addition + and multiplication ∗), a vector space over\nK (or K-vector space) is a set E (of vectors) together with two operations +: E × E → E\n(called vector addition),1 and · : K × E → E (called scalar multiplication) satisfying the\nfollowing conditions for all α, β ∈ K and all u, v ∈ E;\n\n(V0) E is an abelian group w.r.t. +, with identity element 0;2\n\n(V1) α · (u+ v) = (α · u) + (α · v);\n\n(V2) (α + β) · u = (α · u) + (β · u);\n\n(V3) (α ∗ β) · u = α · (β · u);\n\n(V4) 1 · u = u.\n\nIn (V3), ∗ denotes multiplication in the field K.\n\nGiven α ∈ K and v ∈ E, the element α · v is also denoted by αv. The field K is often\ncalled the field of scalars.\n\nUnless specified otherwise or unless we are dealing with several different fields, in the rest\nof this chapter, we assume that all K-vector spaces are defined with respect to a fixed field\nK. Thus, we will refer to a K-vector space simply as a vector space. In most cases, the field\nK will be the field R of reals.\n\nFrom (V0), a vector space always contains the null vector 0, and thus is nonempty.\nFrom (V1), we get α · 0 = 0, and α · (−v) = −(α · v). From (V2), we get 0 · v = 0, and\n(−α) · v = −(α · v).\n\nAnother important consequence of the axioms is the following fact:\n\nProposition 3.1. For any u ∈ E and any λ ∈ K, if λ 6= 0 and λ · u = 0, then u = 0.\n\nProof. Indeed, since λ 6= 0, it has a multiplicative inverse λ−1, so from λ · u = 0, we get\n\nλ−1 · (λ · u) = λ−1 · 0.\n\nHowever, we just observed that λ−1 · 0 = 0, and from (V3) and (V4), we have\n\nλ−1 · (λ · u) = (λ−1λ) · u = 1 · u = u,\n\nand we deduce that u = 0.\n\n1The symbol + is overloaded, since it denotes both addition in the field K and addition of vectors in E.\nIt is usually clear from the context which + is intended.\n\n2The symbol 0 is also overloaded, since it represents both the zero in K (a scalar) and the identity element\nof E (the zero vector). Confusion rarely arises, but one may prefer using 0 for the zero vector.\n\n\n\n3.2. VECTOR SPACES 59\n\nRemark: One may wonder whether axiom (V4) is really needed. Could it be derived from\nthe other axioms? The answer is no. For example, one can take E = Rn and define\n· : R× Rn → Rn by\n\nλ · (x1, . . . , xn) = (0, . . . , 0)\n\nfor all (x1, . . . , xn) ∈ Rn and all λ ∈ R. Axioms (V0)–(V3) are all satisfied, but (V4) fails.\nLess trivial examples can be given using the notion of a basis, which has not been defined\nyet.\n\nThe field K itself can be viewed as a vector space over itself, addition of vectors being\naddition in the field, and multiplication by a scalar being multiplication in the field.\n\nExample 3.1.\n\n1. The fields R and C are vector spaces over R.\n\n2. The groups Rn and Cn are vector spaces over R, with scalar multiplication given by\n\nλ(x1, . . . , xn) = (λx1, . . . , λxn),\n\nfor any λ ∈ R and with (x1, . . . , xn) ∈ Rn or (x1, . . . , xn) ∈ Cn, and Cn is a vector\nspace over C with scalar multiplication as above, but with λ ∈ C.\n\n3. The ring R[X]n of polynomials of degree at most n with real coefficients is a vector\nspace over R, and the ring C[X]n of polynomials of degree at most n with complex\ncoefficients is a vector space over C, with scalar multiplication λ ·P (X) of a polynomial\n\nP (X) = amX\nm + am−1X\n\nm−1 + · · ·+ a1X + a0\n\n(with ai ∈ R or ai ∈ C) by the scalar λ (in R or C), with m ≤ n, given by\n\nλ · P (X) = λamX\nm + λam−1X\n\nm−1 + · · ·+ λa1X + λa0.\n\n4. The ring R[X] of all polynomials with real coefficients is a vector space over R, and the\nring C[X] of all polynomials with complex coefficients is a vector space over C, with\nthe same scalar multiplication as above.\n\n5. The ring of n× n matrices Mn(R) is a vector space over R.\n\n6. The ring of m× n matrices Mm,n(R) is a vector space over R.\n\n7. The ring C(a, b) of continuous functions f : (a, b) → R is a vector space over R, with\nthe scalar multiplication λf of a function f : (a, b)→ R by a scalar λ ∈ R given by\n\n(λf)(x) = λf(x), for all x ∈ (a, b).\n\n\n\n60 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n8. A very important example of vector space is the set of linear maps between two vector\nspaces to be defined in Section 11.1. Here is an example that will prepare us for the\nvector space of linear maps. Let X be any nonempty set and let E be a vector space.\nThe set of all functions f : X → E can be made into a vector space as follows: Given\nany two functions f : X → E and g : X → E, let (f + g) : X → E be defined such that\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ X, and for every λ ∈ R, let λf : X → E be defined such that\n\n(λf)(x) = λf(x)\n\nfor all x ∈ X. The axioms of a vector space are easily verified.\n\nLet E be a vector space. We would like to define the important notions of linear combi-\nnation and linear independence.\n\nBefore defining these notions, we need to discuss a strategic choice which, depending\nhow it is settled, may reduce or increase headaches in dealing with notions such as linear\ncombinations and linear dependence (or independence). The issue has to do with using sets\nof vectors versus sequences of vectors.\n\n3.3 Indexed Families; the Sum Notation\n∑\n\ni∈I ai\n\nOur experience tells us that it is preferable to use sequences of vectors ; even better, indexed\nfamilies of vectors. (We are not alone in having opted for sequences over sets, and we are in\ngood company; for example, Artin [7], Axler [10], and Lang [108] use sequences. Nevertheless,\nsome prominent authors such as Lax [112] use sets. We leave it to the reader to conduct a\nsurvey on this issue.)\n\nGiven a set A, recall that a sequence is an ordered n-tuple (a1, . . . , an) ∈ An of elements\nfrom A, for some natural number n. The elements of a sequence need not be distinct and\nthe order is important. For example, (a1, a2, a1) and (a2, a1, a1) are two distinct sequences\nin A3. Their underlying set is {a1, a2}.\n\nWhat we just defined are finite sequences, which can also be viewed as functions from\n{1, 2, . . . , n} to the set A; the ith element of the sequence (a1, . . . , an) is the image of i under\nthe function. This viewpoint is fruitful, because it allows us to define (countably) infinite\nsequences as functions s : N → A. But then, why limit ourselves to ordered sets such as\n{1, . . . , n} or N as index sets?\n\nThe main role of the index set is to tag each element uniquely, and the order of the tags\nis not crucial, although convenient. Thus, it is natural to define the notion of indexed family.\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 61\n\nDefinition 3.2. Given a set A, an I-indexed family of elements of A, for short a family ,\nis a function a : I → A where I is any set viewed as an index set. Since the function a is\ndetermined by its graph\n\n{(i, a(i)) | i ∈ I},\nthe family a can be viewed as the set of pairs a = {(i, a(i)) | i ∈ I}. For notational simplicity,\nwe write ai instead of a(i), and denote the family a = {(i, a(i)) | i ∈ I} by (ai)i∈I .\n\nFor example, if I = {r, g, b, y} and A = N, the set of pairs\n\na = {(r, 2), (g, 3), (b, 2), (y, 11)}\n\nis an indexed family. The element 2 appears twice in the family with the two distinct tags\nr and b.\n\nWhen the indexed set I is totally ordered, a family (ai)i∈I is often called an I-sequence.\nInterestingly, sets can be viewed as special cases of families. Indeed, a set A can be viewed\nas the A-indexed family {(a, a) | a ∈ I} corresponding to the identity function.\n\nRemark: An indexed family should not be confused with a multiset. Given any set A, a\nmultiset is a similar to a set, except that elements of A may occur more than once. For\nexample, if A = {a, b, c, d}, then {a, a, a, b, c, c, d, d} is a multiset. Each element appears\nwith a certain multiplicity, but the order of the elements does not matter. For example, a\nhas multiplicity 3. Formally, a multiset is a function s : A→ N, or equivalently a set of pairs\n{(a, i) | a ∈ A}. Thus, a multiset is an A-indexed family of elements from N, but not a\nN-indexed family, since distinct elements may have the same multiplicity (such as c an d in\nthe example above). An indexed family is a generalization of a sequence, but a multiset is a\ngeneralization of a set.\n\nWe also need to take care of an annoying technicality, which is to define sums of the\nform\n\n∑\ni∈I ai, where I is any finite index set and (ai)i∈I is a family of elements in some set\n\nA equiped with a binary operation +: A × A → A which is associative (Axiom (G1)) and\ncommutative. This will come up when we define linear combinations.\n\nThe issue is that the binary operation + only tells us how to compute a1 + a2 for two\nelements of A, but it does not tell us what is the sum of three of more elements. For example,\nhow should a1 + a2 + a3 be defined?\n\nWhat we have to do is to define a1+a2+a3 by using a sequence of steps each involving two\nelements, and there are two possible ways to do this: a1 + (a2 +a3) and (a1 +a2) +a3. If our\noperation + is not associative, these are different values. If it associative, then a1+(a2+a3) =\n(a1 + a2) + a3, but then there are still six possible permutations of the indices 1, 2, 3, and if\n+ is not commutative, these values are generally different. If our operation is commutative,\nthen all six permutations have the same value. Thus, if + is associative and commutative,\nit seems intuitively clear that a sum of the form\n\n∑\ni∈I ai does not depend on the order of the\n\noperations used to compute it.\n\n\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form∑\n\ni∈I ai are indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums\n∑\n\ni∈I ai, where I is a finite sequence of distinct natural numbers,\nsay I = (i1, . . . , im). If I = (i1, . . . , im) with m ≥ 2, we denote the sequence (i2, . . . , im) by\nI − {i1}. We proceed by induction on the size m of I. Let∑\n\ni∈I\nai = ai1 , if m = 1,\n\n∑\ni∈I\n\nai = ai1 +\n\n( ∑\ni∈I−{i1}\n\nai\n\n)\n, if m > 1.\n\nFor example, if I = (1, 2, 3, 4), we have∑\ni∈I\n\nai = a1 + (a2 + (a3 + a4)).\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\n\na1 + (a2 + (a3 + a4)) 6= (a1 + a2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum\n∑\n\ni∈I ai should not depend on the grouping\nof the elements in I, as long as their order is preserved. For example, if I = (1, 2, 3, 4, 5),\nJ1 = (1, 2), and J2 = (3, 4, 5), we expect that\n\n∑\ni∈I\n\nai =\n\n(∑\nj∈J1\n\naj\n\n)\n+\n\n(∑\nj∈J2\n\naj\n\n)\n.\n\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: A × A → A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Ik1 , . . . , Ikp, for some nonempty sequence K =\n(k1, . . . , kp) of distinct natural numbers such that ki < kj implies that α < β for all α ∈ Iki\nand all β ∈ Ikj , for every sequence (ai)i∈I of elements in A, we have\n\n∑\nα∈I\n\naα =\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n.\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and Ik1 = I, so the proposition holds trivially.\n\n62 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThis is indeed the case, but a rigorous proof requires induction, and such a proof is\nsurprisingly involved. Readers may accept without proof the fact that sums of the form\nvier 4% ave indeed well defined, and jump directly to Definition 3.3. For those who want to\nsee the gory details, here we go.\n\nFirst, we define sums }7;-,@;, where J is a finite sequence of distinct natural numbers,\nsay I = (i1,...,4m). If I = (i1,...,im) with m > 2, we denote the sequence (i2,...,%m) by\nI — {t,}. We proceed by induction on the size m of I. Let\n\n) a,=a,, Wwm=1,\n\nie]\nLa=a +( S- a). ifm > 1.\nie] i€I—{ir}\n\nFor example, if J = (1,2,3,4), we have\nSai =a, + (ag + (a3 + a4)).\niel\n\nIf the operation + is not associative, the grouping of the terms matters. For instance, in\ngeneral\nay + (a2 + (a3 + a4)) A (a1 + G2) + (a3 + a4).\n\nHowever, if the operation + is associative, the sum }?,., a; should not depend on the grouping\nof the elements in J, as long as their order is preserved. For example, if J = (1,2,3,4,5),\nJ, = (1,2), and Jy = (3,4,5), we expect that\n\nYa=(La)+ (La).\n\niel jel je de\nThis indeed the case, as we have the following proposition.\n\nProposition 3.2. Given any nonempty set A equipped with an associative binary operation\n+: Ax A-— A, for any nonempty finite sequence I of distinct natural numbers and for\nany partition of I into p nonempty sequences Iy,,...,Ik,, for some nonempty sequence K =\n(ky,...,kp) of distinct natural numbers such that k; < k; implies that a < 6 for alla € Ix,\nand all 8 € Iy,, for every sequence (a;)ier of elements in A, we have\n\nProof. We proceed by induction on the size n of I.\n\nIf n = 1, then we must have p = 1 and IJ;, = J, so the proposition holds trivially.\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 63\n\nNext, assume n > 1. If p = 1, then Ik1 = I and the formula is trivial, so assume that\np ≥ 2 and write J = (k2, . . . , kp). There are two cases.\n\nCase 1. The sequence Ik1 has a single element, say β, which is the first element of I.\nIn this case, write C for the sequence obtained from I by deleting its first element β. By\ndefinition, ∑\n\nα∈I\naα = aβ +\n\n(∑\nα∈C\n\naα\n\n)\n,\n\nand ∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n= aβ +\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nSince |C| = n− 1, by the induction hypothesis, we have(∑\nα∈C\n\naα\n\n)\n=\n∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)\n,\n\nwhich yields our identity.\n\nCase 2. The sequence Ik1 has at least two elements. In this case, let β be the first element\nof I (and thus of Ik1), let I ′ be the sequence obtained from I by deleting its first element β,\nlet I ′k1\n\nbe the sequence obtained from Ik1 by deleting its first element β, and let I ′ki = Iki for\ni = 2, . . . , p. Recall that J = (k2, . . . , kp) and K = (k1, . . . , kp). The sequence I ′ has n − 1\nelements, so by the induction hypothesis applied to I ′ and the I ′ki , we get∑\n\nα∈I′\naα =\n\n∑\nk∈K\n\n(∑\nα∈I′k\n\naα\n\n)\n=\n\n(∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n.\n\nIf we add the lefthand side to aβ, by definition we get∑\nα∈I\n\naα.\n\nIf we add the righthand side to aβ, using associativity and the definition of an indexed sum,\nwe get\n\naβ +\n\n((∑\nα∈I′k1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n)))\n=\n\n(\naβ +\n\n(∑\nα∈I′k1\n\naα\n\n))\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n\n(∑\nα∈Ik1\n\naα\n\n)\n+\n\n(∑\nj∈J\n\n(∑\nα∈Ij\n\naα\n\n))\n\n=\n∑\nk∈K\n\n(∑\nα∈Ik\n\naα\n\n)\n,\n\nas claimed.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 63\n\nNext, assume n > 1. If p = 1, then J,, = J and the formula is trivial, so assume that\np > 2 and write J = (ko,...,k,). There are two cases.\n\nCase 1. The sequence J;, has a single element, say 3, which is the first element of I.\nIn this case, write C' for the sequence obtained from I by deleting its first element 6. By\n\ndefinition,\nSi aa =agt+ (Su),\n\nael acC\nand\n\nHE) (E(S\"))\n\nkeK ‘a&€ly, jEtJ Sael;\nSince |C| =n — 1, by the induction hypothesis, we have\n(Se)=X(Le)\naeC jet Sa€l;\n\nwhich yields our identity.\n\nCase 2. The sequence I;, has at least two elements. In this case, let 8 be the first element\nof I (and thus of J;,), let I’ be the sequence obtained from J by deleting its first element £,\nlet Ij, be the sequence obtained from J;,, by deleting its first element 6, and let J, = Ip, for\ni =2,...,p. Recall that J = (ko,...,k,) and kK = (ki,...,k,). The sequence I’ has n — 1\nelements, so by the induction hypothesis applied to J’ and the [;,, we get\n\nYa=O (Lae) = (LH a) + (HL):\nael! keK acl, al}, JET Sal;\nIf we add the lefthand side to ag, by definition we get\non\nael\n\nIf we add the righthand side to ag, using associativity and the definition of an indexed sum,\nwe get\n\nw+ ((X~)+(Z(Le)))= (+ (L4)) +S)\n\nacl, jEJ Sael; acl,\n\n-(L)*(E(E))\n-¥(E«).\n\nkeK \\a€ly\n\nas claimed. Oo\n\n\n\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf I = (1, . . . , n), we also write\n∑n\n\ni=1 ai instead of\n∑\n\ni∈I ai. Since + is associative, Propo-\nsition 3.2 shows that the sum\n\n∑n\ni=1 ai is independent of the grouping of its elements, which\n\njustifies the use the notation a1 + · · ·+ an (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\ncan show that the sum\n\n∑\ni∈I ai does not depend on the ordering of the index set I.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: A× A→ A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (ai)i∈I of elements in A, we have∑\n\nα∈I\naα =\n\n∑\nα∈J\n\naα.\n\nProof. We proceed by induction on the number p of elements in I. If p = 1, we have I = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that I = (1, . . . , p) and that J is a permutation\n(i1, . . . , ip) of I. First, assume that 2 ≤ i1 ≤ p−1, let J ′ be the sequence obtained from J by\ndeleting i1, I ′ be the sequence obtained from I by deleting i1, and let P = (1, 2, . . . , i1−1) and\nQ = (i1 + 1, . . . , p−1, p). Observe that the sequence I ′ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J ′ and I ′, and then by Proposition 3.2\napplied to I ′ and its partition (P,Q), we have\n\n∑\nα∈J ′\n\naα =\n∑\nα∈I′\n\naα =\n\n(i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n.\n\nIf we add the lefthand side to ai1 , by definition we get∑\nα∈J\n\naα.\n\nIf we add the righthand side to ai1 , we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n.\n\nUsing associativity, we get\n\nai1 +\n\n((i1−1∑\ni=1\n\nai\n\n)\n+\n\n( p∑\ni=i1+1\n\nai\n\n))\n=\n\n(\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n,\n\n64 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf J = (1,...,n), we also write }°\"' | a; instead of )7,., a;. Since + is associative, Propo-\nsition 3.2 shows that the sum }>;\"_, a; is independent of the grouping of its elements, which\njustifies the use the notation a, +---+ a, (without any parentheses).\n\nIf we also assume that our associative binary operation on A is commutative, then we\n\ncan show that the sum }°,., a; does not depend on the ordering of the index set J.\n\nProposition 3.3. Given any nonempty set A equipped with an associative and commutative\nbinary operation +: Ax A— A, for any two nonempty finite sequences I and J of distinct\nnatural numbers such that J is a permutation of I (in other words, the underlying sets of I\nand J are identical), for every sequence (a;)icr of elements in A, we have\n\nSota = Soa.\n\naéel aces\n\nProof. We proceed by induction on the number p of elements in J. If p = 1, we have J = J\nand the proposition holds trivially.\n\nIf p > 1, to simplify notation, assume that J = (1,...,p) and that J is a permutation\n(i1,...,%)) of I. First, assume that 2 < 7, < p—1, let J’ be the sequence obtained from J by\ndeleting i, I’ be the sequence obtained from J by deleting 7,, and let P = (1,2,...,i;—1) and\nQ = (%44+1,...,p—1,p). Observe that the sequence I’ is the concatenation of the sequences\nP and Q. By the induction hypothesis applied to J’ and I’, and then by Proposition 3.2\napplied to J’ and its partition (P,Q), we have\n\n44-1 Pp\n) da = u=() a) +() w).\naceJ’ ael’ i=l I=14+1\n\nIf we add the lefthand side to a;,, by definition we get\n\nSo.\n\naed\n\nIf we add the righthand side to a;,, we get\n\n(Eo) (29)\n\nUsing associativity, we get\n\no(E5)+(E9)-+E)-(E9)\n\n\n\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION\n∑\n\ni∈I ai 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i1 − 1), we get(\n\nai1 +\n\n(i1−1∑\ni=1\n\nai\n\n))\n+\n\n( p∑\ni=i1+1\n\nai\n\n)\n=\n\n(i1−1∑\ni=1\n\nai\n\n)\n+ ai1 +\n\n( p∑\ni=i1+1\n\nai\n\n)\n\n=\n\np∑\ni=1\n\nai,\n\nas claimed.\n\nThe cases where i1 = 1 or i1 = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence).\n\nHaving done all this, we can now make sense of sums of the form\n∑\n\ni∈I ai, for any finite\nindexed set I and any family a = (ai)i∈I of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since I is finite, it is in bijection with the set {1, . . . , n} for some n ∈ N, and any\ntotal ordering � on I corresponds to a permutation I� of {1, . . . , n} (where we identify a\npermutation with its image). For any total ordering � on I, we define\n\n∑\ni∈I,� ai as∑\n\ni∈I,�\nai =\n\n∑\nj∈I�\n\naj.\n\nThen for any other total ordering �′ on I, we have∑\ni∈I,�′\n\nai =\n∑\nj∈I�′\n\naj,\n\nand since I� and I�′ are different permutations of {1, . . . , n}, by Proposition 3.3, we have∑\nj∈I�\n\naj =\n∑\nj∈I�′\n\naj.\n\nTherefore, the sum\n∑\n\ni∈I,� ai does not depend on the total ordering on I. We define the sum∑\ni∈I ai as the common value\n\n∑\ni∈I,� ai for all total orderings � of I.\n\nHere are some examples with A = R:\n\n1. If I = {1, 2, 3}, a = {(1, 2), (2,−3), (3,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n2. If I = {2, 5, 7}, a = {(2, 2), (5,−3), (7,\n√\n\n2)}, then\n∑\n\ni∈I ai = 2− 3 +\n√\n\n2 = −1 +\n√\n\n2.\n\n3. If I = {r, g, b}, a = {(r, 2), (g,−3), (b, 1)}, then\n∑\n\ni∈I ai = 2− 3 + 1 = 0.\n\n3.3. INDEXED FAMILIES; THE SUM NOTATION )0,.; a 65\n\nthen using associativity and commutativity several times (more rigorously, using induction\non i; — 1), we get\n\n(+ (E9))(E9)- Ea) ome(E9)\n\nP\n\nS ai,\n\ni=1\n\nas claimed.\n\nThe cases where 7; = 1 or 7; = p are treated similarly, but in a simpler manner since\neither P = () or Q = () (where () denotes the empty sequence). O\n\nHaving done all this, we can now make sense of sums of the form }°,., a;, for any finite\nindexed set J and any family a = (a;);c7 of elements in A, where A is a set equipped with a\nbinary operation + which is associative and commutative.\n\nIndeed, since J is finite, it is in bijection with the set {1,...,n} for some n € N, and any\ntotal ordering < on J corresponds to a permutation J, of {1,...,n} (where we identify a\npermutation with its image). For any total ordering =< on J, we define }0,-; a as\n\nSo a= ray,\n\ni€l,x jel\n\nThen for any other total ordering =<’ on J, we have\n\na= Da\n\ni€1,<! j€ler\nand since J, and J are different permutations of {1,...,n}, by Proposition 3.3, we have\n) ay= ) aj.\nJET JEL:\n\nTherefore, the sum )7,-; . a; does not depend on the total ordering on J. We define the sum\nier % as the common value $7,-;~ a for all total orderings = of J.\n\nHere are some examples with A = R:\n1. If J = {1,2,3}, a = {(1, 2), (2, -3), (3, V2)}, then 0),-,a; = 2-34 V2 =-14 v2.\n2. If I = {2,5,7}, a = {(2, 2), (5, -3), (7, V2)}, then ),-, a; = 2-34 V2 =—-14 v2.\n\n3. If l= {r, 9, Bf, a= {(r, 2), (9, —3), (0, 1)}, then ier ay = 2—3+1=0.\n\n\n\n\n66 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n3.4 Linear Independence, Subspaces\n\nOne of the most useful properties of vector spaces is that they possess bases. What this\nmeans is that in every vector space E, there is some set of vectors, {e1, . . . , en}, such that\nevery vector v ∈ E can be written as a linear combination,\n\nv = λ1e1 + · · ·+ λnen,\n\nof the ei, for some scalars, λ1, . . . , λn ∈ R. Furthermore, the n-tuple, (λ1, . . . , λn), as above\nis unique.\n\nThis description is fine when E has a finite basis, {e1, . . . , en}, but this is not always the\ncase! For example, the vector space of real polynomials, R[X], does not have a finite basis\nbut instead it has an infinite basis, namely\n\n1, X, X2, . . . , Xn, . . .\n\nOne might wonder if it is possible for a vector space to have bases of different sizes, or even\nto have a finite basis as well as an infinite basis. We will see later on that this is not possible;\nall bases of a vector space have the same number of elements (cardinality), which is called\nthe dimension of the space. However, we have the following problem: If a vector space has\nan infinite basis, {e1, e2, . . . , }, how do we define linear combinations? Do we allow linear\ncombinations\n\nλ1e1 + λ2e2 + · · ·\nwith infinitely many nonzero coefficients?\n\nIf we allow linear combinations with infinitely many nonzero coefficients, then we have\nto make sense of these sums and this can only be done reasonably if we define such a sum\nas the limit of the sequence of vectors, s1, s2, . . . , sn, . . ., with s1 = λ1e1 and\n\nsn+1 = sn + λn+1en+1.\n\nBut then, how do we define such limits? Well, we have to define some topology on our space,\nby means of a norm, a metric or some other mechanism. This can indeed be done and this\nis what Banach spaces and Hilbert spaces are all about but this seems to require a lot of\nmachinery.\n\nA way to avoid limits is to restrict our attention to linear combinations involving only\nfinitely many vectors. We may have an infinite supply of vectors but we only form linear\ncombinations involving finitely many nonzero coefficients. Technically, this can be done by\nintroducing families of finite support . This gives us the ability to manipulate families of\nscalars indexed by some fixed infinite set and yet to be treat these families as if they were\nfinite.\n\nWith these motivations in mind, given a set A, recall that an I-indexed family (ai)i∈I\nof elements of A (for short, a family) is a function a : I → A, or equivalently a set of pairs\n{(i, ai) | i ∈ I}. We agree that when I = ∅, (ai)i∈I = ∅. A family (ai)i∈I is finite if I is finite.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 67\n\nRemark: When considering a family (ai)i∈I , there is no reason to assume that I is ordered.\nThe crucial point is that every element of the family is uniquely indexed by an element of\nI. Thus, unless specified otherwise, we do not assume that the elements of an index set are\nordered.\n\nIf A is an abelian group with identity 0, we say that a family (ai)i∈I has finite support if\nai = 0 for all i ∈ I − J , where J is a finite subset of I (the support of the family).\n\nGiven two disjoint sets I and J , the union of two families (ui)i∈I and (vj)j∈J , denoted as\n(ui)i∈I ∪ (vj)j∈J , is the family (wk)k∈(I∪J) defined such that wk = uk if k ∈ I, and wk = vk\nif k ∈ J . Given a family (ui)i∈I and any element v, we denote by (ui)i∈I ∪k (v) the family\n(wi)i∈I∪{k} defined such that, wi = ui if i ∈ I, and wk = v, where k is any index such that\nk /∈ I. Given a family (ui)i∈I , a subfamily of (ui)i∈I is a family (uj)j∈J where J is any subset\nof I.\n\nIn this chapter, unless specified otherwise, is assumed that all families of scalars have\nfinite support .\n\nDefinition 3.3. Let E be a vector space. A vector v ∈ E is a linear combination of a family\n(ui)i∈I of elements of E iff there is a family (λi)i∈I of scalars in K such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nWhen I = ∅, we stipulate that v = 0. (By Proposition 3.3, sums of the form\n∑\n\ni∈I λiui are\nwell defined.) We say that a family (ui)i∈I is linearly independent iff for every family (λi)i∈I\nof scalars in K, ∑\n\ni∈I\nλiui = 0 implies that λi = 0 for all i ∈ I.\n\nEquivalently, a family (ui)i∈I is linearly dependent iff there is some family (λi)i∈I of scalars\nin K such that ∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I.\n\nWe agree that when I = ∅, the family ∅ is linearly independent.\n\nObserve that defining linear combinations for families of vectors rather than for sets of\nvectors has the advantage that the vectors being combined need not be distinct. For example,\nfor I = {1, 2, 3} and the families (u, v, u) and (λ1, λ2, λ1), the linear combination∑\n\ni∈I\nλiui = λ1u+ λ2v + λ1u\n\nmakes sense. Using sets of vectors in the definition of a linear combination does not allow\nsuch linear combinations; this is too restrictive.\n\n\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (ui)i∈I is linearly dependent iff either I consists of a\nsingle element, say i, and ui = 0, or |I| ≥ 2 and some uj in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (λi)i∈I of scalars in K such that∑\n\ni∈I\nλiui = 0 and λj 6= 0 for some j ∈ I,\n\nand since |I| ≥ 2, the set I − {j} is nonempty and we get\n\nuj =\n∑\n\ni∈(I−{j})\n−λ−1\n\nj λiui.\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (ui)i∈I is linearly independent iff either I = ∅, or I\nconsists of a single element i and ui 6= 0, or |I| ≥ 2 and no vector uj in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen I is nonempty, if the family (ui)i∈I is linearly independent, note that ui 6= 0 for\nall i ∈ I. Otherwise, if ui = 0 for some i ∈ I, then we get a nontrivial linear dependence∑\n\ni∈I λiui = 0 by picking any nonzero λi and letting λk = 0 for all k ∈ I with k 6= i, since\nλi0 = 0. If |I| ≥ 2, we must also have ui 6= uj for all i, j ∈ I with i 6= j, since otherwise we\nget a nontrivial linear dependence by picking λi = λ and λj = −λ for any nonzero λ, and\nletting λk = 0 for all k ∈ I with k 6= i, j.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, as we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n\n1. Any two distinct scalars λ, µ 6= 0 in K are linearly dependent.\n\n2. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1) are linearly independent. See Figure\n3.7.\n\n3. In R4, the vectors (1, 1, 1, 1), (0, 1, 1, 1), (0, 0, 1, 1), and (0, 0, 0, 1) are linearly indepen-\ndent.\n\n68 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nUnravelling Definition 3.3, a family (u;);<7 is linearly dependent iff either J consists of a\nsingle element, say 7, and u; = 0, or |I| > 2 and some u; in the family can be expressed as\na linear combination of the other vectors in the family. Indeed, in the second case, there is\nsome family (A;)ic7 of scalars in AK such that\n\nSo dit = 0 and A; #0 for some j € J,\n\ntel\n\nand since |I| > 2, the set J — {7} is nonempty and we get\n\nie(I—{3})\n\nObserve that one of the reasons for defining linear dependence for families of vectors\nrather than for sets of vectors is that our definition allows multiple occurrences of a vector.\nThis is important because a matrix may contain identical columns, and we would like to say\nthat these columns are linearly dependent. The definition of linear dependence for sets does\nnot allow us to do that.\n\nThe above also shows that a family (u;)jc7 is linearly independent iff either J = @, or I\nconsists of a single element i and u; ¥ 0, or |J| > 2 and no vector wu; in the family can be\nexpressed as a linear combination of the other vectors in the family.\n\nWhen J is nonempty, if the family (u;)icr is linearly independent, note that u; 4 0 for\nalli € I. Otherwise, if u; = 0 for some 2 € J, then we get a nontrivial linear dependence\nier Mii = O by picking any nonzero 4; and letting A, = 0 for all k € J with k ¥ 7, since\nA;0 = 0. If |Z] > 2, we must also have u; 4 u; for all 7,7 € J with i 4 j, since otherwise we\nget a nontrivial linear dependence by picking A; = A and A; = —A for any nonzero A, and\nletting A, = 0 for all k € I with k £2, 7.\n\nThus, the definition of linear independence implies that a nontrivial linearly independent\nfamily is actually a set. This explains why certain authors choose to define linear indepen-\ndence for sets of vectors. The problem with this approach is that linear dependence, which\nis the logical negation of linear independence, is then only defined for sets of vectors. How-\never, aS we pointed out earlier, it is really desirable to define linear dependence for families\nallowing multiple occurrences of the same vector.\n\nExample 3.2.\n1. Any two distinct scalars A, #0 in K are linearly dependent.\n\n2. In R°, the vectors (1,0,0), (0,1,0), and (0,0,1) are linearly independent. See Figure\n3.7.\n\n3. In R*, the vectors (1,1,1,1), (0,1,1,1), (0,0,1,1), and (0,0,0,1) are linearly indepen-\ndent.\n\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 69\n\nFigure 3.7: A visual (arrow) depiction of the red vector (1, 0, 0), the green vector (0, 1, 0),\nand the blue vector (0, 0, 1) in R3.\n\n4. In R2, the vectors u = (1, 1), v = (0, 1) and w = (2, 3) are linearly dependent, since\n\nw = 2u+ v.\n\nSee Figure 3.8.\n\n(2,3)\n\n2u\n\nv\n\nw\n\nFigure 3.8: A visual (arrow) depiction of the pink vector u = (1, 1), the dark purple vector\nv = (0, 1), and the vector sum w = 2u+ v.\n\nWhen I is finite, we often assume that it is the set I = {1, 2, . . . , n}. In this case, we\ndenote the family (ui)i∈I as (u1, . . . , un).\n\nThe notion of a subspace of a vector space is defined as follows.\n\nDefinition 3.4. Given a vector space E, a subset F of E is a linear subspace (or subspace)\nof E iff F is nonempty and λu+ µv ∈ F for all u, v ∈ F , and all λ, µ ∈ K.\n\n\n\n70 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is easy to see that a subspace F of E is indeed a vector space, since the restriction\nof +: E × E → E to F × F is indeed a function +: F × F → F , and the restriction of\n· : K × E → E to K × F is indeed a function · : K × F → F .\n\nSince a subspace F is nonempty, if we pick any vector u ∈ F and if we let λ = µ = 0,\nthen λu+ µu = 0u+ 0u = 0, so every subspace contains the vector 0.\n\nThe following facts also hold. The proof is left as an exercise.\n\nProposition 3.4.\n\n(1) The intersection of any family (even infinite) of subspaces of a vector space E is a\nsubspace.\n\n(2) Let F be any subspace of a vector space E. For any nonempty finite index set I,\nif (ui)i∈I is any family of vectors ui ∈ F and (λi)i∈I is any family of scalars, then∑\n\ni∈I λiui ∈ F .\n\nThe subspace {0} will be denoted by (0), or even 0 (with a mild abuse of notation).\n\nExample 3.3.\n\n1. In R2, the set of vectors u = (x, y) such that\n\nx+ y = 0\n\nis the subspace illustrated by Figure 3.9.\n\nFigure 3.9: The subspace x+ y = 0 is the line through the origin with slope −1. It consists\nof all vectors of the form λ(−1, 1).\n\n2. In R3, the set of vectors u = (x, y, z) such that\n\nx+ y + z = 0\n\nis the subspace illustrated by Figure 3.10.\n\n\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+y+z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n ≥ 0, the set of polynomials f(X) ∈ R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n×n matrices is a subspace of the space of n×n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace 〈S〉 (or Span(S)) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\nof E, leaving as an exercise the verification that every subspace containing S also contains\nSpan(S).\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u =\n∑\n\ni∈I λiui\nand v =\n\n∑\nj∈J µjvj are any two linear combinations in Span(S), for any two scalars λ, µ ∈ K,\n\nλu+ µv = λ\n∑\ni∈I\n\nλiui + µ\n∑\nj∈J\n\nµjvj\n\n=\n∑\ni∈I\n\nλλiui +\n∑\nj∈J\n\nµµjvj\n\n=\n∑\ni∈I−J\n\nλλiui +\n∑\ni∈I∩J\n\n(λλi + µµi)ui +\n∑\nj∈J−I\n\nµµjvj,\n\nwhich is a linear combination with index set I ∪ J , and thus λu + µv ∈ Span(S), which\nproves that Span(S) is a subspace.\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n3.4. LINEAR INDEPENDENCE, SUBSPACES 71\n\nFigure 3.10: The subspace x+ y+ z = 0 is the plane through the origin with normal (1, 1, 1).\n\n3. For any n > 0, the set of polynomials f(X) € R[X] of degree at most n is a subspace\nof R[X].\n\n4. The set of upper triangular n x n matrices is a subspace of the space of n x n matrices.\n\nProposition 3.5. Given any vector space E, if S is any nonempty subset of E, then the\nsmallest subspace (S) (or Span(S')) of E containing S is the set of all (finite) linear combi-\nnations of elements from S.\n\nProof. We prove that the set Span(S) of all linear combinations of elements of S is a subspace\n\nof EF, leaving as an exercise the verification that every subspace containing S' also contains\nSpan().\n\nFirst, Span(S) is nonempty since it contains S (which is nonempty). If u = S),.; Aim\nandv=)> jeg Mj¥j are any two linear combinations in Span(.S), for any two scalars A, uw € K,\n\nAu + pv = NSO jue + HD p50;\n\ni€l jed\n= S- ANU; + S- LLjV5\niel jeJ\n= SO Mii + SS OA + mpedur + SS pyr,\nieI—J i€INI jeJ—1\n\nwhich is a linear combination with index set J U J, and thus Au + yu € Span(S), which\nproves that Span(S) is a subspace. O\n\nOne might wonder what happens if we add extra conditions to the coefficients involved\nin forming linear combinations. Here are three natural restrictions which turn out to be\nimportant (as usual, we assume that our index sets are finite):\n\n\n\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations\n∑\n\ni∈I λiui for which∑\ni∈I\n\nλi = 1.\n\nThese are called affine combinations . One should realize that every linear combination∑\ni∈I λiui can be viewed as an affine combination. For example, if k is an index not\n\nin I, if we let J = I ∪ {k}, uk = 0, and λk = 1−∑i∈I λi, then\n∑\n\nj∈J λjuj is an affine\ncombination and ∑\n\ni∈I\nλiui =\n\n∑\nj∈J\n\nλjuj.\n\nHowever, we get new spaces. For example, in R3, the set of all affine combinations of\nthe three vectors e1 = (1, 0, 0), e2 = (0, 1, 0), and e3 = (0, 0, 1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0, 0, 0), it is not a linear\nsubspace.\n\n(2) Consider combinations\n∑\n\ni∈I λiui for which\n\nλi ≥ 0, for all i ∈ I.\n\nThese are called positive (or conic) combinations . It turns out that positive combina-\ntions of families of vectors are cones . They show up naturally in convex optimization.\n\n(3) Consider combinations\n∑\n\ni∈I λiui for which we require (1) and (2), that is∑\ni∈I\n\nλi = 1, and λi ≥ 0 for all i ∈ I.\n\nThese are called convex combinations . Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum\n\n∑\ni∈I λiui makes sense, we restrict our attention to families of finite\n\nsupport.\n\nDefinition 3.5. Given any field K, a family of scalars (λi)i∈I has finite support if λi = 0\nfor all i ∈ I − J , for some finite subset J of I.\n\nIf (λi)i∈I is a family of scalars of finite support, for any vector space E over K, for any\n(possibly infinite) family (ui)i∈I of vectors ui ∈ E, we define the linear combination\n\n∑\ni∈I λiui\n\nas the finite linear combination\n∑\n\nj∈J λjuj, where J is any finite subset of I such that λi = 0\nfor all i ∈ I − J . In general, results stated for finite families also hold for families of finite\nsupport.\n\n72 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n(1) Consider combinations }7,-, A;u; for which\n\nSov = 1.\n\nvel\n\nThese are called affine combinations. One should realize that every linear combination\nier Ais Can be viewed as an affine combination. For example, if k is an index not\nin I, if we let J= JU {k}, up = 0, and Ay =1—)97,-, i, then )7,-, Aju is an affine\n\ncombination and\n\nSs\" AjUj = S- Aj Uy.\n\niel jed\nHowever, we get new spaces. For example, in R°, the set of all affine combinations of\nthe three vectors e; = (1,0,0),e2 = (0,1,0), and e3 = (0,0,1), is the plane passing\nthrough these three points. Since it does not contain 0 = (0,0,0), it is not a linear\nsubspace.\n\n(2) Consider combinations }7,-, A;u; for which\nA; > 0, for allie J.\n\nThese are called positive (or conic) combinations. It turns out that positive combina-\ntions of families of vectors are cones. They show up naturally in convex optimization.\n\n(3) Consider combinations )>,-, A;u; for which we require (1) and (2), that is\n\nwel\nSox Hh, and \\;>0 foralli eT.\n\nwel\n\nThese are called conver combinations. Given any finite family of vectors, the set of all\nconvex combinations of these vectors is a convex polyhedron. Convex polyhedra play a\nvery important role in convex optimization.\n\nRemark: The notion of linear combination can also be defined for infinite index sets I.\nTo ensure that a sum 5°,_, A;u; makes sense, we restrict our attention to families of finite\nsupport.\n\niel\n\nDefinition 3.5. Given any field K, a family of scalars (\\;)ier has finite support if A; = 0\nfor all 2 € I — J, for some finite subset J of J.\n\nIf (A;)icr is a family of scalars of finite support, for any vector space F over K, for any\n(possibly infinite) family (u;);er of vectors u; € E, we define the linear combination 7) ,-, Ait\nas the finite linear combination )> jeg AjUj, Where J is any finite subset of J such that A; = 0\nfor all: € I — J. In general, results stated for finite families also hold for families of finite\nsupport.\n\n\n\n\n3.5. BASES OF A VECTOR SPACE 73\n\n3.5 Bases of a Vector Space\n\nGiven a vector space E, given a family (vi)i∈I , the subset V of E consisting of the null vector\n0 and of all linear combinations of (vi)i∈I is easily seen to be a subspace of E. The family\n(vi)i∈I is an economical way of representing the entire subspace V , but such a family would\nbe even nicer if it was not redundant. Subspaces having such an “efficient” generating family\n(called a basis) play an important role and motivate the following definition.\n\nDefinition 3.6. Given a vector space E and a subspace V of E, a family (vi)i∈I of vectors\nvi ∈ V spans V or generates V iff for every v ∈ V , there is some family (λi)i∈I of scalars in\nK such that\n\nv =\n∑\ni∈I\n\nλivi.\n\nWe also say that the elements of (vi)i∈I are generators of V and that V is spanned by (vi)i∈I ,\nor generated by (vi)i∈I . If a subspace V of E is generated by a finite family (vi)i∈I , we say\nthat V is finitely generated . A family (ui)i∈I that spans V and is linearly independent is\ncalled a basis of V .\n\nExample 3.4.\n\n1. In R3, the vectors (1, 0, 0), (0, 1, 0), and (0, 0, 1), illustrated in Figure 3.9, form a basis.\n\n2. The vectors (1, 1, 1, 1), (1, 1,−1,−1), (1,−1, 0, 0), (0, 0, 1,−1) form a basis of R4 known\nas the Haar basis . This basis and its generalization to dimension 2n are crucial in\nwavelet theory.\n\n3. In the subspace of polynomials in R[X] of degree at most n, the polynomials 1, X,X2,\n. . . , Xn form a basis.\n\n4. The Bernstein polynomials\n\n(\nn\nk\n\n)\n(1 − X)n−kXk for k = 0, . . . , n, also form a basis of\n\nthat space. These polynomials play a major role in the theory of spline curves .\n\nThe first key result of linear algebra is that every vector space E has a basis. We begin\nwith a crucial lemma which formalizes the mechanism for building a basis incrementally.\n\nLemma 3.6. Given a linearly independent family (ui)i∈I of elements of a vector space E, if\nv ∈ E is not a linear combination of (ui)i∈I , then the family (ui)i∈I ∪k (v) obtained by adding\nv to the family (ui)i∈I is linearly independent (where k /∈ I).\n\nProof. Assume that µv+\n∑\n\ni∈I λiui = 0, for any family (λi)i∈I of scalars in K. If µ 6= 0, then\nµ has an inverse (because K is a field), and thus we have v = −∑i∈I(µ\n\n−1λi)ui, showing\nthat v is a linear combination of (ui)i∈I and contradicting the hypothesis. Thus, µ = 0. But\nthen, we have\n\n∑\ni∈I λiui = 0, and since the family (ui)i∈I is linearly independent, we have\n\nλi = 0 for all i ∈ I.\n\n\n\n74 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nThe next theorem holds in general, but the proof is more sophisticated for vector spaces\nthat do not have a finite set of generators. Thus, in this chapter, we only prove the theorem\nfor finitely generated vector spaces.\n\nTheorem 3.7. Given any finite family S = (ui)i∈I generating a vector space E and any\nlinearly independent subfamily L = (uj)j∈J of S (where J ⊆ I), there is a basis B of E such\nthat L ⊆ B ⊆ S.\n\nProof. Consider the set of linearly independent families B such that L ⊆ B ⊆ S. Since this\nset is nonempty and finite, it has some maximal element (that is, a subfamily B = (uh)h∈H\nof S with H ⊆ I of maximum cardinality), say B = (uh)h∈H . We claim that B generates\nE. Indeed, if B does not generate E, then there is some up ∈ S that is not a linear\ncombination of vectors in B (since S generates E), with p /∈ H. Then by Lemma 3.6, the\nfamily B′ = (uh)h∈H∪{p} is linearly independent, and since L ⊆ B ⊂ B′ ⊆ S, this contradicts\nthe maximality of B. Thus, B is a basis of E such that L ⊆ B ⊆ S.\n\nRemark: Theorem 3.7 also holds for vector spaces that are not finitely generated. In this\ncase, the problem is to guarantee the existence of a maximal linearly independent family B\nsuch that L ⊆ B ⊆ S. The existence of such a maximal family can be shown using Zorn’s\nlemma, see Appendix B and the references given there.\n\nA situation where the full generality of Theorem 3.7 is needed is the case of the vector\nspace R over the field of coefficients Q. The numbers 1 and\n\n√\n2 are linearly independent\n\nover Q, so according to Theorem 3.7, the linearly independent family L = (1,\n√\n\n2) can be\nextended to a basis B of R. Since R is uncountable and Q is countable, such a basis must\nbe uncountable!\n\nThe notion of a basis can also be defined in terms of the notion of maximal linearly\nindependent family and minimal generating family.\n\nDefinition 3.7. Let (vi)i∈I be a family of vectors in a vector space E. We say that (vi)i∈I a\nmaximal linearly independent family of E if it is linearly independent, and if for any vector\nw ∈ E, the family (vi)i∈I ∪k {w} obtained by adding w to the family (vi)i∈I is linearly\ndependent. We say that (vi)i∈I a minimal generating family of E if it spans E, and if for\nany index p ∈ I, the family (vi)i∈I−{p} obtained by removing vp from the family (vi)i∈I does\nnot span E.\n\nThe following proposition giving useful properties characterizing a basis is an immediate\nconsequence of Lemma 3.6.\n\nProposition 3.8. Given a vector space E, for any family B = (vi)i∈I of vectors of E, the\nfollowing properties are equivalent:\n\n(1) B is a basis of E.\n\n\n\n3.5. BASES OF A VECTOR SPACE 75\n\n(2) B is a maximal linearly independent family of E.\n\n(3) B is a minimal generating family of E.\n\nProof. We will first prove the equivalence of (1) and (2). Assume (1). Since B is a basis, it is\na linearly independent family. We claim that B is a maximal linearly independent family. If\nB is not a maximal linearly independent family, then there is some vector w ∈ E such that\nthe family B′ obtained by adding w to B is linearly independent. However, since B is a basis\nof E, the vector w can be expressed as a linear combination of vectors in B, contradicting\nthe fact that B′ is linearly independent.\n\nConversely, assume (2). We claim that B spans E. If B does not span E, then there is\nsome vector w ∈ E which is not a linear combination of vectors in B. By Lemma 3.6, the\nfamily B′ obtained by adding w to B is linearly independent. Since B is a proper subfamily\nof B′, this contradicts the assumption that B is a maximal linearly independent family.\nTherefore, B must span E, and since B is also linearly independent, it is a basis of E.\n\nNow we will prove the equivalence of (1) and (3). Again, assume (1). Since B is a basis,\nit is a generating family of E. We claim that B is a minimal generating family. If B is not\na minimal generating family, then there is a proper subfamily B′ of B that spans E. Then,\nevery w ∈ B−B′ can be expressed as a linear combination of vectors from B′, contradicting\nthe fact that B is linearly independent.\n\nConversely, assume (3). We claim that B is linearly independent. If B is not linearly\nindependent, then some vector w ∈ B can be expressed as a linear combination of vectors\nin B′ = B − {w}. Since B generates E, the family B′ also generates E, but B′ is a\nproper subfamily of B, contradicting the minimality of B. Since B spans E and is linearly\nindependent, it is a basis of E.\n\nThe second key result of linear algebra is that for any two bases (ui)i∈I and (vj)j∈J of a\nvector space E, the index sets I and J have the same cardinality. In particular, if E has a\nfinite basis of n elements, every basis of E has n elements, and the integer n is called the\ndimension of the vector space E.\n\nTo prove the second key result, we can use the following replacement lemma due to\nSteinitz. This result shows the relationship between finite linearly independent families and\nfinite families of generators of a vector space. We begin with a version of the lemma which is\na bit informal, but easier to understand than the precise and more formal formulation given\nin Proposition 3.10. The technical difficulty has to do with the fact that some of the indices\nneed to be renamed.\n\nProposition 3.9. (Replacement lemma, version 1) Given a vector space E, let (u1, . . . , um)\nbe any finite linearly independent family in E, and let (v1, . . . , vn) be any finite family such\nthat every ui is a linear combination of (v1, . . . , vn). Then we must have m ≤ n, and there\nis a replacement of m of the vectors vj by (u1, . . . , um), such that after renaming some of the\nindices of the vjs, the families (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate the same\nsubspace of E.\n\n\n\n76 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProof. We proceed by induction on m. When m = 0, the family (u1, . . . , um) is empty, and\nthe proposition holds trivially. For the induction step, we have a linearly independent family\n(u1, . . . , um, um+1). Consider the linearly independent family (u1, . . . , um). By the induction\nhypothesis, m ≤ n, and there is a replacement of m of the vectors vj by (u1, . . . , um), such\nthat after renaming some of the indices of the vs, the families (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace of E. The vector um+1 can also be expressed as a lin-\near combination of (v1, . . . , vn), and since (u1, . . . , um, vm+1, . . . , vn) and (v1, . . . , vn) generate\nthe same subspace, um+1 can be expressed as a linear combination of (u1, . . . , um, vm+1, . . .,\nvn), say\n\num+1 =\nm∑\ni=1\n\nλiui +\nn∑\n\nj=m+1\n\nλjvj.\n\nWe claim that λj 6= 0 for some j with m+ 1 ≤ j ≤ n, which implies that m+ 1 ≤ n.\n\nOtherwise, we would have\n\num+1 =\nm∑\ni=1\n\nλiui,\n\na nontrivial linear dependence of the ui, which is impossible since (u1, . . . , um+1) are linearly\nindependent.\n\nTherefore, m + 1 ≤ n, and after renaming indices if necessary, we may assume that\nλm+1 6= 0, so we get\n\nvm+1 = −\nm∑\ni=1\n\n(λ−1\nm+1λi)ui − λ−1\n\nm+1um+1 −\nn∑\n\nj=m+2\n\n(λ−1\nm+1λj)vj.\n\nObserve that the families (u1, . . . , um, vm+1, . . . , vn) and (u1, . . . , um+1, vm+2, . . . , vn) generate\nthe same subspace, since um+1 is a linear combination of (u1, . . . , um, vm+1, . . . , vn) and vm+1\n\nis a linear combination of (u1, . . . , um+1, vm+2, . . . , vn). Since (u1, . . . , um, vm+1, . . . , vn) and\n(v1, . . . , vn) generate the same subspace, we conclude that (u1, . . . , um+1, vm+2, . . . , vn) and\nand (v1, . . . , vn) generate the same subspace, which concludes the induction hypothesis.\n\nHere is an example illustrating the replacement lemma. Consider sequences (u1, u2, u3)\nand (v1, v2, v3, v4, v5), where (u1, u2, u3) is a linearly independent family and with the uis\nexpressed in terms of the vjs as follows:\n\nu1 = v4 + v5\n\nu2 = v3 + v4 − v5\n\nu3 = v1 + v2 + v3.\n\nFrom the first equation we get\n\nv4 = u1 − v5,\n\n\n\n3.5. BASES OF A VECTOR SPACE 77\n\nand by substituting in the second equation we have\n\nu2 = v3 + v4 − v5 = v3 + u1 − v5 − v5 = u1 + v3 − 2v5.\n\nFrom the above equation we get\n\nv3 = −u1 + u2 + 2v5,\n\nand so\nu3 = v1 + v2 + v3 = v1 + v2 − u1 + u2 + 2v5.\n\nFinally, we get\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nTherefore we have\n\nv1 = u1 − u2 + u3 − v2 − 2v5\n\nv3 = −u1 + u2 + 2v5\n\nv4 = u1 − v5,\n\nwhich shows that (u1, u2, u3, v2, v5) spans the same subspace as (v1, v2, v3, v4, v5). The vectors\n(v1, v3, v4) have been replaced by (u1, u2, u3), and the vectors left over are (v2, v5). We can\nrename them (v4, v5).\n\nFor the sake of completeness, here is a more formal statement of the replacement lemma\n(and its proof).\n\nProposition 3.10. (Replacement lemma, version 2) Given a vector space E, let (ui)i∈I be\nany finite linearly independent family in E, where |I| = m, and let (vj)j∈J be any finite family\nsuch that every ui is a linear combination of (vj)j∈J , where |J | = n. Then there exists a set\nL and an injection ρ : L→ J (a relabeling function) such that L ∩ I = ∅, |L| = n−m, and\nthe families (ui)i∈I ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E. In particular,\nm ≤ n.\n\nProof. We proceed by induction on |I| = m. When m = 0, the family (ui)i∈I is empty, and\nthe proposition holds trivially with L = J (ρ is the identity). Assume |I| = m+ 1. Consider\nthe linearly independent family (ui)i∈(I−{p}), where p is any member of I. By the induction\nhypothesis, there exists a set L and an injection ρ : L → J such that L ∩ (I − {p}) = ∅,\n|L| = n−m, and the families (ui)i∈(I−{p})∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace\nof E. If p ∈ L, we can replace L by (L− {p}) ∪ {p′} where p′ does not belong to I ∪ L, and\nreplace ρ by the injection ρ′ which agrees with ρ on L − {p} and such that ρ′(p′) = ρ(p).\nThus, we can always assume that L ∩ I = ∅. Since up is a linear combination of (vj)j∈J\nand the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (vj)j∈J generate the same subspace of E, up is\na linear combination of (ui)i∈(I−{p}) ∪ (vρ(l))l∈L. Let\n\nup =\n∑\n\ni∈(I−{p})\nλiui +\n\n∑\nl∈L\n\nλlvρ(l). (1)\n\n\n\n78 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIf λl = 0 for all l ∈ L, we have ∑\ni∈(I−{p})\n\nλiui − up = 0,\n\ncontradicting the fact that (ui)i∈I is linearly independent. Thus, λl 6= 0 for some l ∈ L, say\nl = q. Since λq 6= 0, we have\n\nvρ(q) =\n∑\n\ni∈(I−{p})\n(−λ−1\n\nq λi)ui + λ−1\nq up +\n\n∑\nl∈(L−{q})\n\n(−λ−1\nq λl)vρ(l). (2)\n\nWe claim that the families (ui)i∈(I−{p}) ∪ (vρ(l))l∈L and (ui)i∈I ∪ (vρ(l))l∈(L−{q}) generate the\nsame subset of E. Indeed, the second family is obtained from the first by replacing vρ(q) by up,\nand vice-versa, and up is a linear combination of (ui)i∈(I−{p})∪ (vρ(l))l∈L, by (1), and vρ(q) is a\nlinear combination of (ui)i∈I∪(vρ(l))l∈(L−{q}), by (2). Thus, the families (ui)i∈I∪(vρ(l))l∈(L−{q})\nand (vj)j∈J generate the same subspace of E, and the proposition holds for L−{q} and the\nrestriction of the injection ρ : L→ J to L−{q}, since L∩ I = ∅ and |L| = n−m imply that\n(L− {q}) ∩ I = ∅ and |L− {q}| = n− (m+ 1).\n\nThe idea is that m of the vectors vj can be replaced by the linearly independent uis in\nsuch a way that the same subspace is still generated. The purpose of the function ρ : L→ J\nis to pick n −m elements j1, . . . , jn−m of J and to relabel them l1, . . . , ln−m in such a way\nthat these new indices do not clash with the indices in I; this way, the vectors vj1 , . . . , vjn−m\nwho “survive” (i.e. are not replaced) are relabeled vl1 , . . . , vln−m , and the other m vectors vj\nwith j ∈ J −{j1, . . . , jn−m} are replaced by the ui. The index set of this new family is I ∪L.\n\nActually, one can prove that Proposition 3.10 implies Theorem 3.7 when the vector space\nis finitely generated. Putting Theorem 3.7 and Proposition 3.10 together, we obtain the\nfollowing fundamental theorem.\n\nTheorem 3.11. Let E be a finitely generated vector space. Any family (ui)i∈I generating E\ncontains a subfamily (uj)j∈J which is a basis of E. Any linearly independent family (ui)i∈I\ncan be extended to a family (uj)j∈J which is a basis of E (with I ⊆ J). Furthermore, for\nevery two bases (ui)i∈I and (vj)j∈J of E, we have |I| = |J | = n for some fixed integer n ≥ 0.\n\nProof. The first part follows immediately by applying Theorem 3.7 with L = ∅ and S =\n(ui)i∈I . For the second part, consider the family S ′ = (ui)i∈I ∪ (vh)h∈H , where (vh)h∈H is any\nfinitely generated family generating E, and with I ∩ H = ∅. Then apply Theorem 3.7 to\nL = (ui)i∈I and to S ′. For the last statement, assume that (ui)i∈I and (vj)j∈J are bases of\nE. Since (ui)i∈I is linearly independent and (vj)j∈J spans E, Proposition 3.10 implies that\n|I| ≤ |J |. A symmetric argument yields |J | ≤ |I|.\n\nRemark: Theorem 3.11 also holds for vector spaces that are not finitely generated. This\ncan be shown as follows. Let (ui)i∈I be a basis of E, let (vj)j∈J be a generating family of E,\n\n\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that I is infinite. For every j ∈ J , let Lj ⊆ I be the finite set\n\nLj = {i ∈ I | vj =\n∑\ni∈I\n\nλiui, λi 6= 0}.\n\nLet L =\n⋃\nj∈J Lj. By definition L ⊆ I, and since (ui)i∈I is a basis of E, we must have I = L,\n\nsince otherwise (ui)i∈L would be another basis of E, and this would contradict the fact that\n(ui)i∈I is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe Lj are finite, I would be finite. But then, since I =\n\n⋃\nj∈J Lj with J infinite and the Lj\n\nfinite, by a standard result of set theory, |I| ≤ |J |. If (vj)j∈J is also a basis, by a symmetric\nargument, we obtain |J | ≤ |I|, and thus, |I| = |J | for any two bases (ui)i∈I and (vj)j∈J of E.\n\nDefinition 3.8. When a vector space E is not finitely generated, we say that E is of infinite\ndimension. The dimension of a finitely generated vector space E is the common dimension\nn of all of its bases and is denoted by dim(E).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a ∈ K\nand a 6= 0 is a basis. Thus dim(K) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n ≥ 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n−1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (ui)i∈I be a basis of a vector space E. For any vector v ∈ E, since the family (ui)i∈I\ngenerates E, there is a family (λi)i∈I of scalars in K, such that\n\nv =\n∑\ni∈I\n\nλiui.\n\nA very important fact is that the family (λi)i∈I is unique.\n\nProposition 3.12. Given a vector space E, let (ui)i∈I be a family of vectors in E. Let v ∈ E,\nand assume that v =\n\n∑\ni∈I λiui. Then the family (λi)i∈I of scalars such that v =\n\n∑\ni∈I λiui\n\nis unique iff (ui)i∈I is linearly independent.\n\nProof. First, assume that (ui)i∈I is linearly independent. If (µi)i∈I is another family of scalars\nin K such that v =\n\n∑\ni∈I µiui, then we have∑\n\ni∈I\n(λi − µi)ui = 0,\n\nand since (ui)i∈I is linearly independent, we must have λi−µi = 0 for all i ∈ I, that is, λi = µi\nfor all i ∈ I. The converse is shown by contradiction. If (ui)i∈I was linearly dependent, there\nwould be a family (µi)i∈I of scalars not all null such that∑\n\ni∈I\nµiui = 0\n\n3.5. BASES OF A VECTOR SPACE 79\n\nand assume that J is infinite. For every 7 € J, let L; C I be the finite set\n\nL; = {i el | vj = Sodus, Ai A OF-\ntel\nLet L= Uses L,;. By definition L C I, and since (u;);er is a basis of E, we must have J = L,\nsince otherwise (u;);ez would be another basis of FE’, and this would contradict the fact that\n(uwi)icr is linearly independent. Furthermore, J must be infinite, since otherwise, because\nthe L; are finite, J would be finite. But then, since J = U,-; £; with J infinite and the L;\nfinite, by a standard result of set theory, || < |J|. If (v;)j;e, is also a basis, by a symmetric\nargument, we obtain |J| < ||, and thus, |/| = |J| for any two bases (u;)ie7 and (v;)je7 of E.\n\nDefinition 3.8. When a vector space F is not finitely generated, we say that F is of infinite\ndimension. The dimension of a finitely generated vector space FE is the common dimension\nn of all of its bases and is denoted by dim(£).\n\nClearly, if the field K itself is viewed as a vector space, then every family (a) where a €\nand a # 0 is a basis. Thus dim(/‘) = 1. Note that dim({0}) = 0.\n\nDefinition 3.9. If E is a vector space of dimension n > 1, for any subspace U of E, if\ndim(U) = 1, then U is called a line; if dim(U) = 2, then U is called a plane; if dim(U) = n—1,\nthen U is called a hyperplane. If dim(U) = k, then U is sometimes called a k-plane.\n\nLet (u;);er be a basis of a vector space EF. For any vector v € E, since the family (w,;)ie7\ngenerates EF’, there is a family (A;);c, of scalars in A’, such that\n\ni€l\nA very important fact is that the family (\\;)icr is unique.\n\nProposition 3.12. Given a vector space E, let (u;);er be a family of vectors in E. Letv € E,\nand assume that v = Yo,-, Ait. Then the family (Aj)ier of scalars such that v = Doi.) iti\nis unique iff (u;)ier ts linearly independent.\n\nwel\n\nProof. First, assume that (u;)jer is linearly independent. If (u;);er is another family of scalars\nin K such that v = }0,-; fii, then we have\n\nYOu = Hi)us = 0,\niel\n\nand since (w,;)ic7 is linearly independent, we must have \\;—/4; = 0 for alli € J, that is, A; = 14;\nfor alli € I. The converse is shown by contradiction. If (u;);¢7; was linearly dependent, there\nwould be a family (ju;);er of scalars not all null such that\n\nwel\n\n\n\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand µj 6= 0 for some j ∈ I. But then,\n\nv =\n∑\ni∈I\n\nλiui + 0 =\n∑\ni∈I\n\nλiui +\n∑\ni∈I\n\nµiui =\n∑\ni∈I\n\n(λi + µi)ui,\n\nwith λj 6= λj+µj since µj 6= 0, contradicting the assumption that (λi)i∈I is the unique family\nsuch that v =\n\n∑\ni∈I λiui.\n\nDefinition 3.10. If (ui)i∈I is a basis of a vector space E, for any vector v ∈ E, if (xi)i∈I is\nthe unique family of scalars in K such that\n\nv =\n∑\ni∈I\n\nxiui,\n\neach xi is called the component (or coordinate) of index i of v with respect to the basis (ui)i∈I .\n\nGiven a field K and any (nonempty) set I, we can form a vector space K(I) which, in\nsome sense, is the standard vector space of dimension |I|.\nDefinition 3.11. Given a field K and any (nonempty) set I, let K(I) be the subset of the\ncartesian product KI consisting of all families (λi)i∈I with finite support of scalars in K.3\n\nWe define addition and multiplication by a scalar as follows:\n\n(λi)i∈I + (µi)i∈I = (λi + µi)i∈I ,\n\nand\nλ · (µi)i∈I = (λµi)i∈I .\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K(I) is a vector space. Furthermore, because families with finite support are consid-\nered, the family (ei)i∈I of vectors ei, defined such that (ei)j = 0 if j 6= i and (ei)i = 1, is\nclearly a basis of the vector space K(I). When I = {1, . . . , n}, we denote K(I) by Kn. The\nfunction ι : I → K(I), such that ι(i) = ei for every i ∈ I, is clearly an injection.\n\n� When I is a finite set, K(I) = KI , but this is false when I is infinite. In fact, dim(K(I)) =\n|I|, but dim(KI) is strictly greater when I is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where KI denotes the set of all functions from I to K.\n\n80 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nand yu; #0 for some j € J. But then,\n\niel iel ie] ie]\nwith \\; A A; +; since wu; A 0, contradicting the assumption that (A;);er is the unique family\nsuch that v = So,e; Aiti- O\n\nDefinition 3.10. If (u;)jc7 is a basis of a vector space FE, for any vector v € EF, if (x;);e7 is\nthe unique family of scalars in K such that\n\nv= y Tis,\n\ntel\n\neach x; is called the component (or coordinate) of index i of v with respect to the basis (u;)ier-\n\nGiven a field K and any (nonempty) set J, we can form a vector space K“) which, in\nsome sense, is the standard vector space of dimension |J].\n\nDefinition 3.11. Given a field K and any (nonempty) set J, let K“™ be the subset of the\ncartesian product K?‘ consisting of all families (\\;);e¢7 with finite support of scalars in K.°\nWe define addition and multiplication by a scalar as follows:\n\n(Ai)ier + (Mi)ier = (Ai + Midier,\n\nand\nA+ (Mi)ier = (AMi)ier-\n\nIt is immediately verified that addition and multiplication by a scalar are well defined.\nThus, K“ is a vector space. Furthermore, because families with finite support are consid-\nered, the family (e;)ie; of vectors e;, defined such that (e;); = 0 if 7 A i and (e;); = 1, is\nclearly a basis of the vector space K“). When J = {1,...,n}, we denote K“) by K”. The\nfunction 1: [+ K“, such that v(i) = e; for every i € I, is clearly an injection.\n\n© When J is a finite set, K“) = K', but this is false when J is infinite. In fact, dim(K”) =\n|Z|, but dim(K“) is strictly greater when J is infinite.\n\n3.6 Matrices\n\nIn Section 2.1 we introduced informally the notion of a matrix. In this section we define\nmatrices precisely, and also introduce some operations on matrices. It turns out that matri-\nces form a vector space equipped with a multiplication operation which is associative, but\nnoncommutative. We will explain in Section 4.1 how matrices can be used to represent linear\nmaps, defined in the next section.\n\n3Where K! denotes the set of all functions from I to K.\n\n\n\n\n3.6. MATRICES 81\n\nDefinition 3.12. If K = R or K = C, an m×n-matrix over K is a family (ai j)1≤i≤m, 1≤j≤n\nof scalars in K, represented by an array\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nIn the special case where m = 1, we have a row vector , represented by\n\n(a1 1 · · · a1n)\n\nand in the special case where n = 1, we have a column vector , represented bya1 1\n...\n\nam 1\n\n .\n\nIn these last two cases, we usually omit the constant index 1 (first index in case of a row,\nsecond index in case of a column). The set of all m × n-matrices is denoted by Mm,n(K)\nor Mm,n. An n × n-matrix is called a square matrix of dimension n. The set of all square\nmatrices of dimension n is denoted by Mn(K), or Mn.\n\nRemark: As defined, a matrix A = (ai j)1≤i≤m, 1≤j≤n is a family , that is, a function from\n{1, 2, . . . ,m} × {1, 2, . . . , n} to K. As such, there is no reason to assume an ordering on\nthe indices. Thus, the matrix A can be represented in many different ways as an array, by\nadopting different orders for the rows or the columns. However, it is customary (and usually\nconvenient) to assume the natural ordering on the sets {1, 2, . . . ,m} and {1, 2, . . . , n}, and\nto represent A as an array according to this ordering of the rows and columns.\n\nWe define some operations on matrices as follows.\n\nDefinition 3.13. Given two m × n matrices A = (ai j) and B = (bi j), we define their sum\nA+B as the matrix C = (ci j) such that ci j = ai j + bi j; that is,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n+\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbm 1 bm 2 . . . bmn\n\n\n\n=\n\n\na1 1 + b1 1 a1 2 + b1 2 . . . a1n + b1n\n\na2 1 + b2 1 a2 2 + b2 2 . . . a2n + b2n\n...\n\n...\n. . .\n\n...\nam 1 + bm 1 am 2 + bm 2 . . . amn + bmn\n\n .\n\n\n\n82 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nFor any matrix A = (ai j), we let −A be the matrix (−ai j). Given a scalar λ ∈ K, we define\nthe matrix λA as the matrix C = (ci j) such that ci j = λai j; that is\n\nλ\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n =\n\n\nλa1 1 λa1 2 . . . λa1n\n\nλa2 1 λa2 2 . . . λa2n\n...\n\n...\n. . .\n\n...\nλam 1 λam 2 . . . λamn\n\n .\n\nGiven an m×n matrices A = (ai k) and an n× p matrices B = (bk j), we define their product\nAB as the m× p matrix C = (ci j) such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p. In the product AB = C shown below\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nb1 1 b1 2 . . . b1 p\n\nb2 1 b2 2 . . . b2 p\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bn p\n\n =\n\n\nc1 1 c1 2 . . . c1 p\n\nc2 1 c2 2 . . . c2 p\n...\n\n...\n. . .\n\n...\ncm 1 cm 2 . . . cmp\n\n ,\n\nnote that the entry of index i and j of the matrix AB obtained by multiplying the matrices\nA and B can be identified with the product of the row matrix corresponding to the i-th row\nof A with the column matrix corresponding to the j-column of B:\n\n(ai 1 · · · ai n)\n\nb1 j\n...\nbn j\n\n =\nn∑\nk=1\n\nai kbk j.\n\nDefinition 3.14. The square matrix In of dimension n containing 1 on the diagonal and 0\neverywhere else is called the identity matrix . It is denoted by\n\nIn =\n\n\n1 0 . . . 0\n0 1 . . . 0\n...\n\n...\n. . .\n\n...\n0 0 . . . 1\n\n\nDefinition 3.15. Given an m × n matrix A = (ai j), its transpose A> = (a>j i), is the\nn×m-matrix such that a>j i = ai j, for all i, 1 ≤ i ≤ m, and all j, 1 ≤ j ≤ n.\n\nThe transpose of a matrix A is sometimes denoted by At, or even by tA. Note that the\ntranspose A> of a matrix A has the property that the j-th row of A> is the j-th column of\n\n\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5× 6 matrix\n\nA =\n\n\n1 2 3 4 5 6\n7 1 2 3 4 5\n8 7 1 2 3 4\n9 8 7 1 2 3\n10 9 8 7 1 2\n\n ,\n\nthen A> is the 6× 5 matrix\n\nA> =\n\n\n1 7 8 9 10\n2 1 7 8 9\n3 2 1 7 8\n4 3 2 1 7\n5 4 3 2 1\n6 5 4 3 2\n\n .\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm× n matrix A and any n× p matrix B, if we denote the columns of A by A1, . . . , An and\nthe rows of B by B1, . . . , Bn, then we have\n\nAB = A1B1 + · · ·+ AnBn.\n\nFor every square matrix A of dimension n, it is immediately verified that AIn = InA = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = In exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A−1. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = I, or a right\ninverse, that is a matrix C such that AC = I, then A is actually invertible; so B = A−1 and\nC = A−1. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set Mm,n(K) of m×n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m × n-matrices Eij = (eh k), are defined such that ei j = 1, and\neh k = 0, if h 6= i or k 6= j; in other words, the (i, j)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the Eij matrices for m = 2 and n = 3:\n\nE11 =\n\n(\n1 0 0\n0 0 0\n\n)\n, E12 =\n\n(\n0 1 0\n0 0 0\n\n)\n, E13 =\n\n(\n0 0 1\n0 0 0\n\n)\nE21 =\n\n(\n0 0 0\n1 0 0\n\n)\n, E22 =\n\n(\n0 0 0\n0 1 0\n\n)\n, E23 =\n\n(\n0 0 0\n0 0 1\n\n)\n.\n\n3.6. MATRICES 83\n\nA. In other words, transposition exchanges the rows and the columns of a matrix. Here is\nan example. If A is the 5 x 6 matrix\n\n1 23 4 5 6\n\n7 12 3 4 5\nA=/]8 712 3 4],\n\n9 8 712 8\n\n1098 7 1 2\n\nthen A! is the 6 x 5 matrix\n\n1 7 8 9 10\n\n2178 9\n\nTt 3.217 8\n\nA l43217\n\n5 43 2 1\n\n65 4 3 2\n\nThe following observation will be useful later on when we discuss the SVD. Given any\nm Xn matrix A and any n Xx p matrix B, if we denote the columns of A by A!,..., A” and\nthe rows of B by B,,..., By, then we have\n\nAB = A'B, +---+A\"B,.\nFor every square matrix A of dimension n, it is immediately verified that Al, = [,A = A.\n\nDefinition 3.16. For any square matrix A of dimension n, if a matrix B such that AB =\nBA = I, exists, then it is unique, and it is called the inverse of A. The matrix B is also\ndenoted by A+. An invertible matrix is also called a nonsingular matrix, and a matrix that\nis not invertible is called a singular matrix.\n\nUsing Proposition 3.18 and the fact that matrices represent linear maps, it can be shown\nthat if a square matrix A has a left inverse, that is a matrix B such that BA = J, or a right\ninverse, that is a matrix C such that AC = J, then A is actually invertible; so B = A~' and\nC= A7!. These facts also follow from Proposition 6.16.\n\nIt is immediately verified that the set My,,(J¢) of m x n matrices is a vector space under\naddition of matrices and multiplication of a matrix by a scalar.\n\nDefinition 3.17. The m x n-matrices E;; = (en,), are defined such that e;; = 1, and\neng = 0, if h A7 or k ¥ J; in other words, the (7, 7)-entry is equal to 1 and all other entries\nare 0.\n\nHere are the £;; matrices for m = 2 and n = 3:\n\n1 0 0 0 1 0 0\nEy — (; ) ’ Ex, — (; 0 ) ’ E\\3 —_ (;\n0 0 0 0 0 0\nEo, — (; ) ’ Dey) — (j 1 ) ’ Eo —_ (\n\noOo oe\nor\nNW\n\noOo GO\n\n\n\n\n84 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nIt is clear that every matrix A = (ai j) ∈ Mm,n(K) can be written in a unique way as\n\nA =\nm∑\ni=1\n\nn∑\nj=1\n\nai jEij.\n\nThus, the family (Eij)1≤i≤m,1≤j≤n is a basis of the vector space Mm,n(K), which has dimension\nmn.\n\nRemark: Definition 3.12 and Definition 3.13 also make perfect sense when K is a (com-\nmutative) ring rather than a field. In this more general setting, the framework of vector\nspaces is too narrow, but we can consider structures over a commutative ring A satisfying\nall the axioms of Definition 3.1. Such structures are called modules . The theory of modules\nis (much) more complicated than that of vector spaces. For example, modules do not always\nhave a basis, and other properties holding for vector spaces usually fail for modules. When\na module has a basis, it is called a free module. For example, when A is a commutative\nring, the structure An is a module such that the vectors ei, with (ei)i = 1 and (ei)j = 0 for\nj 6= i, form a basis of An. Many properties of vector spaces still hold for An. Thus, An is a\nfree module. As another example, when A is a commutative ring, Mm,n(A) is a free module\nwith basis (Ei,j)1≤i≤m,1≤j≤n. Polynomials over a commutative ring also form a free module\nof infinite dimension.\n\nThe properties listed in Proposition 3.13 are easily verified, although some of the com-\nputations are a bit tedious. A more conceptual proof is given in Proposition 4.1.\n\nProposition 3.13. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nThe properties of Proposition 3.13 together with the fact that AIn = InA = A for all\nsquare n×n matrices show that Mn(K) is a ring with unit In (in fact, an associative algebra).\nThis is a noncommutative ring with zero divisors, as shown by the following example.\n\n\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A,B be the 2× 2-matrices\n\nA =\n\n(\n1 0\n0 0\n\n)\n, B =\n\n(\n0 0\n1 0\n\n)\n,\n\nthen\n\nAB =\n\n(\n1 0\n0 0\n\n)(\n0 0\n1 0\n\n)\n=\n\n(\n0 0\n0 0\n\n)\n,\n\nand\n\nBA =\n\n(\n0 0\n1 0\n\n)(\n1 0\n0 0\n\n)\n=\n\n(\n0 0\n1 0\n\n)\n.\n\nThus AB 6= BA, and AB = 0, even though both A,B 6= 0.\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F . A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\n\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces E and F , a linear map between E and F is a\nfunction f : E → F satisfying the following two conditions:\n\nf(x+ y) = f(x) + f(y) for all x, y ∈ E;\n\nf(λx) = λf(x) for all λ ∈ K, x ∈ E.\n\nSetting x = y = 0 in the first identity, we get f(0) = 0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(ui)i∈I of vectors in E, given any family (λi)i∈I of scalars in K, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui).\n\nThe above identity is shown by induction on |I| using the properties of Definition 3.18.\n\nExample 3.6.\n\n3.7. LINEAR MAPS 85\n\nExample 3.5. For example, letting A, B be the 2 x 2-matrices\n1 0 0 0\n4=(0o)- (0):\n1 0\\ /0 O 0 0\nap=(5 (Co) = (0 0):\n0 0\\ /1l 0 0 0\npa=(1 a) (0.0) =( 0):\n\nThus AB 4 BA, and AB = 0, even though both A, B ¥ 0.\n\nthen\n\nand\n\n3.7 Linear Maps\n\nNow that we understand vector spaces and how to generate them, we would like to be able\nto transform one vector space E into another vector space F’. A function between two vector\nspaces that preserves the vector space structure is called a homomorphism of vector spaces,\nor linear map. Linear maps formalize the concept of linearity of a function.\n\nKeep in mind that linear maps, which are transformations of\nspace, are usually far more important than the spaces\nthemselves.\n\nIn the rest of this section, we assume that all vector spaces are over a given field K (say\nR).\n\nDefinition 3.18. Given two vector spaces EF’ and F’, a linear map between FE and F is a\nfunction f: EF — F satisfying the following two conditions:\n\nf(at+y) = f(x) + f(y) for all x,y € E;\nf(Ar) = Af (x) forallAc Kk, xe LE.\n\nSetting x = y = 0 in the first identity, we get f(0) =0. The basic property of linear maps\nis that they transform linear combinations into linear combinations. Given any finite family\n(u;)icr of vectors in EL, given any family (A;)ier of scalars in K, we have\n\nThe above identity is shown by induction on |/| using the properties of Definition 3.18.\n\nExample 3.6.\n\n\n\n\n86 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n1. The map f : R2 → R2 defined such that\n\nx′ = x− y\ny′ = x+ y\n\nis a linear map. The reader should check that it is the composition of a rotation by\nπ/4 with a magnification of ratio\n\n√\n2.\n\n2. For any vector space E, the identity map id : E → E given by\n\nid(u) = u for all u ∈ E\n\nis a linear map. When we want to be more precise, we write idE instead of id.\n\n3. The map D : R[X]→ R[X] defined such that\n\nD(f(X)) = f ′(X),\n\nwhere f ′(X) is the derivative of the polynomial f(X), is a linear map.\n\n4. The map Φ: C([a, b])→ R given by\n\nΦ(f) =\n\n∫ b\n\na\n\nf(t)dt,\n\nwhere C([a, b]) is the set of continuous functions defined on the interval [a, b], is a linear\nmap.\n\n5. The function 〈−,−〉 : C([a, b])× C([a, b])→ R given by\n\n〈f, g〉 =\n\n∫ b\n\na\n\nf(t)g(t)dt,\n\nis linear in each of the variable f , g. It also satisfies the properties 〈f, g〉 = 〈g, f〉 and\n〈f, f〉 = 0 iff f = 0. It is an example of an inner product .\n\nDefinition 3.19. Given a linear map f : E → F , we define its image (or range) Im f = f(E),\nas the set\n\nIm f = {y ∈ F | (∃x ∈ E)(y = f(x))},\n\nand its Kernel (or nullspace) Ker f = f−1(0), as the set\n\nKer f = {x ∈ E | f(x) = 0}.\n\n86\n\nCHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n. The map f: R? > R? defined such that\n\n/\n\nty\n\n/\n\nis a linear map. The reader should check that it is the composition of a rotation by\nx/4 with a magnification of ratio V2.\n\n. For any vector space E, the identity map id: E > E given by\n\nid(u) =u foralue E\n\nis a linear map. When we want to be more precise, we write idg instead of id.\n\n. The map D: R[X] — R[X] defined such that\n\nwhere f’(X) is the derivative of the polynomial f(X), is a linear map.\n\n. The map ®: C({a,b]) > R given by\n\na(f) = / f(t)at,\n\nwhere C(|a, b]) is the set of continuous functions defined on the interval |a, b], is a linear\nmap.\n\n. The function (—, —): C([a,b]) x C([a, b]) > R given by\n\n(f,9) = / f(t)g(t)dt,\n\nis linear in each of the variable f, g. It also satisfies the properties (f,g) = (g, f) and\n(f, f) = 0 iff f = 0. It is an example of an inner product.\n\nDefinition 3.19. Given a linear map f: EF — F’, we define its image (or range) Im f = f(F),\nas the set\n\nIm f= {ty € F'| (Se € E\\(y = f(2))},\n\nand its Kernel (or nullspace) Ker f = f~'(0), as the set\n\nKer f = {x € E | f(x) =O}.\n\n\n\n\n3.7. LINEAR MAPS 87\n\nThe derivative map D : R[X] → R[X] from Example 3.6(3) has kernel the constant\npolynomials, so KerD = R. If we consider the second derivative D ◦D : R[X]→ R[X], then\nthe kernel of D ◦D consists of all polynomials of degree ≤ 1. The image of D : R[X]→ R[X]\nis actually R[X] itself, because every polynomial P (X) = a0X\n\nn + · · ·+ an−1X + an of degree\nn is the derivative of the polynomial Q(X) of degree n+ 1 given by\n\nQ(X) = a0\nXn+1\n\nn+ 1\n+ · · ·+ an−1\n\nX2\n\n2\n+ anX.\n\nOn the other hand, if we consider the restriction of D to the vector space R[X]n of polyno-\nmials of degree ≤ n, then the kernel of D is still R, but the image of D is the R[X]n−1, the\nvector space of polynomials of degree ≤ n− 1.\n\nProposition 3.14. Given a linear map f : E → F , the set Im f is a subspace of F and the\nset Ker f is a subspace of E. The linear map f : E → F is injective iff Ker f = (0) (where\n(0) is the trivial subspace {0}).\n\nProof. Given any x, y ∈ Im f , there are some u, v ∈ E such that x = f(u) and y = f(v),\nand for all λ, µ ∈ K, we have\n\nf(λu+ µv) = λf(u) + µf(v) = λx+ µy,\n\nand thus, λx+ µy ∈ Im f , showing that Im f is a subspace of F .\n\nGiven any x, y ∈ Ker f , we have f(x) = 0 and f(y) = 0, and thus,\n\nf(λx+ µy) = λf(x) + µf(y) = 0,\n\nthat is, λx+ µy ∈ Ker f , showing that Ker f is a subspace of E.\n\nFirst, assume that Ker f = (0). We need to prove that f(x) = f(y) implies that x = y.\nHowever, if f(x) = f(y), then f(x) − f(y) = 0, and by linearity of f we get f(x − y) = 0.\nBecause Ker f = (0), we must have x − y = 0, that is x = y, so f is injective. Conversely,\nassume that f is injective. If x ∈ Ker f , that is f(x) = 0, since f(0) = 0 we have f(x) = f(0),\nand by injectivity, x = 0, which proves that Ker f = (0). Therefore, f is injective iff\nKer f = (0).\n\nSince by Proposition 3.14, the image Im f of a linear map f is a subspace of F , we can\ndefine the rank rk(f) of f as the dimension of Im f .\n\nDefinition 3.20. Given a linear map f : E → F , the rank rk(f) of f is the dimension of\nthe image Im f of f .\n\nA fundamental property of bases in a vector space is that they allow the definition of\nlinear maps as unique homomorphic extensions, as shown in the following proposition.\n\n\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F , given any basis (ui)i∈I of E,\ngiven any other family of vectors (vi)i∈I in F , there is a unique linear map f : E → F such\nthat f(ui) = vi for all i ∈ I. Furthermore, f is injective iff (vi)i∈I is linearly independent,\nand f is surjective iff (vi)i∈I generates F .\n\nProof. If such a linear map f : E → F exists, since (ui)i∈I is a basis of E, every vector x ∈ E\ncan written uniquely as a linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nDefine the function f : E → F , by letting\n\nf(x) =\n∑\ni∈I\n\nxivi\n\nfor every x =\n∑\n\ni∈I xiui. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(ui) = vi.\n\nNow assume that f is injective. Let (λi)i∈I be any family of scalars, and assume that∑\ni∈I\n\nλivi = 0.\n\nSince vi = f(ui) for every i ∈ I, we have\n\nf(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλif(ui) =\n∑\ni∈I\n\nλivi = 0.\n\nSince f is injective iff Ker f = (0), we have∑\ni∈I\n\nλiui = 0,\n\nand since (ui)i∈I is a basis, we have λi = 0 for all i ∈ I, which shows that (vi)i∈I is linearly\nindependent. Conversely, assume that (vi)i∈I is linearly independent. Since (ui)i∈I is a basis\nof E, every vector x ∈ E is a linear combination x =\n\n∑\ni∈I λiui of (ui)i∈I . If\n\nf(x) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nthen ∑\ni∈I\n\nλivi =\n∑\ni∈I\n\nλif(ui) = f(\n∑\ni∈I\n\nλiui) = 0,\n\nand λi = 0 for all i ∈ I because (vi)i∈I is linearly independent, which means that x = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise.\n\n88 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.15. Given any two vector spaces E and F’, given any basis (u;)icr of E,\ngiven any other family of vectors (v;)icr in F, there is a unique linear map f: E > F such\nthat f(u;) = v; for alli € I. Furthermore, f is injective iff (vi)ier is linearly independent,\nand f is surjective iff (vi)icr generates F.\n\nProof. If such a linear map f: E — F exists, since (u;);c7 is a basis of E, every vector x € E\ncan written uniquely as a linear combination\n\nv= S- VyUj;,\nwel\nand by linearity, we must have\nf(z) = S- aif (ui) = S- LjUi-\niel i€l\nDefine the function f: E > F, by letting\nf(x) = Ss” LiVj\nwel\nfor every © = )0,-, iu. It is easy to verify that f is indeed linear, it is unique by the\nprevious reasoning, and obviously, f(u;) = vu.\nNow assume that f is injective. Let (\\;)jcr be any family of scalars, and assume that\ni€l\nSince uv; = f(u;) for every i € I, we have\nfO, AjUi) = S- Af (ui) = S° Ai; = 0.\nie] i€l ier\nSince f is injective iff Ker f = (0), we have\nS- Aju = 0,\ni€l\nand since (wu;)ier is a basis, we have A; = 0 for all i € J, which shows that (v;)jer is linearly\n\nindependent. Conversely, assume that (v;);c7 is linearly independent. Since (u;);<r is a basis\nof E, every vector x € F is a linear combination « = )0,., Aiui of (u)ier. If\n\nf(z) = 0D Aju) = 9,\n\nthen\n\nSs\" Avi = S- Ai f (ui) = f>~ Aju) = 0,\n\niel iel ier\nand A; = 0 for all i € I because (v;)jer is linearly independent, which means that x7 = 0.\nTherefore, Ker f = (0), which implies that f is injective. The part where f is surjective is\nleft as a simple exercise. im\n\n\n\n\n3.7. LINEAR MAPS 89\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nu = (0,0,1)\n3 v = (1,1)1v = (-1,1)\n\n2\n\nv = (1,0)\n3\n\nf(u )1\nf(u )\n\n2\n-\n\n2f(u  )3\n\nE = \n\nf\n\nF =\nR\n\nR\n2\n\n3\n\nf is not injective\n\ndefining f\n\nFigure 3.11: Given u1 = (1, 0, 0), u2 = (0, 1, 0), u3 = (0, 0, 1) and v1 = (1, 1), v2 = (−1, 1),\nv3 = (1, 0), define the unique linear map f : R3 → R2 by f(u1) = v1, f(u2) = v2, and\nf(u3) = v3. This map is surjective but not injective since f(u1 − u2) = f(u1) − f(u2) =\n(1, 1)− (−1, 1) = (2, 0) = 2f(u3) = f(2u3).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R3 and V = R2\n\nBy the second part of Proposition 3.15, an injective linear map f : E → F sends a basis\n(ui)i∈I to a linearly independent family (f(ui))i∈I of F , which is also a basis when f is\nbijective. Also, when E and F have the same finite dimension n, (ui)i∈I is a basis of E, and\nf : E → F is injective, then (f(ui))i∈I is a basis of F (by Proposition 3.8).\n\nWe can now show that the vector space K(I) of Definition 3.11 has a universal property\nthat amounts to saying that K(I) is the vector space freely generated by I. Recall that\nι : I → K(I), such that ι(i) = ei for every i ∈ I, is an injection from I to K(I).\n\nProposition 3.16. Given any set I, for any vector space F , and for any function f : I → F ,\nthere is a unique linear map f : K(I) → F , such that\n\nf = f ◦ ι,\n\nas in the following diagram:\n\nI ι //\n\nf !!CCCCCCCCC K(I)\n\nf\n��\nF\n\nProof. If such a linear map f : K(I) → F exists, since f = f ◦ ι, we must have\n\nf(i) = f(ι(i)) = f(ei),\n\n3.7. LINEAR MAPS 89\n\ndefining f\n\n2f(u3)\n\nfis not injective\n\nFigure 3.11: Given wu; = (1,0,0), w2 = (0,1,0), us = (0,0,1) and vy; = (1,1), ve = (-1,1),\nv3 = (1,0), define the unique linear map f: R? — R? by f(u1) = v1, flue) = vo, and\nf(u3) = v3. This map is surjective but not injective since f(u; — u2) = f(u1) — f(ue) =\n\n(1, 1) _ (-1, 1) = (2, 0) = 2 f (us) = f(2us).\n\nFigure 3.11 provides an illustration of Proposition 3.15 when E = R? and V = R?\n\nBy the second part of Proposition 3.15, an injective linear map f: E — F sends a basis\n(u;)icr to a linearly independent family (f(wu;))icr of F', which is also a basis when f is\nbijective. Also, when F and F have the same finite dimension n, (u;);ey is a basis of E', and\nf: E > F is injective, then (f(ui))ier is a basis of F' (by Proposition 3.8).\n\nWe can now show that the vector space K™) of Definition 3.11 has a universal property\nthat amounts to saying that AK“) is the vector space freely generated by J. Recall that\nu: I + K“, such that «(i) = e; for every i € I, is an injection from I to KY),\n\nProposition 3.16. Given any set I, for any vector space F’, and for any function f: I > F,\nthere is a unique linear map f: K\\“) — F, such that\n\nf=fou,\n\nas in the following diagram:\n\nProof. If such a linear map f: K“ > F exists, since f = f ov, we must have\n\nF(t) = Fu) = Fei),\n\n\n\n\n90 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nfor every i ∈ I. However, the family (ei)i∈I is a basis of K(I), and (f(i))i∈I is a family of\nvectors in F , and by Proposition 3.15, there is a unique linear map f : K(I) → F such that\nf(ei) = f(i) for every i ∈ I, which proves the existence and uniqueness of a linear map f\nsuch that f = f ◦ ι.\n\nThe following simple proposition is also useful.\n\nProposition 3.17. Given any two vector spaces E and F , with F nontrivial, given any\nfamily (ui)i∈I of vectors in E, the following properties hold:\n\n(1) The family (ui)i∈I generates E iff for every family of vectors (vi)i∈I in F , there is at\nmost one linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\n(2) The family (ui)i∈I is linearly independent iff for every family of vectors (vi)i∈I in F ,\nthere is some linear map f : E → F such that f(ui) = vi for all i ∈ I.\n\nProof. (1) If there is any linear map f : E → F such that f(ui) = vi for all i ∈ I, since\n(ui)i∈I generates E, every vector x ∈ E can be written as some linear combination\n\nx =\n∑\ni∈I\n\nxiui,\n\nand by linearity, we must have\n\nf(x) =\n∑\ni∈I\n\nxif(ui) =\n∑\ni∈I\n\nxivi.\n\nThis shows that f is unique if it exists. Conversely, assume that (ui)i∈I does not generate E.\nSince F is nontrivial, there is some some vector y ∈ F such that y 6= 0. Since (ui)i∈I does\nnot generate E, there is some vector w ∈ E that is not in the subspace generated by (ui)i∈I .\nBy Theorem 3.11, there is a linearly independent subfamily (ui)i∈I0 of (ui)i∈I generating the\nsame subspace. Since by hypothesis, w ∈ E is not in the subspace generated by (ui)i∈I0 , by\nLemma 3.6 and by Theorem 3.11 again, there is a basis (ej)j∈I0∪J of E, such that ei = ui\nfor all i ∈ I0, and w = ej0 for some j0 ∈ J . Letting (vi)i∈I be the family in F such that\nvi = 0 for all i ∈ I, defining f : E → F to be the constant linear map with value 0, we have\na linear map such that f(ui) = 0 for all i ∈ I. By Proposition 3.15, there is a unique linear\nmap g : E → F such that g(w) = y, and g(ej) = 0 for all j ∈ (I0 ∪ J)− {j0}. By definition\nof the basis (ej)j∈I0∪J of E, we have g(ui) = 0 for all i ∈ I, and since f 6= g, this contradicts\nthe fact that there is at most one such map. See Figure 3.12.\n\n(2) If the family (ui)i∈I is linearly independent, then by Theorem 3.11, (ui)i∈I can be\nextended to a basis of E, and the conclusion follows by Proposition 3.15. Conversely, assume\nthat (ui)i∈I is linearly dependent. Then there is some family (λi)i∈I of scalars (not all zero)\nsuch that ∑\n\ni∈I\nλiui = 0.\n\n\n\n3.7. LINEAR MAPS 91\nf\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nu  = (1,0,0)1\n\nu = (0,1,0)\n2\n\nE = F =\nR\n\nR\n2\n\n3\n\nw = (0,0,1)\n\nw = (0,0,1)\n\ndefining f as the zero\n\ndefining g\ny = (1,0)\n\ng(w) = y\n\nFigure 3.12: Let E = R3 and F = R2. The vectors u1 = (1, 0, 0), u2 = (0, 1, 0) do not\ngenerate R3 since both the zero map and the map g, where g(0, 0, 1) = (1, 0), send the peach\nxy-plane to the origin.\n\nBy the assumption, for any nonzero vector y ∈ F , for every i ∈ I, there is some linear map\nfi : E → F , such that fi(ui) = y, and fi(uj) = 0, for j ∈ I − {i}. Then we would get\n\n0 = fi(\n∑\ni∈I\n\nλiui) =\n∑\ni∈I\n\nλifi(ui) = λiy,\n\nand since y 6= 0, this implies λi = 0 for every i ∈ I. Thus, (ui)i∈I is linearly independent.\n\nGiven vector spaces E, F , and G, and linear maps f : E → F and g : F → G, it is easily\nverified that the composition g ◦ f : E → G of f and g is a linear map.\n\nDefinition 3.21. A linear map f : E → F is an isomorphism iff there is a linear map\ng : F → E, such that\n\ng ◦ f = idE and f ◦ g = idF . (∗)\n\nThe map g in Definition 3.21 is unique. This is because if g and h both satisfy g◦f = idE,\nf ◦ g = idF , h ◦ f = idE, and f ◦ h = idF , then\n\ng = g ◦ idF = g ◦ (f ◦ h) = (g ◦ f) ◦ h = idE ◦ h = h.\n\nThe map g satisfying (∗) above is called the inverse of f and it is also denoted by f−1.\n\n\n\n92 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nObserve that Proposition 3.15 shows that if F = Rn, then we get an isomorphism between\nany vector space E of dimension |J | = n and Rn. Proposition 3.15 also implies that if E\nand F are two vector spaces, (ui)i∈I is a basis of E, and f : E → F is a linear map which is\nan isomorphism, then the family (f(ui))i∈I is a basis of F .\n\nOne can verify that if f : E → F is a bijective linear map, then its inverse f−1 : F → E,\nas a function, is also a linear map, and thus f is an isomorphism.\n\nAnother useful corollary of Proposition 3.15 is this:\n\nProposition 3.18. Let E be a vector space of finite dimension n ≥ 1 and let f : E → E be\nany linear map. The following properties hold:\n\n(1) If f has a left inverse g, that is, if g is a linear map such that g ◦ f = id, then f is an\nisomorphism and f−1 = g.\n\n(2) If f has a right inverse h, that is, if h is a linear map such that f ◦ h = id, then f is\nan isomorphism and f−1 = h.\n\nProof. (1) The equation g ◦ f = id implies that f is injective; this is a standard result\nabout functions (if f(x) = f(y), then g(f(x)) = g(f(y)), which implies that x = y since\ng ◦ f = id). Let (u1, . . . , un) be any basis of E. By Proposition 3.15, since f is injective,\n(f(u1), . . . , f(un)) is linearly independent, and since E has dimension n, it is a basis of\nE (if (f(u1), . . . , f(un)) doesn’t span E, then it can be extended to a basis of dimension\nstrictly greater than n, contradicting Theorem 3.11). Then f is bijective, and by a previous\nobservation its inverse is a linear map. We also have\n\ng = g ◦ id = g ◦ (f ◦ f−1) = (g ◦ f) ◦ f−1 = id ◦ f−1 = f−1.\n\n(2) The equation f ◦ h = id implies that f is surjective; this is a standard result about\nfunctions (for any y ∈ E, we have f(h(y)) = y). Let (u1, . . . , un) be any basis of E. By\nProposition 3.15, since f is surjective, (f(u1), . . . , f(un)) spans E, and since E has dimension\nn, it is a basis of E (if (f(u1), . . . , f(un)) is not linearly independent, then because it spans\nE, it contains a basis of dimension strictly smaller than n, contradicting Theorem 3.11).\nThen f is bijective, and by a previous observation its inverse is a linear map. We also have\n\nh = id ◦ h = (f−1 ◦ f) ◦ h = f−1 ◦ (f ◦ h) = f−1 ◦ id = f−1.\n\nThis completes the proof.\n\nDefinition 3.22. The set of all linear maps between two vector spaces E and F is denoted by\nHom(E,F ) or by L(E;F ) (the notation L(E;F ) is usually reserved to the set of continuous\nlinear maps, where E and F are normed vector spaces). When we wish to be more precise and\nspecify the field K over which the vector spaces E and F are defined we write HomK(E,F ).\n\n\n\n3.8. QUOTIENT SPACES 93\n\nThe set Hom(E,F ) is a vector space under the operations defined in Example 3.1, namely\n\n(f + g)(x) = f(x) + g(x)\n\nfor all x ∈ E, and\n(λf)(x) = λf(x)\n\nfor all x ∈ E. The point worth checking carefully is that λf is indeed a linear map, which\nuses the commutativity of ∗ in the field K (typically, K = R or K = C). Indeed, we have\n\n(λf)(µx) = λf(µx) = λµf(x) = µλf(x) = µ(λf)(x).\n\nWhen E and F have finite dimensions, the vector space Hom(E,F ) also has finite di-\nmension, as we shall see shortly.\n\nDefinition 3.23. When E = F , a linear map f : E → E is also called an endomorphism.\nThe space Hom(E,E) is also denoted by End(E).\n\nIt is also important to note that composition confers to Hom(E,E) a ring structure.\nIndeed, composition is an operation ◦ : Hom(E,E) × Hom(E,E) → Hom(E,E), which is\nassociative and has an identity idE, and the distributivity properties hold:\n\n(g1 + g2) ◦ f = g1 ◦ f + g2 ◦ f ;\n\ng ◦ (f1 + f2) = g ◦ f1 + g ◦ f2.\n\nThe ring Hom(E,E) is an example of a noncommutative ring.\n\nIt is easily seen that the set of bijective linear maps f : E → E is a group under compo-\nsition.\n\nDefinition 3.24. Bijective linear maps f : E → E are also called automorphisms . The\ngroup of automorphisms of E is called the general linear group (of E), and it is denoted by\nGL(E), or by Aut(E), or when E = Rn, by GL(n,R), or even by GL(n).\n\nAlthough in this book, we will not have many occasions to use quotient spaces, they are\nfundamental in algebra. The next section may be omitted until needed.\n\n3.8 Quotient Spaces\n\nLet E be a vector space, and let M be any subspace of E. The subspace M induces a relation\n≡M on E, defined as follows: For all u, v ∈ E,\n\nu ≡M v iff u− v ∈M .\n\nWe have the following simple proposition.\n\n\n\n94 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProposition 3.19. Given any vector space E and any subspace M of E, the relation ≡M\nis an equivalence relation with the following two congruential properties:\n\n1. If u1 ≡M v1 and u2 ≡M v2, then u1 + u2 ≡M v1 + v2, and\n\n2. if u ≡M v, then λu ≡M λv.\n\nProof. It is obvious that ≡M is an equivalence relation. Note that u1 ≡M v1 and u2 ≡M v2\n\nare equivalent to u1 − v1 = w1 and u2 − v2 = w2, with w1, w2 ∈M , and thus,\n\n(u1 + u2)− (v1 + v2) = w1 + w2,\n\nand w1 + w2 ∈ M , since M is a subspace of E. Thus, we have u1 + u2 ≡M v1 + v2. If\nu− v = w, with w ∈M , then\n\nλu− λv = λw,\n\nand λw ∈M , since M is a subspace of E, and thus λu ≡M λv.\n\nProposition 3.19 shows that we can define addition and multiplication by a scalar on the\nset E/M of equivalence classes of the equivalence relation ≡M .\n\nDefinition 3.25. Given any vector space E and any subspaceM of E, we define the following\noperations of addition and multiplication by a scalar on the set E/M of equivalence classes\nof the equivalence relation ≡M as follows: for any two equivalence classes [u], [v] ∈ E/M , we\nhave\n\n[u] + [v] = [u+ v],\n\nλ[u] = [λu].\n\nBy Proposition 3.19, the above operations do not depend on the specific choice of represen-\ntatives in the equivalence classes [u], [v] ∈ E/M . It is also immediate to verify that E/M is\na vector space. The function π : E → E/F , defined such that π(u) = [u] for every u ∈ E, is\na surjective linear map called the natural projection of E onto E/F . The vector space E/M\nis called the quotient space of E by the subspace M .\n\nGiven any linear map f : E → F , we know that Ker f is a subspace of E, and it is\nimmediately verified that Im f is isomorphic to the quotient space E/Ker f .\n\n3.9 Linear Forms and the Dual Space\n\nWe already observed that the field K itself (K = R or K = C) is a vector space (over itself).\nThe vector space Hom(E,K) of linear maps from E to the field K, the linear forms, plays\na particular role. In this section, we only define linear forms and show that every finite-\ndimensional vector space has a dual basis. A more advanced presentation of dual spaces and\nduality is given in Chapter 11.\n\n\n\n3.9. LINEAR FORMS AND THE DUAL SPACE 95\n\nDefinition 3.26. Given a vector space E, the vector space Hom(E,K) of linear maps from\nE to the field K is called the dual space (or dual) of E. The space Hom(E,K) is also denoted\nby E∗, and the linear maps in E∗ are called the linear forms , or covectors . The dual space\nE∗∗ of the space E∗ is called the bidual of E.\n\nAs a matter of notation, linear forms f : E → K will also be denoted by starred symbol,\nsuch as u∗, x∗, etc.\n\nIf E is a vector space of finite dimension n and (u1, . . . , un) is a basis of E, for any linear\nform f ∗ ∈ E∗, for every x = x1u1 + · · ·+ xnun ∈ E, by linearity we have\n\nf ∗(x) = f ∗(u1)x1 + · · ·+ f ∗(un)xn\n\n= λ1x1 + · · ·+ λnxn,\n\nwith λi = f ∗(ui) ∈ K for every i, 1 ≤ i ≤ n. Thus, with respect to the basis (u1, . . . , un),\nthe linear form f ∗ is represented by the row vector\n\n(λ1 · · · λn),\n\nwe have\n\nf ∗(x) =\n(\nλ1 · · · λn\n\n)x1\n...\nxn\n\n ,\n\na linear combination of the coordinates of x, and we can view the linear form f ∗ as a linear\nequation. If we decide to use a column vector of coefficients\n\nc =\n\nc1\n...\ncn\n\n\ninstead of a row vector, then the linear form f ∗ is defined by\n\nf ∗(x) = c>x.\n\nThe above notation is often used in machine learning.\n\nExample 3.7. Given any differentiable function f : Rn → R, by definition, for any x ∈ Rn,\nthe total derivative dfx of f at x is the linear form dfx : Rn → R defined so that for all\nu = (u1, . . . , un) ∈ Rn,\n\ndfx(u) =\n\n(\n∂f\n\n∂x1\n\n(x) · · · ∂f\n\n∂xn\n(x)\n\n)u1\n...\nun\n\n =\nn∑\ni=1\n\n∂f\n\n∂xi\n(x)ui.\n\n\n\n96 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nExample 3.8. Let C([0, 1]) be the vector space of continuous functions f : [0, 1]→ R. The\nmap I : C([0, 1])→ R given by\n\nI(f) =\n\n∫ 1\n\n0\n\nf(x)dx for any f ∈ C([0, 1])\n\nis a linear form (integration).\n\nExample 3.9. Consider the vector space Mn(R) of real n×n matrices. Let tr : Mn(R)→ R\nbe the function given by\n\ntr(A) = a11 + a22 + · · ·+ ann,\n\ncalled the trace of A. It is a linear form. Let s : Mn(R)→ R be the function given by\n\ns(A) =\nn∑\n\ni,j=1\n\naij,\n\nwhere A = (aij). It is immediately verified that s is a linear form.\n\nGiven a vector space E and any basis (ui)i∈I for E, we can associate to each ui a linear\nform u∗i ∈ E∗, and the u∗i have some remarkable properties.\n\nDefinition 3.27. Given a vector space E and any basis (ui)i∈I for E, by Proposition 3.15,\nfor every i ∈ I, there is a unique linear form u∗i such that\n\nu∗i (uj) =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor every j ∈ I. The linear form u∗i is called the coordinate form of index i w.r.t. the basis\n(ui)i∈I .\n\nRemark: Given an index set I, authors often define the so called “Kronecker symbol” δi j\nsuch that\n\nδi j =\n\n{\n1 if i = j\n0 if i 6= j,\n\nfor all i, j ∈ I. Then, u∗i (uj) = δi j.\n\nThe reason for the terminology coordinate form is as follows: If E has finite dimension\nand if (u1, . . . , un) is a basis of E, for any vector\n\nv = λ1u1 + · · ·+ λnun,\n\nwe have\n\nu∗i (v) = u∗i (λ1u1 + · · ·+ λnun)\n\n= λ1u\n∗\ni (u1) + · · ·+ λiu\n\n∗\ni (ui) + · · ·+ λnu\n\n∗\ni (un)\n\n= λi,\n\n\n\n3.10. SUMMARY 97\n\nsince u∗i (uj) = δi j. Therefore, u∗i is the linear function that returns the ith coordinate of a\nvector expressed over the basis (u1, . . . , un).\n\nThe following theorem shows that in finite-dimension, every basis (u1, . . . , un) of a vector\nspace E yields a basis (u∗1, . . . , u\n\n∗\nn) of the dual space E∗, called a dual basis .\n\nTheorem 3.20. (Existence of dual bases) Let E be a vector space of dimension n. The\nfollowing properties hold: For every basis (u1, . . . , un) of E, the family of coordinate forms\n(u∗1, . . . , u\n\n∗\nn) is a basis of E∗ (called the dual basis of (u1, . . . , un)).\n\nProof. (a) If v∗ ∈ E∗ is any linear form, consider the linear form\n\nf ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nObserve that because u∗i (uj) = δi j,\n\nf ∗(ui) = (v∗(u1)u∗1 + · · ·+ v∗(un)u∗n)(ui)\n\n= v∗(u1)u∗1(ui) + · · ·+ v∗(ui)u\n∗\ni (ui) + · · ·+ v∗(un)u∗n(ui)\n\n= v∗(ui),\n\nand so f ∗ and v∗ agree on the basis (u1, . . . , un), which implies that\n\nv∗ = f ∗ = v∗(u1)u∗1 + · · ·+ v∗(un)u∗n.\n\nTherefore, (u∗1, . . . , u\n∗\nn) spans E∗. We claim that the covectors u∗1, . . . , u\n\n∗\nn are linearly inde-\n\npendent. If not, we have a nontrivial linear dependence\n\nλ1u\n∗\n1 + · · ·+ λnu\n\n∗\nn = 0,\n\nand if we apply the above linear form to each ui, using a familar computation, we get\n\n0 = λiu\n∗\ni (ui) = λi,\n\nproving that u∗1, . . . , u\n∗\nn are indeed linearly independent. Therefore, (u∗1, . . . , u\n\n∗\nn) is a basis of\n\nE∗.\n\nIn particular, Theorem 3.20 shows a finite-dimensional vector space and its dual E∗ have\nthe same dimension.\n\n3.10 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The notion of a vector space.\n\n• Families of vectors.\n\n\n\n98 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n• Linear combinations of vectors; linear dependence and linear independence of a family\nof vectors.\n\n• Linear subspaces .\n\n• Spanning (or generating) family; generators , finitely generated subspace; basis of a\nsubspace.\n\n• Every linearly independent family can be extended to a basis (Theorem 3.7).\n\n• A family B of vectors is a basis iff it is a maximal linearly independent family iff it is\na minimal generating family (Proposition 3.8).\n\n• The replacement lemma (Proposition 3.10).\n\n• Any two bases in a finitely generated vector space E have the same number of elements ;\nthis is the dimension of E (Theorem 3.11).\n\n• Hyperplanes .\n\n• Every vector has a unique representation over a basis (in terms of its coordinates).\n\n• Matrices\n\n• Column vectors , row vectors .\n\n• Matrix operations : addition, scalar multiplication, multiplication.\n\n• The vector space Mm,n(K) of m × n matrices over the field K; The ring Mn(K) of\nn× n matrices over the field K.\n\n• The notion of a linear map.\n\n• The image Im f (or range) of a linear map f .\n\n• The kernel Ker f (or nullspace) of a linear map f .\n\n• The rank rk(f) of a linear map f .\n\n• The image and the kernel of a linear map are subspaces. A linear map is injective iff\nits kernel is the trivial space (0) (Proposition 3.14).\n\n• The unique homomorphic extension property of linear maps with respect to bases\n(Proposition 3.15 ).\n\n• Quotient spaces .\n\n• The vector space of linear maps HomK(E,F ).\n\n\n\n3.11. PROBLEMS 99\n\n• Linear forms (covectors) and the dual space E∗.\n\n• Coordinate forms.\n\n• The existence of dual bases (in finite dimension).\n\n3.11 Problems\n\nProblem 3.1. Let H be the set of 3× 3 upper triangular matrices given by\n\nH =\n\n\n1 a b\n\n0 1 c\n0 0 1\n\n | a, b, c ∈ R\n\n .\n\n(1) Prove that H with the binary operation of matrix multiplication is a group; find\nexplicitly the inverse of every matrix in H. Is H abelian (commutative)?\n\n(2) Given two groups G1 and G2, recall that a homomorphism if a function ϕ : G1 → G2\n\nsuch that\nϕ(ab) = ϕ(a)ϕ(b), a, b ∈ G1.\n\nProve that ϕ(e1) = e2 (where ei is the identity element of Gi) and that\n\nϕ(a−1) = (ϕ(a))−1, a ∈ G1.\n\n(3) Let S1 be the unit circle, that is\n\nS1 = {eiθ = cos θ + i sin θ | 0 ≤ θ < 2π},\n\nand let ϕ be the function given by\n\nϕ\n\n1 a b\n0 1 c\n0 0 1\n\n = (a, c, eib).\n\nProve that ϕ is a surjective function onto G = R × R × S1, and that if we define\nmultiplication on this set by\n\n(x1, y1, u1) · (x2, y2, u2) = (x1 + x2, y1 + y2, e\nix1y2u1u2),\n\nthen G is a group and ϕ is a group homomorphism from H onto G.\n\n(4) The kernel of a homomorphism ϕ : G1 → G2 is defined as\n\nKer (ϕ) = {a ∈ G1 | ϕ(a) = e2}.\n\nFind explicitly the kernel of ϕ and show that it is a subgroup of H.\n\n\n\n100 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.2. For any m ∈ Z with m > 0, the subset mZ = {mk | k ∈ Z} is an abelian\nsubgroup of Z. Check this.\n\n(1) Give a group isomorphism (an invertible homomorphism) from mZ to Z.\n\n(2) Check that the inclusion map i : mZ→ Z given by i(mk) = mk is a group homomor-\nphism. Prove that if m ≥ 2 then there is no group homomorphism p : Z → mZ such that\np ◦ i = id.\n\nRemark: The above shows that abelian groups fail to have some of the properties of vector\nspaces. We will show later that a linear map satisfying the condition p◦ i = id always exists.\n\nProblem 3.3. Let E = R× R, and define the addition operation\n\n(x1, y1) + (x2, y2) = (x1 + x2, y1,+y2), x1, x2, y1, y2 ∈ R,\n\nand the multiplication operation · : R× E → E by\n\nλ · (x, y) = (λx, y), λ, x, y ∈ R.\n\nShow that E with the above operations + and · is not a vector space. Which of the\naxioms is violated?\n\nProblem 3.4. (1) Prove that the axioms of vector spaces imply that\n\nα · 0 = 0\n\n0 · v = 0\n\nα · (−v) = −(α · v)\n\n(−α) · v = −(α · v),\n\nfor all v ∈ E and all α ∈ K, where E is a vector space over K.\n\n(2) For every λ ∈ R and every x = (x1, . . . , xn) ∈ Rn, define λx by\n\nλx = λ(x1, . . . , xn) = (λx1, . . . , λxn).\n\nRecall that every vector x = (x1, . . . , xn) ∈ Rn can be written uniquely as\n\nx = x1e1 + · · ·+ xnen,\n\nwhere ei = (0, . . . , 0, 1, 0, . . . , 0), with a single 1 in position i. For any operation · : R×Rn →\nRn, if · satisfies the Axiom (V1) of a vector space, then prove that for any α ∈ R, we have\n\nα · x = α · (x1e1 + · · ·+ xnen) = α · (x1e1) + · · ·+ α · (xnen).\n\nConclude that · is completely determined by its action on the one-dimensional subspaces of\nRn spanned by e1, . . . , en.\n\n\n\n3.11. PROBLEMS 101\n\n(3) Use (2) to define operations · : R × Rn → Rn that satisfy the Axioms (V1–V3), but\nfor which Axiom V4 fails.\n\n(4) For any operation · : R×Rn → Rn, prove that if · satisfies the Axioms (V2–V3), then\nfor every rational number r ∈ Q and every vector x ∈ Rn, we have\n\nr · x = r(1 · x).\n\nIn the above equation, 1 · x is some vector (y1, . . . , yn) ∈ Rn not necessarily equal to x =\n(x1, . . . , xn), and\n\nr(1 · x) = (ry1, . . . , ryn),\n\nas in Part (2).\n\nUse (4) to conclude that any operation · : Q×Rn → Rn that satisfies the Axioms (V1–V3)\nis completely determined by the action of 1 on the one-dimensional subspaces of Rn spanned\nby e1, . . . , en.\n\nProblem 3.5. Let A1 be the following matrix:\n\nA1 =\n\n 2 3 1\n1 2 −1\n−3 −5 1\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 2,−7) over the basis consisting of the column vectors of A1.\n\nProblem 3.6. Let A2 be the following matrix:\n\nA2 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 3 0\n\n .\n\nExpress the fourth column of A2 as a linear combination of the first three columns of A2. Is\nthe vector x = (7, 14,−1, 2) a linear combination of the columns of A2?\n\nProblem 3.7. Let A3 be the following matrix:\n\nA3 =\n\n1 1 1\n1 1 2\n1 2 3\n\n .\n\nProve that the columns of A1 are linearly independent. Find the coordinates of the vector\nx = (6, 9, 14) over the basis consisting of the column vectors of A3.\n\n\n\n102 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.8. Let A4 be the following matrix:\n\nA4 =\n\n\n1 2 1 1\n2 3 2 3\n−1 0 1 −1\n−2 −1 4 0\n\n .\n\nProve that the columns of A4 are linearly independent. Find the coordinates of the vector\nx = (7, 14,−1, 2) over the basis consisting of the column vectors of A4.\n\nProblem 3.9. Consider the following Haar matrix\n\nH =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\nProve that the columns of H are linearly independent.\n\nHint . Compute the product H>H.\n\nProblem 3.10. Consider the following Hadamard matrix\n\nH4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n .\n\nProve that the columns of H4 are linearly independent.\n\nHint . Compute the product H>4 H4.\n\nProblem 3.11. In solving this problem, do not use determinants.\n\n(1) Let (u1, . . . , um) and (v1, . . . , vm) be two families of vectors in some vector space E.\nAssume that each vi is a linear combination of the ujs, so that\n\nvi = ai 1u1 + · · ·+ aimum, 1 ≤ i ≤ m,\n\nand that the matrix A = (ai j) is an upper-triangular matrix, which means that if 1 ≤ j <\ni ≤ m, then ai j = 0. Prove that if (u1, . . . , um) are linearly independent and if all the\ndiagonal entries of A are nonzero, then (v1, . . . , vm) are also linearly independent.\n\nHint . Use induction on m.\n\n(2) Let A = (ai j) be an upper-triangular matrix. Prove that if all the diagonal entries of\nA are nonzero, then A is invertible and the inverse A−1 of A is also upper-triangular.\n\nHint . Use induction on m.\n\nProve that if A is invertible, then all the diagonal entries of A are nonzero.\n\n(3) Prove that if the families (u1, . . . , um) and (v1, . . . , vm) are related as in (1), then\n(u1, . . . , um) are linearly independent iff (v1, . . . , vm) are linearly independent.\n\n\n\n3.11. PROBLEMS 103\n\nProblem 3.12. In solving this problem, do not use determinants. Consider the n × n\nmatrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n0 1 2 0 . . . 0 0\n0 0 1 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 0 1 2 0\n0 0 . . . 0 0 1 2\n0 0 . . . 0 0 0 1\n\n\n.\n\n(1) Find the solution x = (x1, . . . , xn) of the linear system\n\nAx = b,\n\nfor\n\nb =\n\n\nb1\n\nb2\n...\nbn\n\n .\n\n(2) Prove that the matrix A is invertible and find its inverse A−1. Given that the number\nof atoms in the universe is estimated to be ≤ 1082, compare the size of the coefficients the\ninverse of A to 1082, if n ≥ 300.\n\n(3) Assume b is perturbed by a small amount δb (note that δb is a vector). Find the new\nsolution of the system\n\nA(x+ δx) = b+ δb,\n\nwhere δx is also a vector. In the case where b = (0, . . . , 0, 1), and δb = (0, . . . , 0, ε), show\nthat\n\n|(δx)1| = 2n−1|ε|.\n(where (δx)1 is the first component of δx).\n\n(4) Prove that (A− I)n = 0.\n\nProblem 3.13. An n × n matrix N is nilpotent if there is some integer r ≥ 1 such that\nN r = 0.\n\n(1) Prove that if N is a nilpotent matrix, then the matrix I −N is invertible and\n\n(I −N)−1 = I +N +N2 + · · ·+N r−1.\n\n(2) Compute the inverse of the following matrix A using (1):\n\nA =\n\n\n1 2 3 4 5\n0 1 2 3 4\n0 0 1 2 3\n0 0 0 1 2\n0 0 0 0 1\n\n .\n\n\n\n104 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\nProblem 3.14. (1) Let A be an n×n matrix. If A is invertible, prove that for any x ∈ Rn,\nif Ax = 0, then x = 0.\n\n(2) Let A be an m × n matrix and let B be an n ×m matrix. Prove that Im − AB is\ninvertible iff In −BA is invertible.\n\nHint . If for all x ∈ Rn, Mx = 0 implies that x = 0, then M is invertible.\n\nProblem 3.15. Consider the following n× n matrix, for n ≥ 3:\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n(1) If we denote the columns of B by b1, . . . , bn, prove that\n\n(n− 3)b1 − (b2 + · · ·+ bn) = 2(n− 2)e1\n\nb1 − b2 = 2(e1 + e2)\n\nb1 − b3 = 2(e1 + e3)\n\n...\n...\n\nb1 − bn = 2(e1 + en),\n\nwhere e1, . . . , en are the canonical basis vectors of Rn.\n\n(2) Prove that B is invertible and that its inverse A = (aij) is given by\n\na11 =\n(n− 3)\n\n2(n− 2)\n, ai1 = − 1\n\n2(n− 2)\n2 ≤ i ≤ n\n\nand\n\naii = − (n− 3)\n\n2(n− 2)\n, 2 ≤ i ≤ n\n\naji =\n1\n\n2(n− 2)\n, 2 ≤ i ≤ n, j 6= i.\n\n(3) Show that the n diagonal n× n matrices Di defined such that the diagonal entries of\nDi are equal the entries (from top down) of the ith column of B form a basis of the space of\n\n\n\n3.11. PROBLEMS 105\n\nn × n diagonal matrices (matrices with zeros everywhere except possibly on the diagonal).\nFor example, when n = 4, we have\n\nD1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n D2 =\n\n\n−1 0 0 0\n0 −1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nD3 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 −1 0\n0 0 0 1\n\n , D4 =\n\n\n−1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 −1\n\n .\n\nProblem 3.16. Given any m×n matrix A and any n×p matrix B, if we denote the columns\nof A by A1, . . . , An and the rows of B by B1, . . . , Bn, prove that\n\nAB = A1B1 + · · ·+ AnBn.\n\nProblem 3.17. Let f : E → F be a linear map which is also a bijection (it is injective and\nsurjective). Prove that the inverse function f−1 : F → E is linear.\n\nProblem 3.18. Given two vectors spaces E and F , let (ui)i∈I be any basis of E and let\n(vi)i∈I be any family of vectors in F . Prove that the unique linear map f : E → F such that\nf(ui) = vi for all i ∈ I is surjective iff (vi)i∈I spans F .\n\nProblem 3.19. Let f : E → F be a linear map with dim(E) = n and dim(F ) = m. Prove\nthat f has rank 1 iff f is represented by an m× n matrix of the form\n\nA = uv>\n\nwith u a nonzero column vector of dimension m and v a nonzero column vector of dimension\nn.\n\nProblem 3.20. Find a nontrivial linear dependence among the linear forms\n\nϕ1(x, y, z) = 2x− y + 3z, ϕ2(x, y, z) = 3x− 5y + z, ϕ3(x, y, z) = 4x− 7y + z.\n\nProblem 3.21. Prove that the linear forms\n\nϕ1(x, y, z) = x+ 2y + z, ϕ2(x, y, z) = 2x+ 3y + 3z, ϕ3(x, y, z) = 3x+ 7y + z\n\nare linearly independent. Express the linear form ϕ(x, y, z) = x+y+z as a linear combination\nof ϕ1, ϕ2, ϕ3.\n\n\n\n106 CHAPTER 3. VECTOR SPACES, BASES, LINEAR MAPS\n\n\n\nChapter 4\n\nMatrices and Linear Maps\n\nIn this chapter, all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n4.1 Representation of Linear Maps by Matrices\n\nProposition 3.15 shows that given two vector spaces E and F and a basis (uj)j∈J of E, every\nlinear map f : E → F is uniquely determined by the family (f(uj))j∈J of the images under\nf of the vectors in the basis (uj)j∈J .\n\nIf we also have a basis (vi)i∈I of F , then every vector f(uj) can be written in a unique\nway as\n\nf(uj) =\n∑\ni∈I\n\nai jvi,\n\nwhere j ∈ J , for a family of scalars (ai j)i∈I . Thus, with respect to the two bases (uj)j∈J\nof E and (vi)i∈I of F , the linear map f is completely determined by a “I × J-matrix”\nM(f) = (ai j)i∈I, j∈J .\n\nRemark: Note that we intentionally assigned the index set J to the basis (uj)j∈J of E, and\nthe index set I to the basis (vi)i∈I of F , so that the rows of the matrix M(f) associated\nwith f : E → F are indexed by I, and the columns of the matrix M(f) are indexed by J .\nObviously, this causes a mildly unpleasant reversal. If we had considered the bases (ui)i∈I of\nE and (vj)j∈J of F , we would obtain a J × I-matrix M(f) = (aj i)j∈J, i∈I . No matter what\nwe do, there will be a reversal! We decided to stick to the bases (uj)j∈J of E and (vi)i∈I of\nF , so that we get an I × J-matrix M(f), knowing that we may occasionally suffer from this\ndecision!\n\nWhen I and J are finite, and say, when |I| = m and |J | = n, the linear map f is\ndetermined by the matrix M(f) whose entries in the j-th column are the components of the\n\n107\n\n\n\n108 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nvector f(uj) over the basis (v1, . . . , vm), that is, the matrix\n\nM(f) =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\nwhose entry on Row i and Column j is ai j (1 ≤ i ≤ m, 1 ≤ j ≤ n).\n\nWe will now show that when E and F have finite dimension, linear maps can be very\nconveniently represented by matrices, and that composition of linear maps corresponds to\nmatrix multiplication. We will follow rather closely an elegant presentation method due to\nEmil Artin.\n\nLet E and F be two vector spaces, and assume that E has a finite basis (u1, . . . , un) and\nthat F has a finite basis (v1, . . . , vm). Recall that we have shown that every vector x ∈ E\ncan be written in a unique way as\n\nx = x1u1 + · · ·+ xnun,\n\nand similarly every vector y ∈ F can be written in a unique way as\n\ny = y1v1 + · · ·+ ymvm.\n\nLet f : E → F be a linear map between E and F . Then for every x = x1u1 + · · ·+ xnun in\nE, by linearity, we have\n\nf(x) = x1f(u1) + · · ·+ xnf(un).\n\nLet\nf(uj) = a1 jv1 + · · ·+ amjvm,\n\nor more concisely,\n\nf(uj) =\nm∑\ni=1\n\nai jvi,\n\nfor every j, 1 ≤ j ≤ n. This can be expressed by writing the coefficients a1j, a2j, . . . , amj of\nf(uj) over the basis (v1, . . . , vm), as the jth column of a matrix, as shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThen substituting the right-hand side of each f(uj) into the expression for f(x), we get\n\nf(x) = x1(\nm∑\ni=1\n\nai 1vi) + · · ·+ xn(\nm∑\ni=1\n\nai nvi),\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 109\n\nwhich, by regrouping terms to obtain a linear combination of the vi, yields\n\nf(x) = (\nn∑\nj=1\n\na1 jxj)v1 + · · ·+ (\nn∑\nj=1\n\namjxj)vm.\n\nThus, letting f(x) = y = y1v1 + · · ·+ ymvm, we have\n\nyi =\nn∑\nj=1\n\nai jxj (1)\n\nfor all i, 1 ≤ i ≤ m.\n\nTo make things more concrete, let us treat the case where n = 3 and m = 2. In this case,\n\nf(u1) = a11v1 + a21v2\n\nf(u2) = a12v1 + a22v2\n\nf(u3) = a13v1 + a23v2,\n\nwhich in matrix form is expressed by\n\nf(u1) f(u2) f(u3)\n\nv1\n\nv2\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)\n,\n\nand for any x = x1u1 + x2u2 + x3u3, we have\n\nf(x) = f(x1u1 + x2u2 + x3u3)\n\n= x1f(u1) + x2f(u2) + x3f(u3)\n\n= x1(a11v1 + a21v2) + x2(a12v1 + a22v2) + x3(a13v1 + a23v2)\n\n= (a11x1 + a12x2 + a13x3)v1 + (a21x1 + a22x2 + a23x3)v2.\n\nConsequently, since\ny = y1v1 + y2v2,\n\nwe have\n\ny1 = a11x1 + a12x2 + a13x3\n\ny2 = a21x1 + a22x2 + a23x3.\n\nThis agrees with the matrix equation(\ny1\n\ny2\n\n)\n=\n\n(\na11 a12 a13\n\na21 a22 a23\n\n)x1\n\nx2\n\nx3\n\n .\n\nWe now formalize the representation of linear maps by matrices.\n\n\n\n110 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nDefinition 4.1. Let E and F be two vector spaces, and let (u1, . . . , un) be a basis for E,\nand (v1, . . . , vm) be a basis for F . Each vector x ∈ E expressed in the basis (u1, . . . , un) as\nx = x1u1 + · · ·+ xnun is represented by the column matrix\n\nM(x) =\n\nx1\n...\nxn\n\n\nand similarly for each vector y ∈ F expressed in the basis (v1, . . . , vm).\n\nEvery linear map f : E → F is represented by the matrix M(f) = (ai j), where ai j is the\ni-th component of the vector f(uj) over the basis (v1, . . . , vm), i.e., where\n\nf(uj) =\nm∑\ni=1\n\nai jvi, for every j, 1 ≤ j ≤ n.\n\nThe coefficients a1j, a2j, . . . , amj of f(uj) over the basis (v1, . . . , vm) form the jth column of\nthe matrix M(f) shown below:\n\nf(u1) f(u2) . . . f(un)\n\nv1\n\nv2\n...\nvm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n .\n\nThe matrix M(f) associated with the linear map f : E → F is called the matrix of f with\nrespect to the bases (u1, . . . , un) and (v1, . . . , vm). When E = F and the basis (v1, . . . , vm)\nis identical to the basis (u1, . . . , un) of E, the matrix M(f) associated with f : E → E (as\nabove) is called the matrix of f with respect to the basis (u1, . . . , un).\n\nRemark: As in the remark after Definition 3.12, there is no reason to assume that the\nvectors in the bases (u1, . . . , un) and (v1, . . . , vm) are ordered in any particular way. However,\nit is often convenient to assume the natural ordering. When this is so, authors sometimes\nrefer to the matrix M(f) as the matrix of f with respect to the ordered bases (u1, . . . , un)\nand (v1, . . . , vm).\n\nLet us illustrate the representation of a linear map by a matrix in a concrete situation.\nLet E be the vector space R[X]4 of polynomials of degree at most 4, let F be the vector\nspace R[X]3 of polynomials of degree at most 3, and let the linear map be the derivative\nmap d: that is,\n\nd(P +Q) = dP + dQ\n\nd(λP ) = λdP,\n\n\n\n4.1. REPRESENTATION OF LINEAR MAPS BY MATRICES 111\n\nwith λ ∈ R. We choose (1, x, x2, x3, x4) as a basis of E and (1, x, x2, x3) as a basis of F .\nThen the 4 × 5 matrix D associated with d is obtained by expressing the derivative dxi of\neach basis vector xi for i = 0, 1, 2, 3, 4 over the basis (1, x, x2, x3). We find\n\nD =\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n .\n\nIf P denotes the polynomial\n\nP = 3x4 − 5x3 + x2 − 7x+ 5,\n\nwe have\ndP = 12x3 − 15x2 + 2x− 7.\n\nThe polynomial P is represented by the vector (5,−7, 1,−5, 3), the polynomial dP is repre-\nsented by the vector (−7, 2,−15, 12), and we have\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n5\n−7\n1\n−5\n3\n\n =\n\n\n−7\n2\n−15\n12\n\n ,\n\nas expected! The kernel (nullspace) of d consists of the polynomials of degree 0, that is, the\nconstant polynomials. Therefore dim(Ker d) = 1, and from\n\ndim(E) = dim(Ker d) + dim(Im d)\n\n(see Theorem 6.13), we get dim(Im d) = 4 (since dim(E) = 5).\n\nFor fun, let us figure out the linear map from the vector space R[X]3 to the vector space\nR[X]4 given by integration (finding the primitive, or anti-derivative) of xi, for i = 0, 1, 2, 3).\nThe 5× 4 matrix S representing\n\n∫\nwith respect to the same bases as before is\n\nS =\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n .\n\nWe verify that DS = I4,\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n\n\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n\n\n112 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThis is to be expected by the fundamental theorem of calculus since the derivative of an\nintegral returns the function. As we will shortly see, the above matrix product corresponds\nto this functional composition. The equation DS = I4 shows that S is injective and has D\nas a left inverse. However, SD 6= I5, and instead\n\n0 0 0 0\n1 0 0 0\n0 1/2 0 0\n0 0 1/3 0\n0 0 0 1/4\n\n\n\n\n0 1 0 0 0\n0 0 2 0 0\n0 0 0 3 0\n0 0 0 0 4\n\n =\n\n\n0 0 0 0 0\n0 1 0 0 0\n0 0 1 0 0\n0 0 0 1 0\n0 0 0 0 1\n\n ,\n\nbecause constant polynomials (polynomials of degree 0) belong to the kernel of D.\n\n4.2 Composition of Linear Maps and Matrix\n\nMultiplication\n\nLet us now consider how the composition of linear maps is expressed in terms of bases.\n\nLet E, F , and G, be three vectors spaces with respective bases (u1, . . . , up) for E,\n(v1, . . . , vn) for F , and (w1, . . . , wm) for G. Let g : E → F and f : F → G be linear maps.\nAs explained earlier, g : E → F is determined by the images of the basis vectors uj, and\nf : F → G is determined by the images of the basis vectors vk. We would like to understand\nhow f ◦ g : E → G is determined by the images of the basis vectors uj.\n\nRemark: Note that we are considering linear maps g : E → F and f : F → G, instead\nof f : E → F and g : F → G, which yields the composition f ◦ g : E → G instead of\ng ◦ f : E → G. Our perhaps unusual choice is motivated by the fact that if f is represented\nby a matrix M(f) = (ai k) and g is represented by a matrix M(g) = (bk j), then f ◦g : E → G\nis represented by the product AB of the matrices A and B. If we had adopted the other\nchoice where f : E → F and g : F → G, then g ◦ f : E → G would be represented by the\nproduct BA. Personally, we find it easier to remember the formula for the entry in Row i and\nColumn j of the product of two matrices when this product is written by AB, rather than\nBA. Obviously, this is a matter of taste! We will have to live with our perhaps unorthodox\nchoice.\n\nThus, let\n\nf(vk) =\nm∑\ni=1\n\nai kwi,\n\nfor every k, 1 ≤ k ≤ n, and let\n\ng(uj) =\nn∑\nk=1\n\nbk jvk,\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 113\n\nfor every j, 1 ≤ j ≤ p; in matrix form, we have\n\nf(v1) f(v2) . . . f(vn)\n\nw1\n\nw2\n...\nwm\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nam1 am2 . . . amn\n\n\nand\n\ng(u1) g(u2) . . . g(up)\n\nv1\n\nv2\n...\nvn\n\n\nb11 b12 . . . b1p\n\nb21 b22 . . . b2p\n...\n\n...\n. . .\n\n...\nbn1 bn2 . . . bnp\n\n .\n\nBy previous considerations, for every\n\nx = x1u1 + · · ·+ xpup,\n\nletting g(x) = y = y1v1 + · · ·+ ynvn, we have\n\nyk =\n\np∑\nj=1\n\nbk jxj (2)\n\nfor all k, 1 ≤ k ≤ n, and for every\n\ny = y1v1 + · · ·+ ynvn,\n\nletting f(y) = z = z1w1 + · · ·+ zmwm, we have\n\nzi =\nn∑\nk=1\n\nai kyk (3)\n\nfor all i, 1 ≤ i ≤ m. Then if y = g(x) and z = f(y), we have z = f(g(x)), and in view of (2)\nand (3), we have\n\nzi =\nn∑\nk=1\n\nai k(\n\np∑\nj=1\n\nbk jxj)\n\n=\nn∑\nk=1\n\np∑\nj=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\nn∑\nk=1\n\nai kbk jxj\n\n=\n\np∑\nj=1\n\n(\nn∑\nk=1\n\nai kbk j)xj.\n\n\n\n114 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nThus, defining ci j such that\n\nci j =\nn∑\nk=1\n\nai kbk j,\n\nfor 1 ≤ i ≤ m, and 1 ≤ j ≤ p, we have\n\nzi =\n\np∑\nj=1\n\nci jxj (4)\n\nIdentity (4) shows that the composition of linear maps corresponds to the product of\nmatrices.\n\nThen given a linear map f : E → F represented by the matrix M(f) = (ai j) w.r.t. the\nbases (u1, . . . , un) and (v1, . . . , vm), by Equation (1), namely\n\nyi =\nn∑\nj=1\n\nai jxj 1 ≤ i ≤ m,\n\nand the definition of matrix multiplication, the equation y = f(x) corresponds to the matrix\nequation M(y) = M(f)M(x), that is,y1\n\n...\nym\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nam 1 . . . amn\n\n\nx1\n\n...\nxn\n\n .\n\nRecall that\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nam 1 am 2 . . . amn\n\n\n\nx1\n\nx2\n...\nxn\n\n = x1\n\n\na1 1\n\na2 1\n...\n\nam 1\n\n+ x2\n\n\na1 2\n\na2 2\n...\n\nam 2\n\n+ · · ·+ xn\n\n\na1n\n\na2n\n...\n\namn\n\n .\n\nSometimes, it is necessary to incorporate the bases (u1, . . . , un) and (v1, . . . , vm) in the\nnotation for the matrix M(f) expressing f with respect to these bases. This turns out to be\na messy enterprise!\n\nWe propose the following course of action:\n\nDefinition 4.2. Write U = (u1, . . . , un) and V = (v1, . . . , vm) for the bases of E and F , and\ndenote by MU ,V(f) the matrix of f with respect to the bases U and V . Furthermore, write\nxU for the coordinates M(x) = (x1, . . . , xn) of x ∈ E w.r.t. the basis U and write yV for the\ncoordinates M(y) = (y1, . . . , ym) of y ∈ F w.r.t. the basis V . Then\n\ny = f(x)\n\n\n\n4.2. COMPOSITION OF LINEAR MAPS AND MATRIX MULTIPLICATION 115\n\nis expressed in matrix form by\nyV = MU ,V(f)xU .\n\nWhen U = V , we abbreviate MU ,V(f) as MU(f).\n\nThe above notation seems reasonable, but it has the slight disadvantage that in the\nexpression MU ,V(f)xU , the input argument xU which is fed to the matrix MU ,V(f) does not\nappear next to the subscript U in MU ,V(f). We could have used the notation MV,U(f), and\nsome people do that. But then, we find a bit confusing that V comes before U when f maps\nfrom the space E with the basis U to the space F with the basis V . So, we prefer to use the\nnotation MU ,V(f).\n\nBe aware that other authors such as Meyer [124] use the notation [f ]U ,V , and others such\nas Dummit and Foote [55] use the notation MV\n\nU (f), instead of MU ,V(f). This gets worse!\nYou may find the notation MU\n\nV (f) (as in Lang [108]), or U [f ]V , or other strange notations.\n\nDefinition 4.2 shows that the function which associates to a linear map f : E → F the\nmatrix M(f) w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm) has the property that matrix mul-\ntiplication corresponds to composition of linear maps. This allows us to transfer properties\nof linear maps to matrices. Here is an illustration of this technique:\n\nProposition 4.1. (1) Given any matrices A ∈ Mm,n(K), B ∈ Mn,p(K), and C ∈ Mp,q(K),\nwe have\n\n(AB)C = A(BC);\n\nthat is, matrix multiplication is associative.\n\n(2) Given any matrices A,B ∈ Mm,n(K), and C,D ∈ Mn,p(K), for all λ ∈ K, we have\n\n(A+B)C = AC +BC\n\nA(C +D) = AC + AD\n\n(λA)C = λ(AC)\n\nA(λC) = λ(AC),\n\nso that matrix multiplication · : Mm,n(K)×Mn,p(K)→ Mm,p(K) is bilinear.\n\nProof. (1) Every m× n matrix A = (ai j) defines the function fA : Kn → Km given by\n\nfA(x) = Ax,\n\nfor all x ∈ Kn. It is immediately verified that fA is linear and that the matrix M(fA)\nrepresenting fA over the canonical bases in Kn and Km is equal to A. Then Formula (4)\nproves that\n\nM(fA ◦ fB) = M(fA)M(fB) = AB,\n\nso we get\nM((fA ◦ fB) ◦ fC) = M(fA ◦ fB)M(fC) = (AB)C\n\n\n\n116 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nand\n\nM(fA ◦ (fB ◦ fC)) = M(fA)M(fB ◦ fC) = A(BC),\n\nand since composition of functions is associative, we have (fA ◦ fB) ◦ fC = fA ◦ (fB ◦ fC),\nwhich implies that\n\n(AB)C = A(BC).\n\n(2) It is immediately verified that if f1, f2 ∈ HomK(E,F ), A,B ∈ Mm,n(K), (u1, . . . , un) is\nany basis of E, and (v1, . . . , vm) is any basis of F , then\n\nM(f1 + f2) = M(f1) +M(f2)\n\nfA+B = fA + fB.\n\nThen we have\n\n(A+B)C = M(fA+B)M(fC)\n\n= M(fA+B ◦ fC)\n\n= M((fA + fB) ◦ fC))\n\n= M((fA ◦ fC) + (fB ◦ fC))\n\n= M(fA ◦ fC) +M(fB ◦ fC)\n\n= M(fA)M(fC) +M(fB)M(fC)\n\n= AC +BC.\n\nThe equation A(C + D) = AC + AD is proven in a similar fashion, and the last two\nequations are easily verified. We could also have verified all the identities by making matrix\ncomputations.\n\nNote that Proposition 4.1 implies that the vector space Mn(K) of square matrices is a\n(noncommutative) ring with unit In. (It even shows that Mn(K) is an associative algebra.)\n\nThe following proposition states the main properties of the mapping f 7→M(f) between\nHom(E,F ) and Mm,n. In short, it is an isomorphism of vector spaces.\n\nProposition 4.2. Given three vector spaces E, F , G, with respective bases (u1, . . . , up),\n(v1, . . . , vn), and (w1, . . . , wm), the mapping M : Hom(E,F )→ Mn,p that associates the ma-\ntrix M(g) to a linear map g : E → F satisfies the following properties for all x ∈ E, all\ng, h : E → F , and all f : F → G:\n\nM(g(x)) = M(g)M(x)\n\nM(g + h) = M(g) +M(h)\n\nM(λg) = λM(g)\n\nM(f ◦ g) = M(f)M(g),\n\n\n\n4.3. CHANGE OF BASIS MATRIX 117\n\nwhere M(x) is the column vector associated with the vector x and M(g(x)) is the column\nvector associated with g(x), as explained in Definition 4.1.\n\nThus, M : Hom(E,F ) → Mn,p is an isomorphism of vector spaces, and when p = n\nand the basis (v1, . . . , vn) is identical to the basis (u1, . . . , up), M : Hom(E,E) → Mn is an\nisomorphism of rings.\n\nProof. That M(g(x)) = M(g)M(x) was shown by Definition 4.2 or equivalently by Formula\n(1). The identities M(g+ h) = M(g) +M(h) and M(λg) = λM(g) are straightforward, and\nM(f ◦ g) = M(f)M(g) follows from Identity (4) and the definition of matrix multiplication.\nThe mapping M : Hom(E,F ) → Mn,p is clearly injective, and since every matrix defines a\nlinear map (see Proposition 4.1), it is also surjective, and thus bijective. In view of the above\nidentities, it is an isomorphism (and similarly for M : Hom(E,E)→ Mn, where Proposition\n4.1 is used to show that Mn is a ring).\n\nIn view of Proposition 4.2, it seems preferable to represent vectors from a vector space\nof finite dimension as column vectors rather than row vectors. Thus, from now on, we will\ndenote vectors of Rn (or more generally, of Kn) as column vectors.\n\n4.3 Change of Basis Matrix\n\nIt is important to observe that the isomorphism M : Hom(E,F )→ Mn,p given by Proposition\n4.2 depends on the choice of the bases (u1, . . . , up) and (v1, . . . , vn), and similarly for the\nisomorphism M : Hom(E,E) → Mn, which depends on the choice of the basis (u1, . . . , un).\nThus, it would be useful to know how a change of basis affects the representation of a linear\nmap f : E → F as a matrix. The following simple proposition is needed.\n\nProposition 4.3. Let E be a vector space, and let (u1, . . . , un) be a basis of E. For every\nfamily (v1, . . . , vn), let P = (ai j) be the matrix defined such that vj =\n\n∑n\ni=1 ai jui. The matrix\n\nP is invertible iff (v1, . . . , vn) is a basis of E.\n\nProof. Note that we have P = M(f), the matrix associated with the unique linear map\nf : E → E such that f(ui) = vi. By Proposition 3.15, f is bijective iff (v1, . . . , vn) is a basis\nof E. Furthermore, it is obvious that the identity matrix In is the matrix associated with the\nidentity id : E → E w.r.t. any basis. If f is an isomorphism, then f ◦f−1 = f−1◦f = id, and\nby Proposition 4.2, we get M(f)M(f−1) = M(f−1)M(f) = In, showing that P is invertible\nand that M(f−1) = P−1.\n\nProposition 4.3 suggests the following definition.\n\nDefinition 4.3. Given a vector space E of dimension n, for any two bases (u1, . . . , un) and\n(v1, . . . , vn) of E, let P = (ai j) be the invertible matrix defined such that\n\nvj =\nn∑\ni=1\n\nai jui,\n\n\n\n118 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nwhich is also the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), in that order . Indeed, we express each id(vj) = vj over the basis (u1, . . . , un).\nThe coefficients a1j, a2j, . . . , anj of vj over the basis (u1, . . . , un) form the jth column of the\nmatrix P shown below:\n\nv1 v2 . . . vn\n\nu1\n\nu2\n...\nun\n\n\na11 a12 . . . a1n\n\na21 a22 . . . a2n\n...\n\n...\n. . .\n\n...\nan1 an2 . . . ann\n\n .\n\nThe matrix P is called the change of basis matrix from (u1, . . . , un) to (v1, . . . , vn).\n\nClearly, the change of basis matrix from (v1, . . . , vn) to (u1, . . . , un) is P−1. Since P =\n(ai j) is the matrix of the identity id : E → E with respect to the bases (v1, . . . , vn) and\n(u1, . . . , un), given any vector x ∈ E, if x = x1u1 + · · ·+xnun over the basis (u1, . . . , un) and\nx = x′1v1 + · · ·+ x′nvn over the basis (v1, . . . , vn), from Proposition 4.2, we havex1\n\n...\nxn\n\n =\n\na1 1 . . . a1n\n...\n\n. . .\n...\n\nan 1 . . . ann\n\n\nx\n\n′\n1\n...\nx′n\n\n ,\n\nshowing that the old coordinates (xi) of x (over (u1, . . . , un)) are expressed in terms of the\nnew coordinates (x′i) of x (over (v1, . . . , vn)).\n\nNow we face the painful task of assigning a “good” notation incorporating the bases\nU = (u1, . . . , un) and V = (v1, . . . , vn) into the notation for the change of basis matrix from\nU to V . Because the change of basis matrix from U to V is the matrix of the identity map\nidE with respect to the bases V and U in that order , we could denote it by MV,U(id) (Meyer\n[124] uses the notation [I]V,U). We prefer to use an abbreviation for MV,U(id).\n\nDefinition 4.4. The change of basis matrix from U to V is denoted\n\nPV,U .\n\nNote that\nPU ,V = P−1\n\nV,U .\n\nThen, if we write xU = (x1, . . . , xn) for the old coordinates of x with respect to the basis U\nand xV = (x′1, . . . , x\n\n′\nn) for the new coordinates of x with respect to the basis V , we have\n\nxU = PV,U xV , xV = P−1\nV,U xU .\n\nThe above may look backward, but remember that the matrix MU ,V(f) takes input\nexpressed over the basis U to output expressed over the basis V . Consequently, PV,U takes\ninput expressed over the basis V to output expressed over the basis U , and xU = PV,U xV\nmatches this point of view!\n\n\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n� Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto V as PU ,V = P−1\n\nV,U . Under this point of view, the old basis U is expressed in terms of\nthe new basis V . We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = PV,U expresses the new basis (v1, . . . , vn) in terms of the old basis\n(u1, . . ., un), we observe that the coordinates (xi) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant .\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R2, with u1 = (1, 0), u2 = (0, 1), v1 = (1, 1) and v2 = (−1, 1).\nThe change of basis matrix P from the basis U = (u1, u2) to the basis V = (v1, v2) is\n\nP =\n\n(\n1 −1\n1 1\n\n)\nand its inverse is\n\nP−1 =\n\n(\n1/2 1/2\n−1/2 1/2\n\n)\n.\n\nThe old coordinates (x1, x2) with respect to (u1, u2) are expressed in terms of the new\ncoordinates (x′1, x\n\n′\n2) with respect to (v1, v2) by(\n\nx1\n\nx2\n\n)\n=\n\n(\n1 −1\n1 1\n\n)(\nx′1\nx′2\n\n)\n,\n\nand the new coordinates (x′1, x\n′\n2) with respect to (v1, v2) are expressed in terms of the old\n\ncoordinates (x1, x2) with respect to (u1, u2) by(\nx′1\nx′2\n\n)\n=\n\n(\n1/2 1/2\n−1/2 1/2\n\n)(\nx1\n\nx2\n\n)\n.\n\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\nand consider the bases U = (1, x, x2, x3) and V = (B3\n\n0(x), B3\n1(x), B3\n\n2(x), B3\n3(x)), where\n\nB3\n0(x), B3\n\n1(x), B3\n2(x), B3\n\n3(x) are the Bernstein polynomials of degree 3, given by\n\nB3\n0(x) = (1− x)3 B3\n\n1(x) = 3(1− x)2x B3\n2(x) = 3(1− x)x2 B3\n\n3(x) = x3.\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix PV,U is\ngiven by\n\nPV,U =\n\n\n1 0 0 0\n−3 3 0 0\n3 −6 3 0\n−1 3 −3 1\n\n .\n\n4.3. CHANGE OF BASIS MATRIX 119\n\n© Beware that some authors (such as Artin [7]) define the change of basis matrix from U\nto Vas Puy = Py 1. Under this point of view, the old basis U/ is expressed in terms of\nthe new basis V. We find this a bit unnatural. Also, in practice, it seems that the new basis\nis often expressed in terms of the old basis, rather than the other way around.\n\nSince the matrix P = P)y expresses the new basis (v1,...,Un) in terms of the old basis\n(U1,.--, Un), we observe that the coordinates (;) of a vector x vary in the opposite direction\nof the change of basis. For this reason, vectors are sometimes said to be contravariant.\nHowever, this expression does not make sense! Indeed, a vector in an intrinsic quantity that\ndoes not depend on a specific basis. What makes sense is that the coordinates of a vector\nvary in a contravariant fashion.\n\nLet us consider some concrete examples of change of bases.\n\nExample 4.1. Let E = F = R’, with wu, = (1,0), wa = (0,1), vy = (1,1) and ve = (-1,1).\nThe change of basis matrix P from the basis U = (uy, ug) to the basis V = (vj, v2) is\n\np=(, 7)\n\npte (1 ta):\n\nThe old coordinates (21,22) with respect to (ui,u2) are expressed in terms of the new\ncoordinates (x, 24) with respect to (v1, v2) by\n\n(2) = 7) @)\n\nand the new coordinates (2,25) with respect to (v1, v2) are expressed in terms of the old\ncoordinates (21,22) with respect to (ui, U2) by\n\nvy) _ ( 1/2 1/2) (x\nX5 —1/2 1/2) \\a)\nExample 4.2. Let E = F = R[X]3 be the set of polynomials of degree at most 3,\n\nand consider the bases UY = (1,2,x?,73) and V = (B3(x), B3(x), B3(x), B3(x)), where\nBe (x), Bi(x), B3(x), B3(x) are the Bernstein polynomials of degree 3, given by\n\nB3(x) = (1-2)? B3(x) = 3(1—2)?x B3 (x) = 3(1 — x)2? B3 (x) = 2°.\n\nand its inverse is\n\nBy expanding the Bernstein polynomials, we find that the change of basis matrix Pyy is\ngiven by\n\n1 0 0 0\n3 3 0 0\nPou = 3-6 3 0\n-1 3 -3 1\n\n\n\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of PV,U is\n\nP−1\nV,U =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n .\n\nTherefore, the coordinates of the polynomial 2x3 − x+ 1 over the basis V are\n1\n\n2/3\n1/3\n2\n\n =\n\n\n1 0 0 0\n1 1/3 0 0\n1 2/3 1/3 0\n1 1 1 1\n\n\n\n\n1\n−1\n0\n2\n\n ,\n\nand so\n\n2x3 − x+ 1 = B3\n0(x) +\n\n2\n\n3\nB3\n\n1(x) +\n1\n\n3\nB3\n\n2(x) + 2B3\n3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn)\n\nbe two bases of E, and let V = (v1, . . . , vm) and V ′ = (v′1, . . . , v\n′\nm) be two bases of F . Let\n\nP = PU ′,U be the change of basis matrix from U to U ′, and let Q = PV ′,V be the change of\nbasis matrix from V to V ′. For any linear map f : E → F , let M(f) = MU ,V(f) be the matrix\nassociated to f w.r.t. the bases U and V, and let M ′(f) = MU ′,V ′(f) be the matrix associated\nto f w.r.t. the bases U ′ and V ′. We have\n\nM ′(f) = Q−1M(f)P,\n\nor more explicitly\n\nMU ′,V ′(f) = P−1\nV ′,VMU ,V(f)PU ′,U = PV,V ′MU ,V(f)PU ′,U .\n\nProof. Since f : E → F can be written as f = idF ◦ f ◦ idE, since P is the matrix of idE\nw.r.t. the bases (u′1, . . . , u\n\n′\nn) and (u1, . . . , un), and Q−1 is the matrix of idF w.r.t. the bases\n\n(v1, . . . , vm) and (v′1, . . . , v\n′\nm), by Proposition 4.2, we have M ′(f) = Q−1M(f)P .\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (u1, . . . , un) and U ′ = (u′1, . . . , u\n′\nn) be\n\ntwo bases of E. Let P = PU ′,U be the change of basis matrix from U to U ′. For any linear\n\n120 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nWe also find that the inverse of Pyy is\n\n10 0 0\npi_|{i 1/3 0 0\nU~11 9/3 1/3 0\n\n11 11\n\nTherefore, the coordinates of the polynomial 27° — x + 1 over the basis V are\n\n1 10 0 0\\/1\n\n2/3) [1 1/3 0 oO] f-1\n\n1/3 1 2/3 1/3 of | 0 J?\n2 11 1 1) \\2\n\nand so\n\n2 1\n2x? —x2+1= Be(x) + 3 Pile) + 3 B2(@) + 2B3(x).\n\n4.4 The Effect of a Change of Bases on Matrices\n\nThe effect of a change of bases on the representation of a linear map is described in the\nfollowing proposition.\n\n/\n\nProposition 4.4. Let E and F be vector spaces, let U = (u1,..., tn) andU' = (u},...,ui,)\n\nan\n\nbe two bases of E, and let V = (u,...,Um) and V! = (v},...,v/,) be two bases of F'. Let\nP= Pyy be the change of basis matriz from U to U', and let Q = Pyy be the change of\nbasis matriz from V to V'. For any linear map f: E > F, let M(f) = Mu y(f) be the matriz\nassociated to f w.r.t. the basesU and V, and let M'(f) = My y(f) be the matrix associated\n\nto f w.r.t. the bases UW’ and VY’. We have\n\nM'(f) =Q°M(f)P,\n\nor more explicitly\nMuy (f) = Poy Muy (Pf) Pau = Poy Muy (f)Puru-\n\nProof. Since f: E — F can be written as f = idg o f oidg, since P is the matrix of idg\n\nw.r.t. the bases (uj,...,u/,) and (w1,...,Un), and Q7! is the matrix of idp w.r.t. the bases\n\nan\n\n(v1,---,Um) and (v},...,v/,), by Proposition 4.2, we have M’(f) = Q-'M(f)P. O\n\nAs a corollary, we get the following result.\n\nCorollary 4.5. Let E be a vector space, and let U = (uj,...,Un) andU’ = (u},...,ui,) be\ntwo bases of E. Let P = Pyy be the change of basis matrix from U to U'. For any linear\n\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f : E → E, let M(f) = MU(f) be the matrix associated to f w.r.t. the basis U , and let\nM ′(f) = MU ′(f) be the matrix associated to f w.r.t. the basis U ′. We have\n\nM ′(f) = P−1M(f)P,\n\nor more explicitly,\n\nMU ′(f) = P−1\nU ′,UMU(f)PU ′,U = PU ,U ′MU(f)PU ′,U .\n\nExample 4.3. Let E = R2, U = (e1, e2) where e1 = (1, 0) and e2 = (0, 1) are the canonical\nbasis vectors, let V = (v1, v2) = (e1, e1 − e2), and let\n\nA =\n\n(\n2 1\n0 1\n\n)\n.\n\nThe change of basis matrix P = PV,U from U to V is\n\nP =\n\n(\n1 1\n0 −1\n\n)\n,\n\nand we check that\nP−1 = P.\n\nTherefore, in the basis V , the matrix representing the linear map f defined by A is\n\nA′ = P−1AP = PAP =\n\n(\n1 1\n0 −1\n\n)(\n2 1\n0 1\n\n)(\n1 1\n0 −1\n\n)\n=\n\n(\n2 0\n0 1\n\n)\n= D,\n\na diagonal matrix. In the basis V , it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v1 direction and it is the identity in the v2 direction. Observe that v1 and\nv2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v1 and v2 are corresponding eigenvectors . We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two n×n matrices A and B are said to be similar iff there is some invertible\nmatrix P such that\n\nB = P−1AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n × n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 121\n\nmap f: E + E, let M(f) = Mu (f) be the matrix associated to f w.r.t. the basis U, and let\nM'(f) = Mw(f) be the matrix associated to f w.r.t. the basis U'. We have\n\nM'(f) =PUM(f)P.\nor more explicitly,\nMu(f) = Pry Mul f) Pau = Puw Mu(f) Pew.\n\nExample 4.3. Let E = R?, U = (e1, e2) where e; = (1,0) and e2 = (0,1) are the canonical\nbasis vectors, let V = (v1, v2) = (€1, €1 — €2), and let\n\n=)\n\nThe change of basis matrix P = Pyy from U to Y is\n\n11\np=(p 4):\n\nP'=P.\n\nand we check that\n\nTherefore, in the basis Y, the matrix representing the linear map f defined by A is\n\nb plyap.. —f1 1\\(2 I) fa 1\\_ (2 0) ~\nserarcrare( EDGE )-2\n\na diagonal matrix. In the basis VY, it is clear what the action of f is: it is a stretch by a\nfactor of 2 in the v, direction and it is the identity in the v2 direction. Observe that v, and\nV2 are not orthogonal.\n\nWhat happened is that we diagonalized the matrix A. The diagonal entries 2 and 1 are\nthe eigenvalues of A (and f), and v; and vy are corresponding eigenvectors. We will come\nback to eigenvalues and eigenvectors later on.\n\nThe above example showed that the same linear map can be represented by different\nmatrices. This suggests making the following definition:\n\nDefinition 4.5. Two nxn matrices A and B are said to be similar iff there is some invertible\n\nmatrix P such that\nB=P'AP.\n\nIt is easily checked that similarity is an equivalence relation. From our previous consid-\nerations, two n x n matrices A and B are similar iff they represent the same linear map\nwith respect to two different bases. The following surprising fact can be shown: Every square\n\n\n\n\n122 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nmatrix A is similar to its transpose A>. The proof requires advanced concepts (the Jordan\nform or similarity invariants).\n\nIf U = (u1, . . . , un) and V = (v1, . . . , vn) are two bases of E, the change of basis matrix\n\nP = PV,U =\n\n\na11 a12 · · · a1n\n\na21 a22 · · · a2n\n...\n\n...\n. . .\n\n...\nan1 an2 · · · ann\n\n\nfrom (u1, . . . , un) to (v1, . . . , vn) is the matrix whose jth column consists of the coordinates\nof vj over the basis (u1, . . . , un), which means that\n\nvj =\nn∑\ni=1\n\naijui.\n\nIt is natural to extend the matrix notation and to express the vector\n\nv1\n...\nvn\n\n in En as the\n\nproduct of a matrix times the vector\n\nu1\n...\nun\n\n in En, namely as\n\n\nv1\n\nv2\n...\nvn\n\n =\n\n\na11 a21 · · · an1\n\na12 a22 · · · an2\n...\n\n...\n. . .\n\n...\na1n a2n · · · ann\n\n\n\nu1\n\nu2\n...\nun\n\n ,\n\nbut notice that the matrix involved is not P , but its transpose P>.\n\nThis observation has the following consequence: if U = (u1, . . . , un) and V = (v1, . . . , vn)\nare two bases of E and if v1\n\n...\nvn\n\n = A\n\nu1\n...\nun\n\n ,\n\nthat is,\n\nvi =\nn∑\nj=1\n\naijuj,\n\nfor any vector w ∈ E, if\n\nw =\nn∑\ni=1\n\nxiui =\nn∑\nk=1\n\nykvk,\n\n\n\n4.4. THE EFFECT OF A CHANGE OF BASES ON MATRICES 123\n\nthen x1\n...\nxn\n\n = A>\n\ny1\n...\nyn\n\n ,\n\nand so y1\n...\nyn\n\n = (A>)−1\n\nx1\n...\nxn\n\n .\n\nIt is easy to see that (A>)−1 = (A−1)>. Also, if U = (u1, . . . , un), V = (v1, . . . , vn), and\nW = (w1, . . . , wn) are three bases of E, and if the change of basis matrix from U to V is\nP = PV,U and the change of basis matrix from V to W is Q = PW,V , thenv1\n\n...\nvn\n\n = P>\n\nu1\n...\nun\n\n ,\n\nw1\n...\nwn\n\n = Q>\n\nv1\n...\nvn\n\n ,\n\nso w1\n...\nwn\n\n = Q>P>\n\nu1\n...\nun\n\n = (PQ)>\n\nu1\n...\nun\n\n ,\n\nwhich means that the change of basis matrix PW,U from U to W is PQ. This proves that\n\nPW,U = PV,UPW,V .\n\nEven though matrices are indispensable since they are the major tool in applications of\nlinear algebra, one should not lose track of the fact that\n\nlinear maps are more fundamental because they are intrinsic\nobjects that do not depend on the choice of bases.\n\nConsequently, we advise the reader to try to think in terms of\nlinear maps rather than reduce everything to matrices.\n\nIn our experience, this is particularly effective when it comes to proving results about\nlinear maps and matrices, where proofs involving linear maps are often more “conceptual.”\nThese proofs are usually more general because they do not depend on the fact that the\ndimension is finite. Also, instead of thinking of a matrix decomposition as a purely algebraic\noperation, it is often illuminating to view it as a geometric decomposition. This is the case of\nthe SVD, which in geometric terms says that every linear map can be factored as a rotation,\nfollowed by a rescaling along orthogonal axes and then another rotation.\n\nAfter all,\n\n\n\n124 CHAPTER 4. MATRICES AND LINEAR MAPS\n\na matrix is a representation of a linear map,\n\nand most decompositions of a matrix reflect the fact that with a suitable choice of a basis\n(or bases), the linear map is a represented by a matrix having a special shape. The problem\nis then to find such bases.\n\nStill, for the beginner, matrices have a certain irresistible appeal, and we confess that\nit takes a certain amount of practice to reach the point where it becomes more natural to\ndeal with linear maps. We still recommend it! For example, try to translate a result stated\nin terms of matrices into a result stated in terms of linear maps. Whenever we tried this\nexercise, we learned something.\n\nAlso, always try to keep in mind that\n\nlinear maps are geometric in nature; they act on space.\n\n4.5 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• The representation of linear maps by matrices .\n\n• The matrix representation mapping M : Hom(E,F ) → Mn,p and the representation\nisomorphism (Proposition 4.2).\n\n• Change of basis matrix and Proposition 4.4.\n\n4.6 Problems\n\nProblem 4.1. Prove that the column vectors of the matrix A1 given by\n\nA1 =\n\n1 2 3\n2 3 7\n1 3 1\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B1 over the basis consisting\nof the column vectors of A1 given by\n\nB1 =\n\n3 5 1\n1 2 1\n4 3 −6\n\n\n\n\n\n4.6. PROBLEMS 125\n\nare the columns of the matrix P1 given by\n\nP1 =\n\n−27 −61 −41\n9 18 9\n4 10 8\n\n .\n\nGive a nontrivial linear dependence of the columns of P1. Check that B1 = A1P1. Is the\nmatrix B1 invertible?\n\nProblem 4.2. Prove that the column vectors of the matrix A2 given by\n\nA2 =\n\n\n1 1 1 1\n1 2 1 3\n1 1 2 2\n1 1 1 3\n\n\nare linearly independent.\n\nProve that the column vectors of the matrix B2 given by\n\nB2 =\n\n\n1 −2 2 −2\n0 −3 2 −3\n3 −5 5 −4\n3 −4 4 −4\n\n\nare linearly independent.\n\nProve that the coordinates of the column vectors of the matrix B2 over the basis consisting\nof the column vectors of A2 are the columns of the matrix P2 given by\n\nP2 =\n\n\n2 0 1 −1\n−3 1 −2 1\n1 −2 2 −1\n1 −1 1 −1\n\n .\n\nCheck that A2P2 = B2. Prove that\n\nP−1\n2 =\n\n\n−1 −1 −1 1\n2 1 1 −2\n2 1 2 −3\n−1 −1 0 −1\n\n .\n\nWhat are the coordinates over the basis consisting of the column vectors of B2 of the vector\nwhose coordinates over the basis consisting of the column vectors of A1 are (2,−3, 0, 0)?\n\n\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nB2\n0(t) = (1− t)2 B2\n\n1(t) = 2(1− t)t B2\n2(t) = t2\n\nB3\n0(t) = (1− t)3 B3\n\n1(t) = 3(1− t)2t B3\n2(t) = 3(1− t)t2 B3\n\n3(t) = t3,\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B2\n0(t), B2\n\n1(t), B2\n2(t) are expressed as linear com-\n\nbinations of the basis (1, t, t2) of the vector space of polynomials of degree at most 2 as\nfollows: B2\n\n0(t)\nB2\n\n1(t)\nB2\n\n2(t)\n\n =\n\n1 −2 1\n0 2 −2\n0 0 1\n\n1\nt\nt2\n\n .\n\nProve that\nB2\n\n0(t) +B2\n1(t) +B2\n\n2(t) = 1.\n\n(2) Show that the Bernstein polynomials B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t) are expressed as linear\ncombinations of the basis (1, t, t2, t3) of the vector space of polynomials of degree at most 3\nas follows: \n\nB3\n0(t)\n\nB3\n1(t)\n\nB3\n2(t)\n\nB3\n3(t)\n\n =\n\n\n1 −3 3 −1\n0 3 −6 3\n0 0 3 −3\n0 0 0 1\n\n\n\n\n1\nt\nt2\n\nt3\n\n .\n\nProve that\nB3\n\n0(t) +B3\n1(t) +B3\n\n2(t) +B3\n3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient\n(\nm\nk\n\n)\nis given by(\n\nm\n\nk\n\n)\n=\n\nm!\n\nk!(m− k)!\n,\n\nwith 0 ≤ k ≤ m.\n\nFor any m ≥ 1, we have the m+ 1 Bernstein polynomials of degree m given by\n\nBm\nk (t) =\n\n(\nm\n\nk\n\n)\n(1− t)m−ktk, 0 ≤ k ≤ m.\n\n(1) Prove that\n\nBm\nk (t) =\n\nm∑\nj=k\n\n(−1)j−k\n(\nm\n\nj\n\n)(\nj\n\nk\n\n)\ntj. (∗)\n\n126 CHAPTER 4. MATRICES AND LINEAR MAPS\n\nProblem 4.3. Consider the polynomials\n\nBU) =(-0) BA) =BU—N* BA =30—HP BR) =e\n\nknown as the Bernstein polynomials of degree 2 and 3.\n\n(1) Show that the Bernstein polynomials B3(t), B?(t), B3(t) are expressed as linear com-\nbinations of the basis (1,¢,¢?) of the vector space of polynomials of degree at most 2 as\nfollows:\n\nBe(t) 1-2 1\nBet)}=|0 2 -2 t\nB3(t) 0 0 1 ?\n\nProve that\nB3(t) + Be(t) + BS(t) = 1.\n\n(2) Show that the Bernstein polynomials B3(t), B}(t), B3(t), B3(t) are expressed as linear\ncombinations of the basis (1, t, ¢?, #2) of the vector space of polynomials of degree at most 3\nas follows:\n\nBe(t) 1-3 3 -1 1\nBeth} {0 3 -6 3 t\nBt)} {0 0 3 -3) |\nB3(t) 00 0 1 t?\n\nProve that\nBe (t) + Be(t) + B3(t) + B3(t) = 1.\n\n(3) Prove that the Bernstein polynomials of degree 2 are linearly independent, and that\nthe Bernstein polynomials of degree 3 are linearly independent.\n\nProblem 4.4. Recall that the binomial coefficient (\"\") is given by\nm\\ m!\nkk} kM(m—k)V\n\nFor any m > 1, we have the m-+1 Bernstein polynomials of degree m given by\n\nwithO<k<m.\n\nBrit) = (‘\") (1—t)\"*t*®, O<k<m.\n\n(1) Prove that\n\n\n\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bm\n0 (t), . . . , Bm\n\nm(t) are linearly independent.\n\n(2) Prove that\n\nBm\n0 (t) + · · ·+Bm\n\nm(t) = 1.\n\n(3) What can you say about the symmetries of the (m+ 1)× (m+ 1) matrix expressing\nBm\n\n0 , . . . , B\nm\nm in terms of the basis 1, t, . . . , tm?\n\nProve your claim (beware that in equation (∗) the coefficient of tj in Bm\nk is the entry on\n\nthe (k+1)th row of the (j+1)th column, since 0 ≤ k, j ≤ m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the ti in terms of the Bernstein polynomials\nBm\n\n0 (t), . . . , Bm\nm(t), with 0 ≤ i ≤ m.\n\nFirst, prove that\n\nti =\nm−i∑\nj=0\n\ntiBm−i\nj (t), 0 ≤ i ≤ m.\n\nThen prove that (\nm\n\ni\n\n)(\nm− i\nj\n\n)\n=\n\n(\nm\n\ni+ j\n\n)(\ni+ j\n\ni\n\n)\n.\n\nUse the above facts to prove that\n\nti =\nm−i∑\nj=0\n\n(\ni+j\ni\n\n)(\nm\ni\n\n) Bm\ni+j(t).\n\nConclude that the Bernstein polynomials Bm\n0 (t), . . . , Bm\n\nm(t) form a basis of the vector\nspace of polynomials of degree ≤ m.\n\nCompute the matrix expressing 1, t, t2 in terms of B2\n0(t), B2\n\n1(t), B2\n2(t), and the matrix\n\nexpressing 1, t, t2, t3 in terms of B3\n0(t), B3\n\n1(t), B3\n2(t), B3\n\n3(t).\n\nYou should find 1 1 1\n0 1/2 1\n0 0 1\n\n\nand \n\n1 1 1 1\n0 1/3 2/3 1\n0 0 1/3 1\n0 0 0 1\n\n .\n\n4.6. PROBLEMS 127\n\nUse the above to prove that Bj’(t),..., Bi (t) are linearly independent.\n(2) Prove that\nBo (t) +--+ Bt) =1.\n(3) What can you say about the symmetries of the (m+ 1) x (m+ 1) matrix expressing\nBo',..., BM in terms of the basis 1,t,...,t?\n\nProve your claim (beware that in equation («) the coefficient of t? in Bi” is the entry on\nthe (k+1)th row of the (j+1)th column, since 0 < k, 7 < m. Make appropriate modifications\nto the indices).\n\nWhat can you say about the sum of the entries on each row of the above matrix? What\nabout the sum of the entries on each column?\n\n(4) The purpose of this question is to express the t’ in terms of the Bernstein polynomials\nBy (t),..., B(t), with 0<i<m.\n\nFirst, prove that\n\n= S UB (t), O<i<m.\nj=0\n\nOlen monies)\n\nUse the above facts to prove that\n\nThen prove that\n\ni’ Bm (t).\n(\"\") ig (t)\n\nConclude that the Bernstein polynomials Bj’(t),...,B/\"(t) form a basis of the vector\nspace of polynomials of degree < m.\n\nCompute the matrix expressing 1,t,¢? in terms of B3(t), B?(t), B3(t), and the matrix\nexpressing 1,t,t*,¢° in terms of B3(t), BR(t), B3(t), B3(t).\n\nYou should find\n\nand\n\noO CF\n\n_\noOo ™.\n\nw\nre bo\n—~=\nwm w\nRee\n\n\n\n\n128 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) A polynomial curve C(t) of degree m in the plane is the set of points\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\ngiven by two polynomials of degree ≤ m,\n\nx(t) = α0t\nm1 + α1t\n\nm1−1 + · · ·+ αm1\n\ny(t) = β0t\nm2 + β1t\n\nm2−1 + · · ·+ βm2 ,\n\nwith 1 ≤ m1,m2 ≤ m and α0, β0 6= 0.\n\nProve that there exist m+ 1 points b0, . . . , bm ∈ R2 so that\n\nC(t) =\n\n(\nx(t)\ny(t)\n\n)\n= Bm\n\n0 (t)b0 +Bm\n1 (t)b1 + · · ·+Bm\n\nm(t)bm\n\nfor all t ∈ R, with C(0) = b0 and C(1) = bm. Are the points b1, . . . , bm−1 generally on the\ncurve?\n\nWe say that the curve C is a Bézier curve and (b0, . . . , bm) is the list of control points of\nthe curve (control points need not be distinct).\n\nRemark: Because Bm\n0 (t) + · · · + Bm\n\nm(t) = 1 and Bm\ni (t) ≥ 0 when t ∈ [0, 1], the curve\n\nsegment C[0, 1] corresponding to t ∈ [0, 1] belongs to the convex hull of the control points.\nThis is an important property of Bézier curves which is used in geometric modeling to\nfind the intersection of curve segments. Bézier curves play an important role in computer\ngraphics and geometric modeling, but also in robotics because they can be used to model\nthe trajectories of moving objects.\n\nProblem 4.5. Consider the n× n matrix\n\nA =\n\n\n\n0 0 0 · · · 0 −an\n1 0 0 · · · 0 −an−1\n\n0 1 0 · · · 0 −an−2\n...\n\n. . . . . . . . .\n...\n\n...\n\n0 0 0\n. . . 0 −a2\n\n0 0 0 · · · 1 −a1\n\n\n,\n\nwith an 6= 0.\n\n(1) Find a matrix P such that\nA> = P−1AP.\n\nWhat happens when an = 0?\n\nHint . First, try n = 3, 4, 5. Such a matrix must have zeros above the “antidiagonal,” and\nidentical entries pij for all i, j ≥ 0 such that i+ j = n+ k, where k = 1, . . . , n.\n\n(2) Prove that if an = 1 and if a1, . . . , an−1 are integers, then P can be chosen so that\nthe entries in P−1 are also integers.\n\n\n\n4.6. PROBLEMS 129\n\nProblem 4.6. For any matrix A ∈ Mn(C), let RA and LA be the maps from Mn(C) to itself\ndefined so that\n\nLA(B) = AB, RA(B) = BA, for all B ∈ Mn(C).\n\n(1) Check that LA and RA are linear, and that LA and RB commute for all A,B.\n\nLet adA : Mn(C)→ Mn(C) be the linear map given by\n\nadA(B) = LA(B)−RA(B) = AB −BA = [A,B], for all B ∈ Mn(C).\n\nNote that [A,B] is the Lie bracket.\n\n(2) Prove that if A is invertible, then LA and RA are invertible; in fact, (LA)−1 = LA−1\n\nand (RA)−1 = RA−1 . Prove that if A = PBP−1 for some invertible matrix P , then\n\nLA = LP ◦ LB ◦ L−1\nP , RA = R−1\n\nP ◦RB ◦RP .\n\n(3) Recall that the n2 matrices Eij defined such that all entries in Eij are zero except\nthe (i, j)th entry, which is equal to 1, form a basis of the vector space Mn(C). Consider the\npartial ordering of the Eij defined such that for i = 1, . . . , n, if n ≥ j > k ≥ 1, then then Eij\nprecedes Eik, and for j = 1, . . . , n, if 1 ≤ i < h ≤ n, then Eij precedes Ehj.\n\nDraw the Hasse diagram of the partial order defined above when n = 3.\n\nThere are total orderings extending this partial ordering. How would you find them\nalgorithmically? Check that the following is such a total order:\n\n(1, 3), (1, 2), (1, 1), (2, 3), (2, 2), (2, 1), (3, 3), (3, 2), (3, 1).\n\n(4) Let the total order of the basis (Eij) extending the partial ordering defined in (2) be\ngiven by\n\n(i, j) < (h, k) iff\n\n{\ni = h and j > k\nor i < h.\n\nLet R be the n× n permutation matrix given by\n\nR =\n\n\n0 0 . . . 0 1\n0 0 . . . 1 0\n...\n\n...\n. . .\n\n...\n...\n\n0 1 . . . 0 0\n1 0 . . . 0 0\n\n .\n\nObserve that R−1 = R. Prove that for any n ≥ 1, the matrix of LA is given by A⊗In, and the\nmatrix of RA is given by In⊗RA>R (over the basis (Eij) ordered as specified above), where\n⊗ is the Kronecker product (also called tensor product) of matrices defined in Definition 5.4.\n\nHint . Figure out what are RB(Eij) = EijB and LB(Eij) = BEij.\n\n\n\n130 CHAPTER 4. MATRICES AND LINEAR MAPS\n\n(5) Prove that if A is upper triangular, then the matrices representing LA and RA are\nalso upper triangular.\n\nNote that if instead of the ordering\n\nE1n, E1n−1, . . . , E11, E2n, . . . , E21, . . . , Enn, . . . , En1,\n\nthat I proposed you use the standard lexicographic ordering\n\nE11, E12, . . . , E1n, E21, . . . , E2n, . . . , En1, . . . , Enn,\n\nthen the matrix representing LA is still A⊗ In, but the matrix representing RA is In ⊗ A>.\nIn this case, if A is upper-triangular, then the matrix of RA is lower triangular . This is the\nmotivation for using the first basis (avoid upper becoming lower).\n\n\n\nChapter 5\n\nHaar Bases, Haar Wavelets,\nHadamard Matrices\n\nIn this chapter, we discuss two types of matrices that have applications in computer science\nand engineering:\n\n(1) Haar matrices and the corresponding Haar wavelets, a fundamental tool in signal pro-\ncessing and computer graphics.\n\n2) Hadamard matrices which have applications in error correcting codes, signal processing,\nand low rank approximation.\n\n5.1 Introduction to Signal Compression Using Haar\n\nWavelets\n\nWe begin by considering Haar wavelets in R4. Wavelets play an important role in audio\nand video signal processing, especially for compressing long signals into much smaller ones\nthat still retain enough information so that when they are played, we can’t see or hear any\ndifference.\n\nConsider the four vectors w1, w2, w3, w4 given by\n\nw1 =\n\n\n1\n1\n1\n1\n\n w2 =\n\n\n1\n1\n−1\n−1\n\n w3 =\n\n\n1\n−1\n0\n0\n\n w4 =\n\n\n0\n0\n1\n−1\n\n .\n\nNote that these vectors are pairwise orthogonal, so they are indeed linearly independent\n(we will see this in a later chapter). Let W = {w1, w2, w3, w4} be the Haar basis , and let\nU = {e1, e2, e3, e4} be the canonical basis of R4. The change of basis matrix W = PW,U from\n\n131\n\n\n\n132 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nU to W is given by\n\nW =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nand we easily find that the inverse of W is given by\n\nW−1 =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n .\n\nSo the vector v = (6, 4, 5, 1) over the basis U becomes c = (c1, c2, c3, c4) over the Haar basis\nW , with \n\nc1\n\nc2\n\nc3\n\nc4\n\n =\n\n\n1/4 0 0 0\n0 1/4 0 0\n0 0 1/2 0\n0 0 0 1/2\n\n\n\n\n1 1 1 1\n1 1 −1 −1\n1 −1 0 0\n0 0 1 −1\n\n\n\n\n6\n4\n5\n1\n\n =\n\n\n4\n1\n1\n2\n\n .\n\nGiven a signal v = (v1, v2, v3, v4), we first transform v into its coefficients c = (c1, c2, c3, c4)\nover the Haar basis by computing c = W−1v. Observe that\n\nc1 =\nv1 + v2 + v3 + v4\n\n4\n\nis the overall average value of the signal v. The coefficient c1 corresponds to the background\nof the image (or of the sound). Then, c2 gives the coarse details of v, whereas, c3 gives the\ndetails in the first part of v, and c4 gives the details in the second half of v.\n\nReconstruction of the signal consists in computing v = Wc. The trick for good compres-\nsion is to throw away some of the coefficients of c (set them to zero), obtaining a compressed\nsignal ĉ, and still retain enough crucial information so that the reconstructed signal v̂ = Wĉ\nlooks almost as good as the original signal v. Thus, the steps are:\n\ninput v −→ coefficients c = W−1v −→ compressed ĉ −→ compressed v̂ = Wĉ.\n\nThis kind of compression scheme makes modern video conferencing possible.\n\nIt turns out that there is a faster way to find c = W−1v, without actually using W−1.\nThis has to do with the multiscale nature of Haar wavelets.\n\nGiven the original signal v = (6, 4, 5, 1) shown in Figure 5.1, we compute averages and\nhalf differences obtaining Figure 5.2. We get the coefficients c3 = 1 and c4 = 2. Then\nagain we compute averages and half differences obtaining Figure 5.3. We get the coefficients\nc1 = 4 and c2 = 1. Note that the original signal v can be reconstructed from the two signals\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 133\n\n6 4 5 1\n\nFigure 5.1: The original signal v.\n\n5 5 3 3\n\n1\n\n−1\n\n2\n\n−2\n\nFigure 5.2: First averages and first half differences.\n\nin Figure 5.2, and the signal on the left of Figure 5.2 can be reconstructed from the two\nsignals in Figure 5.3. In particular, the data from Figure 5.2 gives us\n\n5 + 1 =\nv1 + v2\n\n2\n+\nv1 − v2\n\n2\n= v1\n\n5− 1 =\nv1 + v2\n\n2\n− v1 − v2\n\n2\n= v2\n\n3 + 2 =\nv3 + v4\n\n2\n+\nv3 − v4\n\n2\n= v3\n\n3− 2 =\nv3 + v4\n\n2\n− v3 − v4\n\n2\n= v4.\n\n5.2 Haar Bases and Haar Matrices, Scaling Properties\n\nof Haar Wavelets\n\nThe method discussed in Section 5.2 can be generalized to signals of any length 2n. The\nprevious case corresponds to n = 2. Let us consider the case n = 3. The Haar basis\n\n\n\n134 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n4 4 4 4\n1 1\n\n−1 −1\n\nFigure 5.3: Second averages and second half differences.\n\n(w1, w2, w3, w4, w5, w6, w7, w8) is given by the matrix\n\nW =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nThe columns of this matrix are orthogonal, and it is easy to see that\n\nW−1 = diag(1/8, 1/8, 1/4, 1/4, 1/2, 1/2, 1/2, 1/2)W>.\n\nA pattern is beginning to emerge. It looks like the second Haar basis vector w2 is the\n“mother” of all the other basis vectors, except the first, whose purpose is to perform aver-\naging. Indeed, in general, given\n\nw2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n2n\n\n,\n\nthe other Haar basis vectors are obtained by a “scaling and shifting process.” Starting from\nw2, the scaling process generates the vectors\n\nw3, w5, w9, . . . , w2j+1, . . . , w2n−1+1,\n\nsuch that w2j+1+1 is obtained from w2j+1 by forming two consecutive blocks of 1 and −1\nof half the size of the blocks in w2j+1, and setting all other entries to zero. Observe that\nw2j+1 has 2j blocks of 2n−j elements. The shifting process consists in shifting the blocks of\n1 and −1 in w2j+1 to the right by inserting a block of (k − 1)2n−j zeros from the left, with\n0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Note that our convention is to use j as the scaling index and\nk as the shifting index. Thus, we obtain the following formula for w2j+k:\n\nw2j+k(i) =\n\n\n0 1 ≤ i ≤ (k − 1)2n−j\n\n1 (k − 1)2n−j + 1 ≤ i ≤ (k − 1)2n−j + 2n−j−1\n\n−1 (k − 1)2n−j + 2n−j−1 + 1 ≤ i ≤ k2n−j\n\n0 k2n−j + 1 ≤ i ≤ 2n,\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 135\n\nwith 0 ≤ j ≤ n− 1 and 1 ≤ k ≤ 2j. Of course\n\nw1 = (1, . . . , 1)︸ ︷︷ ︸\n2n\n\n.\n\nThe above formulae look a little better if we change our indexing slightly by letting k vary\nfrom 0 to 2j − 1, and using the index j instead of 2j.\n\nDefinition 5.1. The vectors of the Haar basis of dimension 2n are denoted by\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\nwhere\n\nhjk(i) =\n\n\n0 1 ≤ i ≤ k2n−j\n\n1 k2n−j + 1 ≤ i ≤ k2n−j + 2n−j−1\n\n−1 k2n−j + 2n−j−1 + 1 ≤ i ≤ (k + 1)2n−j\n\n0 (k + 1)2n−j + 1 ≤ i ≤ 2n,\n\nwith 0 ≤ j ≤ n− 1 and 0 ≤ k ≤ 2j − 1. The 2n × 2n matrix whose columns are the vectors\n\nw1, h\n0\n0, h\n\n1\n0, h\n\n1\n1, h\n\n2\n0, h\n\n2\n1, h\n\n2\n2, h\n\n2\n3, . . . , h\n\nj\nk, . . . , h\n\nn−1\n2n−1−1,\n\n(in that order), is called the Haar matrix of dimension 2n, and is denoted by Wn.\n\nIt turns out that there is a way to understand these formulae better if we interpret a\nvector u = (u1, . . . , um) as a piecewise linear function over the interval [0, 1).\n\nDefinition 5.2. Given a vector u = (u1, . . . , um), the piecewise linear function plf(u) is\ndefined such that\n\nplf(u)(x) = ui,\ni− 1\n\nm\n≤ x <\n\ni\n\nm\n, 1 ≤ i ≤ m.\n\nIn words, the function plf(u) has the value u1 on the interval [0, 1/m), the value u2 on\n[1/m, 2/m), etc., and the value um on the interval [(m− 1)/m, 1).\n\nFor example, the piecewise linear function associated with the vector\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3)\n\nis shown in Figure 5.4.\nThen each basis vector hjk corresponds to the function\n\nψjk = plf(hjk).\n\nIn particular, for all n, the Haar basis vectors\n\nh0\n0 = w2 = (1, . . . , 1,−1, . . . ,−1)︸ ︷︷ ︸\n\n2n\n\n\n\n136 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n−2\n\n−1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nFigure 5.4: The piecewise linear function plf(u).\n\nyield the same piecewise linear function ψ given by\n\nψ(x) =\n\n\n1 if 0 ≤ x < 1/2\n\n−1 if 1/2 ≤ x < 1\n\n0 otherwise,\n\nwhose graph is shown in Figure 5.5. It is easy to see that ψjk is given by the simple expression\n\n1\n\n1\n\n−1\n\n0\n\nFigure 5.5: The Haar wavelet ψ.\n\nψjk(x) = ψ(2jx− k), 0 ≤ j ≤ n− 1, 0 ≤ k ≤ 2j − 1.\n\nThe above formula makes it clear that ψjk is obtained from ψ by scaling and shifting.\n\nDefinition 5.3. The function φ0\n0 = plf(w1) is the piecewise linear function with the constant\n\nvalue 1 on [0, 1), and the functions ψjk = plf(hjk) together with φ0\n0 are known as the Haar\n\nwavelets .\n\nRather than using W−1 to convert a vector u to a vector c of coefficients over the Haar\nbasis, and the matrix W to reconstruct the vector u from its Haar coefficients c, we can use\nfaster algorithms that use averaging and differencing.\n\n\n\n5.2. HAAR MATRICES, SCALING PROPERTIES OF HAAR WAVELETS 137\n\nIf c is a vector of Haar coefficients of dimension 2n, we compute the sequence of vectors\nu0, u1, . . ., un as follows:\n\nu0 = c\n\nuj+1 = uj\n\nuj+1(2i− 1) = uj(i) + uj(2j + i)\n\nuj+1(2i) = uj(i)− uj(2j + i),\n\nfor j = 0, . . . , n− 1 and i = 1, . . . , 2j. The reconstructed vector (signal) is u = un.\n\nIf u is a vector of dimension 2n, we compute the sequence of vectors cn, cn−1, . . . , c0 as\nfollows:\n\ncn = u\n\ncj = cj+1\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/2,\n\nfor j = n− 1, . . . , 0 and i = 1, . . . , 2j. The vector over the Haar basis is c = c0.\n\nWe leave it as an exercise to implement the above programs in Matlab using two variables\nu and c, and by building iteratively 2j. Here is an example of the conversion of a vector to\nits Haar coefficients for n = 3.\n\nGiven the sequence u = (31, 29, 23, 17,−6,−8,−2,−4), we get the sequence\n\nc3 = (31, 29, 23, 17,−6,−8, 2,−4)\n\nc2 =\n\n(\n31 + 29\n\n2\n,\n23 + 17\n\n2\n,\n−6− 8\n\n2\n,\n−2− 4\n\n2\n,\n31− 29\n\n2\n,\n23− 17\n\n2\n,\n−6− (−8)\n\n2\n,\n−2− (−4)\n\n2\n\n)\n= (30, 20,−7,−3, 1, 3, 1, 1)\n\nc1 =\n\n(\n30 + 20\n\n2\n,\n−7− 3\n\n2\n,\n30− 20\n\n2\n,\n−7− (−3)\n\n2\n, 1, 3, 1, 1\n\n)\n= (25,−5, 5,−2, 1, 3, 1, 1)\n\nc0 =\n\n(\n25− 5\n\n2\n,\n25− (−5)\n\n2\n, 5,−2, 1, 3, 1, 1\n\n)\n= (10, 15, 5,−2, 1, 3, 1, 1)\n\nso c = (10, 15, 5,−2, 1, 3, 1, 1). Conversely, given c = (10, 15, 5,−2, 1, 3, 1, 1), we get the\nsequence\n\nu0 = (10, 15, 5,−2, 1, 3, 1, 1)\n\nu1 = (10 + 15, 10− 15, 5,−2, 1, 3, 1, 1) = (25,−5, 5,−2, 1, 3, 1, 1)\n\nu2 = (25 + 5, 25− 5,−5 + (−2),−5− (−2), 1, 3, 1, 1) = (30, 20,−7,−3, 1, 3, 1, 1)\n\nu3 = (30 + 1, 30− 1, 20 + 3, 20− 3,−7 + 1,−7− 1,−3 + 1,−3− 1)\n\n= (31, 29, 23, 17,−6,−8,−2,−4),\n\nwhich gives back u = (31, 29, 23, 17,−6,−8,−2,−4).\n\n\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix Wn of dimension 2n\n\nthat makes it clearer why the columns of Wn are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split Wn into two\n2n × 2n−1 matrices, then the second matrix containing the last 2n−1 columns of Wn has a\nvery simple structure: it consists of the vector\n\n(1,−1, 0, . . . , 0)︸ ︷︷ ︸\n2n\n\nand 2n−1 − 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 0 0\n−1 0 0 0\n0 1 0 0\n0 −1 0 0\n0 0 1 0\n0 0 −1 0\n0 0 0 1\n0 0 0 −1\n\n\n.\n\nObserve that this matrix can be obtained from the identity matrix I2n−1 , in our example\n\nI4 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n ,\n\nby forming the 2n × 2n−1 matrix obtained by replacing each 1 by the column vector(\n1\n−1\n\n)\nand each zero by the column vector (\n\n0\n0\n\n)\n.\n\nNow the first half of Wn, that is the matrix consisting of the first 2n−1 columns of Wn, can\nbe obtained from Wn−1 by forming the 2n× 2n−1 matrix obtained by replacing each 1 by the\ncolumn vector (\n\n1\n1\n\n)\n,\n\neach −1 by the column vector (\n−1\n−1\n\n)\n,\n\n138 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n5.3 Kronecker Product Construction of Haar Matrices\n\nThere is another recursive method for constructing the Haar matrix W,, of dimension 2”\nthat makes it clearer why the columns of W,, are pairwise orthogonal, and why the above\nalgorithms are indeed correct (which nobody seems to prove!). If we split W,, into two\n2” x 2\"-! matrices, then the second matrix containing the last 2\"~' columns of W,, has a\nvery simple structure: it consists of the vector\n\n(1, -1,0,...,0)\n\nXX w)\n\\-\n\nQn\n\nand 2”~! — 1 shifted copies of it, as illustrated below for n = 3:\n\n1 0 OO\n-l1 0 O 0\n0 1 0 0\n0 -l 0 0O\n0 0 1 0\n0 O -l 0O\n0 O O 1\n0 O QO -Il\n\nObserve that this matrix can be obtained from the identity matrix [gn-1, in our example\n\n0 0\n\n0\nI, 0 ’\n1\n\noOo OrF\noor ©\nor c&\n\nby forming the 2” x 2”~! matrix obtained by replacing each 1 by the column vector\n\n(4)\n()\n\nNow the first half of W,,, that is the matrix consisting of the first 2”~! columns of W,,, can\nbe obtained from W,,_; by forming the 2” x 2”! matrix obtained by replacing each 1 by the\ncolumn vector\n\nand each zero by the column vector\n\neach —1 by the column vector\n\n\n\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector (\n0\n0\n\n)\n.\n\nFor n = 3, the first half of W3 is the matrix\n\n1 1 1 0\n1 1 1 0\n1 1 −1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 1\n1 −1 0 −1\n1 −1 0 −1\n\n\nwhich is indeed obtained from\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given a m×n matrix A = (aij) and a p×q matrix B = (bij), the Kronecker\nproduct (or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined. Then it is immediately verified that Wn is given by\nthe following neat recursive equations:\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\n5.3. KRONECKER PRODUCT CONSTRUCTION OF HAAR MATRICES 139\n\nand each zero by the column vector\n\nFor n = 3, the first half of W3 is the matrix\n\nje ee ee ee ee oe oe\n\nwhich is indeed obtained from\n\nW2 =\n\nfo oe ee ee\n\n-1 0 -l\n\nusing the process that we just described.\n\nThese matrix manipulations can be described conveniently using a product operation on\nmatrices known as the Kronecker product.\n\nDefinition 5.4. Given am xn matrix A = (a;;) anda pxq matrix B = (b;;), the Kronecker\nproduct (or tensor product) A® B of A and B is the mp x nq matrix\n\nay,B ai2B te QinB\nA 2 B= an B an 7 : Amn B\nAmB Am2PB vee AmnP\n\nIt can be shown that © is associative and that\n\n(A@ B)\\(C®D) =AC@BD\n(A@B)'=A' @B',\n\nwhenever AC and BD are well defined. Then it is immediately verified that W,, is given by\nthe following neat recursive equations:\n\nHe-(000(0) 2(2))\n\n\n\n\n140 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nwith W0 = (1). If we let\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n,\n\nthen it is not hard to use the Kronecker product formulation of Wn to obtain a rigorous\nproof of the equation\n\nW>\nn Wn = Bn, for all n ≥ 1.\n\nThe above equation offers a clean justification of the fact that the columns of Wn are pairwise\northogonal.\n\nObserve that the right block (of size 2n × 2n−1) shows clearly how the detail coefficients\nin the second half of the vector c are added and subtracted to the entries in the first half of\nthe partially reconstructed vector after n− 1 steps.\n\n5.4 Multiresolution Signal Analysis with Haar Bases\n\nAn important and attractive feature of the Haar basis is that it provides a multiresolution\nanalysis of a signal. Indeed, given a signal u, if c = (c1, . . . , c2n) is the vector of its Haar coef-\nficients, the coefficients with low index give coarse information about u, and the coefficients\nwith high index represent fine information. For example, if u is an audio signal corresponding\nto a Mozart concerto played by an orchestra, c1 corresponds to the “background noise,” c2\n\nto the bass, c3 to the first cello, c4 to the second cello, c5, c6, c7, c7 to the violas, then the\nviolins, etc. This multiresolution feature of wavelets can be exploited to compress a signal,\nthat is, to use fewer coefficients to represent it. Here is an example.\n\nConsider the signal\n\nu = (2.4, 2.2, 2.15, 2.05, 6.8, 2.8,−1.1,−1.3),\n\nwhose Haar transform is\nc = (2, 0.2, 0.1, 3, 0.1, 0.05, 2, 0.1).\n\nThe piecewise-linear curves corresponding to u and c are shown in Figure 5.6. Since some of\nthe coefficients in c are small (smaller than or equal to 0.2) we can compress c by replacing\nthem by 0. We get\n\nc2 = (2, 0, 0, 3, 0, 0, 2, 0),\n\nand the reconstructed signal is\n\nu2 = (2, 2, 2, 2, 7, 3,−1,−1).\n\nThe piecewise-linear curves corresponding to u2 and c2 are shown in Figure 5.7.\n\n\n\n5.4. MULTIRESOLUTION SIGNAL ANALYSIS WITH HAAR BASES 141\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n2\n\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.6: A signal and its Haar transform.\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n1\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\n0\n\n0.5\n\n1\n\n1.5\n\n2\n\n2.5\n\n3\n\nFigure 5.7: A compressed signal and its compressed Haar transform.\n\nAn interesting (and amusing) application of the Haar wavelets is to the compression of\naudio signals. It turns out that if your type load handel in Matlab an audio file will be\nloaded in a vector denoted by y, and if you type sound(y), the computer will play this piece\nof music. You can convert y to its vector of Haar coefficients c. The length of y is 73113,\nso first tuncate the tail of y to get a vector of length 65536 = 216. A plot of the signals\ncorresponding to y and c is shown in Figure 5.8. Then run a program that sets all coefficients\nof c whose absolute value is less that 0.05 to zero. This sets 37272 coefficients to 0. The\nresulting vector c2 is converted to a signal y2. A plot of the signals corresponding to y2 and\nc2 is shown in Figure 5.9. When you type sound(y2), you find that the music doesn’t differ\nmuch from the original, although it sounds less crisp. You should play with other numbers\ngreater than or less than 0.05. You should hear what happens when you type sound(c). It\nplays the music corresponding to the Haar transform c of y, and it is quite funny.\n\n\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n0 1 2 3 4 5 6 7\nx 104\n\n−1\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0 1 2 3 4 5 6 7\nx 104\n\n−0.8\n\n−0.6\n\n−0.4\n\n−0.2\n\n0\n\n0.2\n\n0.4\n\n0.6\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2n × 2n matrix Wn of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W>\n\nn is not the inverse of Wn,\nbut rather the matrix\n\nW−1\nn = DnW\n\n>\nn\n\nwith Dn = diag\n(\n\n2−n, 2−n︸︷︷︸\n20\n\n, 2−(n−1), 2−(n−1)︸ ︷︷ ︸\n21\n\n, 2−(n−2), . . . , 2−(n−2)︸ ︷︷ ︸\n22\n\n, . . . , 2−1, . . . , 2−1︸ ︷︷ ︸\n2n−1\n\n)\n.\n\n142 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n-0.6-\n\n-0.8\n0\n\nx10\" x10!\n\nFigure 5.8: The signal “handel” and its Haar transform.\n\n1 7 7 7 7 7 7 0.6\n\n-0.6+\n\nA L L L L L L 08\n\nFigure 5.9: The compressed signal “handel” and its Haar transform.\n\n5.5 Haar Transform for Digital Images\n\nAnother neat property of the Haar transform is that it can be instantly generalized to\nmatrices (even rectangular) without any extra effort! This allows for the compression of\ndigital images. But first we address the issue of normalization of the Haar coefficients. As\nwe observed earlier, the 2” x 2” matrix W,, of Haar basis vectors has orthogonal columns,\nbut its columns do not have unit length. As a consequence, W,! is not the inverse of Wp,\nbut rather the matrix\n\nW,' = D,,W,!\n\nwith D,, = diag (2-\", gam Q-(M-V) Q-(M=1) g-(n-2)g(n-2)— gd ).\nae 2 .\n2° 21 22 gn—1\n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n\nHn = WnD\n1\n2\nn\n\nwhose columns are the normalized Haar basis vectors, with\n\nD\n1\n2\nn = diag\n\n(\n2−\n\nn\n2 , 2−\n\nn\n2︸︷︷︸\n\n20\n\n, 2−\nn−1\n\n2 , 2−\nn−1\n\n2︸ ︷︷ ︸\n21\n\n, 2−\nn−2\n\n2 , . . . , 2−\nn−2\n\n2︸ ︷︷ ︸\n22\n\n, . . . , 2−\n1\n2 , . . . , 2−\n\n1\n2︸ ︷︷ ︸\n\n2n−1\n\n)\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c = H>n u\nthe normalized Haar coefficients of u.\n\nBecause Hn is orthogonal, H−1\nn = H>n .\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H>n u and Hnc as follows: When computing the sequence of ujs, use\n\nuj+1(2i− 1) = (uj(i) + uj(2j + i))/\n√\n\n2\n\nuj+1(2i) = (uj(i)− uj(2j + i))/\n√\n\n2,\n\nand when computing the sequence of cjs, use\n\ncj(i) = (cj+1(2i− 1) + cj+1(2i))/\n√\n\n2\n\ncj(2j + i) = (cj+1(2i− 1)− cj+1(2i))/\n√\n\n2.\n\nNote that things are now more symmetric, at the expense of a division by\n√\n\n2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/\n√\n\n2n\n\nand u by\n√\n\n2n. This is because the norm of the basis functions ψjk is not equal to 1 (under\n\nthe inner product 〈f, g〉 =\n∫ 1\n\n0\nf(t)g(t)dt). The normalized basis functions are the functions√\n\n2jψjk.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix Wn, the method using Hn being identical (except that H−1\n\nn = H>n , but this does\nnot hold for W−1\n\nn ). Given a 2m × 2n matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W−1\n\nn , obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W−1\n\nm . Because columns and rows\nare exchanged in the first step,\n\nB = A(W−1\nn )>,\n\nand in the second step C = W−1\nm B, thus, we have\n\nC = W−1\nm A(W−1\n\nn )> = DmW\n>\nmAWnDn.\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 143\n\nDefinition 5.5. The orthogonal matrix\n1\nHH, = W,D;i\nwhose columns are the normalized Haar basis vectors, with\n\n4 . -R 9 m jg mel Qj m=1 2-2 _n=2 = _1\nDy = diag(272,2°2,2°> 2,27 2 272 ,...,2707 ,...,272,...,2°2\n— A A\n“ a “- -~_—\"’\n20 Q1 92 gn-1\n\nis called the normalized Haar transform matrix. Given a vector (signal) u, we call c= Hu\nthe normalized Haar coefficients of wu.\n\nBecause H,, is orthogonal, Hy! = H,.\n\nThen a moment of reflection shows that we have to slightly modify the algorithms to\ncompute H,'u and H,,c as follows: When computing the sequence of u/s, use\n\nw+1(2i — 1) = (w(t) + wi (2’ +: 1))/V2\nw*\"(2i) = (wi (i) — w (2? + i))/v2,\n\nand when computing the sequence of c’s, use\n\n(i) = (At (21 — 1) + At (21))/V2\n(2) +i) = (F*1(2i — 1) — F121) /Vv2.\n\nNote that things are now more symmetric, at the expense of a division by 2. However, for\nlong vectors, it turns out that these algorithms are numerically more stable.\n\nRemark: Some authors (for example, Stollnitz, Derose and Salesin [166]) rescale c by 1/2”\nand u by V2”. This is because el norm t the basis functions wi is not equal to 1 (under\n\nthe inner product (f,g) = fr fl . The normalized basis functions are the functions\nV2.\n\nLet us now explain the 2D version of the Haar transform. We describe the version using\nthe matrix W,,, the method using H,, being identical (except that H7' = H,’, but this does\nnot hold for W,-'). Given a 2” x 2” matrix A, we can first convert the rows of A to their\nHaar coefficients using the Haar transform W,-', obtaining a matrix B, and then convert the\ncolumns of B to their Haar coefficients, using the matrix W,,'. Because columns and rows\nare exchanged in the first step,\n\nB= A(W,,')\",\nand in the second step C = W,;'B, thus, we have\n\nC=W,'A(W,')' = D,W,) AW, Dn-\n\n\n\n\n144 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIn the other direction, given a 2m × 2n matrix C of Haar coefficients, we reconstruct the\nmatrix A (the image) by first applying Wm to the columns of C, obtaining B, and then W>\n\nn\n\nto the rows of B. Therefore\n\nA = WmCW\n>\nn .\n\nOf course, we don’t actually have to invert Wm and Wn and perform matrix multiplications.\nWe just have to use our algorithms using averaging and differencing. Here is an example.\n\nIf the data matrix (the image) is the 8× 8 matrix\n\nA =\n\n\n\n64 2 3 61 60 6 7 57\n9 55 54 12 13 51 50 16\n17 47 46 20 21 43 42 24\n40 26 27 37 36 30 31 33\n32 34 35 29 28 38 39 25\n41 23 22 44 45 19 18 48\n49 15 14 52 53 11 10 56\n8 58 59 5 4 62 63 1\n\n\n,\n\nthen applying our algorithms, we find that\n\nC =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0.5 0.5 27 −25 23 −21\n0 0 −0.5 −0.5 −11 9 −7 5\n0 0 0.5 0.5 −5 7 −9 11\n0 0 −0.5 −0.5 21 −23 25 −27\n\n\n.\n\nAs we can see, C has more zero entries than A; it is a compressed version of A. We can\nfurther compress C by setting to 0 all entries of absolute value at most 0.5. Then we get\n\nC2 =\n\n\n\n32.5 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0\n0 0 0 0 4 −4 4 −4\n0 0 0 0 4 −4 4 −4\n0 0 0 0 27 −25 23 −21\n0 0 0 0 −11 9 −7 5\n0 0 0 0 −5 7 −9 11\n0 0 0 0 21 −23 25 −27\n\n\n.\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 145\n\nWe find that the reconstructed image is\n\nA2 =\n\n\n\n63.5 1.5 3.5 61.5 59.5 5.5 7.5 57.5\n9.5 55.5 53.5 11.5 13.5 51.5 49.5 15.5\n17.5 47.5 45.5 19.5 21.5 43.5 41.5 23.5\n39.5 25.5 27.5 37.5 35.5 29.5 31.5 33.5\n31.5 33.5 35.5 29.5 27.5 37.5 39.5 25.5\n41.5 23.5 21.5 43.5 45.5 19.5 17.5 47.5\n49.5 15.5 13.5 51.5 53.5 11.5 9.5 55.5\n7.5 57.5 59.5 5.5 3.5 61.5 63.5 1.5\n\n\n,\n\nwhich is pretty close to the original image matrix A.\n\nIt turns out that Matlab has a wonderful command, image(X) (also imagesc(X), which\noften does a better job), which displays the matrix X has an image in which each entry\nis shown as a little square whose gray level is proportional to the numerical value of that\nentry (lighter if the value is higher, darker if the value is closer to zero; negative values are\ntreated as zero). The images corresponding to A and C are shown in Figure 5.10. The\n\nFigure 5.10: An image and its Haar transform.\n\ncompressed images corresponding to A2 and C2 are shown in Figure 5.11. The compressed\nversions appear to be indistinguishable from the originals!\n\nIf we use the normalized matrices Hm and Hn, then the equations relating the image\nmatrix A and its normalized Haar transform C are\n\nC = H>mAHn\n\nA = HmCH\n>\nn .\n\nThe Haar transform can also be used to send large images progressively over the internet.\nIndeed, we can start sending the Haar coefficients of the matrix C starting from the coarsest\n\n\n\n146 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.11: Compressed image and its Haar transform.\n\ncoefficients (the first column from top down, then the second column, etc.), and at the\nreceiving end we can start reconstructing the image as soon as we have received enough\ndata.\n\nObserve that instead of performing all rounds of averaging and differencing on each row\nand each column, we can perform partial encoding (and decoding). For example, we can\nperform a single round of averaging and differencing for each row and each column. The\nresult is an image consisting of four subimages, where the top left quarter is a coarser version\nof the original, and the rest (consisting of three pieces) contain the finest detail coefficients.\nWe can also perform two rounds of averaging and differencing, or three rounds, etc. The\nsecond round of averaging and differencing is applied to the top left quarter of the image.\nGenerally, the kth round is applied to the 2m+1−k × 2n+1−k submatrix consisting of the first\n2m+1−k rows and the first 2n+1−k columns (1 ≤ k ≤ n) of the matrix obtained at the end of\nthe previous round. This process is illustrated on the image shown in Figure 5.12. The result\nof performing one round, two rounds, three rounds, and nine rounds of averaging is shown in\nFigure 5.13. Since our images have size 512× 512, nine rounds of averaging yields the Haar\ntransform, displayed as the image on the bottom right. The original image has completely\ndisappeared! We leave it as a fun exercise to modify the algorithms involving averaging and\ndifferencing to perform k rounds of averaging/differencing. The reconstruction algorithm is\na little tricky.\n\nA nice and easily accessible account of wavelets and their uses in image processing and\ncomputer graphics can be found in Stollnitz, Derose and Salesin [166]. A very detailed\naccount is given in Strang and and Nguyen [170], but this book assumes a fair amount of\nbackground in signal processing.\n\nWe can find easily a basis of 2n × 2n = 22n vectors wij (2n × 2n matrices) for the linear\nmap that reconstructs an image from its Haar coefficients, in the sense that for any 2n × 2n\n\n\n\n5.5. HAAR TRANSFORM FOR DIGITAL IMAGES 147\n\nFigure 5.12: Original drawing by Durer.\n\nmatrix C of Haar coefficients, the image matrix A is given by\n\nA =\n2n∑\ni=1\n\n2n∑\nj=1\n\ncijwij.\n\nIndeed, the matrix wij is given by the so-called outer product\n\nwij = wi(wj)\n>.\n\nSimilarly, there is a basis of 2n × 2n = 22n vectors hij (2n × 2n matrices) for the 2D Haar\ntransform, in the sense that for any 2n × 2n matrix A, its matrix C of Haar coefficients is\ngiven by\n\nC =\n2n∑\ni=1\n\n2n∑\nj=1\n\naijhij.\n\nIf the columns of W−1 are w′1, . . . , w\n′\n2n , then\n\nhij = w′i(w\n′\nj)\n>.\n\nWe leave it as exercise to compute the bases (wij) and (hij) for n = 2, and to display the\ncorresponding images using the command imagesc.\n\n\n\n148 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nFigure 5.13: Haar tranforms after one, two, three, and nine rounds of averaging.\n\n\n\n5.6. HADAMARD MATRICES 149\n\n5.6 Hadamard Matrices\n\nThere is another famous family of matrices somewhat similar to Haar matrices, but these\nmatrices have entries +1 and −1 (no zero entries).\n\nDefinition 5.6. A real n × n matrix H is a Hadamard matrix if hij = ±1 for all i, j such\nthat 1 ≤ i, j ≤ n and if\n\nH>H = nIn.\n\nThus the columns of a Hadamard matrix are pairwise orthogonal. Because H is a square\nmatrix, the equation H>H = nIn shows that H is invertible, so we also have HH> = nIn.\nThe following matrices are example of Hadamard matrices:\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n, H4 =\n\n\n1 1 1 1\n1 −1 1 −1\n1 1 −1 −1\n1 −1 −1 1\n\n ,\n\nand\n\nH8 =\n\n\n\n1 1 1 1 1 1 1 1\n1 −1 1 −1 1 −1 1 −1\n1 1 −1 −1 1 1 −1 −1\n1 −1 −1 1 1 −1 −1 1\n1 1 1 1 −1 −1 −1 −1\n1 −1 1 −1 −1 1 −1 1\n1 1 −1 −1 −1 −1 1 1\n1 −1 −1 1 −1 1 1 −1\n\n\n.\n\nA natural question is to determine the positive integers n for which a Hadamard matrix\nof dimension n exists, but surprisingly this is an open problem. The Hadamard conjecture is\nthat for every positive integer of the form n = 4k, there is a Hadamard matrix of dimension\nn.\n\nWhat is known is a necessary condition and various sufficient conditions.\n\nTheorem 5.1. If H is an n×n Hadamard matrix, then either n = 1, 2, or n = 4k for some\npositive integer k.\n\nSylvester introduced a family of Hadamard matrices and proved that there are Hadamard\nmatrices of dimension n = 2m for all m ≥ 1 using the following construction.\n\nProposition 5.2. (Sylvester, 1867) If H is a Hadamard matrix of dimension n, then the\nblock matrix of dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\n\n\n150 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nIf we start with\n\nH2 =\n\n(\n1 1\n1 −1\n\n)\n,\n\nwe obtain an infinite family of symmetric Hadamard matrices usually called Sylvester–\nHadamard matrices and denoted by H2m . The Sylvester–Hadamard matrices H2, H4 and\nH8 are shown on the previous page.\n\nIn 1893, Hadamard gave examples of Hadamard matrices for n = 12 and n = 20. At the\npresent, Hadamard matrices are known for all n = 4k ≤ 1000, except for n = 668, 716, and\n892.\n\nHadamard matrices have various applications to error correcting codes, signal processing,\nand numerical linear algebra; see Seberry, Wysocki and Wysocki [152] and Tropp [175]. For\nexample, there is a code based on H32 that can correct 7 errors in any 32-bit encoded block,\nand can detect an eighth. This code was used on a Mariner spacecraft in 1969 to transmit\npictures back to the earth.\n\nFor every m ≥ 0, the piecewise affine functions plf((H2m)i) associated with the 2m rows\nof the Sylvester–Hadamard matrix H2m are functions on [0, 1] known as the Walsh functions .\nIt is customary to index these 2m functions by the integers 0, 1, . . . , 2m−1 in such a way that\nthe Walsh function Wal(k, t) is equal to the function plf((H2m)i) associated with the Row i\nof H2m that contains k changes of signs between consecutive groups of +1 and consecutive\ngroups of −1. For example, the fifth row of H8, namely(\n\n1 −1 −1 1 1 −1 −1 1\n)\n,\n\nhas five consecutive blocks of +1s and −1s, four sign changes between these blocks, and thus\nis associated with Wal(4, t). In particular, Walsh functions corresponding to the rows of H8\n\n(from top down) are:\n\nWal(0, t), Wal(7, t), Wal(3, t), Wal(4, t), Wal(1, t), Wal(6, t), Wal(2, t), Wal(5, t).\n\nBecause of the connection between Sylvester–Hadamard matrices and Walsh functions,\nSylvester–Hadamard matrices are called Walsh–Hadamard matrices by some authors. For\nevery m, the 2m Walsh functions are pairwise orthogonal. The countable set of Walsh\nfunctions Wal(k, t) for all m ≥ 0 and all k such that 0 ≤ k ≤ 2m − 1 can be ordered in\nsuch a way that it is an orthogonal Hilbert basis of the Hilbert space L2([0, 1)]; see Seberry,\nWysocki and Wysocki [152].\n\nThe Sylvester–Hadamard matrix H2m plays a role in various algorithms for dimension\nreduction and low-rank matrix approximation. There is a type of structured dimension-\nreduction map known as the subsampled randomized Hadamard transform, for short SRHT;\nsee Tropp [175] and Halko, Martinsson and Tropp [86]. For ` � n = 2m, an SRHT matrix\nis an `× n matrix of the form\n\nΦ =\n\n√\nn\n\n`\nRHD,\n\nwhere\n\n\n\n5.7. SUMMARY 151\n\n1. D is a random n× n diagonal matrix whose entries are independent random signs.\n\n2. H = n−1/2Hn, a normalized Sylvester–Hadamard matrix of dimension n.\n\n3. R is a random ` × n matrix that restricts an n-dimensional vector to ` coordinates,\nchosen uniformly at random.\n\nIt is explained in Tropp [175] that for any input x such that ‖x‖2 = 1, the probability\n\nthat |(HDx)i| ≥\n√\nn−1 log(n) for any i is quite small. Thus HD has the effect of “flattening”\n\nthe input x. The main result about the SRHT is that it preserves the geometry of an entire\nsubspace of vectors; see Tropp [175] (Theorem 1.3).\n\n5.7 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Haar basis vectors and a glimpse at Haar wavelets .\n\n• Kronecker product (or tensor product) of matrices.\n\n• Hadamard and Sylvester–Hadamard matrices.\n\n• Walsh functions.\n\n5.8 Problems\n\nProblem 5.1. (Haar extravaganza) Consider the matrix\n\nW3,3 =\n\n\n\n1 0 0 0 1 0 0 0\n1 0 0 0 −1 0 0 0\n0 1 0 0 0 1 0 0\n0 1 0 0 0 −1 0 0\n0 0 1 0 0 0 1 0\n0 0 1 0 0 0 −1 0\n0 0 0 1 0 0 0 1\n0 0 0 1 0 0 0 −1\n\n\n.\n\n(1) Show that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,3c of applying\nW3,3 to c is\n\nW3,3c = (c1 + c5, c1 − c5, c2 + c6, c2 − c6, c3 + c7, c3 − c7, c4 + c8, c4 − c8),\n\nthe last step in reconstructing a vector from its Haar coefficients.\n\n\n\n152 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\n(2) Prove that the inverse of W3,3 is (1/2)W>\n3,3. Prove that the columns and the rows of\n\nW3,3 are orthogonal.\n\n(3) Let W3,2 and W3,1 be the following matrices:\n\nW3,2 =\n\n\n\n1 0 1 0 0 0 0 0\n1 0 −1 0 0 0 0 0\n0 1 0 1 0 0 0 0\n0 1 0 −1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n, W3,1 =\n\n\n\n1 1 0 0 0 0 0 0\n1 −1 0 0 0 0 0 0\n0 0 1 0 0 0 0 0\n0 0 0 1 0 0 0 0\n0 0 0 0 1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 0\n0 0 0 0 0 0 0 1\n\n\n.\n\nShow that given any vector c = (c1, c2, c3, c4, c5, c6, c7, c8), the result W3,2c of applying W3,2\n\nto c is\nW3,2c = (c1 + c3, c1 − c3, c2 + c4, c2 − c4, c5, c6, c7, c8),\n\nthe second step in reconstructing a vector from its Haar coefficients, and the result W3,1c of\napplying W3,1 to c is\n\nW3,1c = (c1 + c2, c1 − c2, c3, c4, c5, c6, c7, c8),\n\nthe first step in reconstructing a vector from its Haar coefficients.\n\nConclude that\nW3,3W3,2W3,1 = W3,\n\nthe Haar matrix\n\nW3 =\n\n\n\n1 1 1 0 1 0 0 0\n1 1 1 0 −1 0 0 0\n1 1 −1 0 0 1 0 0\n1 1 −1 0 0 −1 0 0\n1 −1 0 1 0 0 1 0\n1 −1 0 1 0 0 −1 0\n1 −1 0 −1 0 0 0 1\n1 −1 0 −1 0 0 0 −1\n\n\n.\n\nHint . First check that\n\nW3,2W3,1 =\n\n(\nW2 04,4\n\n04,4 I4\n\n)\n,\n\nwhere\n\nW2 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n .\n\n\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W3,2 and W3,1 are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W−1\n\n3 are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W−1\n\n3 orthogonal? Find the inverse of W3,2 and\nthe inverse of W3,1.\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n ≥ 2, the 2n × 2n matrix Wn,n is obtained form the two rows\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n, 1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\n,−1, 0, . . . , 0︸ ︷︷ ︸\n2n−1\n\nby shifting them 2n−1 − 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (c1, c2, . . . , c2n), show that Wn,nc is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W−1\n\nn,n = (1/2)W>\nn,n,\n\nand that the columns and the rows of Wn,n are orthogonal.\n\n(2) Given a m× n matrix A = (aij) and a p× q matrix B = (bij), the Kronecker product\n(or tensor product) A⊗B of A and B is the mp× nq matrix\n\nA⊗B =\n\n\na11B a12B · · · a1nB\na21B a22B · · · a2nB\n\n...\n...\n\n. . .\n...\n\nam1B am2B · · · amnB\n\n .\n\nIt can be shown (and you may use these facts without proof) that ⊗ is associative and that\n\n(A⊗B)(C ⊗D) = AC ⊗BD\n(A⊗B)> = A> ⊗B>,\n\nwhenever AC and BD are well defined.\n\nCheck that\n\nWn,n =\n\n(\nI2n−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n,\n\nand that\n\nWn =\n\n(\nWn−1 ⊗\n\n(\n1\n1\n\n)\nI2n−1 ⊗\n\n(\n1\n−1\n\n))\n.\n\nUse the above to reprove that\n\nWn,nW\n>\nn,n = 2I2n .\n\n5.8. PROBLEMS 153\n\n(4) Prove that the columns and the rows of W35 and W3, are orthogonal. Deduce from\nthis that the columns of W3 are orthogonal, and the rows of W; ' are orthogonal. Are the\nrows of W3 orthogonal? Are the columns of W;' orthogonal? Find the inverse of W3,9 and\nthe inverse of Ws).\n\nProblem 5.2. This is a continuation of Problem 5.1.\n\n(1) For any n > 2, the 2” x 2” matrix W,,,, is obtained form the two rows\n\nwe we\n\ngn-1 gn-1\n1,0,.. ,0, —1, 0, ,0\n\na ww\n\ngn-1 gn—-1\n\nby shifting them 2”~! — 1 times over to the right by inserting a zero on the left each time.\n\nGiven any vector c = (C1, C2,...,C2n), show that W,,,c is the result of the last step in the\nprocess of reconstructing a vector from its Haar coefficients c. Prove that W,,, = (1/2)W,),,\nand that the columns and the rows of W,,,, are orthogonal.\n\n(2) Given am x n matrix A = (a;;) and a p x q matrix B = (b;;), the Kronecker product\n(or tensor product) A ® B of A and B is the mp x nq matrix\n\nayiB aygB -:+ ayy,B\nAg pa | OW CR G08\nAmB Am2B vee AmnP\n\nIt can be shown (and you may use these facts without proof) that @ is associative and that\n\n(A@ B)(C®D) =AC @ BD\n(A@B)'=A' @B',\n\nwhenever AC’ and BD are well defined.\n\nCheck that\n1 1\nHan=(t09(2) tove(4)),\n1 1\nHis (tise(?) tes0(3)),\n\nUse the above to reprove that\n\nand that\n\nWrnW. n= 2lon.\n\n\n\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n\nB1 = 2\n\n(\n1 0\n0 1\n\n)\n=\n\n(\n2 0\n0 2\n\n)\nand for n ≥ 1,\n\nBn+1 = 2\n\n(\nBn 0\n0 I2n\n\n)\n.\n\nProve that\nW>\nn Wn = Bn, for all n ≥ 1.\n\n(3) The matrix Wn,i is obtained from the matrix Wi,i (1 ≤ i ≤ n− 1) as follows:\n\nWn,i =\n\n(\nWi,i 02i,2n−2i\n\n02n−2i,2i I2n−2i\n\n)\n.\n\nIt consists of four blocks, where 02i,2n−2i and 02n−2i,2i are matrices of zeros and I2n−2i is the\nidentity matrix of dimension 2n − 2i.\n\nExplain what Wn,i does to c and prove that\n\nWn,nWn,n−1 · · ·Wn,1 = Wn,\n\nwhere Wn is the Haar matrix of dimension 2n.\n\nHint . Use induction on k, with the induction hypothesis\n\nWn,kWn,k−1 · · ·Wn,1 =\n\n(\nWk 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProve that the columns and rows of Wn,k are orthogonal, and use this to prove that the\ncolumns of Wn and the rows of W−1\n\nn are orthogonal. Are the rows of Wn orthogonal? Are\nthe columns of W−1\n\nn orthogonal? Prove that\n\nW−1\nn,k =\n\n(\n1\n2\nW>\nk,k 02k,2n−2k\n\n02n−2k,2k I2n−2k\n\n)\n.\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\nof dimension 2n, (\n\nH H\nH −H\n\n)\n,\n\nis a Hadamard matrix.\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k, t) for k = 0, 1, . . . , 7.\n\nProblem 5.5. Describe a recursive algorithm to compute the productH2m x of the Sylvester–\nHadamard matrix H2m by a vector x ∈ R2m that uses m recursive calls.\n\n154 CHAPTER 5. HAAR BASES, HAAR WAVELETS, HADAMARD MATRICES\n\nLet\n1 0 2 0\nm=2(5 t)= (0 3)\nand for n > 1,\nB, 0\nBri = 2 ( 0 .)\nProve that\n\nW/W, =Bn, for alln>1.\n\n(3) The matrix W,,; is obtained from the matrix W;; (1 <7 <n -—1) as follows:\n\nWri = ( Wii Man)\n\nQon_9i, 24 Ton _9i\n\nIt consists of four blocks, where 09: 9n_9i and Ogn_9i 9: are matrices of zeros and Jn_»: is the\nidentity matrix of dimension 2” — 2°.\n\nExplain what W,,; does to c and prove that\nWrinWrn—1 ute Writ = Wr,\n\nwhere W,, is the Haar matrix of dimension 2”.\n\nHint. Use induction on k, with the induction hypothesis\n\nWrrWnr pis Wra = ( We 2)\n\nOon_9k 9k Ton _9k\n\nProve that the columns and rows of W,,, are orthogonal, and use this to prove that the\ncolumns of W,, and the rows of W,! are orthogonal. Are the rows of W,, orthogonal? Are\nthe columns of W,-! orthogonal? Prove that\n\n1 T\nw-! _ 5W ik Ook on_9k\nnk .\n\nOon_9k 9k Ton _9k\n\nProblem 5.3. Prove that if H is a Hadamard matrix of dimension n, then the block matrix\n\nof dimension 2n,\nH H\nH —-H]’\n\nProblem 5.4. Plot the graphs of the eight Walsh functions Wal(k,t) for k = 0,1,..., 7.\n\nis a Hadamard matrix.\n\nProblem 5.5. Describe a recursive algorithm to compute the product Hom x of the Sylvester—\nHadamard matrix Hym by a vector x € R?” that uses m recursive calls.\n\n\n\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E\n\n∐\nF of two vector spaces using the cartesian\n\nproduct E × F , we don’t quite get the right notion because elements of E × F are ordered\npairs, but we want E\n\n∐\nF = F\n\n∐\nE. Thus, we want to think of the elements of E\n\n∐\nF as\n\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(Ei)i∈{1,2}, and more generally of a family (Ei)i∈I . For simplicity, we begin by considering\nthe case where I = {1, 2}.\nDefinition 6.1. Given a family (Ei)i∈{1,2} of two vector spaces, we define the (external)\ndirect sum E1\n\n∐\nE2 (or coproduct) of the family (Ei)i∈{1,2} as the set\n\nE1\n\n∐\nE2 = {{〈1, u〉, 〈2, v〉} | u ∈ E1, v ∈ E2},\n\nwith addition\n\n{〈1, u1〉, 〈2, v1〉}+ {〈1, u2〉, 〈2, v2〉} = {〈1, u1 + u2〉, 〈2, v1 + v2〉},\nand scalar multiplication\n\nλ{〈1, u〉, 〈2, v〉} = {〈1, λu〉, 〈2, λv〉}.\nWe define the injections in1 : E1 → E1\n\n∐\nE2 and in2 : E2 → E1\n\n∐\nE2 as the linear maps\n\ndefined such that,\nin1(u) = {〈1, u〉, 〈2, 0〉},\n\nand\nin2(v) = {〈1, 0〉, 〈2, v〉}.\n\n155\n\nChapter 6\n\nDirect Sums\n\nIn this chapter all vector spaces are defined over an arbitrary field AK. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n6.1 Sums, Direct Sums, Direct Products\n\nThere are some useful ways of forming new vector spaces from older ones, in particular,\ndirect products and direct sums. Regarding direct sums, there is a subtle point, which is\nthat if we attempt to define the direct sum E' || F of two vector spaces using the cartesian\nproduct E x F’, we don’t quite get the right notion because elements of EF x F' are ordered\npairs, but we want E || F = F [| £. Thus, we want to think of the elements of E'[] F as\nunordrered pairs of elements. It is possible to do so by considering the direct sum of a family\n(E;)iefi2}, and more generally of a family (£;)ier. For simplicity, we begin by considering\nthe case where I = {1,2}.\n\nDefinition 6.1. Given a family (F;)ic{1,2} of two vector spaces, we define the (external)\ndirect sum E\\ [| E, (or coproduct) of the family (E;):e(1,2} as the set\n\nE, [| & = {{(1,u), (2,0)} we Ey, v € By},\nwith addition\n{(1, u1), (2, v1) } + {(1, U2), (2, v2) } = {(1, U1 + U2), (2, Ut + V2) },\n\nand scalar multiplication\n\nA{(1,u), (2,v)} = {(1, Nu), (2, rv) }-\n\nWe define the injections inj: Ey > E, [|] EF, and ing: Ey > E, [|] FE as the linear maps\n\ndefined such that,\niny(u) = {(1, u), (2, 0),\n\nand\n\ning(v) = {(1, 0), (2, v)}.\n\n155\n\n\n\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\n\nE2\n\n∐\nE1 = {{〈2, v〉, 〈1, u〉} | v ∈ E2, u ∈ E1} = E1\n\n∐\nE2.\n\nThus, every member {〈1, u〉, 〈2, v〉} of E1\n\n∐\nE2 can be viewed as an unordered pair consisting\n\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, E1\n\n∐\nE2 is just the product\n\n∏\ni∈{1,2}Ei of the family (Ei)i∈{1,2}.\n\n� This is not to be confused with the cartesian product E1×E2. The vector space E1×E2\n\nis the set of all ordered pairs 〈u, v〉, where u ∈ E1, and v ∈ E2, with addition and\nmultiplication by a scalar defined such that\n\n〈u1, v1〉+ 〈u2, v2〉 = 〈u1 + u2, v1 + v2〉,\nλ〈u, v〉 = 〈λu, λv〉.\n\nThere is a bijection between\n∏\n\ni∈{1,2}Ei and E1 × E2, but as we just saw, elements of∏\ni∈{1,2}Ei are certain sets. The product E1 × · · · × En of any number of vector spaces\n\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E1 and E2, the set E1\n\n∐\nE2 is a vector\n\nspace. For every pair of linear maps, f : E1 → G and g : E2 → G, there is a unique linear\nmap, f + g : E1\n\n∐\nE2 → G, such that (f + g) ◦ in1 = f and (f + g) ◦ in2 = g, as in the\n\nfollowing diagram:\nE1\n\nin1\n\n��\n\nf\n\n''PPPPPPPPPPPPPPPP\n\nE1\n\n∐\nE2\n\nf+g // G\n\nE2\n\nin2\n\nOO\n\ng\n\n77nnnnnnnnnnnnnnnn\n\nProof. Define\n(f + g)({〈1, u〉, 〈2, v〉}) = f(u) + g(v),\n\nfor every u ∈ E1 and v ∈ E2. It is immediately verified that f + g is the unique linear map\nwith the required properties.\n\nWe already noted that E1\n\n∐\nE2 is in bijection with E1 ×E2. If we define the projections\n\nπ1 : E1\n\n∐\nE2 → E1 and π2 : E1\n\n∐\nE2 → E2, such that\n\nπ1({〈1, u〉, 〈2, v〉}) = u,\n\nand\nπ2({〈1, u〉, 〈2, v〉}) = v,\n\nwe have the following proposition.\n\n156 CHAPTER 6. DIRECT SUMS\n\nNote that\nFy] ] FE. = {{(2,v), (l.u)} |v € Be, we BY} = E, [] F.\n\nThus, every member {(1, wu), (2,v)} of E [] £2 can be viewed as an unordered pair consisting\nof the two vectors u and v, tagged with the index 1 and 2, respectively.\n\nRemark: In fact, FE, [[ E> is just the product [Hien.23 EF; of the family (2; )ieg2}-\n\n© This is not to be confused with the cartesian product EF, x E,. The vector space EF; x Es\nis the set of all ordered pairs (u,v), where u € Fy, and v € £5, with addition and\nmultiplication by a scalar defined such that\n\n(U1, U1) + (Ug, V2) = (uy + Ug, U1 + V2),\nA(u,v) = (Au, Av).\n\nThere is a bijection between [];. 12} E, and E, x E,, but as we just saw, elements of\n\nIL- 4.2} Ej, are certain sets. The product FE, x --- x E, of any number of vector spaces\ncan also be defined. We will do this shortly.\n\nThe following property holds.\n\nProposition 6.1. Given any two vector spaces, E, and E>, the set E, [| E> is a vector\nspace. For every pair of linear maps, f: FE, >~ G and g: Ep > G, there is a unique linear\nmap, f +g: E, [|] Ek, - G, such that (f +g) coin, = f and (f + 9) cing = g, as in the\nfollowing diagram:\n\nEy\namy\nEi Il Ey f+g G\nin| a\nEy\n\nProof. Define\n(f + 9) ACL, u), (2,0)}) = Flu) + g(r),\n\nfor every u € FE; and v € Eg. It is immediately verified that f + g is the unique linear map\nwith the required properties. im\n\nWe already noted that FE, [| £2 is in bijection with FE, x E>. If we define the projections\nTy: Fy [| £2 —> EF, and T9: EF, [| £2 > Es, such that\n\nm({(1,u), (2, v)}) =U,\nand\nmo({(1, u), (2, v)}) =v,\n\nwe have the following proposition.\n\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E1 and E2, for every pair of linear maps,\nf : D → E1 and g : D → E2, there is a unique linear map, f × g : D → E1\n\n∐\nE2, such that\n\nπ1 ◦ (f × g) = f and π2 ◦ (f × g) = g, as in the following diagram:\n\nE1\n\nD\nf×g //\n\nf\n\n77nnnnnnnnnnnnnnnn\n\ng\n((PPPPPPPPPPPPPPPP E1\n\n∐\nE2\n\nπ1\n\nOO\n\nπ2\n\n��\nE2\n\nProof. Define\n(f × g)(w) = {〈1, f(w)〉, 〈2, g(w)〉},\n\nfor every w ∈ D. It is immediately verified that f × g is the unique linear map with the\nrequired properties.\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U, V are subspaces of a vector space E, letting i1 : U → E and i2 : V → E be the\ninclusion maps, if U\n\n∐\nV is isomomorphic to E under the map i1 + i2 given by Proposition\n\n6.1, we say that E is a direct sum of U and V , and we write E = U\n∐\nV (with a slight abuse\n\nof notation, since E and U\n∐\nV are only isomorphic). It is also convenient to define the sum\n\nU1 + · · ·+ Up and the internal direct sum U1 ⊕ · · · ⊕ Up of any number of subspaces of E.\n\nDefinition 6.2. Given p ≥ 2 vector spaces E1, . . . , Ep, the product F = E1 × · · · × Ep can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(u1, . . . , up) + (v1, . . . , vp) = (u1 + v1, . . . , up + vp)\n\nλ(u1, . . . , up) = (λu1, . . . , λup),\n\nfor all ui, vi ∈ Ei and all λ ∈ R. The zero vector of E1 × · · · × Ep is the p-tuple\n\n( 0, . . . , 0︸ ︷︷ ︸\np\n\n),\n\nwhere the ith zero is the zero vector of Ei.\n\nWith the above addition and multiplication, the vector space F = E1× · · ·×Ep is called\nthe direct product of the vector spaces E1, . . . , Ep.\n\nAs a special case, when E1 = · · · = Ep = R, we find again the vector space F = Rp. The\nprojection maps pri : E1 × · · · × Ep → Ei given by\n\npri(u1, . . . , up) = ui\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 157\n\nProposition 6.2. Given any two vector spaces, E, and E>, for every pair of linear maps,\nf: D> E, and g: D> En, there is a unique linear map, f x g: D > E, [| Es, such that\n™0(f xg) =f andm0(f x g) = 4, as in the following diagram:\n\nProof. Define\n(f x g)(w) = {C1 F(w)), (2; g(w)) fs\n\nfor every w € D. It is immediately verified that f x g is the unique linear map with the\nrequired properties. C]\n\nRemark: It is a peculiarity of linear algebra that direct sums and products of finite families\nare isomorphic. However, this is no longer true for products and sums of infinite families.\n\nWhen U,V are subspaces of a vector space E, letting i;: U > E and 12: V > E be the\ninclusion maps, if U]]V is isomomorphic to F under the map 7; + 22 given by Proposition\n6.1, we say that E is a direct sum of U and V, and we write E = U|[V (with a slight abuse\nof notation, since F and U [| V are only isomorphic). It is also convenient to define the sum\nU, +---+U, and the internal direct sum U; © --- @ U, of any number of subspaces of F.\n\nDefinition 6.2. Given p > 2 vector spaces E},...,E,, the product F' = EF, x --- x E, can\nbe made into a vector space by defining addition and scalar multiplication as follows:\n\n(U1,.--,Up) + (U1,---, Up) = (Ur + U1, ---, Up + Up)\n\nA(u1,-+-,Up) = (Au, --, AUp),\n\nfor all u;,v; € E; and all A € R. The zero vector of EF, x --- x E, is the p-tuple\n\nwhere the ith zero is the zero vector of E;.\n\nWith the above addition and multiplication, the vector space fF’ = LE) x --- x E, is called\nthe direct product of the vector spaces F),..., Ep.\n\nAs a special case, when EF, = --- = E, = R, we find again the vector space F = R?. The\nprojection maps pr;: Ey x --- x E, + E; given by\n\npri(uy,...,Up) = Uj\n\n\n\n\n158 CHAPTER 6. DIRECT SUMS\n\nare clearly linear. Similarly, the maps ini : Ei → E1 × · · · × Ep given by\n\nini(ui) = (0, . . . , 0, ui, 0, . . . , 0)\n\nare injective and linear. If dim(Ei) = ni and if (ei1, . . . , e\ni\nni\n\n) is a basis of Ei for i = 1, . . . , p,\nthen it is easy to see that the n1 + · · ·+ np vectors\n\n(e1\n1, 0, . . . , 0), . . . , (e1\n\nn1\n, 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ei1, 0, . . . , 0), . . . , (0, . . . , 0, eini , 0, . . . , 0),\n\n...\n...\n\n...\n(0, . . . , 0, ep1), . . . , (0, . . . , 0, epnp)\n\nform a basis of E1 × · · · × Ep, and so\n\ndim(E1 × · · · × Ep) = dim(E1) + · · ·+ dim(Ep).\n\nLet us now consider a vector space E and p subspaces U1, . . . , Up of E. We have a map\n\na : U1 × · · · × Up → E\n\ngiven by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p. It is clear that this map is linear, and so its image is a subspace\nof E denoted by\n\nU1 + · · ·+ Up\n\nand called the sum of the subspaces U1, . . . , Up. By definition,\n\nU1 + · · ·+ Up = {u1 + · · ·+ up | ui ∈ Ui, 1 ≤ i ≤ p},\n\nand it is immediately verified that U1 + · · · + Up is the smallest subspace of E containing\nU1, . . . , Up. This also implies that U1 + · · ·+ Up does not depend on the order of the factors\nUi; in particular,\n\nU1 + U2 = U2 + U1.\n\nDefinition 6.3. For any vector space E and any p ≥ 2 subspaces U1, . . . , Up of E, if the\nmap a defined above is injective, then the sum U1 + · · ·+ Up is called a direct sum and it is\ndenoted by\n\nU1 ⊕ · · · ⊕ Up.\nThe space E is the direct sum of the subspaces Ui if\n\nE = U1 ⊕ · · · ⊕ Up.\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 159\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1.\n\nIf the map a is injective, then by Proposition 3.14 we have Ker a = {( 0, . . . , 0︸ ︷︷ ︸\np\n\n)} where\n\neach 0 is the zero vector of E, which means that if ui ∈ Ui for i = 1, . . . , p and if\n\nu1 + · · ·+ up = 0,\n\nthen (u1, . . . , up) = (0, . . . , 0), that is, u1 = 0, . . . , up = 0.\n\nProposition 6.3. If the map a : U1×· · ·×Up → E is injective, then every u ∈ U1 + · · ·+Up\nhas a unique expression as a sum\n\nu = u1 + · · ·+ up,\n\nwith ui ∈ Ui, for i = 1, . . . , p.\n\nProof. If\nu = v1 + · · ·+ vp = w1 + · · ·+ wp,\n\nwith vi, wi ∈ Ui, for i = 1, . . . , p, then we have\n\nw1 − v1 + · · ·+ wp − vp = 0,\n\nand since vi, wi ∈ Ui and each Ui is a subspace, wi−vi ∈ Ui. The injectivity of a implies that\nwi−vi = 0, that is, wi = vi for i = 1, . . . , p, which shows the uniqueness of the decomposition\nof u.\n\nProposition 6.4. If the map a : U1 × · · · ×Up → E is injective, then any p nonzero vectors\nu1, . . . , up with ui ∈ Ui are linearly independent.\n\nProof. To see this, assume that\n\nλ1u1 + · · ·+ λpup = 0\n\nfor some λi ∈ R. Since ui ∈ Ui and Ui is a subspace, λiui ∈ Ui, and the injectivity of a\nimplies that λiui = 0, for i = 1, . . . , p. Since ui 6= 0, we must have λi = 0 for i = 1, . . . , p;\nthat is, u1, . . . , up with ui ∈ Ui and ui 6= 0 are linearly independent.\n\nObserve that if a is injective, then we must have Ui ∩Uj = (0) whenever i 6= j. However,\nthis condition is generally not sufficient if p ≥ 3. For example, if E = R2 and U1 the line\nspanned by e1 = (1, 0), U2 is the line spanned by d = (1, 1), and U3 is the line spanned by\ne2 = (0, 1), then U1∩U2 = U1∩U3 = U2∩U3 = {(0, 0)}, but U1+U2 = U1+U3 = U2+U3 = R2,\nso U1 + U2 + U3 is not a direct sum. For example, d is expressed in two different ways as\n\nd = (1, 1) = (1, 0) + (0, 1) = e1 + e2.\n\n\n\n160 CHAPTER 6. DIRECT SUMS\n\nSee Figure 6.1.\n\ne1\nU1\n\ne\n\nU3\n\n2 (1,1)\n\nU2\n\nFigure 6.1: The linear subspaces U1, U2, and U3 illustrated as lines in R2.\n\nAs in the case of a sum, U1 ⊕ U2 = U2 ⊕ U1. Observe that when the map a is injective,\nthen it is a linear isomorphism between U1 × · · · × Up and U1 ⊕ · · · ⊕ Up. The difference is\nthat U1 × · · · × Up is defined even if the spaces Ui are not assumed to be subspaces of some\ncommon space.\n\nIf E is a direct sum E = U1⊕· · ·⊕Up, since any p nonzero vectors u1, . . . , up with ui ∈ Ui\nare linearly independent, if we pick a basis (uk)k∈Ij in Uj for j = 1, . . . , p, then (ui)i∈I with\nI = I1 ∪ · · · ∪ Ip is a basis of E. Intuitively, E is split into p independent subspaces.\n\nConversely, given a basis (ui)i∈I of E, if we partition the index set I as I = I1 ∪ · · · ∪ Ip,\nthen each subfamily (uk)k∈Ij spans some subspace Uj of E, and it is immediately verified\nthat we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\nDefinition 6.4. Let f : E → E be a linear map. For any subspace U of E, if f(U) ⊆ U we\nsay that U is invariant under f .\n\nAssume that E is finite-dimensional, a direct sum E = U1 ⊕ · · · ⊕ Up, and that each Uj\nis invariant under f . If we pick a basis (ui)i∈I as above with I = I1 ∪ · · · ∪ Ip and with\neach (uk)k∈Ij a basis of Uj, since each Uj is invariant under f , the image f(uk) of every basis\nvector uk with k ∈ Ij belongs to Uj, so the matrix A representing f over the basis (ui)i∈I is\na block diagonal matrix of the form\n\nA =\n\n\nA1\n\nA2\n\n. . .\n\nAp\n\n ,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 161\n\nwith each block Aj a dj × dj-matrix with dj = dim(Uj) and all other entries equal to 0. If\ndj = 1 for j = 1, . . . , p, the matrix A is a diagonal matrix.\n\nThere are natural injections from each Ui to E denoted by ini : Ui → E.\n\nNow, if p = 2, it is easy to determine the kernel of the map a : U1 × U2 → E. We have\n\na(u1, u2) = u1 + u2 = 0 iff u1 = −u2, u1 ∈ U1, u2 ∈ U2,\n\nwhich implies that\n\nKer a = {(u,−u) | u ∈ U1 ∩ U2}.\nNow, U1 ∩ U2 is a subspace of E and the linear map u 7→ (u,−u) is clearly an isomorphism\nbetween U1 ∩U2 and Ker a, so Ker a is isomorphic to U1 ∩U2. As a consequence, we get the\nfollowing result:\n\nProposition 6.5. Given any vector space E and any two subspaces U1 and U2, the sum\nU1 + U2 is a direct sum iff U1 ∩ U2 = (0).\n\nAn interesting illustration of the notion of direct sum is the decomposition of a square\nmatrix into its symmetric part and its skew-symmetric part. Recall that an n × n matrix\nA ∈ Mn is symmetric if A> = A, skew -symmetric if A> = −A. It is clear that s\n\nS(n) = {A ∈ Mn | A> = A} and Skew(n) = {A ∈ Mn | A> = −A}\n\nare subspaces of Mn, and that S(n)∩Skew(n) = (0). Observe that for any matrix A ∈ Mn,\nthe matrix H(A) = (A + A>)/2 is symmetric and the matrix S(A) = (A − A>)/2 is skew-\nsymmetric. Since\n\nA = H(A) + S(A) =\nA+ A>\n\n2\n+\nA− A>\n\n2\n,\n\nwe see that Mn = S(n) + Skew(n), and since S(n)∩Skew(n) = (0), we have the direct sum\n\nMn = S(n)⊕ Skew(n).\n\nRemark: The vector space Skew(n) of skew-symmetric matrices is also denoted by so(n).\nIt is the Lie algebra of the group SO(n).\n\nProposition 6.5 can be generalized to any p ≥ 2 subspaces at the expense of notation.\nThe proof of the following proposition is left as an exercise.\n\nProposition 6.6. Given any vector space E and any p ≥ 2 subspaces U1, . . . , Up, the fol-\nlowing properties are equivalent:\n\n(1) The sum U1 + · · ·+ Up is a direct sum.\n\n\n\n162 CHAPTER 6. DIRECT SUMS\n\n(2) We have\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0), i = 1, . . . , p.\n\n(3) We have\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nBecause of the isomorphism\n\nU1 × · · · × Up ≈ U1 ⊕ · · · ⊕ Up,\n\nwe have\n\nProposition 6.7. If E is any vector space, for any (finite-dimensional) subspaces U1, . . .,\nUp of E, we have\n\ndim(U1 ⊕ · · · ⊕ Up) = dim(U1) + · · ·+ dim(Up).\n\nIf E is a direct sum\nE = U1 ⊕ · · · ⊕ Up,\n\nsince every u ∈ E can be written in a unique way as\n\nu = u1 + · · ·+ up\n\nwith ui ∈ Ui for i = 1 . . . , p, we can define the maps πi : E → Ui, called projections , by\n\nπi(u) = πi(u1 + · · ·+ up) = ui.\n\nIt is easy to check that these maps are linear and satisfy the following properties:\n\nπj ◦ πi =\n\n{\nπi if i = j\n\n0 if i 6= j,\n\nπ1 + · · ·+ πp = idE.\n\nFor example, in the case of the direct sum\n\nMn = S(n)⊕ Skew(n),\n\nthe projection onto S(n) is given by\n\nπ1(A) = H(A) =\nA+ A>\n\n2\n,\n\n\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nπ2(A) = S(A) =\nA− A>\n\n2\n.\n\nClearly, H(A)+S(A) = A, H(H(A)) = H(A), S(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that f ◦ f = f is said to be idempotent . Thus, the projections πi are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p ≥ 2 linear maps fi : E → E, if\n\nfj ◦ fi =\n\n{\nfi if i = j\n\n0 if i 6= j,\n\nf1 + · · ·+ fp = idE,\n\nthen if we let Ui = fi(E), we have a direct sum\n\nE = U1 ⊕ · · · ⊕ Up.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f : E → E is an idempotent linear map, i.e.,\nf ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set I. First,\nlet us recall the notion of the product of a family (Ei)i∈I . Given a family of sets (Ei)i∈I , its\nproduct\n\n∏\ni∈I Ei, is the set of all functions f : I → ⋃\n\ni∈I Ei, such that, f(i) ∈ Ei, for every\ni ∈ I. It is one of the many versions of the axiom of choice, that, if Ei 6= ∅ for every i ∈ I,\nthen\n\n∏\ni∈I Ei 6= ∅. A member f ∈ ∏i∈I Ei, is often denoted as (fi)i∈I . For every i ∈ I, we\n\nhave the projection πi :\n∏\n\ni∈I Ei → Ei, defined such that, πi((fi)i∈I) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let I be any nonempty set, and let (Ei)i∈I be a family of vector spaces.\nThe (external) direct sum\n\n∐\ni∈I Ei (or coproduct) of the family (Ei)i∈I is defined as follows:∐\n\ni∈I Ei consists of all f ∈ ∏i∈I Ei, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(fi)i∈I + (gi)i∈I = (fi + gi)i∈I ,\n\nλ(fi)i∈I = (λfi)i∈I .\n\nWe also have injection maps ini : Ei →\n∐\n\ni∈I Ei, defined such that, ini(x) = (fi)i∈I , where\nfi = x, and fj = 0, for all j ∈ (I − {i}).\n\n6.1. SUMS, DIRECT SUMS, DIRECT PRODUCTS 163\n\nand the projection onto Skew(n) is given by\n\nm™(A) = S(A) =\n\nClearly, H(A) +$(A) = A, H(H(A)) = H(A), $(S(A)) = S(A), and H(S(A)) = S(H(A)) =\n0.\n\nA function f such that fo f = f is said to be idempotent. Thus, the projections 7; are\nidempotent. Conversely, the following proposition can be shown:\n\nProposition 6.8. Let E be a vector space. For any p > 2 linear maps f;: E > E, if\n\nfi ifisj\nfo fi= “pe ye\n0 ift AF),\nfitet fp = ide,\nthen if we let U; = f,(E), we have a direct sum\n\nE=U,®-:®@U,.\n\nWe also have the following proposition characterizing idempotent linear maps whose proof\nis also left as an exercise.\n\nProposition 6.9. For every vector space E, if f: E > E is an idempotent linear map, 1.e.,\nfof=f, then we have a direct sum\n\nE=Kerf @lImf,\n\nso that f is the projection onto its image Im f.\n\nWe now give the definition of a direct sum for any arbitrary nonempty index set J. First,\nlet us recall the notion of the product of a family (£;);e7. Given a family of sets (F;)jer, its\nproduct [],-; i, is the set of all functions f: J > U,_, Ei, such that, f(i) € Ej, for every\ni € I. It is one of the many versions of the axiom of choice, that, if E; 4 @ for every 7 € J,\nthen [],<; £; 4 0. A member f € [J,-; Fi, is often denoted as (f;)ier. For every i € I, we\nhave the projection 7: [[,<; Ei + Ej, defined such that, m((fi)ier) = fi. We now define\ndirect sums.\n\nDefinition 6.5. Let J be any nonempty set, and let (F;)ic7 be a family of vector spaces.\nThe (external) direct sum [],-; E; (or coproduct) of the family (£;)ier is defined as follows:\n\nL<, £i consists of all f € [],<; i, which have finite support, and addition and multi-\nplication by a scalar are defined as follows:\n\n(flier + (Qi)ier = (fi + M)ier,\nA fidier = (Afidier-\n\nWe also have injection maps in;: E; > |],-; Fi, defined such that, in;(x) = (fi)ier, where\n\nfi =x, and f; =0, for all 7 € (J — {i}).\n\ntel\n\n\n\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (Ei)i∈I be a family of vector spaces, and\nlet G be any vector space. The direct sum\n\n∐\ni∈I Ei is a vector space, and for every family\n\n(hi)i∈I of linear maps hi : Ei → G, there is a unique linear map(∑\ni∈I\n\nhi\n\n)\n:\n∐\ni∈I\n\nEi → G,\n\nsuch that, (\n∑\n\ni∈I hi) ◦ ini = hi, for every i ∈ I.\n\nRemarks:\n\n(1) One might wonder why the direct sum\n∐\n\ni∈I Ei consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (Ei)i∈I as\n\n∏\ni∈I Ei? The product space\n\n∏\ni∈I Ei with addition and scalar\n\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map ĥ :\n\n∏\ni∈I Ei → G such that ĥ ◦ ini = hi for all ∈ I must be given by\n\nh̃((ui)∈I) =\n∑\ni∈I\n\nhi(ui),\n\nand if I is infinite, the sum on the right-hand side is infinite, and thus undefined! If I\nis finite then\n\n∏\ni∈I Ei and\n\n∐\ni∈I Ei are isomorphic.\n\n(2) When Ei = E, for all i ∈ I, we denote\n∐\n\ni∈I Ei by E(I). In particular, when Ei = K,\nfor all i ∈ I, we find the vector space K(I) of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f : E → F be a linear map. If\nf : E → F is injective, then there is a surjective linear map r : F → E called a retraction,\nsuch that r ◦ f = idE. See Figure 6.2. If f : E → F is surjective, then there is an injective\nlinear map s : F → E called a section, such that f ◦ s = idF . See Figure 6.3.\n\nProof. Let (ui)i∈I be a basis of E. Since f : E → F is an injective linear map, by Proposition\n3.15, (f(ui))i∈I is linearly independent in F . By Theorem 3.7, there is a basis (vj)j∈J of F ,\nwhere I ⊆ J , and where vi = f(ui), for all i ∈ I. By Proposition 3.15, a linear map r : F → E\ncan be defined such that r(vi) = ui, for all i ∈ I, and r(vj) = w for all j ∈ (J − I), where w\nis any given vector in E, say w = 0. Since r(f(ui)) = ui for all i ∈ I, by Proposition 3.15,\nwe have r ◦ f = idE.\n\nNow, assume that f : E → F is surjective. Let (vj)j∈J be a basis of F . Since f : E → F\nis surjective, for every vj ∈ F , there is some uj ∈ E such that f(uj) = vj. Since (vj)j∈J is a\nbasis of F , by Proposition 3.15, there is a unique linear map s : F → E such that s(vj) = uj.\nAlso, since f(s(vj)) = vj, by Proposition 3.15 (again), we must have f ◦ s = idF .\n\n164 CHAPTER 6. DIRECT SUMS\n\nThe following proposition is an obvious generalization of Proposition 6.1.\n\nProposition 6.10. Let I be any nonempty set, let (E;)icr be a family of vector spaces, and\nlet G be any vector space. The direct sum [],-,; Ei; 1s a vector space, and for every family\n(hi)icr of linear maps h;: E; + G, there is a unique linear map\n\n(Soni): []2-¢\n\n1EL tel\n\nsuch that, (Yoje, hi) 9 ini = hy, for every i € I.\n\nRemarks:\n\n(1) One might wonder why the direct sum [],_,; £; consists of familes of finite support\ninstead of arbitrary families; in other words, why didn’t we define the direct sum of\nthe family (E;)ier as [[,<; H;? The product space [],., £; with addition and scalar\nmultiplication defined as above is also a vector space but the problem is that any\nlinear map h: [Ler E; > G such that ho in; = h; for all € J must be given by\n\nh((ui)er) = >, hi(ws);\niel\nand if J is infinite, the sum on the right-hand side is infinite, and thus undefined! If /\n\nis finite then [[,-, £; and [],_,; £; are isomorphic.\n\n(2) When E; = £, for all i € I, we denote [],-, E; by E. In particular, when E; = K,\nfor all i € I, we find the vector space K“ of Definition 3.11.\n\nWe also have the following basic proposition about injective or surjective linear maps.\n\nProposition 6.11. Let E and F be vector spaces, and let f: E + F be a linear map. If\nf: E - F is injective, then there is a surjective linear map r: F — E called a retraction,\nsuch that ro f =idg. See Figure 6.2. If f: E > F 1s surjective, then there is an injective\nlinear map s: F — E called a section, such that fos =idp. See Figure 6.3.\n\nProof. Let (u;)ier be a basis of E. Since f: E — F is an injective linear map, by Proposition\n3.15, (f(us))ier is linearly independent in F. By Theorem 3.7, there is a basis (v;)jey of F,\nwhere J C J, and where v; = f(u;), for alli € J. By Proposition 3.15, a linear map r: F > EF\ncan be defined such that r(v;) = u;, for all i € I, and r(v;) = w for all j € (J — J), where w\nis any given vector in FE, say w = 0. Since r(f(u;)) = u; for all i € I, by Proposition 3.15,\nwe have ro f = idg.\n\nNow, assume that f: E — F is surjective. Let (v;)je7 be a basis of F. Since f: E> F\nis surjective, for every v; € F, there is some u; € E such that f(u;) = v;. Since (v;)jey is a\nbasis of F’, by Proposition 3.15, there is a unique linear map s: F' > E such that s(v;) = uj;.\nAlso, since f(s(v;)) = v;, by Proposition 3.15 (again), we must have f os = idp. O\n\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 165\n\nu = (1,0)\n1\n\nu = (1,1)\n2\n\nf(x,y) = (x,y,0)\n\nv = f(u ) = (1,0,0)1 1 v  = f(u ) = (1,1,0)2 2\n\nv = (0,0,1)\n3\n\nr(x,y,z) = (x,y)\nE = R\n\n2\n\nF = R3\n\nFigure 6.2: Let f : E → F be the injective linear map from R2 to R3 given by f(x, y) =\n(x, y, 0). Then a surjective retraction is given by r : R3 → R2 is given by r(x, y, z) = (x, y).\nObserve that r(v1) = u1, r(v2) = u2, and r(v3) = 0 .\n\nThe converse of Proposition 6.11 is obvious.\n\nWe are now ready to prove a very crucial result relating the rank and the dimension of\nthe kernel of a linear map.\n\n6.2 The Rank-Nullity Theorem; Grassmann’s Relation\n\nWe begin with the following fundamental proposition.\n\nProposition 6.12. Let E, F and G, be three vector spaces, f : E → F an injective linear\nmap, g : F → G a surjective linear map, and assume that Im f = Ker g. Then, the following\nproperties hold. (a) For any section s : G → F of g, we have F = Ker g ⊕ Im s, and the\nlinear map f + s : E ⊕G→ F is an isomorphism.1\n\n(b) For any retraction r : F → E of f , we have F = Im f ⊕Ker r.2\n\nE\nf //\n\nF\nr\noo\n\ng //\nG\n\ns\noo\n\nProof. (a) Since s : G→ F is a section of g, we have g ◦ s = idG, and for every u ∈ F ,\n\ng(u− s(g(u))) = g(u)− g(s(g(u))) = g(u)− g(u) = 0.\n\n1The existence of a section s : G→ F of g follows from Proposition 6.11.\n2The existence of a retraction r : F → E of f follows from Proposition 6.11.\n\n\n\n166 CHAPTER 6. DIRECT SUMS\n\nThus, u − s(g(u)) ∈ Ker g, and we have F = Ker g + Im s. On the other hand, if u ∈\nKer g ∩ Im s, then u = s(v) for some v ∈ G because u ∈ Im s, g(u) = 0 because u ∈ Ker g,\nand so,\n\ng(u) = g(s(v)) = v = 0,\n\nbecause g ◦ s = idG, which shows that u = s(v) = 0. Thus, F = Ker g ⊕ Im s, and since by\nassumption, Im f = Ker g, we have F = Im f ⊕ Im s. But then, since f and s are injective,\nf + s : E ⊕G→ F is an isomorphism. The proof of (b) is very similar.\n\nNote that we can choose a retraction r : F → E so that Ker r = Im s, since\nF = Ker g ⊕ Im s = Im f ⊕ Im s and f is injective so we can set r ≡ 0 on Im s.\n\nGiven a sequence of linear maps E\nf−→ F\n\ng−→ G, when Im f = Ker g, we say that the\n\nsequence E\nf−→ F\n\ng−→ G is exact at F . If in addition to being exact at F , f is injective\nand g is surjective, we say that we have a short exact sequence, and this is denoted as\n\n0 −→ E\nf−→ F\n\ng−→ G −→ 0.\n\nThe property of a short exact sequence given by Proposition 6.12 is often described by saying\n\nthat 0 −→ E\nf−→ F\n\ng−→ G −→ 0 is a (short) split exact sequence.\n\nAs a corollary of Proposition 6.12, we have the following result which shows that given\na linear map f : E → F , its domain E is the direct sum of its kernel Ker f with some\nisomorphic copy of its image Im f .\n\nTheorem 6.13. (Rank-nullity theorem) Let E and F be vector spaces, and let f : E → F\nbe a linear map. Then, E is isomorphic to Ker f ⊕ Im f , and thus,\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f).\n\nSee Figure 6.3.\n\nProof. Consider\n\nKer f\ni−→ E\n\nf ′−→ Im f,\n\nwhere Ker f\ni−→ E is the inclusion map, and E\n\nf ′−→ Im f is the surjection associated\n\nwith E\nf−→ F . Then, we apply Proposition 6.12 to any section Im f\n\ns−→ E of f ′ to\nget an isomorphism between E and Ker f ⊕ Im f , and Proposition 6.7, to get dim(E) =\ndim(Ker f) + dim(Im f).\n\nDefinition 6.6. The dimension dim(Ker f) of the kernel of a linear map f is called the\nnullity of f .\n\nWe now derive some important results using Theorem 6.13.\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 167\n\nu = (0,1,1)\n\n2\n\nu = (1,0,1)\n1\n\nKer f\n\nf  = f(u  ) = (1,0)11\n\nf  =  f(u  ) = (0, 1)2 2 f(u) = (1,1)\n\nf(x,y,z) = (x,y)\n\ns(x,y) = (x,y,x+y)\n\nu = (1,1,1)\n\ns (f(u)) = (1,1,2)\n\nh = (0,0,-1)\n\nFigure 6.3: Let f : E → F be the linear map from R3 to R2 given by f(x, y, z) = (x, y).\nThen s : R2 → R3 is given by s(x, y) = (x, y, x + y) and maps the pink R2 isomorphically\nonto the slanted pink plane of R3 whose equation is −x − y + z = 0. Theorem 6.13 shows\nthat R3 is the direct sum of the plane −x− y + z = 0 and the kernel of f which the orange\nz-axis.\n\nProposition 6.14. Given a vector space E, if U and V are any two subspaces of E, then\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nan equation known as Grassmann’s relation.\n\nProof. Recall that U + V is the image of the linear map\n\na : U × V → E\n\ngiven by\n\na(u, v) = u+ v,\n\nand that we proved earlier that the kernel Ker a of a is isomorphic to U ∩ V . By Theorem\n6.13,\n\ndim(U × V ) = dim(Ker a) + dim(Im a),\n\nbut dim(U × V ) = dim(U) + dim(V ), dim(Ker a) = dim(U ∩ V ), and Im a = U + V , so the\nGrassmann relation holds.\n\nThe Grassmann relation can be very useful to figure out whether two subspace have a\nnontrivial intersection in spaces of dimension > 3. For example, it is easy to see that in R5,\nthere are subspaces U and V with dim(U) = 3 and dim(V ) = 2 such that U ∩ V = (0); for\nexample, let U be generated by the vectors (1, 0, 0, 0, 0), (0, 1, 0, 0, 0), (0, 0, 1, 0, 0), and V be\n\n\n\n168 CHAPTER 6. DIRECT SUMS\n\ngenerated by the vectors (0, 0, 0, 1, 0) and (0, 0, 0, 0, 1). However, we claim that if dim(U) = 3\nand dim(V ) = 3, then dim(U ∩ V ) ≥ 1. Indeed, by the Grassmann relation, we have\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ),\n\nnamely\n\n3 + 3 = 6 = dim(U + V ) + dim(U ∩ V ),\n\nand since U + V is a subspace of R5, dim(U + V ) ≤ 5, which implies\n\n6 ≤ 5 + dim(U ∩ V ),\n\nthat is 1 ≤ dim(U ∩ V ).\n\nAs another consequence of Proposition 6.14, if U and V are two hyperplanes in a vector\nspace of dimension n, so that dim(U) = n− 1 and dim(V ) = n− 1, the reader should show\nthat\n\ndim(U ∩ V ) ≥ n− 2,\n\nand so, if U 6= V , then\n\ndim(U ∩ V ) = n− 2.\n\nHere is a characterization of direct sums that follows directly from Theorem 6.13.\n\nProposition 6.15. If U1, . . . , Up are any subspaces of a finite dimensional vector space E,\nthen\n\ndim(U1 + · · ·+ Up) ≤ dim(U1) + · · ·+ dim(Up),\n\nand\n\ndim(U1 + · · ·+ Up) = dim(U1) + · · ·+ dim(Up)\n\niff the Uis form a direct sum U1 ⊕ · · · ⊕ Up.\n\nProof. If we apply Theorem 6.13 to the linear map\n\na : U1 × · · · × Up → U1 + · · ·+ Up\n\ngiven by a(u1, . . . , up) = u1 + · · ·+ up, we get\n\ndim(U1 + · · ·+ Up) = dim(U1 × · · · × Up)− dim(Ker a)\n\n= dim(U1) + · · ·+ dim(Up)− dim(Ker a),\n\nso the inequality follows. Since a is injective iff Ker a = (0), the Uis form a direct sum iff\nthe second equation holds.\n\nAnother important corollary of Theorem 6.13 is the following result:\n\n\n\n6.2. THE RANK-NULLITY THEOREM; GRASSMANN’S RELATION 169\n\nProposition 6.16. Let E and F be two vector spaces with the same finite dimension\ndim(E) = dim(F ) = n. For every linear map f : E → F , the following properties are\nequivalent:\n\n(a) f is bijective.\n\n(b) f is surjective.\n\n(c) f is injective.\n\n(d) Ker f = (0).\n\nProof. Obviously, (a) implies (b).\n\nIf f is surjective, then Im f = F , and so dim(Im f) = n. By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(E) = n and dim(Im f) = n, we get dim(Ker f) = 0, which means that\nKer f = (0), and so f is injective (see Proposition 3.14). This proves that (b) implies (c).\n\nIf f is injective, then by Proposition 3.14, Ker f = (0), so (c) implies (d).\n\nFinally, assume that Ker f = (0), so that dim(Ker f) = 0 and f is injective (by Proposi-\ntion 3.14). By Theorem 6.13,\n\ndim(E) = dim(Ker f) + dim(Im f),\n\nand since dim(Ker f) = 0, we get\n\ndim(Im f) = dim(E) = dim(F ),\n\nwhich proves that f is also surjective, and thus bijective. This proves that (d) implies (a)\nand concludes the proof.\n\nOne should be warned that Proposition 6.16 fails in infinite dimension.\n\nThe following Proposition will also be useful.\n\nProposition 6.17. Let E be a vector space. If E = U ⊕ V and E = U ⊕W , then there is\nan isomorphism f : V → W between V and W .\n\nProof. Let R be the relation between V and W , defined such that\n\n〈v, w〉 ∈ R iff w − v ∈ U.\n\nWe claim that R is a functional relation that defines a linear isomorphism f : V → W\nbetween V and W , where f(v) = w iff 〈v, w〉 ∈ R (R is the graph of f). If w − v ∈ U and\nw′ − v ∈ U , then w′ − w ∈ U , and since U ⊕W is a direct sum, U ∩W = (0), and thus\n\n\n\n170 CHAPTER 6. DIRECT SUMS\n\nw′ − w = 0, that is w′ = w. Thus, R is functional. Similarly, if w − v ∈ U and w − v′ ∈ U ,\nthen v′ − v ∈ U , and since U ⊕ V is a direct sum, U ∩ V = (0), and v′ = v. Thus, f is\ninjective. Since E = U ⊕ V , for every w ∈ W , there exists a unique pair 〈u, v〉 ∈ U × V ,\nsuch that w = u+ v. Then, w− v ∈ U , and f is surjective. We also need to verify that f is\nlinear. If\n\nw − v = u\n\nand\nw′ − v′ = u′,\n\nwhere u, u′ ∈ U , then, we have\n\n(w + w′)− (v + v′) = (u+ u′),\n\nwhere u+ u′ ∈ U . Similarly, if\nw − v = u\n\nwhere u ∈ U , then we have\nλw − λv = λu,\n\nwhere λu ∈ U . Thus, f is linear.\n\nGiven a vector space E and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U ⊕ V depends only on U . We call dim(V ) the\ncodimension of U , and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f : E → F is the dimension dim(Im f) of the image\nsubspace Im f of F .\n\nWe have the following simple proposition.\n\nProposition 6.18. Given a linear map f : E → F , the following properties hold:\n\n(i) rk(f) = codim(Ker f).\n\n(ii) rk(f) + dim(Ker f) = dim(E).\n\n(iii) rk(f) ≤ min(dim(E), dim(F )).\n\nProof. Since by Proposition 6.13, dim(E) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = codim(Ker f). Since rk(f) = dim(Im f), (ii) follows\nfrom dim(E) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F , we have\nrk(f) ≤ dim(F ), and since rk(f) + dim(Ker f) = dim(E), we have rk(f) ≤ dim(E).\n\nThe rank of a matrix is defined as follows.\n\n170 CHAPTER 6. DIRECT SUMS\n\nw' —w = 0, that is w’ = w. Thus, R is functional. Similarly, if w—-v ¢€U andw—v' €U,\nthen v' — v € U, and since U @ V is a direct sum, UNV = (0), and v’ = v. Thus, f is\ninjective. Since EF = U @V, for every w € W, there exists a unique pair (u,v) € U x V,\nsuch that w = u+v. Then, w—v € U, and f is surjective. We also need to verify that f is\nlinear. If\n\nw-v=U\n\nand\n/ / /\nw—-v =u,\n\nwhere u, u’ € U, then, we have\n(wtw')—(v+v') =(utv),\n\nwhere u+u' € U. Similarly, if\n\nwhere u € U, then we have\nAw — Av = Au,\n\nwhere Au € U. Thus, f is linear. O\n\nGiven a vector space F and any subspace U of E, Proposition 6.17 shows that the\ndimension of any subspace V such that E = U @V depends only on U. We call dim(V) the\ncodimension of U, and we denote it by codim(U). A subspace U of codimension 1 is called\na hyperplane.\n\nThe notion of rank of a linear map or of a matrix is an important one, both theoretically\nand practically, since it is the key to the solvability of linear equations. Recall from Definition\n3.19 that the rank rk(f) of a linear map f: E — F is the dimension dim(Im f) of the image\nsubspace Im f of F’.\n\nWe have the following simple proposition.\nProposition 6.18. Given a linear map f: E — F, the following properties hold:\n(i) rk(f) = codim(Ker f).\n(ii) rk(f) + dim(Ker f) = dim(E).\n(iit) rk(f) < min(dim(£), dim(F)).\n\nProof. Since by Proposition 6.13, dim(#) = dim(Ker f) + dim(Im f), and by definition,\nrk(f) = dim(Im f), we have rk(f) = = codim(Ker f). Since rk(f) = dim/(Im f), (ii) follows\nfrom dim(F) = dim(Ker f) + dim(Im f). As for (iii), since Im f is a subspace of F’, we have\nrk(f) < dim(F), and since rk(f) + dim(Ker f) = dim(£), we have rk(f) < dim(E). O\n\nThe rank of a matrix is defined as follows.\n\n\n\n\n6.3. SUMMARY 171\n\nDefinition 6.7. Given a m × n-matrix A = (ai j) over the field K, the rank rk(A) of the\nmatrix A is the maximum number of linearly independent columns of A (viewed as vectors\nin Km).\n\nIn view of Proposition 3.8, the rank of a matrix A is the dimension of the subspace of\nKm generated by the columns of A. Let E and F be two vector spaces, and let (u1, . . . , un)\nbe a basis of E, and (v1, . . . , vm) a basis of F . Let f : E → F be a linear map, and let M(f)\nbe its matrix w.r.t. the bases (u1, . . . , un) and (v1, . . . , vm). Since the rank rk(f) of f is the\ndimension of Im f , which is generated by (f(u1), . . . , f(un)), the rank of f is the maximum\nnumber of linearly independent vectors in (f(u1), . . . , f(un)), which is equal to the number\nof linearly independent columns of M(f), since F and Km are isomorphic. Thus, we have\nrk(f) = rk(M(f)), for every matrix representing f .\n\nWe will see later, using duality, that the rank of a matrix A is also equal to the maximal\nnumber of linearly independent rows of A.\n\nIf U is a hyperplane, then E = U ⊕ V for some subspace V of dimension 1. However, a\nsubspace V of dimension 1 is generated by any nonzero vector v ∈ V , and thus we denote\nV by Kv, and we write E = U ⊕ Kv. Clearly, v /∈ U . Conversely, let x ∈ E be a vector\nsuch that x /∈ U (and thus, x 6= 0). We claim that E = U ⊕ Kx. Indeed, since U is a\nhyperplane, we have E = U ⊕Kv for some v /∈ U (with v 6= 0). Then, x ∈ E can be written\nin a unique way as x = u + λv, where u ∈ U , and since x /∈ U , we must have λ 6= 0, and\nthus, v = −λ−1u + λ−1x. Since E = U ⊕Kv, this shows that E = U + Kx. Since x /∈ U ,\nwe have U ∩Kx = 0, and thus E = U ⊕Kx. This argument shows that a hyperplane is a\nmaximal proper subspace H of E.\n\nIn Chapter 11, we shall see that hyperplanes are precisely the Kernels of nonnull linear\nmaps f : E → K, called linear forms.\n\n6.3 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Direct products, sums, direct sums .\n\n• Projections .\n\n• The fundamental equation\n\ndim(E) = dim(Ker f) + dim(Im f) = dim(Ker f) + rk(f)\n\n(Proposition 6.13).\n\n• Grassmann’s relation\n\ndim(U) + dim(V ) = dim(U + V ) + dim(U ∩ V ).\n\n\n\n172 CHAPTER 6. DIRECT SUMS\n\n• Characterizations of a bijective linear map f : E → F .\n\n• Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E. Prove that if V ∪W is\na subspace of E, then either V ⊆ W or W ⊆ V .\n\nProblem 6.2. Prove that for every vector space E, if f : E → E is an idempotent linear\nmap, i.e., f ◦ f = f , then we have a direct sum\n\nE = Ker f ⊕ Im f,\n\nso that f is the projection onto its image Im f .\n\nProblem 6.3. Let U1, . . . , Up be any p ≥ 2 subspaces of some vector space E and recall\nthat the linear map\n\na : U1 × · · · × Up → E\n\nis given by\na(u1, . . . , up) = u1 + · · ·+ up,\n\nwith ui ∈ Ui for i = 1, . . . , p.\n\n(1) If we let Zi ⊆ U1 × · · · × Up be given by\n\nZi =\n\n{(\nu1, . . . , ui−1,−\n\np∑\nj=1,j 6=i\n\nuj, ui+1, . . . , up\n\n) ∣∣∣∣∣\np∑\n\nj=1,j 6=i\nuj ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)}\n,\n\nfor i = 1, . . . , p, then prove that\n\nKer a = Z1 = · · · = Zp.\n\nIn general, for any given i, the condition Ui ∩\n(∑p\n\nj=1,j 6=i Uj\n\n)\n= (0) does not necessarily\n\nimply that Zi = (0). Thus, let\n\nZ =\n\n{(\nu1, . . . , ui−1, ui, ui+1, . . . , up\n\n) ∣∣∣∣ ui = −\np∑\n\nj=1,j 6=i\nuj, ui ∈ Ui ∩\n\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n, 1 ≤ i ≤ p\n\n}\n.\n\nSince Ker a = Z1 = · · · = Zp, we have Z = Ker a. Prove that if\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p,\n\n172 CHAPTER 6. DIRECT SUMS\n\ne Characterizations of a bijective linear map f: E > F.\n\ne Rank of a matrix.\n\n6.4 Problems\n\nProblem 6.1. Let V and W be two subspaces of a vector space E’. Prove that if V UW is\na subspace of £, then either V CW or W CV.\n\nProblem 6.2. Prove that for every vector space E, if f: E — E is an idempotent linear\nmap, i.e., fo f = f, then we have a direct sum\n\nE=Ker f @Imf,\nso that f is the projection onto its image Im f.\n\nProblem 6.3. Let U;,...,U, be any p > 2 subspaces of some vector space F and recall\nthat the linear map\na: U,xX+:+xU,> EF\n\nis given by\nA(U1,-.-,Up) = Uy +--+ + Up,\n\nwith u; € U; fori =1,...,p.\n(1) If we let Z; C U; x --- x U, be given by\n\nP\nZi, = { (tus tian ) Uj trys +5 Up)\n\nj=l ji\n\nS wevin( Su}.\n\njHljxi j=l jAt\n\nfori =1,...,p, then prove that\n\nKera = 24, =-:- = Z,.\n\nIn general, for any given i, the condition U;M ( eer v;) = (0) does not necessarily\nimply that Z; = (0). Thus, let\n\nUz=— 3 ww EU 3 uj), 1sisoh,\n\nj=l ix j=l j#i\n\nZ= { (tases tira thstssss sty)\n\nSince Kera = Z; =--: = Z,, we have Z = Kera. Prove that if\n\nun ( 3 5) =) l<ic<p,\n\nj=l Hi\n\n\n\n\n6.4. PROBLEMS 173\n\nthen Z = Ker a = (0).\n\n(2) Prove that U1 + · · ·+ Up is a direct sum iff\n\nUi ∩\n( p∑\nj=1,j 6=i\n\nUj\n\n)\n= (0) 1 ≤ i ≤ p.\n\nProblem 6.4. Assume that E is finite-dimensional, and let fi : E → E be any p ≥ 2 linear\nmaps such that\n\nf1 + · · ·+ fp = idE.\n\nProve that the following properties are equivalent:\n\n(1) f 2\ni = fi, 1 ≤ i ≤ p.\n\n(2) fj ◦ fi = 0, for all i 6= j, 1 ≤ i, j ≤ p.\n\nHint . Use Problem 6.2.\n\nLet U1, . . . , Up be any p ≥ 2 subspaces of some vector space E. Prove that U1 + · · ·+ Up\nis a direct sum iff\n\nUi ∩\n( i−1∑\n\nj=1\n\nUj\n\n)\n= (0), i = 2, . . . , p.\n\nProblem 6.5. Given any vector space E, a linear map f : E → E is an involution if\nf ◦ f = id.\n\n(1) Prove that an involution f is invertible. What is its inverse?\n\n(2) Let E1 and E−1 be the subspaces of E defined as follows:\n\nE1 = {u ∈ E | f(u) = u}\nE−1 = {u ∈ E | f(u) = −u}.\n\nProve that we have a direct sum\nE = E1 ⊕ E−1.\n\nHint . For every u ∈ E, write\n\nu =\nu+ f(u)\n\n2\n+\nu− f(u)\n\n2\n.\n\n(3) If E is finite-dimensional and f is an involution, prove that there is some basis of E\nwith respect to which the matrix of f is of the form\n\nIk,n−k =\n\n(\nIk 0\n0 −In−k\n\n)\n,\n\nwhere Ik is the k × k identity matrix (similarly for In−k) and k = dim(E1). Can you give a\ngeometric interpretation of the action of f (especially when k = n− 1)?\n\n\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n × n matrix H is upper Hessenberg if hjk = 0 for all (j, k) such that\nj − k ≥ 0. An upper Hessenberg matrix is unreduced if hi+1i 6= 0 for i = 1, . . . , n− 1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n× k matrix.\n\n(1) Prove that the k × k matrix A>A and the matrix A have the same nullspace. Use\nthis to prove that rank(A>A) = rank(A). Similarly, prove that the n × n matrix AA> and\nthe matrix A> have the same nullspace, and conclude that rank(AA>) = rank(A>).\n\nWe will prove later that rank(A>) = rank(A).\n\n(2) Let a1, . . . , ak be k linearly independent vectors in Rn (1 ≤ k ≤ n), and let A be the\nn× k matrix whose ith column is ai. Prove that A>A has rank k, and that it is invertible.\nLet P = A(A>A)−1A> (an n× n matrix). Prove that\n\nP 2 = P\n\nP> = P.\n\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a1, . . . , ak, or equivalently\nthe set of all vectors in Rn of the form Ax, with x ∈ Rk. Prove that the nullspace U of P is\nthe set of vectors u ∈ Rn such that A>u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of Rn onto the subspace V spanned by a1, . . . , ak, and\nthat\n\nRn = U ⊕ V.\n\nProblem 6.8. A rotation Rθ in the plane R2 is given by the matrix\n\nRθ =\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)\n.\n\n(1) Use Matlab to show the action of a rotation Rθ on a simple figure such as a triangle\nor a rectangle, for various values of θ, including θ = π/6, π/4, π/3, π/2.\n\n(2) Prove that Rθ is invertible and that its inverse is R−θ.\n\n(3) For any two rotations Rα and Rβ, prove that\n\nRβ ◦Rα = Rα ◦Rβ = Rα+β.\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\nSO(2).\n\n174 CHAPTER 6. DIRECT SUMS\n\nProblem 6.6. An n x n matrix H is upper Hessenberg if hj, = 0 for all (j,k) such that\nj —k > 0. An upper Hessenberg matrix is unreduced if hii4; 4 0 for? =1,...,n—1.\n\nProve that if H is a singular unreduced upper Hessenberg matrix, then dim(Ker (H)) = 1.\n\nProblem 6.7. Let A be any n x k matrix.\n\n(1) Prove that the k x k matrix A'A and the matrix A have the same nullspace. Use\nthis to prove that rank(A' A) = rank(A). Similarly, prove that the n x n matrix AA’ and\nthe matrix A’ have the same nullspace, and conclude that rank(AA') = rank(A').\n\nWe will prove later that rank(A') = rank(A).\n\n(2) Let a1,...,a, be & linearly independent vectors in R\" (1 < k <n), and let A be the\nn x k matrix whose ith column is a;. Prove that A'A has rank k, and that it is invertible.\nLet P = A(A'A)~'A! (an n x n matrix). Prove that\n\nP? =P\nP'=P.\nWhat is the matrix P when k = 1?\n\n(3) Prove that the image of P is the subspace V spanned by a,...,a,, or equivalently\nthe set of all vectors in R” of the form Az, with x € R*. Prove that the nullspace U of P is\nthe set of vectors u € R” such that A'u = 0. Can you give a geometric interpretation of U?\n\nConclude that P is a projection of R” onto the subspace V spanned by ay,,...,a,, and\nthat\n\nR\"=U®6V.\n\nProblem 6.8. A rotation Ro in the plane R? is given by the matrix\ncos@ —sin@\nRo = er cos 6 )\n\n(1) Use Matlab to show the action of a rotation Ry on a simple figure such as a triangle\nor a rectangle, for various values of 0, including 0 = 1/6, 7/4, 7/3, 7/2.\n\n(2) Prove that Rg is invertible and that its inverse is R_g.\n\n(3) For any two rotations Ry and Rg, prove that\nRg ° Ra = Ra ie) Rg = Ra+p-\n\nUse (2)-(3) to prove that the rotations in the plane form a commutative group denoted\n\nSO(2).\n\n\n\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Rθ,(a1,a2) in R2 given by(\ny1\n\ny2\n\n)\n=\n\n(\ncos θ − sin θ\nsin θ cos θ\n\n)(\nx1\n\nx2\n\n)\n+\n\n(\na1\n\na2\n\n)\n.\n\n(1) Prove that if θ 6= k2π, with k ∈ Z, then Rθ,(a1,a2) has a unique fixed point (c1, c2),\nthat is, there is a unique point (c1, c2) such that(\n\nc1\n\nc2\n\n)\n= Rθ,(a1,a2)\n\n(\nc1\n\nc2\n\n)\n,\n\nand this fixed point is given by(\nc1\n\nc2\n\n)\n=\n\n1\n\n2 sin(θ/2)\n\n(\ncos(π/2− θ/2) − sin(π/2− θ/2)\nsin(π/2− θ/2) cos(π/2− θ/2)\n\n)(\na1\n\na2\n\n)\n.\n\n(2) In this question we still assume that θ 6= k2π, with k ∈ Z. By translating the\ncoordinate system with origin (0, 0) to the new coordinate system with origin (c1, c2), which\nmeans that if (x1, x2) are the coordinates with respect to the standard origin (0, 0) and if\n(x′1, x\n\n′\n2) are the coordinates with respect to the new origin (c1, c2), we have\n\nx1 = x′1 + c1\n\nx2 = x′2 + c2\n\nand similarly for (y1, y2) and (y′1, y\n′\n2), then show that(\ny1\n\ny2\n\n)\n= Rθ,(a1,a2)\n\n(\nx1\n\nx2\n\n)\nbecomes (\n\ny′1\ny′2\n\n)\n= Rθ\n\n(\nx′1\nx′2\n\n)\n.\n\nConclude that with respect to the new origin (c1, c2), the affine map Rθ,(a1,a2) becomes\nthe rotation Rθ. We say that Rθ,(a1,a2) is a rotation of center (c1, c2).\n\n(3) Use Matlab to show the action of the affine map Rθ,(a1,a2) on a simple figure such as a\ntriangle or a rectangle, for θ = π/3 and various values of (a1, a2). Display the center (c1, c2)\nof the rotation.\n\nWhat kind of transformations correspond to θ = k2π, with k ∈ Z?\n\n(4) Prove that the inverse of Rθ,(a1,a2) is of the form R−θ,(b1,b2), and find (b1, b2) in terms\nof θ and (a1, a2).\n\n(5) Given two affine maps Rα,(a1,a2) and Rβ,(b1,b2), prove that\n\nRβ,(b1,b2) ◦Rα,(a1,a2) = Rα+β,(t1,t2)\n\n6.4. PROBLEMS 175\n\nProblem 6.9. Consider the affine map Ro (a,,a2) in R? given by\n\nV1 cos@ —sind L1 a1\n=(_ + .\nYo sind cos v9 a9\n(1) Prove that if 0 A k27, with k € Z, then Rova,,a,) has a unique fixed point (cy, ce),\nthat is, there is a unique point (c),c2) such that\n\nCy \\ Cy\n(‘:) = Ro (01,02) (“) )\nand this fixed point is given by\n\nc\\ 1 cos(7/2 — 0/2) —sin(a/2—6/2)\\ (a,\nC2)  2sin(@/2) \\sin(/2— 6/2) — cos(m/2 — 6/2) a2)”\n(2) In this question we still assume that 0 # k27, with k € Z. By translating the\ncoordinate system with origin (0,0) to the new coordinate system with origin (c1,c2), which\n\nmeans that if (7,22) are the coordinates with respect to the standard origin (0,0) and if\n(x, 75) are the coordinates with respect to the new origin (c;,c2), we have\n\n!\nT= XU,+\n\n!\nUq = Lo + CQ\n\nand similarly for (y1, y2) and (y{, 45), then show that\n\nY\\ Ly\n(32) = Boon (22)\nYi\\ ry\n(,) = Re (\").\n\nConclude that with respect to the new origin (ci, c2), the affine map Ro (a,,a,) becomes\nthe rotation Rg. We say that Rg (a,,a.) is a rotation of center (c1, C2).\n\nbecomes\n\n(3) Use Matlab to show the action of the affine map Rg (a,,a,) on a simple figure such as a\ntriangle or a rectangle, for 6 = 7/3 and various values of (a1, a2). Display the center (c1, ce)\nof the rotation.\n\nWhat kind of transformations correspond to 6 = k27, with k € Z?\n\n(4) Prove that the inverse of Rg (a,,a.) is of the form R_g (5,,.), and find (b), b2) in terms\nof @ and (a1, a2).\n\n(5) Given two affine maps Ro (a;,a.) and Rgo,,b), prove that\n\nR3.(b1,b2) ° Ra,(ar,a2) = Rots, (t1,t2)\n\n\n\n\n176 CHAPTER 6. DIRECT SUMS\n\nfor some (t1, t2), and find (t1, t2) in terms of β, (a1, a2) and (b1, b2).\n\nEven in the case where (a1, a2) = (0, 0), prove that in general\n\nRβ,(b1,b2) ◦Rα 6= Rα ◦Rβ,(b1,b2).\n\nUse (4)-(5) to show that the affine maps of the plane defined in this problem form a\nnonabelian group denoted SE(2).\n\nProve that Rβ,(b1,b2) ◦Rα,(a1,a2) is not a translation (possibly the identity) iff α+β 6= k2π,\nfor all k ∈ Z. Find its center of rotation when (a1, a2) = (0, 0).\n\nIf α+β = k2π, then Rβ,(b1,b2) ◦Rα,(a1,a2) is a pure translation. Find the translation vector\nof Rβ,(b1,b2) ◦Rα,(a1,a2).\n\nProblem 6.10. (Affine subspaces) A subset A of Rn is called an affine subspace if either\nA = ∅, or there is some vector a ∈ Rn and some subspace U of Rn such that\n\nA = a+ U = {a+ u | u ∈ U}.\nWe define the dimension dim(A) of A as the dimension dim(U) of U .\n\n(1) If A = a+ U , why is a ∈ A?\n\nWhat are affine subspaces of dimension 0? What are affine subspaces of dimension 1\n(begin with R2)? What are affine subspaces of dimension 2 (begin with R3)?\n\nProve that any nonempty affine subspace is closed under affine combinations.\n\n(2) Prove that if A = a + U is any nonempty affine subspace, then A = b + U for any\nb ∈ A.\n\n(3) Let A be any nonempty subset of Rn closed under affine combinations. For any\na ∈ A, prove that\n\nUa = {x− a ∈ Rn | x ∈ A}\nis a (linear) subspace of Rn such that\n\nA = a+ Ua.\n\nProve that Ua does not depend on the choice of a ∈ A; that is, Ua = Ub for all a, b ∈ A. In\nfact, prove that\n\nUa = U = {y − x ∈ Rn | x, y ∈ A}, for all a ∈ A,\nand so\n\nA = a+ U, for any a ∈ A.\n\nRemark: The subspace U is called the direction of A.\n\n(4) Two nonempty affine subspaces A and B are said to be parallel iff they have the same\ndirection. Prove that that if A 6= B and A and B are parallel, then A ∩ B = ∅.\n\nRemark: The above shows that affine subspaces behave quite differently from linear sub-\nspaces.\n\n\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v1, . . . , vn) ∈ Rn, let\nv̂ ∈ Rn+1 be the vector v̂ = (v1, . . . , vn, 1). Equivalently, v̂ = (v̂1, . . . , v̂n+1) ∈ Rn+1 is the\nvector defined by\n\nv̂i =\n\n{\nvi if 1 ≤ i ≤ n,\n\n1 if i = n+ 1.\n\n(1) For any m+ 1 vectors (u0, u1, . . . , um) with ui ∈ Rn and m ≤ n, prove that if the m\nvectors (u1 − u0, . . . , um − u0) are linearly independent, then the m+ 1 vectors (û0, . . . , ûm)\nare linearly independent.\n\n(2) Prove that if the m + 1 vectors (û0, . . . , ûm) are linearly independent, then for any\nchoice of i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are\nlinearly independent.\n\nAny m+ 1 vectors (u0, u1, . . . , um) such that the m+ 1 vectors (û0, . . . , ûm) are linearly\nindependent are said to be affinely independent .\n\nFrom (1) and (2), the vector (u0, u1, . . . , um) are affinely independent iff for any any choice\nof i, with 0 ≤ i ≤ m, the m vectors uj − ui for j ∈ {0, . . . ,m} with j − i 6= 0 are linearly\nindependent. If m = n, we say that n+ 1 affinely independent vectors (u0, u1, . . . , un) form\nan affine frame of Rn.\n\n(3) if (u0, u1, . . . , un) is an affine frame of Rn, then prove that for every vector v ∈ Rn,\nthere is a unique (n+ 1)-tuple (λ0, λ1, . . . , λn) ∈ Rn+1, with λ0 +λ1 + · · ·+λn = 1, such that\n\nv = λ0u0 + λ1u1 + · · ·+ λnun.\n\nThe scalars (λ0, λ1, . . . , λn) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (u0, u1, . . . , un).\n\nIf we write ei = ui − u0, for i = 1, . . . , n, then prove that we have\n\nv = u0 + λ1e1 + · · ·+ λnen,\n\nand since (e1, . . . , en) is a basis of Rn (by (1) & (2)), the n-tuple (λ1, . . . , λn) consists of the\nstandard coordinates of v − u0 over the basis (e1, . . . , en).\n\nConversely, for any vector u0 ∈ Rn and for any basis (e1, . . . , en) of Rn, let ui = u0 + ei\nfor i = 1, . . . , n. Prove that (u0, u1, . . . , un) is an affine frame of Rn, and for any v ∈ Rn, if\n\nv = u0 + x1e1 + · · ·+ xnen,\n\nwith (x1, . . . , xn) ∈ Rn (unique), then\n\nv = (1− (x1 + · · ·+ xx))u0 + x1u1 + · · ·+ xnun,\n\nso that (1− (x1 + · · ·+xx)), x1, · · · , xn), are the barycentric coordinates of v w.r.t. the affine\nframe (u0, u1, . . . , un).\n\n6.4. PROBLEMS 177\n\nProblem 6.11. (Affine frames and affine maps) For any vector v = (v),...,Un) € R”, let\nv0 € R\"! be the vector 0 = (v,...,Un, 1). Equivalently, 0 = (01,...,0n41) € R°! is the\nvector defined by\n\n. {! ifl<i<n,\n\nUi= Lp\n1 ift=n-+1.\n(1) For any m+ 1 vectors (uo, U1,...,Um) with u; € R” and m <n, prove that if the m\nvectors (U1; — Uo,---,;Um — Uo) are linearly independent, then the m+ 1 vectors (to,...,Um)\n\nare linearly independent.\n\n(2) Prove that if the m+ 1 vectors (Uo,...,Um) are linearly independent, then for any\nchoice of 7, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with 7 —i # 0 are\nlinearly independent.\n\nAny m+ 1 vectors (uo, U1,---,Um) such that the m+ 1 vectors (Uo,..., Um) are linearly\nindependent are said to be affinely independent.\n\nFrom (1) and (2), the vector (uo, w1,..., Um) are affinely independent iff for any any choice\nof i, with 0 <i < m, the m vectors u; — u; for 7 € {0,...,m} with j —i ¥ 0 are linearly\nindependent. If m = n, we say that n +1 affinely independent vectors (uo, u1,...,Un) form\nan affine frame of R”.\n\n(3) if (uo, U1,...,Un) is an affine frame of R”, then prove that for every vector v € R”,\nthere is a unique (n+ 1)-tuple (Ag, \\1,---; An) € R°*!, with Ag+ Ai +--+: +An = 1, such that\n\nv= Agtto + Aut + +++ + Antn-\n\nThe scalars (Ao, A1,---,; An) are called the barycentric (or affine) coordinates of v w.r.t. the\naffine frame (uo, U1,.--, Un).\nIf we write e; = uj; — uo, for = 1,...,n, then prove that we have\n\nVv = Uo + Azer +++ + Anen,\n\nand since (€1,...,€n) is a basis of R” (by (1) & (2)), the n-tuple (\\1,..., An) consists of the\n\nstandard coordinates of v — uo over the basis (e1,...,€n).\nConversely, for any vector uo € R” and for any basis (€1,...,€n) of R”, let wu; = uo + e;\nfori =1,...,n. Prove that (uo, u1,..., Un) is an affine frame of R”, and for any v € R”, if\n\nU = Up + LE, Ft + Len,\nwith (a,...,%,) € R” (unique), then\nv= (1— (ay +--+ + 2z))uo + oyu H+ F 2p Un,\n\nso that (1— (a1 +--:+2z)),%1,-+++ ,2n), are the barycentric coordinates of v w.r.t. the affine\nframe (uo, U41,---, Un).\n\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (u0, . . .,\nun) and pairs (u0, (e1, . . . , en)), with (e1, . . . , en) a basis. Given an affine frame (u0, . . . , un),\nwe obtain the basis (e1, . . . , en) with ei = ui−u0, for i = 1, . . . , n; given the pair (u0, (e1, . . .,\nen)) where (e1, . . . , en) is a basis, we obtain the affine frame (u0, . . . , un), with ui = u0 + ei,\nfor i = 1, . . . , n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (u0, . . . , un) and standard coordinates w.r.t. the basis (e1, . . . , en).\nThe barycentric cordinates (λ0, λ1, . . . , λn) of v (with λ0 + λ1 + · · · + λn = 1) yield the\nstandard coordinates (λ1, . . . , λn) of v − u0; the standard coordinates (x1, . . . , xn) of v − u0\n\nyield the barycentric coordinates (1− (x1 + · · ·+ xn), x1, . . . , xn) of v.\n\n(4) Recall that an affine map is a map f : E → F between vector spaces that preserves\naffine combinations ; that is,\n\nf\n\n(\nm∑\ni=1\n\nλiui\n\n)\n=\n\nm∑\ni=1\n\nλif(ui),\n\nfor all u1 . . . , um ∈ E and all λi ∈ K such that\n∑m\n\ni=1 λi = 1.\n\nLet (u0, . . . , un) be any affine frame in Rn and let (v0, . . . , vn) be any vectors in Rm. Prove\nthat there is a unique affine map f : Rn → Rm such that\n\nf(ui) = vi, i = 0, . . . , n.\n\n(5) Let (a0, . . . , an) be any affine frame in Rn and let (b0, . . . , bn) be any n+ 1 points in\nRn. Prove that there is a unique (n+ 1)× (n+ 1) matrix\n\nA =\n\n(\nB w\n0 1\n\n)\ncorresponding to the unique affine map f such that\n\nf(ai) = bi, i = 0, . . . , n,\n\nin the sense that\nAâi = b̂i, i = 0, . . . , n,\n\nand that A is given by\n\nA =\n(\nb̂0 b̂1 · · · b̂n\n\n) (\nâ0 â1 · · · ân\n\n)−1\n.\n\nMake sure to prove that the bottom row of A is (0, . . . , 0, 1).\n\nIn the special case where (a0, . . . , an) is the canonical affine frame with ai = ei+1 for\ni = 0, . . . , n− 1 and an = (0, . . . , 0) (where ei is the ith canonical basis vector), show that\n\n(\nâ0 â1 · · · ân\n\n)\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n1 1 · · · 1 1\n\n\n\n178 CHAPTER 6. DIRECT SUMS\n\nThe above shows that there is a one-to-one correspondence between affine frames (up, ...,\nUn) and pairs (uo, (€1,.--,@n)), with (e1,...,e,) a basis. Given an affine frame (uo,..., Un),\nwe obtain the basis (€1,...,€,) with e; = u; — uo, for i = 1,...,; given the pair (uo, (e1,...,\n€,,)) where (€1,...,€n) is a basis, we obtain the affine frame (uo,..., Un), with wu; = uo + &;,\nfor 2 =1,...,n. There is also a one-to-one correspondence between barycentric coordinates\nw.r.t. the affine frame (uo,...,Un) and standard coordinates w.r.t. the basis (e1,...,€n).\nThe barycentric cordinates (Xo, A1,---,;An) of v (with Ap + Ay +--+: + An = 1) yield the\nstandard coordinates (Ai,...,An) of vu — uo; the standard coordinates (1%1,...,2n) of v — uo\nyield the barycentric coordinates (1 — (a1 +-+++%n),%1,.--,2n) of v.\n\n(4) Recall that an affine map is a map f: EF — F' between vector spaces that preserves\naffine combinations; that is,\n\nf (> se) = » Af (ui);\n\nfor all uy ...,Um € E and all A; € K such that $0\", A; = 1.\n\nLet (uo,..., Un) be any affine frame in R” and let (vo,..., Un) be any vectors in R™. Prove\nthat there is a unique affine map f: R” > R” such that\n\nf(ui) =v;, t=0,...,n.\n\n(5) Let (ao,...,@n) be any affine frame in R” and let (bo,...,b,) be any n + 1 points in\nR”. Prove that there is a unique (n + 1) x (n + 1) matrix\n\nBow\na=(0 4)\ncorresponding to the unique affine map f such that\n\nf(a;) = b;, 1=0,...,N,\n\nin the sense that .\nAa; = b;, 1=0,...,N,\n\nand that A is given by\nA- (i Boe in) (@ @ +++ Gi).\n\nMake sure to prove that the bottom row of A is (0,...,0,1).\n\nIn the special case where (ao,...,@n) is the canonical affine frame with a; = e;4, for\ni=0,...,n—1 and a, = (0,...,0) (where e; is the 7th canonical basis vector), show that\n1 0.:--. 0 0\n\n0 1\n\na\n\nQ)\n\nOo\n\nQ)\n\n=\n\nQ\n\n=\n\n—\"\n\nI|\n\nOeee\nrer © oS\nFe Oa ©\n\ne\ne\n\n\n\n\n6.4. PROBLEMS 179\n\nand\n\n(\nâ0 â1 · · · ân\n\n)−1\n=\n\n\n1 0 · · · 0 0\n0 1 · · · 0 0\n...\n\n...\n. . . 0 0\n\n0 0 · · · 1 0\n−1 −1 · · · −1 1\n\n .\n\nFor example, when n = 2, if we write bi = (xi, yi), then we have\n\nA =\n\nx1 x2 x3\n\ny1 y2 y3\n\n1 1 1\n\n 1 0 0\n0 1 0\n−1 −1 1\n\n =\n\nx1 − x3 x2 − x3 x3\n\ny1 − y3 y2 − y3 y3\n\n0 0 1\n\n .\n\n(6) Recall that a nonempty affine subspace A of Rn is any nonempty subset of Rn closed\nunder affine combinations. For any affine map f : Rn → Rm, for any affine subspace A of\nRn, and any affine subspace B of Rm, prove that f(A) is an affine subspace of Rm, and that\nf−1(B) is an affine subspace of Rn.\n\n\n\n180 CHAPTER 6. DIRECT SUMS\n\n\n\nChapter 7\n\nDeterminants\n\nIn this chapter all vector spaces are defined over an arbitrary field K. For the sake of\nconcreteness, the reader may safely assume that K = R.\n\n7.1 Permutations, Signature of a Permutation\n\nThis chapter contains a review of determinants and their use in linear algebra. We begin\nwith permutations and the signature of a permutation. Next, we define multilinear maps\nand alternating multilinear maps. Determinants are introduced as alternating multilinear\nmaps taking the value 1 on the unit matrix (following Emil Artin). It is then shown how\nto compute a determinant using the Laplace expansion formula, and the connection with\nthe usual definition is made. It is shown how determinants can be used to invert matrices\nand to solve (at least in theory!) systems of linear equations (the Cramer formulae). The\ndeterminant of a linear map is defined. We conclude by defining the characteristic polynomial\nof a matrix (and of a linear map) and by proving the celebrated Cayley-Hamilton theorem\nwhich states that every matrix is a “zero” of its characteristic polynomial (we give two proofs;\none computational, the other one more conceptual).\n\nDeterminants can be defined in several ways. For example, determinants can be defined\nin a fancy way in terms of the exterior algebra (or alternating algebra) of a vector space.\nWe will follow a more algorithmic approach due to Emil Artin. No matter which approach\nis followed, we need a few preliminaries about permutations on a finite set. We need to\nshow that every permutation on n elements is a product of transpositions, and that the\nparity of the number of transpositions involved is an invariant of the permutation. Let\n[n] = {1, 2 . . . , n}, where n ∈ N, and n > 0.\n\nDefinition 7.1. A permutation on n elements is a bijection π : [n]→ [n]. When n = 1, the\nonly function from [1] to [1] is the constant map: 1 7→ 1. Thus, we will assume that n ≥ 2.\nA transposition is a permutation τ : [n]→ [n] such that, for some i < j (with 1 ≤ i < j ≤ n),\nτ(i) = j, τ(j) = i, and τ(k) = k, for all k ∈ [n] − {i, j}. In other words, a transposition\nexchanges two distinct elements i, j ∈ [n]. A cyclic permutation of order k (or k-cycle) is a\n\n181\n\n\n\n182 CHAPTER 7. DETERMINANTS\n\npermutation σ : [n]→ [n] such that, for some sequence (i1, i2, . . . , ik) of distinct elements of\n[n] with 2 ≤ k ≤ n,\n\nσ(i1) = i2, σ(i2) = i3, . . . , σ(ik−1) = ik, σ(ik) = i1,\n\nand σ(j) = j, for j ∈ [n]−{i1, . . . , ik}. The set {i1, . . . , ik} is called the domain of the cyclic\npermutation, and the cyclic permutation is usually denoted by (i1 i2 . . . ik).\n\nIf τ is a transposition, clearly, τ ◦ τ = id. Also, a cyclic permutation of order 2 is a\ntransposition, and for a cyclic permutation σ of order k, we have σk = id. Clearly, the\ncomposition of two permutations is a permutation and every permutation has an inverse\nwhich is also a permutation. Therefore, the set of permutations on [n] is a group often\ndenoted Sn. It is easy to show by induction that the group Sn has n! elements. We will\nalso use the terminology product of permutations (or transpositions), as a synonym for\ncomposition of permutations.\n\nA permutation σ on n elements, say σ(i) = ki for i = 1, . . . , n, can be represented in\nfunctional notation by the 2× n array(\n\n1 · · · i · · · n\nk1 · · · ki · · · kn\n\n)\nknown as Cauchy two-line notation. For example, we have the permutation σ denoted by(\n\n1 2 3 4 5 6\n2 4 3 6 5 1\n\n)\n.\n\nA more concise notation often used in computer science and in combinatorics is to rep-\nresent a permutation by its image, namely by the sequence\n\nσ(1) σ(2) · · · σ(n)\n\nwritten as a row vector without commas separating the entries. The above is known as\nthe one-line notation. For example, in the one-line notation, our previous permutation σ is\nrepresented by\n\n2 4 3 6 5 1.\n\nThe reason for not enclosing the above sequence within parentheses is avoid confusion with\nthe notation for cycles, for which is it customary to include parentheses.\n\nThe following proposition shows the importance of cyclic permutations and transposi-\ntions.\n\nProposition 7.1. For every n ≥ 2, for every permutation π : [n]→ [n], there is a partition\nof [n] into r subsets called the orbits of π, with 1 ≤ r ≤ n, where each set J in this partition\nis either a singleton {i}, or it is of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\n\n\n\n7.1. PERMUTATIONS, SIGNATURE OF A PERMUTATION 183\n\nwhere ri is the smallest integer, such that, πri(i) = i and 2 ≤ ri ≤ n. If π is not the identity,\nthen it can be written in a unique way (up to the order) as a composition π = σ1 ◦ . . . ◦ σs\nof cyclic permutations with disjoint domains, where s is the number of orbits with at least\ntwo elements. Every permutation π : [n]→ [n] can be written as a nonempty composition of\ntranspositions.\n\nProof. Consider the relation Rπ defined on [n] as follows: iRπj iff there is some k ≥ 1 such\nthat j = πk(i). We claim that Rπ is an equivalence relation. Transitivity is obvious. We\nclaim that for every i ∈ [n], there is some least r (1 ≤ r ≤ n) such that πr(i) = i.\n\nIndeed, consider the following sequence of n+ 1 elements:\n\n〈i, π(i), π2(i), . . . , πn(i)〉.\n\nSince [n] only has n distinct elements, there are some h, k with 0 ≤ h < k ≤ n such that\n\nπh(i) = πk(i),\n\nand since π is a bijection, this implies πk−h(i) = i, where 0 ≤ k − h ≤ n. Thus, we proved\nthat there is some integer m ≥ 1 such that πm(i) = i, so there is such a smallest integer r.\n\nConsequently, Rπ is reflexive. It is symmetric, since if j = πk(i), letting r be the least\nr ≥ 1 such that πr(i) = i, then\n\ni = πkr(i) = πk(r−1)(πk(i)) = πk(r−1)(j).\n\nNow, for every i ∈ [n], the equivalence class (orbit) of i is a subset of [n], either the singleton\n{i} or a set of the form\n\nJ = {i, π(i), π2(i), . . . , πri−1(i)},\nwhere ri is the smallest integer such that πri(i) = i and 2 ≤ ri ≤ n, and in the second case,\nthe restriction of π to J induces a cyclic permutation σi, and π = σ1 ◦ . . . ◦σs, where s is the\nnumber of equivalence classes having at least two elements.\n\nFor the second part of the proposition, we proceed by induction on n. If n = 2, there are\nexactly two permutations on [2], the transposition τ exchanging 1 and 2, and the identity.\nHowever, id2 = τ 2. Now, let n ≥ 3. If π(n) = n, since by the induction hypothesis, the\nrestriction of π to [n − 1] can be written as a product of transpositions, π itself can be\nwritten as a product of transpositions. If π(n) = k 6= n, letting τ be the transposition such\nthat τ(n) = k and τ(k) = n, it is clear that τ ◦ π leaves n invariant, and by the induction\nhypothesis, we have τ ◦ π = τm ◦ . . . ◦ τ1 for some transpositions, and thus\n\nπ = τ ◦ τm ◦ . . . ◦ τ1,\n\na product of transpositions (since τ ◦ τ = idn).\n\n\n\n184 CHAPTER 7. DETERMINANTS\n\nRemark: When π = idn is the identity permutation, we can agree that the composition of\n0 transpositions is the identity. The second part of Proposition 7.1 shows that the transpo-\nsitions generate the group of permutations Sn.\n\nIn writing a permutation π as a composition π = σ1 ◦ . . . ◦ σs of cyclic permutations, it\nis clear that the order of the σi does not matter, since their domains are disjoint. Given\na permutation written as a product of transpositions, we now show that the parity of the\nnumber of transpositions is an invariant.\n\nDefinition 7.2. For every n ≥ 2, since every permutation π : [n] → [n] defines a partition\nof r subsets over which π acts either as the identity or as a cyclic permutation, let ε(π),\ncalled the signature of π, be defined by ε(π) = (−1)n−r, where r is the number of sets in the\npartition.\n\nIf τ is a transposition exchanging i and j, it is clear that the partition associated with\nτ consists of n − 1 equivalence classes, the set {i, j}, and the n − 2 singleton sets {k}, for\nk ∈ [n]− {i, j}, and thus, ε(τ) = (−1)n−(n−1) = (−1)1 = −1.\n\nProposition 7.2. For every n ≥ 2, for every permutation π : [n] → [n], for every transpo-\nsition τ , we have\n\nε(τ ◦ π) = −ε(π).\n\nConsequently, for every product of transpositions such that π = τm ◦ . . . ◦ τ1, we have\n\nε(π) = (−1)m,\n\nwhich shows that the parity of the number of transpositions is an invariant.\n\nProof. Assume that τ(i) = j and τ(j) = i, where i < j. There are two cases, depending\nwhether i and j are in the same equivalence class Jl of Rπ, or if they are in distinct equivalence\nclasses. If i and j are in the same class Jl, then if\n\nJl = {i1, . . . , ip, . . . iq, . . . ik},\n\nwhere ip = i and iq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = iq\n\nand\nτ(π(iq−1)) = τ(iq) = τ(j) = i = ip,\n\nit is clear that Jl splits into two subsets, one of which is {ip, . . . , iq−1}, and thus, the number\nof classes associated with τ ◦ π is r + 1, and ε(τ ◦ π) = (−1)n−r−1 = −(−1)n−r = −ε(π). If i\nand j are in distinct equivalence classes Jl and Jm, say\n\n{i1, . . . , ip, . . . ih}\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{j1, . . . , jq, . . . jk},\n\nwhere ip = i and jq = j, since\n\nτ(π(π−1(ip))) = τ(ip) = τ(i) = j = jq\n\nand\nτ(π(π−1(jq))) = τ(jq) = τ(j) = i = ip,\n\nwe see that the classes Jl and Jm merge into a single class, and thus, the number of classes\nassociated with τ ◦ π is r − 1, and ε(τ ◦ π) = (−1)n−r+1 = −(−1)n−r = −ε(π).\n\nNow, let π = τm ◦ . . . ◦ τ1 be any product of transpositions. By the first part of the\nproposition, we have\n\nε(π) = (−1)m−1ε(τ1) = (−1)m−1(−1) = (−1)m,\n\nsince ε(τ1) = −1 for a transposition.\n\nRemark: When π = idn is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (−1)0 = ε(id) = +1. From the\nproposition, it is immediate that ε(π′ ◦ π) = ε(π′)ε(π). In particular, since π−1 ◦ π = idn, we\nget ε(π−1) = ε(π).\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over K (free modules, when bases are\nneeded).\n\nLet E1, . . . , En, and F , be vector spaces over a field K, where n ≥ 1.\n\nDefinition 7.3. A function f : E1 × . . . × En → F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every i,\n1 ≤ i ≤ n, for all x1 ∈ E1, . . ., xi−1 ∈ Ei−1, xi+1 ∈ Ei+1, . . ., xn ∈ En, for all x, y ∈ Ei, for all\nλ ∈ K,\n\nf(x1, . . . , xi−1, x+ y, xi+1, . . . , xn) = f(x1, . . . , xi−1, x, xi+1, . . . , xn)\n\n+ f(x1, . . . , xi−1, y, xi+1, . . . , xn),\n\nf(x1, . . . , xi−1, λx, xi+1, . . . , xn) = λf(x1, . . . , xi−1, x, xi+1, . . . , xn).\n\n7.2. ALTERNATING MULTILINEAR MAPS 185\n\nand\n{i1, see Jar : Jkt,\n\nwhere 7, = 7% and j, = j, since\n\nand\n\nT(m(m~*(jq))) =T(Jq) =TU) = t= ty,\nwe see that the classes J; and J;, merge into a single class, and thus, the number of classes\nassociated with 7 om is r—1, and e(7 om) = (—1)\"\"*1 = —(-1)\"\" = —c(z).\n\nNow, let 7 = Tm 0...07, be any product of transpositions. By the first part of the\nproposition, we have\n\nsince €(7,) = —1 for a transposition. O\n\nRemark: When z = id, is the identity permutation, since we agreed that the composition\nof 0 transpositions is the identity, it it still correct that (—1)° = e(id) = +1. From the\nproposition, it is immediate that e(a’ om) = e(n’)e(z). In particular, since 77!\nget e(7!) = e(z).\n\nom =id,, we\n\nWe can now proceed with the definition of determinants.\n\n7.2 Alternating Multilinear Maps\n\nFirst we define multilinear maps, symmetric multilinear maps, and alternating multilinear\nmaps.\n\nRemark: Most of the definitions and results presented in this section also hold when K is\na commutative ring and when we consider modules over A (free modules, when bases are\n\nneeded).\nLet E,,...,E,, and F, be vector spaces over a field K, where n > 1.\n\nDefinition 7.3. A function f: FE, x... x E, > F is a multilinear map (or an n-linear\nmap) if it is linear in each argument, holding the others fixed. More explicitly, for every 1,\n1<i<n, for all x, € Fy,..., m1 © Fj-4, tin. © Figs, --, Un © En, for all x,y € K, for all\nAE K,\n\nf(@1,---, Ui, + Y, Vig, ---5 En) = f(41,-- +, Vi-1, V, Lig,---, Ln)\n+ f(a, wee Vi-1,Y, Vit1,--- En),\n\nf(“1, te ,Tj—1, AL, Vin, te Ln) = Af (£1, vee Di-1, VU, Vi41,--- En).\n\n\n\n\n186 CHAPTER 7. DETERMINANTS\n\nWhen F = K, we call f an n-linear form (or multilinear form). If n ≥ 2 and E1 =\nE2 = . . . = En, an n-linear map f : E × . . .×E → F is called symmetric, if f(x1, . . . , xn) =\nf(xπ(1), . . . , xπ(n)) for every permutation π on {1, . . . , n}. An n-linear map f : E×. . .×E → F\nis called alternating , if f(x1, . . . , xn) = 0 whenever xi = xi+1 for some i, 1 ≤ i ≤ n − 1 (in\nother words, when two adjacent arguments are equal). It does no harm to agree that when\nn = 1, a linear map is considered to be both symmetric and alternating, and we will do so.\n\nWhen n = 2, a 2-linear map f : E1 × E2 → F is called a bilinear map. We have already\nseen several examples of bilinear maps. Multiplication · : K × K → K is a bilinear map,\ntreating K as a vector space over itself.\n\nThe operation 〈−,−〉 : E∗×E → K applying a linear form to a vector is a bilinear map.\n\nSymmetric bilinear maps (and multilinear maps) play an important role in geometry\n(inner products, quadratic forms) and in differential calculus (partial derivatives).\n\nA bilinear map is symmetric if f(u, v) = f(v, u), for all u, v ∈ E.\n\nAlternating multilinear maps satisfy the following simple but crucial properties.\n\nProposition 7.3. Let f : E× . . .×E → F be an n-linear alternating map, with n ≥ 2. The\nfollowing properties hold:\n\n(1)\nf(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .)\n\n(2)\nf(. . . , xi, . . . , xj, . . .) = 0,\n\nwhere xi = xj, and 1 ≤ i < j ≤ n.\n\n(3)\nf(. . . , xi, . . . , xj, . . .) = −f(. . . , xj, . . . , xi, . . .),\n\nwhere 1 ≤ i < j ≤ n.\n\n(4)\nf(. . . , xi, . . .) = f(. . . , xi + λxj, . . .),\n\nfor any λ ∈ K, and where i 6= j.\n\nProof. (1) By multilinearity applied twice, we have\n\nf(. . . , xi + xi+1, xi + xi+1, . . .) = f(. . . , xi, xi, . . .) + f(. . . , xi, xi+1, . . .)\n\n+ f(. . . , xi+1, xi, . . .) + f(. . . , xi+1, xi+1, . . .),\n\nand since f is alternating, this yields\n\n0 = f(. . . , xi, xi+1, . . .) + f(. . . , xi+1, xi, . . .),\n\n\n\n7.2. ALTERNATING MULTILINEAR MAPS 187\n\nthat is, f(. . . , xi, xi+1, . . .) = −f(. . . , xi+1, xi, . . .).\n\n(2) If xi = xj and i and j are not adjacent, we can interchange xi and xi+1, and then xi\nand xi+2, etc, until xi and xj become adjacent. By (1),\n\nf(. . . , xi, . . . , xj, . . .) = εf(. . . , xi, xj, . . .),\n\nwhere ε = +1 or −1, but f(. . . , xi, xj, . . .) = 0, since xi = xj, and (2) holds.\n\n(3) follows from (2) as in (1). (4) is an immediate consequence of (2).\n\nProposition 7.3 will now be used to show a fundamental property of alternating multilin-\near maps. First we need to extend the matrix notation a little bit. Let E be a vector space\nover K. Given an n× n matrix A = (ai j) over K, we can define a map L(A) : En → En as\nfollows:\n\nL(A)1(u) = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nL(A)n(u) = an 1u1 + · · ·+ annun,\n\nfor all u1, . . . , un ∈ E and with u = (u1, . . . , un). It is immediately verified that L(A) is\nlinear. Then given two n×n matrices A = (ai j) and B = (bi j), by repeating the calculations\nestablishing the product of matrices (just before Definition 3.12), we can show that\n\nL(AB) = L(A) ◦ L(B).\n\nIt is then convenient to use the matrix notation to describe the effect of the linear map L(A),\nas \n\nL(A)1(u)\nL(A)2(u)\n\n...\nL(A)n(u)\n\n =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nu1\n\nu2\n...\nun\n\n .\n\nLemma 7.4. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un) and\n(v1, . . . , vn) be two families of n vectors, such that,\n\nv1 = a1 1u1 + · · ·+ an 1un,\n\n. . .\n\nvn = a1nu1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A>\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nf(u1, . . . , un),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nProof. Expanding f(v1, . . . , vn) by multilinearity, we get a sum of terms of the form\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)),\n\nfor all possible functions π : {1, . . . , n} → {1, . . . , n}. However, because f is alternating, only\nthe terms for which π is a permutation are nonzero. By Proposition 7.1, every permutation\nπ is a product of transpositions, and by Proposition 7.2, the parity ε(π) of the number of\ntranspositions only depends on π. Then applying Proposition 7.3 (3) to each transposition\nin π, we get\n\naπ(1) 1 · · · aπ(n)nf(uπ(1), . . . , uπ(n)) = ε(π)aπ(1) 1 · · · aπ(n)nf(u1, . . . , un).\n\nThus, we get the expression of the lemma.\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, v2) = f(a11u1 + a21u2, a12u1 + a22u2)\n\n= f(a11u1 + a21u2, a12u1) + f(a11u1 + a21u2, a22u2)\n\n= f(a11u1, a12u1) + f(a21u2, a12u1) + f(a11ua, a22u2) + f(a21u2, a22u2)\n\n= a11a12f(u1, u1) + a21a12f(u2, u1) + a11a22f(u1, u2) + a21a22f(u2, u2)\n\n= a21a12f(u2, u1)a11a22f(u1, u2)\n\n= (a11a22 − a12a22) f(u1, u2).\n\nHopefully the reader will recognize the quantity a11a22− a12a22. It is the determinant of the\n2× 2 matrix\n\nA =\n\n(\na11 a12\n\na21 a22\n\n)\n.\n\nThis is no accident. The quantity\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n188 CHAPTER 7. DETERMINANTS\n\nassume that we have\n\nUy U1\nU2 _ At U2\nUn Un\nThen,\nf(ur, te ,Un) = ( S- €(T)an(1)1 oe Arin)n) f(t, te , Un),\nTEGn\nwhere the sum ranges over all permutations 7 on {1,...,n}.\nProof. Expanding f(v1,...,Un) by multilinearity, we get a sum of terms of the form\nAn(1)1*** On(n) nd (Un(1)s+++5Um(n))s\nfor all possible functions 7: {1,...,n}— {1,...,n}. However, because f is alternating, only\n\nthe terms for which 7 is a permutation are nonzero. By Proposition 7.1, every permutation\nm is a product of transpositions, and by Proposition 7.2, the parity e(7) of the number of\ntranspositions only depends on 7. Then applying Proposition 7.3 (3) to each transposition\nin 77, we get\n\nAn(1)1°** An(n) nd (Un(1)>+ «+5 Um(n)) = E(T)An(1) 1° ** Ar(n) nd (Ua, -.. 5 Un):\nThus, we get the expression of the lemma. im\n\nFor the case of n = 2, the proof details of Lemma 7.4 become\n\nf(v1, 02) = f\n=f\nf\n\nay U + Gg1U2, A121 + A22QU2)\nQ41Uy + Ag U2, A12U1) + f (Qiu + Goi U2, do2U2)\n\nQy1U1, A121) + f(aeit2, di2t1) + f(a11ta, d22U2) + f(ao1U2, d22U2)\n\n—_~~ ~~\n\n= dz G12 f (U2, U1) A114 f (ur, U2)\n\n= (a4 122 _— 12422) f (ui, Ug).\n\nHopefully the reader will recognize the quantity a11d@92 — @12d22. It is the determinant of the\n2 xX 2 matrix\nA- ai1 G12\na21 422\nThis is no accident. The quantity\n\ndet(A) = S° €(T)Ax(1)1°** Ax(n)n\n\nTEGH\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 189\n\nis in fact the value of the determinant of A (which, as we shall see shortly, is also equal to the\ndeterminant of A>). However, working directly with the above definition is quite awkward,\nand we will proceed via a slightly indirect route\n\nRemark: The reader might have been puzzled by the fact that it is the transpose matrix\nA> rather than A itself that appears in Lemma 7.4. The reason is that if we want the generic\nterm in the determinant to be\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the permutation applies to the first index, then we have to express the vjs in terms\nof the uis in terms of A> as we did. Furthermore, since\n\nvj = a1 ju1 + · · ·+ ai jui + · · ·+ an jun,\n\nwe see that vj corresponds to the jth column of the matrix A, and so the determinant is\nviewed as a function of the columns of A.\n\nThe literature is split on this point. Some authors prefer to define a determinant as we\ndid. Others use A itself, which amounts to viewing det as a function of the rows, in which\ncase we get the expression ∑\n\nσ∈Sn\nε(σ)a1σ(1) · · · anσ(n).\n\nCorollary 7.7 show that these two expressions are equal, so it doesn’t matter which is chosen.\nThis is a matter of taste.\n\n7.3 Definition of a Determinant\n\nRecall that the set of all square n × n-matrices with coefficients in a field K is denoted by\nMn(K).\n\nDefinition 7.4. A determinant is defined as any map\n\nD : Mn(K)→ K,\n\nwhich, when viewed as a map on (Kn)n, i.e., a map of the n columns of a matrix, is n-linear\nalternating and such that D(In) = 1 for the identity matrix In. Equivalently, we can consider\na vector space E of dimension n, some fixed basis (e1, . . . , en), and define\n\nD : En → K\n\nas an n-linear alternating map such that D(e1, . . . , en) = 1.\n\n\n\n190 CHAPTER 7. DETERMINANTS\n\nFirst we will show that such maps D exist, using an inductive definition that also gives\na recursive method for computing determinants. Actually, we will define a family (Dn)n≥1\n\nof (finite) sets of maps D : Mn(K)→ K. Second we will show that determinants are in fact\nuniquely defined, that is, we will show that each Dn consists of a single map. This will show\nthe equivalence of the direct definition det(A) of Lemma 7.4 with the inductive definition\nD(A). Finally, we will prove some basic properties of determinants, using the uniqueness\ntheorem.\n\nGiven a matrix A ∈ Mn(K), we denote its n columns by A1, . . . , An. In order to describe\nthe recursive process to define a determinant we need the notion of a minor.\n\nDefinition 7.5. Given any n×n matrix with n ≥ 2, for any two indices i, j with 1 ≤ i, j ≤ n,\nlet Aij be the (n − 1) × (n − 1) matrix obtained by deleting Row i and Column j from A\nand called a minor :\n\nAij =\n\n\n\n×\n×\n\n× × × × × × ×\n×\n×\n×\n×\n\n\n.\n\nFor example, if\n\nA =\n\n\n2 −1 0 0 0\n−1 2 −1 0 0\n0 −1 2 −1 0\n0 0 −1 2 −1\n0 0 0 −1 2\n\n\nthen\n\nA2 3 =\n\n\n2 −1 0 0\n0 −1 −1 0\n0 0 2 −1\n0 0 −1 2\n\n .\n\nDefinition 7.6. For every n ≥ 1, we define a finite set Dn of maps D : Mn(K) → K\ninductively as follows:\n\nWhen n = 1, D1 consists of the single map D such that, D(A) = a, where A = (a), with\na ∈ K.\n\nAssume that Dn−1 has been defined, where n ≥ 2. Then Dn consists of all the maps D\nsuch that, for some i, 1 ≤ i ≤ n,\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+nai nD(Ai n),\n\nwhere for every j, 1 ≤ j ≤ n, D(Ai j) is the result of applying any D in Dn−1 to the minor\nAi j.\n\n\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n� We confess that the use of the same letter D for the member of Dn being defined, and\nfor members of Dn−1, may be slightly confusing. We considered using subscripts to\n\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each Dn contains just one map.\n\nEach (−1)i+jD(Ai j) is called the cofactor of ai j, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row . Given a matrix A ∈ Mn(K),\neach D(A) is called a determinant of A.\n\nWe can think of each member of Dn as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nDn consists of a single map). Assuming this fact, given a n× n-matrix A = (ai j),\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\ndet(A) =\n\n∣∣∣∣∣∣∣∣∣\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n∣∣∣∣∣∣∣∣∣ .\n\nLet us first consider some examples.\n\nExample 7.1.\n\n1. When n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen by expanding according to any row, we have\n\nD(A) = ad− bc.\n\n2. When n = 3, if\n\nA =\n\na1 1 a1 2 a1 3\n\na2 1 a2 2 a2 3\n\na3 1 a3 2 a3 3\n\n ,\n\n7.3. DEFINITION OF A DETERMINANT 191\n\n© We confess that the use of the same letter D for the member of D,, being defined, and\nfor members of D,_1, may be slightly confusing. We considered using subscripts to\ndistinguish, but this seems to complicate things unnecessarily. One should not worry too\nmuch anyway, since it will turn out that each D, contains just one map.\n\nEach (—1)'*? D(A;;) is called the cofactor of a;;, and the inductive expression for D(A)\nis called a Laplace expansion of D according to the i-th Row. Given a matrix A € M,(KkK),\neach D(A) is called a determinant of A.\n\nWe can think of each member of D,, as an algorithm to evaluate “the” determinant of A.\nThe main point is that these algorithms, which recursively evaluate a determinant using all\npossible Laplace row expansions, all yield the same result, det(A).\n\nWe will prove shortly that D(A) is uniquely defined (at the moment, it is not clear that\nD,, consists of a single map). Assuming this fact, given a n x n-matrix A = (a;,;),\n\nQi, Qay2q ..-- Atn\n\na2, G22 ... Aan\nA= . . ;\n\nQn1 GQn2 ---» Ann\n\nits determinant is denoted by D(A) or det(A), or more explicitly by\n\na1, %aaq .-- Ain\n\na2, G22 ... QAan\ndet(A) =] . j\n\nQn1 Qn2 +--+. Ann\n\nLet us first consider some examples.\nExample 7.1.\n1. When n = 2, if\nA= (: ‘) .\n\nthen by expanding according to any row, we have\nD(A) = ad — be.\n\n2. When n = 3, if\nQ11 G12 13\nA= |do1 G2 93\n\n431 432 433\n\n\n\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\nD(A) = a1 1\n\n∣∣∣∣a2 2 a2 3\n\na3 2 a3 3\n\n∣∣∣∣− a1 2\n\n∣∣∣∣a2 1 a2 3\n\na3 1 a3 3\n\n∣∣∣∣+ a1 3\n\n∣∣∣∣a2 1 a2 2\n\na3 1 a3 2\n\n∣∣∣∣ ,\nthat is,\n\nD(A) = a1 1(a2 2a3 3 − a3 2a2 3)− a1 2(a2 1a3 3 − a3 1a2 3) + a1 3(a2 1a3 2 − a3 1a2 2),\n\nwhich gives the explicit formula\n\nD(A) = a1 1a2 2a3 3 + a2 1a3 2a1 3 + a3 1a1 2a2 3 − a1 1a3 2a2 3 − a2 1a1 2a3 3 − a3 1a2 2a1 3.\n\nWe now show that each D ∈ Dn is a determinant (map).\n\nLemma 7.5. For every n ≥ 1, for every D ∈ Dn as defined in Definition 7.6, D is an\nalternating multilinear map such that D(In) = 1.\n\nProof. By induction on n, it is obvious that D(In) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (−1)i+1ai 1D(Ai 1) + · · ·+ (−1)i+jai jD(Ai j) + · · ·+ (−1)i+nai nD(Ai n),\n\nif j 6= k, then by induction, D(Ai j) is linear in Column k, and ai j does not belong to Column\nk, so (−1)i+jai jD(Ai j) is linear in Column k. If j = k, then D(Ai j) does not depend on\nColumn k = j, since Ai j is obtained from A by deleting Row i and Column j = k, and ai j\nbelongs to Column j = k. Thus, (−1)i+jai jD(Ai j) is linear in Column k. Consequently, in\nall cases, (−1)i+jai jD(Ai j) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say Ak = Ak+1. Assume that j 6= k and j 6= k + 1. Then the matrix Ai j has two\nidentical adjacent columns, and by the induction hypothesis, D(Ai j) = 0. The remaining\nterms of D(A) are\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1).\n\nHowever, the two matrices Ai k and Ai k+1 are equal, since we are assuming that Columns k\nand k + 1 of A are identical and Ai k is obtained from A by deleting Row i and Column k\nwhile Ai k+1 is obtained from A by deleting Row i and Column k+ 1. Similarly, ai k = ai k+1,\nsince Columns k and k + 1 of A are equal. But then,\n\n(−1)i+kai kD(Ai k) + (−1)i+k+1ai k+1D(Ai k+1) = (−1)i+kai kD(Ai k)− (−1)i+kai kD(Ai k) = 0.\n\nThis shows that D is alternating and completes the proof.\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n192 CHAPTER 7. DETERMINANTS\n\nthen by expanding according to the first row, we have\n\na21 422\na31 432\n\na21 423\n\n2 + a13\n431 433\n\nv]\n\nthat is,\nD(A) = 11 (22033 _ 32023) _ 1 2(a21433 _ 31423) + 1 3(G21432 _ 31422),\nwhich gives the explicit formula\n\nD(A) = 011422433 + 421432013 + 431412423 — 411032423 — 421012433 — 4314220)3.\n\nWe now show that each D € D,, is a determinant (map).\n\nLemma 7.5. For every n > 1, for every D € Dy, as defined in Definition 7.6, D is an\nalternating multilinear map such that D(I,) = 1.\n\nProof. By induction on n, it is obvious that D(/,) = 1. Let us now prove that D is\nmultilinear. Let us show that D is linear in each column. Consider any Column k. Since\n\nD(A) = (-1)'*\"a;1D(Ajit) +--+ + (-1)' aj D(Aij) + + (-1)GinD(Ain),\n\nif 7 # k, then by induction, D(A;,) is linear in Column k, and a;; does not belong to Column\nk, so (—1)'a,;;D(A;,;) is linear in Column k. If 7 = k, then D(A;;) does not depend on\nColumn k = j, since A;; is obtained from A by deleting Row 7 and Column j = k, and a; ;\nbelongs to Column j = k. Thus, (—1)'’a;;D(A;;) is linear in Column k. Consequently, in\nall cases, (—1)'*7a;;D(A;,;) is linear in Column k, and thus, D(A) is linear in Column k.\n\nLet us now prove that D is alternating. Assume that two adjacent columns of A are\nequal, say A* = A**t!, Assume that 7 # k and j 4 k+1. Then the matrix A;; has two\nidentical adjacent columns, and by the induction hypothesis, D(A;;) = 0. The remaining\nterms of D(A) are\n\n(—1)'*ajxD(Ain) + (1) ain D(Ainyt)-\n\nHowever, the two matrices A;, and A;,4; are equal, since we are assuming that Columns k\nand k +1 of A are identical and A;; is obtained from A by deleting Row i and Column k\nwhile A;;41 is obtained from A by deleting Row i and Column k +1. Similarly, aj, = ajx4i,\nsince Columns k and k + 1 of A are equal. But then,\n\n(—1)*8a;4D(Aig) + (—1) ai ng (Aina) = (-1)' aie D(Ain) — (-1) \"ain D(Aix) = 0.\nThis shows that D is alternating and completes the proof. im\n\nLemma 7.5 shows the existence of determinants. We now prove their uniqueness.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 193\n\nTheorem 7.6. For every n ≥ 1, for every D ∈ Dn, for every matrix A ∈ Mn(K), we have\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. As a consequence, Dn consists\nof a single map for every n ≥ 1, and this map is given by the above explicit formula.\n\nProof. Consider the standard basis (e1, . . . , en) of Kn, where (ei)i = 1 and (ei)j = 0, for\nj 6= i. Then each column Aj of A corresponds to a vector vj whose coordinates over the\nbasis (e1, . . . , en) are the components of Aj, that is, we can write\n\nv1 = a1 1e1 + · · ·+ an 1en,\n\n. . .\n\nvn = a1ne1 + · · ·+ annen.\n\nSince by Lemma 7.5, each D is a multilinear alternating map, by applying Lemma 7.4, we\nget\n\nD(A) = D(v1, . . . , vn) =\n(∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n\n\n)\nD(e1, . . . , en),\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. But D(e1, . . . , en) = D(In),\nand by Lemma 7.5, we have D(In) = 1. Thus,\n\nD(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}.\n\nFrom now on we will favor the notation det(A) over D(A) for the determinant of a square\nmatrix.\n\nRemark: There is a geometric interpretation of determinants which we find quite illumi-\nnating. Given n linearly independent vectors (u1, . . . , un) in Rn, the set\n\nPn = {λ1u1 + · · ·+ λnun | 0 ≤ λi ≤ 1, 1 ≤ i ≤ n}\n\nis called a parallelotope. If n = 2, then P2 is a parallelogram and if n = 3, then P3 is a\nparallelepiped , a skew box having u1, u2, u3 as three of its corner sides. See Figures 7.1 and\n7.2.\n\nThen it turns out that det(u1, . . . , un) is the signed volume of the parallelotope Pn (where\nvolume means n-dimensional volume). The sign of this volume accounts for the orientation\nof Pn in Rn.\n\nWe can now prove some properties of determinants.\n\n\n\n194 CHAPTER 7. DETERMINANTS\n\nu = (1,0)1\n\nu = (1,1)\n2\n\nFigure 7.1: The parallelogram in Rw spanned by the vectors u1 = (1, 0) and u2 = (1, 1).\n\nCorollary 7.7. For every matrix A ∈ Mn(K), we have det(A) = det(A>).\n\nProof. By Theorem 7.6, we have\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n,\n\nwhere the sum ranges over all permutations π on {1, . . . , n}. Since a permutation is invertible,\nevery product\n\naπ(1) 1 · · · aπ(n)n\n\ncan be rewritten as\na1π−1(1) · · · anπ−1(n),\n\nand since ε(π−1) = ε(π) and the sum is taken over all permutations on {1, . . . , n}, we have∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n),\n\nwhere π and σ range over all permutations. But it is immediately verified that\n\ndet(A>) =\n∑\nσ∈Sn\n\nε(σ)a1σ(1) · · · anσ(n).\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n194 CHAPTER 7. DETERMINANTS\n\n0.8 4\n0.64\n0.44\n\n0.24\n\n1\nu= (1,0)\n\nFigure 7.1: The parallelogram in R” spanned by the vectors u; = (1,0) and uz = (1,1).\n\nCorollary 7.7. For every matriz A € M,,(K), we have det(A) = det(A‘).\n\nProof. By Theorem 7.6, we have\n\ndet (A) = S- €(17)ax(1) 1° °° An(n)ns\n\nTEGn\n\nwhere the sum ranges over all permutations 7 on {1,...,}. Since a permutation is invertible,\nevery product\n\nAn(1)1°°* Ax(n)n\ncan be rewritten as\n\nA1q-1(1) °° Anal (n)s\n\nand since e(7~!) = e(7) and the sum is taken over all permutations on {1,...,n}, we have\nS- €(1)Ax(1) 1° °° Ag(n)n = S- €(o)ay o(1) °°\" Gno(n)>\nTEGn aEGn\n\nwhere 7 and o range over all permutations. But it is immediately verified that\n\ndet(A') = S- €(7)Q1 (1) *** An o(n): d\n\naEGn\n\nA useful consequence of Corollary 7.7 is that the determinant of a matrix is also a multi-\nlinear alternating map of its rows. This fact, combined with the fact that the determinant of\na matrix is a multilinear alternating map of its columns, is often useful for finding short-cuts\nin computing determinants. We illustrate this point on the following example which shows\nup in polynomial interpolation.\n\n\n\n\n7.3. DEFINITION OF A DETERMINANT 195\n\nu = (1,1,0)\n1\n\nu = (0,1,0)\n2\n\nu = (1,1,1)\n3\n\nFigure 7.2: The parallelepiped in R3 spanned by the vectors u1 = (1, 1, 0), u2 = (0, 1, 0), and\nu3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\nx1 x2 . . . xn\nx2\n\n1 x2\n2 . . . x2\n\nn\n...\n\n...\n. . .\n\n...\nxn−1\n\n1 xn−1\n2 . . . xn−1\n\nn\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nWe claim that\n\nV (x1, . . . , xn) =\n∏\n\n1≤i<j≤n\n(xj − xi),\n\nwith V (x1, . . . , xn) = 1, when n = 1. We prove it by induction on n ≥ 1. The case n = 1 is\nobvious. Assume n ≥ 2. We proceed as follows: multiply Row n − 1 by x1 and subtract it\nfrom Row n (the last row), then multiply Row n− 2 by x1 and subtract it from Row n− 1,\netc, multiply Row i− 1 by x1 and subtract it from row i, until we reach Row 1. We obtain\n\n7.3. DEFINITION OF A DETERMINANT\n\n14\n084\nv4\n044\n\n0.25\n\n195\n\nFigure 7.2: The parallelepiped in R® spanned by the vectors u, = (1, 1,0), we = (0,1,0), and\n\nU3 = (0, 0, 1).\n\nExample 7.2. Consider the so-called Vandermonde determinant\n\n1 1 1\n\nLy 2 In\n\n2 2 2\n\nV(a1,...,%n) =] %1  % Ly,\ncp! at grt\n\nWe claim that\n\nV(a1,.--,2%n) = II (x; — 7),\n\n1<i<j<n\n\nwith V(a1,...,%) = 1, when n = 1. We prove it by induction on n > 1. The case n = 1 is\nobvious. Assume n > 2. We proceed as follows: multiply Row n — 1 by x, and subtract it\nfrom Row n (the last row), then multiply Row n — 2 by x; and subtract it from Row n— 1,\netc, multiply Row 7 — 1 by x, and subtract it from row 7, until we reach Row 1. We obtain\n\n\n\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\nV (x1, . . . , xn) =\n\n∣∣∣∣∣∣∣∣∣∣∣\n\n1 1 . . . 1\n0 x2 − x1 . . . xn − x1\n\n0 x2(x2 − x1) . . . xn(xn − x1)\n...\n\n...\n. . .\n\n...\n0 xn−2\n\n2 (x2 − x1) . . . xn−2\nn (xn − x1)\n\n∣∣∣∣∣∣∣∣∣∣∣\n.\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (xi − x1) from the column of index i − 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV (x1, . . . , xn) = (x2 − x1)(x3 − x1) · · · (xn − x1)V (x2, . . . , xn),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f : E × . . .×E → F be an n-linear alternating map. Let (u1, . . . , un)\nand (v1, . . . , vn) be two families of n vectors, such that\n\nv1 = a1 1u1 + · · ·+ a1nun,\n\n. . .\n\nvn = an 1u1 + · · ·+ annun.\n\nEquivalently, letting\n\nA =\n\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n ,\n\nassume that we have \nv1\n\nv2\n...\nvn\n\n = A\n\n\nu1\n\nu2\n...\nun\n\n .\n\nThen,\n\nf(v1, . . . , vn) = det(A)f(u1, . . . , un).\n\nProof. The only difference with Lemma 7.4 is that here we are using A> instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result.\n\n196 CHAPTER 7. DETERMINANTS\n\nthe following determinant:\n\n1 1 a 1\n\n0 L— Ly a In — Ly\nV(a1,---,2n) = 0 %2(%2—- 21)... En (4% — 11)\n\n0 af 7(ag-91) ... 2\"? (a, — 21)\n\nNow expanding this determinant according to the first column and using multilinearity,\nwe can factor (7; — x1) from the column of index i — 1 of the matrix obtained by deleting\nthe first row and the first column, and thus\n\nV(a1,.--,%n) = (\"2 — ©1)(43 — 1) +++ (pn — 21)V (H0,...,Ln),\n\nwhich establishes the induction step.\n\nLemma 7.4 can be reformulated nicely as follows.\n\nProposition 7.8. Let f: Ex...x E— F be an n-linear alternating map. Let (u1,..., Un)\nand (U1,...,Un) be two families of n vectors, such that\n\nVy = 441Uy +--+ + A nUn,\n\nUn = An{Uy + +++ + AnnUn-\n\nEquivalently, letting\n\na11 G12 «+. An\nA= a21 (122 -.. Qan .\nQn1 Gn2 Ann\nassume that we have\nU1 U1\nV2 _A U2\nUn, Un\n\nThen,\nf(v1,-.-,Un) = det(A)f (ur, ...,Un)-\n\nProof. The only difference with Lemma 7.4 is that here we are using A' instead of A. Thus,\nby Lemma 7.4 and Corollary 7.7, we get the desired result. im\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 197\n\nAs a consequence, we get the very useful property that the determinant of a product of\nmatrices is the product of the determinants of these matrices.\n\nProposition 7.9. For any two n×n-matrices A and B, we have det(AB) = det(A) det(B).\n\nProof. We use Proposition 7.8 as follows: let (e1, . . . , en) be the standard basis of Kn, and\nlet \n\nw1\n\nw2\n...\nwn\n\n = AB\n\n\ne1\n\ne2\n...\nen\n\n .\n\nThen we get\n\ndet(w1, . . . , wn) = det(AB) det(e1, . . . , en) = det(AB),\n\nsince det(e1, . . . , en) = 1. Now letting\nv1\n\nv2\n...\nvn\n\n = B\n\n\ne1\n\ne2\n...\nen\n\n ,\n\nwe get\n\ndet(v1, . . . , vn) = det(B),\n\nand since \nw1\n\nw2\n...\nwn\n\n = A\n\n\nv1\n\nv2\n...\nvn\n\n ,\n\nwe get\n\ndet(w1, . . . , wn) = det(A) det(v1, . . . , vn) = det(A) det(B).\n\nIt should be noted that all the results of this section, up to now, also hold when K is a\ncommutative ring and not necessarily a field. We can now characterize when an n×n-matrix\nA is invertible in terms of its determinant det(A).\n\n7.4 Inverse Matrices and Determinants\n\nIn the next two sections, K is a commutative ring and when needed a field.\n\n\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A ∈ Mn(K), let Ã = (bi j)\nbe the matrix defined such that\n\nbi j = (−1)i+j det(Aj i),\n\nthe cofactor of aj i. The matrix Ã is called the adjugate of A, and each matrix Aj i is called\na minor of the matrix A.\n\nFor example, if\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n ,\n\nwe have\n\nb11 = det(A11) =\n\n∣∣∣∣ −2 −2\n3 −3\n\n∣∣∣∣ = 12 b12 = − det(A21) = −\n∣∣∣∣ 1 1\n\n3 −3\n\n∣∣∣∣ = 6\n\nb13 = det(A31) =\n\n∣∣∣∣ 1 1\n−2 −2\n\n∣∣∣∣ = 0 b21 = − det(A12) = −\n∣∣∣∣ 2 −2\n\n3 −3\n\n∣∣∣∣ = 0\n\nb22 = det(A22) =\n\n∣∣∣∣ 1 1\n3 −3\n\n∣∣∣∣ = −6 b23 = − det(A32) = −\n∣∣∣∣ 1 1\n\n2 −2\n\n∣∣∣∣ = 4\n\nb31 = det(A13) =\n\n∣∣∣∣ 2 −2\n3 3\n\n∣∣∣∣ = 12 b32 = − det(A23) = −\n∣∣∣∣ 1 1\n\n3 3\n\n∣∣∣∣ = 0\n\nb33 = det(A33) =\n\n∣∣∣∣ 1 1\n2 −2\n\n∣∣∣∣ = −4,\n\nwe find that\n\nÃ =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n .\n\n� Note the reversal of the indices in\n\nbi j = (−1)i+j det(Aj i).\n\nThus, Ã is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A ∈ Mn(K), we have\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, A−1 = (det(A))−1Ã.\n\n198 CHAPTER 7. DETERMINANTS\n\nDefinition 7.7. Let K be a commutative ring. Given a matrix A € M,(K), let A = (0; ;)\nbe the matrix defined such that\n\nbij = (-1)'™ det(A;,),\n\nthe cofactor of a;;. The matrix A is called the adjugate of A, and each matrix A,; is called\na minor of the matrix A.\n\nFor example, if\n\n1 1 1\nA= |{2 -—2 -2],\n3 3 -3\nwe have\n2 ~9 1 1\nby, = det(Ay1) = 3 3 | = 12 byg = — det (Agi) = — | 3 3 | = 6\n1 1 2 —2\nbi3 = det(A31) = 2 _2 | =0 by = —det(Aiz) = — | 3-3 | =O\n1 1 1 1\nboo = det (A22) = 3 3 =—6 bos = — det (Az) =~ | 2 —2 | = 4\n2 —2 1 1\n531 = det (Aj3) = 3 3 = 12 b32 = — det (Ags) =~ | 3 3 | =0\n1 1\n633 = det (A33) = a) = —4,\nwe find that\n7 12 6 O\nA=|0 -6 4\n12 0 —4\n\n© Note the reversal of the indices in\nbi = (-1)'\" det (Aj;;).\nThus, A is the transpose of the matrix of cofactors of elements of A.\n\nWe have the following proposition.\n\nProposition 7.10. Let K be a commutative ring. For every matrix A € M,,(K), we have\nAA = AA = det(A)Iy.\n\nAs a consequence, A is invertible iff det(A) is invertible, and if so, AT! = (det(A))~1A.\n\n\n\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If Ã = (bi j) and AÃ = (ci j), we know that the entry ci j in row i and column j of AÃ\nis\n\nci j = ai 1b1 j + · · ·+ ai kbk j + · · ·+ ai nbn j,\n\nwhich is equal to\n\nai 1(−1)j+1 det(Aj 1) + · · ·+ ai n(−1)j+n det(Aj n).\n\nIf j = i, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\n\nci i = det(A) = ai 1(−1)i+1 det(Ai 1) + · · ·+ ai n(−1)i+n det(Ai n).\n\nIf j 6= i, we can form the matrix A′ by replacing the j-th row of A by the i-th row of A.\nNow the matrix Aj k obtained by deleting row j and column k from A is equal to the matrix\nA′j k obtained by deleting row j and column k from A′, since A and A′ only differ by the j-th\nrow. Thus,\n\ndet(Aj k) = det(A′j k),\n\nand we have\n\nci j = ai 1(−1)j+1 det(A′j 1) + · · ·+ ai n(−1)j+n det(A′j n).\n\nHowever, this is the expansion of det(A′) according to the j-th row, since the j-th row of A′\n\nis equal to the i-th row of A. Furthermore, since A′ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A′) = 0. Thus,\nwe have shown that ci i = det(A), and ci j = 0, when j 6= i, and so\n\nAÃ = det(A)In.\n\nIt is also obvious from the definition of Ã, that\n\nÃ> = Ã>.\n\nThen applying the first part of the argument to A>, we have\n\nA>Ã> = det(A>)In,\n\nand since det(A>) = det(A), Ã> = Ã>, and (ÃA)> = A>Ã>, we get\n\ndet(A)In = A>Ã> = A>Ã> = (ÃA)>,\n\nthat is,\n\n(ÃA)> = det(A)In,\n\nwhich yields\n\nÃA = det(A)In,\n\n7.4. INVERSE MATRICES AND DETERMINANTS 199\n\nProof. If A= (b;;) and AA= (c;;), we know that the entry c¢;; in row 7 and column j of AA\nis\nCig = Aii1d1g + +++ + Gindeg + +++ + Ginbns,\n\nwhich is equal to\naj1(—1)7*\" det(A; 1) feee Hb in(—1)7*\" det(A;,,).\n\nIf 7 = 7, then we recognize the expression of the expansion of det(A) according to the i-th\nrow:\nCi = det (A) = aj1(—1)\"*! det (A;1) fteeet din(—1)'*” det (A; »).\n\nIf 7 4 7%, we can form the matrix A’ by replacing the j-th row of A by the i-th row of A.\nNow the matrix A;;, obtained by deleting row 7 and column k from A is equal to the matrix\nA’,,, obtained by deleting row j and column k from A’, since A and A’ only differ by the j-th\nrow. Thus,\n\ndet(Aj,) = det(Aj,),\n\nand we have\nCijy = ay1(—1)'** det( Aj) +--+ + ain(-1)7*™ det(4j,,,).\n\nHowever, this is the expansion of det(A’) according to the j-th row, since the j-th row of A’\nis equal to the i-th row of A. Furthermore, since A’ has two identical rows i and j, because\ndet is an alternating map of the rows (see an earlier remark), we have det(A’) = 0. Thus,\nwe have shown that c;; = det(A), and c;; = 0, when j 47, and so\nAA = det(A) In.\nIt is also obvious from the definition of A, that\nwaa\nThen applying the first part of the argument to A', we have\nA'AT = det(A')In,\nand since det(A‘) = det(A), AY = AT, and (AA)\" = A™AT, we get\ndet(A)I, = AA’ = ATA! = (AA)’,\n\nthat is, 7\n(AA)! = det(A)In,\n\nwhich yields 7\nAA = det(A)In,\n\n\n\n\n200 CHAPTER 7. DETERMINANTS\n\nsince I>n = In. This proves that\n\nAÃ = ÃA = det(A)In.\n\nAs a consequence, if det(A) is invertible, we have A−1 = (det(A))−1Ã. Conversely, if A is\ninvertible, from AA−1 = In, by Proposition 7.9, we have det(A) det(A−1) = 1, and det(A) is\ninvertible.\n\nFor example, we saw earlier that\n\nA =\n\n1 1 1\n2 −2 −2\n3 3 −3\n\n and Ã =\n\n12 6 0\n0 −6 4\n12 0 −4\n\n ,\n\nand we have 1 1 1\n2 −2 −2\n3 3 −3\n\n12 6 0\n0 −6 4\n12 0 −4\n\n = 24\n\n1 0 0\n0 1 0\n0 0 1\n\n\nwith det(A) = 24.\n\nWhen K is a field, an element a ∈ K is invertible iff a 6= 0. In this case, the second part\nof the proposition can be stated as A is invertible iff det(A) 6= 0. Note in passing that this\nmethod of computing the inverse of a matrix is usually not practical.\n\n7.5 Systems of Linear Equations and Determinants\n\nWe now consider some applications of determinants to linear independence and to solving\nsystems of linear equations. Although these results hold for matrices over certain rings, their\nproofs require more sophisticated methods. Therefore, we assume again that K is a field\n(usually, K = R or K = C).\n\nLet A be an n×n-matrix, x a column vectors of variables, and b another column vector,\nand let A1, . . . , An denote the columns of A. Observe that the system of equations Ax = b,\n\na1 1 a1 2 . . . a1n\n\na2 1 a2 2 . . . a2n\n...\n\n...\n. . .\n\n...\nan 1 an 2 . . . ann\n\n\n\nx1\n\nx2\n...\nxn\n\n =\n\n\nb1\n\nb2\n...\nbn\n\n\nis equivalent to\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = b,\n\nsince the equation corresponding to the i-th row is in both cases\n\nai 1x1 + · · ·+ ai jxj + · · ·+ ai nxn = bi.\n\nFirst we characterize linear independence of the column vectors of a matrix A in terms\nof its determinant.\n\n\n\n7.5. SYSTEMS OF LINEAR EQUATIONS AND DETERMINANTS 201\n\nProposition 7.11. Given an n × n-matrix A over a field K, the columns A1, . . . , An of\nA are linearly dependent iff det(A) = det(A1, . . . , An) = 0. Equivalently, A has rank n iff\ndet(A) 6= 0.\n\nProof. First assume that the columns A1, . . . , An of A are linearly dependent. Then there\nare x1, . . . , xn ∈ K, such that\n\nx1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn = 0,\n\nwhere xj 6= 0 for some j. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , 0, . . . , An) = 0,\n\nwhere 0 occurs in the j-th position. By multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = xj det(A1, . . . , An) = 0.\n\nSince xj 6= 0 and K is a field, we must have det(A1, . . . , An) = 0.\n\nConversely, we show that if the columns A1, . . . , An of A are linearly independent, then\ndet(A1, . . . , An) 6= 0. If the columns A1, . . . , An of A are linearly independent, then they\nform a basis of Kn, and we can express the standard basis (e1, . . . , en) of Kn in terms of\nA1, . . . , An. Thus, we have\n\ne1\n\ne2\n...\nen\n\n =\n\n\nb1 1 b1 2 . . . b1n\n\nb2 1 b2 2 . . . b2n\n...\n\n...\n. . .\n\n...\nbn 1 bn 2 . . . bnn\n\n\n\nA1\n\nA2\n\n...\nAn\n\n ,\n\nfor some matrix B = (bi j), and by Proposition 7.8, we get\n\ndet(e1, . . . , en) = det(B) det(A1, . . . , An),\n\nand since det(e1, . . . , en) = 1, this implies that det(A1, . . . , An) 6= 0 (and det(B) 6= 0). For\nthe second assertion, recall that the rank of a matrix is equal to the maximum number of\nlinearly independent columns, and the conclusion is clear.\n\nWe now characterize when a system of linear equations of the form Ax = b has a unique\nsolution.\n\nProposition 7.12. Given an n× n-matrix A over a field K, the following properties hold:\n\n(1) For every column vector b, there is a unique column vector x such that Ax = b iff the\nonly solution to Ax = 0 is the trivial vector x = 0, iff det(A) 6= 0.\n\n\n\n202 CHAPTER 7. DETERMINANTS\n\n(2) If det(A) 6= 0, the unique solution of Ax = b is given by the expressions\n\nxj =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n,\n\nknown as Cramer’s rules.\n\n(3) The system of linear equations Ax = 0 has a nonzero solution iff det(A) = 0.\n\nProof. (1) Assume that Ax = b has a single solution x0, and assume that Ay = 0 with y 6= 0.\nThen,\n\nA(x0 + y) = Ax0 + Ay = Ax0 + 0 = b,\n\nand x0 + y 6= x0 is another solution of Ax = b, contradicting the hypothesis that Ax = b has\na single solution x0. Thus, Ax = 0 only has the trivial solution. Now assume that Ax = 0\nonly has the trivial solution. This means that the columns A1, . . . , An of A are linearly\nindependent, and by Proposition 7.11, we have det(A) 6= 0. Finally, if det(A) 6= 0, by\nProposition 7.10, this means that A is invertible, and then for every b, Ax = b is equivalent\nto x = A−1b, which shows that Ax = b has a single solution.\n\n(2) Assume that Ax = b. If we compute\n\ndet(A1, . . . , x1A\n1 + · · ·+ xjA\n\nj + · · ·+ xnA\nn, . . . , An) = det(A1, . . . , b, . . . , An),\n\nwhere b occurs in the j-th position, by multilinearity, all terms containing two identical\ncolumns Ak for k 6= j vanish, and we get\n\nxj det(A1, . . . , An) = det(A1, . . . , Aj−1, b, Aj+1, . . . , An),\n\nfor every j, 1 ≤ j ≤ n. Since we assumed that det(A) = det(A1, . . . , An) 6= 0, we get the\ndesired expression.\n\n(3) Note that Ax = 0 has a nonzero solution iff A1, . . . , An are linearly dependent (as\nobserved in the proof of Proposition 7.11), which, by Proposition 7.11, is equivalent to\ndet(A) = 0.\n\nAs pleasing as Cramer’s rules are, it is usually impractical to solve systems of linear\nequations using the above expressions. However, these formula imply an interesting fact,\nwhich is that the solution of the system Ax = b are continuous in A and b. If we assume that\nthe entries in A are continuous functions aij(t) and the entries in b are are also continuous\nfunctions bj(t) of a real parameter t, since determinants are polynomial functions of their\nentries, the expressions\n\nxj(t) =\ndet(A1, . . . , Aj−1, b, Aj+1, . . . , An)\n\ndet(A1, . . . , Aj−1, Aj, Aj+1, . . . , An)\n\nare ratios of polynomials, and thus are also continuous as long as det(A(t)) is nonzero.\nSimilarly, if the functions aij(t) and bj(t) are differentiable, so are the xj(t).\n\n\n\n7.6. DETERMINANT OF A LINEAR MAP 203\n\n7.6 Determinant of a Linear Map\n\nGiven a vector space E of finite dimension n, given a basis (u1, . . . , un) of E, for every linear\nmap f : E → E, if M(f) is the matrix of f w.r.t. the basis (u1, . . . , un), we can define\ndet(f) = det(M(f)). If (v1, . . . , vn) is any other basis of E, and if P is the change of basis\nmatrix, by Corollary 4.5, the matrix of f with respect to the basis (v1, . . . , vn) is P−1M(f)P .\nBy Proposition 7.9, we have\n\ndet(P−1M(f)P ) = det(P−1) det(M(f)) det(P ) = det(P−1) det(P ) det(M(f)) = det(M(f)).\n\nThus, det(f) is indeed independent of the basis of E.\n\nDefinition 7.8. Given a vector space E of finite dimension, for any linear map f : E → E,\nwe define the determinant det(f) of f as the determinant det(M(f)) of the matrix of f in\nany basis (since, from the discussion just before this definition, this determinant does not\ndepend on the basis).\n\nThen we have the following proposition.\n\nProposition 7.13. Given any vector space E of finite dimension n, a linear map f : E → E\nis invertible iff det(f) 6= 0.\n\nProof. The linear map f : E → E is invertible iff its matrix M(f) in any basis is invertible\n(by Proposition 4.2), iff det(M(f)) 6= 0, by Proposition 7.10.\n\nGiven a vector space of finite dimension n, it is easily seen that the set of bijective linear\nmaps f : E → E such that det(f) = 1 is a group under composition. This group is a\nsubgroup of the general linear group GL(E). It is called the special linear group (of E), and\nit is denoted by SL(E), or when E = Kn, by SL(n,K), or even by SL(n).\n\n7.7 The Cayley–Hamilton Theorem\n\nWe next discuss an interesting and important application of Proposition 7.10, the Cayley–\nHamilton theorem. The results of this section apply to matrices over any commutative ring\nK. First we need the concept of the characteristic polynomial of a matrix.\n\nDefinition 7.9. If K is any commutative ring, for every n × n matrix A ∈ Mn(K), the\ncharacteristic polynomial PA(X) of A is the determinant\n\nPA(X) = det(XI − A).\n\n\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial PA(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\nA =\n\n(\na b\nc d\n\n)\n,\n\nthen\n\nPA(X) =\n\n∣∣∣∣X − a −b\n−c X − d\n\n∣∣∣∣ = X2 − (a+ d)X + ad− bc.\n\nWe can substitute the matrix A for the variable X in the polynomial PA(X), obtaining a\nmatrix PA. If we write\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn,\n\nthen\nPA = An + c1A\n\nn−1 + · · ·+ cnI.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley–Hamilton) If K is any commutative ring, for every n× n matrix\nA ∈ Mn(K), if we let\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nbe the characteristic polynomial of A, then\n\nPA = An + c1A\nn−1 + · · ·+ cnI = 0.\n\nProof. We can view the matrix B = XI −A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B̃ which is the transpose of the matrix of\n\ncofactors of elements of B. Each entry in B̃ is an (n− 1)× (n− 1) determinant, and thus a\n\npolynomial of degree a most n− 1, so we can write B̃ as\n\nB̃ = Xn−1B0 +Xn−2B1 + · · ·+Bn−1,\n\nfor some n× n matrices B0, . . . , Bn−1 with coefficients in K. For example, when n = 2, we\nhave\n\nB =\n\n(\nX − a −b\n−c X − d\n\n)\n, B̃ =\n\n(\nX − d b\nc X − a\n\n)\n= X\n\n(\n1 0\n0 1\n\n)\n+\n\n(\n−d b\nc −a\n\n)\n.\n\nBy Proposition 7.10, we have\n\nBB̃ = det(B)I = PA(X)I.\n\nOn the other hand, we have\n\nBB̃ = (XI − A)(Xn−1B0 +Xn−2B1 + · · ·+Xn−j−1Bj + · · ·+Bn−1),\n\n204 CHAPTER 7. DETERMINANTS\n\nThe characteristic polynomial P4(X) is a polynomial in K[X], the ring of polynomials\nin the indeterminate X with coefficients in the ring K. For example, when n = 2, if\n\na b\na=(\" i)\n\nX—a —b\n—c X-d\n\nthen\nPa(X) =\n\n=X? (wa) + ad be\n\nWe can substitute the matrix A for the variable X in the polynomial P4(X), obtaining a\nmatrix P,. If we write\nP4(X) =X\" +E, X\" 1 +++ +en,\n\nthen\nPy =A\" +c A\" '+---+e, 1.\n\nWe have the following remarkable theorem.\n\nTheorem 7.14. (Cayley-Hamilton) If K is any commutative ring, for every n x n matriz\nAEM, (kK), if we let\nPy(X) =X\" +X\" 1 +---+e\n\nbe the characteristic polynomial of A, then\nPy =A\" +c, A\" 14+---+e,1 =0.\n\nProof. We can view the matrix B = XJ — A as a matrix with coefficients in the polynomial\nring K[X], and then we can form the matrix B which is the transpose of the matrix of\ncofactors of elements of B. Each entry in B is an (n — 1) x (n — 1) determinant, and thus a\npolynomial of degree a most n — 1, so we can write Bas\n\nB= X\"'Bo +X\"? By, +--+ Baa,\n\nfor some n x n matrices Bo,..., By, with coefficients in K. For example, when n = 2, we\n\nhave\nX—-a@a —b ~ X—d b 1 0 —d 0b\nB= (“7 yea) B= ( C xa) =* (i N+ 1).\n\nBy Proposition 7.10, we have\nBB = det(B)I = Pa(X)I.\nOn the other hand, we have\n\nBB=(XI-A)(X\"'Bo +X\" 7B, + +--+ X77 'B; +---+ Br),\n\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 205\n\nand by multiplying out the right-hand side, we get\n\nBB̃ = XnD0 +Xn−1D1 + · · ·+Xn−jDj + · · ·+Dn,\n\nwith\n\nD0 = B0\n\nD1 = B1 − AB0\n\n...\n\nDj = Bj − ABj−1\n\n...\n\nDn−1 = Bn−1 − ABn−2\n\nDn = −ABn−1.\n\nSince\nPA(X)I = (Xn + c1X\n\nn−1 + · · ·+ cn)I,\n\nthe equality\nXnD0 +Xn−1D1 + · · ·+Dn = (Xn + c1X\n\nn−1 + · · ·+ cn)I\n\nis an equality between two matrices, so it requires that all corresponding entries are equal,\nand since these are polynomials, the coefficients of these polynomials must be identical,\nwhich is equivalent to the set of equations\n\nI = B0\n\nc1I = B1 − AB0\n\n...\n\ncjI = Bj − ABj−1\n\n...\n\ncn−1I = Bn−1 − ABn−2\n\ncnI = −ABn−1,\n\nfor all j, with 1 ≤ j ≤ n− 1. If, as in the table below,\n\nAn = AnB0\n\nc1A\nn−1 = An−1(B1 − AB0)\n\n...\n\ncjA\nn−j = An−j(Bj − ABj−1)\n\n...\n\ncn−1A = A(Bn−1 − ABn−2)\n\ncnI = −ABn−1,\n\n\n\n206 CHAPTER 7. DETERMINANTS\n\nwe multiply the first equation by An, the last by I, and generally the (j + 1)th by An−j,\nwhen we add up all these new equations, we see that the right-hand side adds up to 0, and\nwe get our desired equation\n\nAn + c1A\nn−1 + · · ·+ cnI = 0,\n\nas claimed.\n\nAs a concrete example, when n = 2, the matrix\n\nA =\n\n(\na b\nc d\n\n)\nsatisfies the equation\n\nA2 − (a+ d)A+ (ad− bc)I = 0.\n\nMost readers will probably find the proof of Theorem 7.14 rather clever but very myste-\nrious and unmotivated. The conceptual difficulty is that we really need to understand how\npolynomials in one variable “act” on vectors in terms of the matrix A. This can be done and\nyields a more “natural” proof. Actually, the reasoning is simpler and more general if we free\nourselves from matrices and instead consider a finite-dimensional vector space E and some\ngiven linear map f : E → E. Given any polynomial p(X) = a0X\n\nn + a1X\nn−1 + · · ·+ an with\n\ncoefficients in the field K, we define the linear map p(f) : E → E by\n\np(f) = a0f\nn + a1f\n\nn−1 + · · ·+ anid,\n\nwhere fk = f ◦ · · · ◦ f , the k-fold composition of f with itself. Note that\n\np(f)(u) = a0f\nn(u) + a1f\n\nn−1(u) + · · ·+ anu,\n\nfor every vector u ∈ E. Then we define a new kind of scalar multiplication · : K[X]×E → E\nby polynomials as follows: for every polynomial p(X) ∈ K[X], for every u ∈ E,\n\np(X) · u = p(f)(u).\n\nIt is easy to verify that this is a “good action,” which means that\n\np · (u+ v) = p · u+ p · v\n(p+ q) · u = p · u+ q · u\n\n(pq) · u = p · (q · u)\n\n1 · u = u,\n\nfor all p, q ∈ K[X] and all u, v ∈ E. With this new scalar multiplication, E is a K[X]-module.\n\nIf p = λ is just a scalar in K (a polynomial of degree 0), then\n\nλ · u = (λid)(u) = λu,\n\n\n\n7.7. THE CAYLEY–HAMILTON THEOREM 207\n\nwhich means that K acts on E by scalar multiplication as before. If p(X) = X (the monomial\nX), then\n\nX · u = f(u).\n\nNow if we pick a basis (e1, . . . , en) of E, if a polynomial p(X) ∈ K[X] has the property\nthat\n\np(X) · ei = 0, i = 1, . . . , n,\n\nthen this means that p(f)(ei) = 0 for i = 1, . . . , n, which means that the linear map p(f)\nvanishes on E. We can also check, as we did in Section 7.2, that if A and B are two n× n\nmatrices and if (u1, . . . , un) are any n vectors, then\n\nA ·\n\nB ·\nu1\n\n...\nun\n\n\n = (AB) ·\n\nu1\n...\nun\n\n .\n\nThis suggests the plan of attack for our second proof of the Cayley–Hamilton theorem.\nFor simplicity, we prove the theorem for vector spaces over a field. The proof goes through\nfor a free module over a commutative ring.\n\nTheorem 7.15. (Cayley–Hamilton) For every finite-dimensional vector space over a field\nK, for every linear map f : E → E, for every basis (e1, . . . , en), if A is the matrix over f\nover the basis (e1, . . . , en) and if\n\nPA(X) = Xn + c1X\nn−1 + · · ·+ cn\n\nis the characteristic polynomial of A, then\n\nPA(f) = fn + c1f\nn−1 + · · ·+ cnid = 0.\n\nProof. Since the columns of A consist of the vector f(ej) expressed over the basis (e1, . . . , en),\nwe have\n\nf(ej) =\nn∑\ni=1\n\nai jei, 1 ≤ j ≤ n.\n\nUsing our action of K[X] on E, the above equations can be expressed as\n\nX · ej =\nn∑\ni=1\n\nai j · ei, 1 ≤ j ≤ n,\n\nwhich yields\n\nj−1∑\ni=1\n\n−ai j · ei + (X − aj j) · ej +\nn∑\n\ni=j+1\n\n−ai j · ei = 0, 1 ≤ j ≤ n.\n\n\n\n208 CHAPTER 7. DETERMINANTS\n\nObserve that the transpose of the characteristic polynomial shows up, so the above system\ncan be written as\n\nX − a1 1 −a2 1 · · · −an 1\n\n−a1 2 X − a2 2 · · · −an 2\n...\n\n...\n...\n\n...\n−a1n −a2n · · · X − ann\n\n ·\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n .\n\nIf we let B = XI −A>, then as in the previous proof, if B̃ is the transpose of the matrix of\ncofactors of B, we have\n\nB̃B = det(B)I = det(XI − A>)I = det(XI − A)I = PAI.\n\nBut since\n\nB ·\n\n\ne1\n\ne2\n...\nen\n\n =\n\n\n0\n0\n...\n0\n\n ,\n\nand since B̃ is matrix whose entries are polynomials in K[X], it makes sense to multiply on\n\nthe left by B̃ and we get\n\nB̃ ·B ·\n\n\ne1\n\ne2\n...\nen\n\n = (B̃B) ·\n\n\ne1\n\ne2\n...\nen\n\n = PAI ·\n\n\ne1\n\ne2\n...\nen\n\n = B̃ ·\n\n\n0\n0\n...\n0\n\n =\n\n\n0\n0\n...\n0\n\n ;\n\nthat is,\nPA · ej = 0, j = 1, . . . , n,\n\nwhich proves that PA(f) = 0, as claimed.\n\nIfK is a field, then the characteristic polynomial of a linear map f : E → E is independent\nof the basis (e1, . . . , en) chosen in E. To prove this, observe that the matrix of f over another\nbasis will be of the form P−1AP , for some inverible matrix P , and then\n\ndet(XI − P−1AP ) = det(XP−1IP − P−1AP )\n\n= det(P−1(XI − A)P )\n\n= det(P−1) det(XI − A) det(P )\n\n= det(XI − A).\n\nTherefore, the characteristic polynomial of a linear map is intrinsic to f , and it is denoted\nby Pf .\n\nThe zeros (roots) of the characteristic polynomial of a linear map f are called the eigen-\nvalues of f . They play an important role in theory and applications. We will come back to\nthis topic later on.\n\n\n\n7.8. PERMANENTS 209\n\n7.8 Permanents\n\nRecall that the explicit formula for the determinant of an n× n matrix is\n\ndet(A) =\n∑\nπ∈Sn\n\nε(π)aπ(1) 1 · · · aπ(n)n.\n\nIf we drop the sign ε(π) of every permutation from the above formula, we obtain a quantity\nknown as the permanent :\n\nper(A) =\n∑\nπ∈Sn\n\naπ(1) 1 · · · aπ(n)n.\n\nPermanents and determinants were investigated as early as 1812 by Cauchy. It is clear from\nthe above definition that the permanent is a multilinear symmetric form. We also have\n\nper(A) = per(A>),\n\nand the following unsigned version of the Laplace expansion formula:\n\nper(A) = ai 1per(Ai 1) + · · ·+ ai jper(Ai j) + · · ·+ ai nper(Ai n),\n\nfor i = 1, . . . , n. However, unlike determinants which have a clear geometric interpretation as\nsigned volumes, permanents do not have any natural geometric interpretation. Furthermore,\ndeterminants can be evaluated efficiently, for example using the conversion to row reduced\nechelon form, but computing the permanent is hard.\n\nPermanents turn out to have various combinatorial interpretations. One of these is in\nterms of perfect matchings of bipartite graphs which we now discuss.\n\nSee Definition 20.5 for the definition of an undirected graph. A bipartite (undirected)\ngraph G = (V,E) is a graph whose set of nodes V can be partitioned into two nonempty\ndisjoint subsets V1 and V2, such that every edge e ∈ E has one endpoint in V1 and one\nendpoint in V2.\n\nAn example of a bipartite graph with 14 nodes is shown in Figure 7.3; its nodes are\npartitioned into the two sets {x1, x2, x3, x4, x5, x6, x7} and {y1, y2, y3, y4, y5, y6, y7}.\n\nA matching in a graph G = (V,E) (bipartite or not) is a set M of pairwise non-adjacent\nedges, which means that no two edges in M share a common vertex. A perfect matching is\na matching such that every node in V is incident to some edge in the matching M (every\nnode in V is an endpoint of some edge in M). Figure 7.4 shows a perfect matching (in red)\nin the bipartite graph G.\n\nObviously, a perfect matching in a bipartite graph can exist only if its set of nodes has\na partition in two blocks of equal size, say {x1, . . . , xm} and {y1, . . . , ym}. Then there is\na bijection between perfect matchings and bijections π : {x1, . . . , xm} → {y1, . . . , ym} such\nthat π(xi) = yj iff there is an edge between xi and yj.\n\nNow every bipartite graph G with a partition of its nodes into two sets of equal size as\nabove is represented by an m × m matrix A = (aij) such that aij = 1 iff there is an edge\n\n\n\n210 CHAPTER 7. DETERMINANTS\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.3: A bipartite graph G.\n\nx1 x2 x3 x4 x5 x6 x7\n\ny1 y2 y3 y4 y5 y6 y7\n\nFigure 7.4: A perfect matching in the bipartite graph G.\n\nbetween xi and yj, and aij = 0 otherwise. Using the interpretation of perfect matchings as\nbijections π : {x1, . . . , xm} → {y1, . . . , ym}, we see that the permanent per(A) of the (0, 1)-\nmatrix A representing the bipartite graph G counts the number of perfect matchings in G.\n\nIn a famous paper published in 1979, Leslie Valiant proves that computing the permanent\nis a #P-complete problem. Such problems are suspected to be intractable. It is known that\nif a polynomial-time algorithm existed to solve a #P-complete problem, then we would have\nP = NP , which is believed to be very unlikely.\n\nAnother combinatorial interpretation of the permanent can be given in terms of systems\nof distinct representatives. Given a finite set S, let (A1, . . . , An) be any sequence of nonempty\nsubsets of S (not necessarily distinct). A system of distinct representatives (for short SDR)\nof the sets A1, . . . , An is a sequence of n distinct elements (a1, . . . , an), with ai ∈ Ai for i =\n1, . . . , n. The number of SDR’s of a sequence of sets plays an important role in combinatorics.\nNow, if S = {1, 2, . . . , n} and if we associate to any sequence (A1, . . . , An) of nonempty\nsubsets of S the matrix A = (aij) defined such that aij = 1 if j ∈ Ai and aij = 0 otherwise,\nthen the permanent per(A) counts the number of SDR’s of the sets A1, . . . , An.\n\nThis interpretation of permanents in terms of SDR’s can be used to prove bounds for the\npermanents of various classes of matrices. Interested readers are referred to van Lint and\n\n\n\n7.9. SUMMARY 211\n\nWilson [178] (Chapters 11 and 12). In particular, a proof of a theorem known as Van der\nWaerden conjecture is given in Chapter 12. This theorem states that for any n × n matrix\nA with nonnegative entries in which all row-sums and column-sums are 1 (doubly stochastic\nmatrices), we have\n\nper(A) ≥ n!\n\nnn\n,\n\nwith equality for the matrix in which all entries are equal to 1/n.\n\n7.9 Summary\n\nThe main concepts and results of this chapter are listed below:\n\n• Permutations , transpositions , basics transpositions .\n\n• Every permutation can be written as a composition of permutations.\n\n• The parity of the number of transpositions involved in any decomposition of a permu-\ntation σ is an invariant; it is the signature ε(σ) of the permutation σ.\n\n• Multilinear maps (also called n-linear maps); bilinear maps .\n\n• Symmetric and alternating multilinear maps.\n\n• A basic property of alternating multilinear maps (Lemma 7.4) and the introduction of\nthe formula expressing a determinant.\n\n• Definition of a determinant as a multlinear alternating map D : Mn(K)→ K such that\nD(I) = 1.\n\n• We define the set of algorithms Dn, to compute the determinant of an n× n matrix.\n\n• Laplace expansion according to the ith row ; cofactors .\n\n• We prove that the algorithms in Dn compute determinants (Lemma 7.5).\n\n• We prove that all algorithms in Dn compute the same determinant (Theorem 7.6).\n\n• We give an interpretation of determinants as signed volumes .\n\n• We prove that det(A) = det(A>).\n\n• We prove that det(AB) = det(A) det(B).\n\n• The adjugate Ã of a matrix A.\n\n• Formula for the inverse in terms of the adjugate.\n\n\n\n212 CHAPTER 7. DETERMINANTS\n\n• A matrix A is invertible iff det(A) 6= 0.\n\n• Solving linear equations using Cramer’s rules .\n\n• Determinant of a linear map.\n\n• The characteristic polynomial of a matrix.\n\n• The Cayley–Hamilton theorem.\n\n• The action of the polynomial ring induced by a linear map on a vector space.\n\n• Permanents .\n\n• Permanents count the number of perfect matchings in bipartite graphs.\n\n• Computing the permanent is a #P-perfect problem (L. Valiant).\n\n• Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3–6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R2 of coordinates (c1−a1, c2−a2) and (b1−a1, b2−a2),\nprove that they are linearly dependent iff∣∣∣∣∣∣\n\na1 b1 c1\n\na2 b2 c2\n\n1 1 1\n\n∣∣∣∣∣∣ = 0.\n\n(2) Given three vectors in R3 of coordinates (d1−a1, d2−a2, d3−a3), (c1−a1, c2−a2, c3−a3),\nand (b1 − a1, b2 − a2, b3 − a3), prove that they are linearly dependent iff∣∣∣∣∣∣∣∣\n\na1 b1 c1 d1\n\na2 b2 c2 d2\n\na3 b3 c3 d3\n\n1 1 1 1\n\n∣∣∣∣∣∣∣∣ = 0.\n\n212 CHAPTER 7. DETERMINANTS\n\nA matrix A is invertible iff det(A) ¥ 0.\n\ne Solving linear equations using Cramer’s rules.\n\ne Determinant of a linear map.\n\ne The characteristic polynomial of a matrix.\n\ne The Cayley—Hamilton theorem.\n\ne The action of the polynomial ring induced by a linear map on a vector space.\ne Permanents.\n\ne Permanents count the number of perfect matchings in bipartite graphs.\n\ne Computing the permanent is a #P-perfect problem (L. Valiant).\n\ne Permanents count the number of SDRs of sequences of subsets of a given set.\n\n7.10 Further Readings\n\nThorough expositions of the material covered in Chapter 3-6 and 7 can be found in Strang\n[168, 167], Lax [112], Lang [108], Artin [7], Mac Lane and Birkhoff [117], Hoffman and Kunze\n[100], Dummit and Foote [55], Bourbaki [25, 26], Van Der Waerden [177], Serre [154], Horn\nand Johnson [93], and Bertin [15]. These notions of linear algebra are nicely put to use in\nclassical geometry, see Berger [11, 12], Tisseron [173] and Dieudonné [50].\n\n7.11 Problems\n\nProblem 7.1. Prove that every transposition can be written as a product of basic transpo-\nsitions.\n\nProblem 7.2. (1) Given two vectors in R? of coordinates (cj —a,, C2—a@2) and (bj —a, by—a2),\nprove that they are linearly dependent iff\n\nay by Cy\nag bo C2) = 0.\n1 11\n\n(2) Given three vectors in R® of coordinates (d;—a1, d2—a2, d3—a3), (c1—a1, C2—2, C343),\nand (b1 — a1, bg — dg, b3 — a3), prove that they are linearly dependent iff\n\nay by Cc dy\naz by C2 dy\naz bg cz dg\n\n1 1 1éiéid1\n\n= 0.\n\n\n\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+ n)× (m+ n) block matrix (over any field K) given by\n\nA =\n\n(\nA1 A2\n\n0 A4\n\n)\n,\n\nwhere A1 is an m×m matrix, A2 is an m×n matrix, and A4 is an n×n matrix. Prove that\ndet(A) = det(A1) det(A4).\n\nUse the above result to prove that if A is an upper triangular n×n matrix, then det(A) =\na11a22 · · · ann.\n\nProblem 7.4. Prove that if n ≥ 3, then\n\ndet\n\n\n1 + x1y1 1 + x1y2 · · · 1 + x1yn\n1 + x2y1 1 + x2y2 · · · 1 + x2yn\n\n...\n...\n\n...\n...\n\n1 + xny1 1 + xny2 · · · 1 + xnyn\n\n = 0.\n\nProblem 7.5. Prove that ∣∣∣∣∣∣∣∣\n1 4 9 16\n4 9 16 25\n9 16 25 36\n16 25 36 49\n\n∣∣∣∣∣∣∣∣ = 0.\n\nProblem 7.6. Consider the n× n symmetric matrix\n\nA =\n\n\n\n1 2 0 0 . . . 0 0\n2 5 2 0 . . . 0 0\n0 2 5 2 . . . 0 0\n...\n\n...\n. . . . . . . . .\n\n...\n...\n\n0 0 . . . 2 5 2 0\n0 0 . . . 0 2 5 2\n0 0 . . . 0 0 2 5\n\n\n.\n\n(1) Find an upper-triangular matrix R such that A = R>R.\n\n(2) Prove that det(A) = 1.\n\n(3) Consider the sequence\n\np0(λ) = 1\n\np1(λ) = 1− λ\npk(λ) = (5− λ)pk−1(λ)− 4pk−2(λ) 2 ≤ k ≤ n.\n\nProve that\ndet(A− λI) = pn(λ).\n\nRemark: It can be shown that pn(λ) has n distinct (real) roots and that the roots of pk(λ)\nseparate the roots of pk+1(λ).\n\n7.11. PROBLEMS 213\n\nProblem 7.3. Let A be the (m+n) x (m+n) block matrix (over any field AK’) given by\n\nA, Ape\na=(o 4).\nwhere A, is an m Xm matrix, Ag is an m X n matrix, and A, is an n x n matrix. Prove that\ndet(A) = det(A;) det(Ay).\nUse the above result to prove that if A is an upper triangular n x n matrix, then det(A) =\n\n411422 °°* Ann-\n\nProblem 7.4. Prove that if n > 3, then\n\nL+ayyr Llt+ayye +++ 1+21y,\n1 + T2Y1 1 + T2Y2 °°\" 1 + T2Un\ndet . . . . = 0.\nL+apy1 L4+%nyo +++ 14+ 2nyn\nProblem 7.5. Prove that\n1 4 9 16\n4 9 16 25) _ 0\n9 16 25 36, ©\n16 25 36 49\nProblem 7.6. Consider the n x n symmetric matrix\n12 0 0 . 0 0\n25 2 O 0 0\n02 5 2 . 0 0\nA=]: : tet\n0 0 2 5 2 0\n0 0 0 2 5 2\n0 0 0 0 2 5\n\n(1) Find an upper-triangular matrix R such that A = R'R.\n(2) Prove that det(A) = 1.\n(3) Consider the sequence\npo(A) = 1\npi(A) =1-A\npr(A) = (5 — A)pe_-1(A) — 4pp-2(A) 2<k <n.\n\nProve that\ndet(A — AT) = pp (A).\n\nRemark: It can be shown that p,(A) has n distinct (real) roots and that the roots of pz (A)\nseparate the roots of pz41(A).\n\n\n\n\n214 CHAPTER 7. DETERMINANTS\n\nProblem 7.7. Let B be the n× n matrix (n ≥ 3) given by\n\nB =\n\n\n\n1 −1 −1 −1 · · · −1 −1\n1 −1 1 1 · · · 1 1\n1 1 −1 1 · · · 1 1\n1 1 1 −1 · · · 1 1\n...\n\n...\n...\n\n...\n...\n\n...\n...\n\n1 1 1 1 · · · −1 1\n1 1 1 1 · · · 1 −1\n\n\n.\n\nProve that\ndet(B) = (−1)n(n− 2)2n−1.\n\nProblem 7.8. Given a field K (say K = R or K = C), given any two polynomials\np(X), q(X) ∈ K[X], we says that q(X) divides p(X) (and that p(X) is a multiple of q(X))\niff there is some polynomial s(X) ∈ K[X] such that\n\np(X) = q(X)s(X).\n\nIn this case we say that q(X) is a factor of p(X), and if q(X) has degree at least one, we\nsay that q(X) is a nontrivial factor of p(X).\n\nLet f(X) and g(X) be two polynomials in K[X] with\n\nf(X) = a0X\nm + a1X\n\nm−1 + · · ·+ am\n\nof degree m ≥ 1 and\ng(X) = b0X\n\nn + b1X\nn−1 + · · ·+ bn\n\nof degree n ≥ 1 (with a0, b0 6= 0).\n\nYou will need the following result which you need not prove:\n\nTwo polynomials f(X) and g(X) with deg(f) = m ≥ 1 and deg(g) = n ≥ 1 have some\ncommon nontrivial factor iff there exist two nonzero polynomials p(X) and q(X) such that\n\nfp = gq,\n\nwith deg(p) ≤ n− 1 and deg(q) ≤ m− 1.\n\n(1) Let Pm denote the vector space of all polynomials in K[X] of degree at most m− 1,\nand let T : Pn × Pm → Pm+n be the map given by\n\nT (p, q) = fp+ gq, p ∈ Pn, q ∈ Pm,\n\nwhere f and g are some fixed polynomials of degree m ≥ 1 and n ≥ 1.\n\nProve that the map T is linear.\n\n\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f, g) = 0, where R(f, g) is\nthe determinant given by\n\nR(f, g) =\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\na0 a1 · · · · · · am 0 · · · · · · · · · · · · 0\n0 a0 a1 · · · · · · am 0 · · · · · · · · · 0\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · · · · · · · · · · 0 a0 a1 · · · · · · am\nb0 b1 · · · · · · · · · · · · · · · bn 0 · · · 0\n0 b0 b1 · · · · · · · · · · · · · · · bn 0 · · ·\n· · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · · ·\n0 · · · 0 b0 b1 · · · · · · · · · · · · · · · bn\n\n∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣∣\n\n.\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n+m)× (n+m) matrix, with the first row\n(involving the ais) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the bjs) occurring m times, each time shifted over to the right\nby one column.\n\nHint . Express the matrix of T over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a) m = n = 1, and write f(X) = aX + b and g(X) = cX + d.\n\n(b) m = 1 and n ≥ 2 arbitrary.\n\n(c) f(X) = aX2 + bX + c and g(X) = 2aX + b.\n\n(5) Compute the resultant of f(X) = X3 + pX + q and g(X) = 3X2 + p, and\n\nf(X) = a0X\n2 + a1X + a2\n\ng(X) = b0X\n2 + b1X + b2.\n\nIn the second case, you should get\n\n4R(f, g) = (2a0b2 − a1b1 + 2a2b0)2 − (4a0a2 − a2\n1)(4b0b2 − b2\n\n1).\n\nProblem 7.9. Let A,B,C,D be n× n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\ndet\n\n(\nA B\nC D\n\n)\n= det(AD − CB).\n\n7.11. PROBLEMS 215\n\n(2) Prove that T is not injective iff f and g have a common nontrivial factor.\n\n(3) Prove that f and g have a nontrivial common factor iff R(f,g) =0, where R(f,g) is\nthe determinant given by\n\nao ay eee wee Am 0 cee eee eee eee 0\n\n0 ag Gy cts tt Gm Oo ret tee eee O\nRg) =f\n\n0 eee eee eee wee 0 ao ay eee eee Am\n\na | 0\n\nOS | 0\n\nOe 0 dg Bp tet tte tet tee ee Dy\n\nThe above determinant is called the resultant of f and g.\n\nNote that the matrix of the resultant is an (n +m) x (n +m) matrix, with the first row\n(involving the a;s) occurring n times, each time shifted over to the right by one column, and\nthe (n + 1)th row (involving the b;s) occurring m times, each time shifted over to the right\nby one column.\n\nHint. Express the matrix of T’ over some suitable basis.\n\n(4) Compute the resultant in the following three cases:\n\n(a\n\n)\n(b) m=1 and n > 2 arbitrary.\n)\n\n(c\n\n3\n\n= n=l, and write f(X) =aX +b and g(X) =cX +d.\n\nf(X) =aX? + bX +c and g(X) = 2aX +b.\n\n(5) Compute the resultant of f(X) = X°+pX +q and g(X) = 3X? +p, and\n\nf(X) = ag.X? +a,X + ag\ng(X) => by X? + bX + bo.\n\nIn the second case, you should get\nAR(f, g) = (2Qagbe _ a,b, + 2aybp)? _ (daga2 —_ a*) (4bgbo _ bi).\n\nProblem 7.9. Let A,B,C, D be n x n real or complex matrices.\n\n(1) Prove that if A is invertible and if AC = CA, then\n\nA B\ndet (< b) = det(AD — CB).\n\n\n\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then | det(H)| = nn/2.\n\n(3) Prove that if H is an n× n Hadamard matrix (n ≥ 2), then\n\ndet\n\n(\nH H\nH −H\n\n)\n= (2n)n.\n\nProblem 7.10. Compute the product of the following determinants∣∣∣∣∣∣∣∣\na −b −c −d\nb a −d c\nc d a −b\nd −c b a\n\n∣∣∣∣∣∣∣∣\n∣∣∣∣∣∣∣∣\nx −y −z −t\ny x −t z\nz t x −y\nt −z y x\n\n∣∣∣∣∣∣∣∣\nto prove the following identity (due to Euler):\n\n(a2 + b2 + c2 + d2)(x2 + y2 + z2 + t2) = (ax+ by + cz + dt)2 + (ay − bx+ ct− dz)2\n\n+ (az − bt− cx+ dy)2 + (at+ bz − cy + dx)2.\n\nProblem 7.11. Let A be an n × n matrix with integer entries. Prove that A−1 exists and\nhas integer entries if and only if det(A) = ±1.\n\nProblem 7.12. Let A be an n× n real or complex matrix.\n\n(1) Prove that if A> = −A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n\n(2) Prove that ∣∣∣∣∣∣∣∣\n0 a b c\n−a 0 d e\n−b −d 0 f\n−c −e −f 0\n\n∣∣∣∣∣∣∣∣ = (af − be+ dc)2.\n\nProblem 7.13. A Cauchy matrix is a matrix of the form\n\n1\n\nλ1 − σ1\n\n1\n\nλ1 − σ2\n\n· · · 1\n\nλ1 − σn\n1\n\nλ2 − σ1\n\n1\n\nλ2 − σ2\n\n· · · 1\n\nλ2 − σn\n...\n\n...\n...\n\n...\n1\n\nλn − σ1\n\n1\n\nλn − σ2\n\n· · · 1\n\nλn − σn\n\n\nwhere λi 6= σj, for all i, j, with 1 ≤ i, j ≤ n. Prove that the determinant Cn of a Cauchy\nmatrix as above is given by\n\nCn =\n\n∏n\ni=2\n\n∏i−1\nj=1(λi − λj)(σj − σi)∏n\ni=1\n\n∏n\nj=1(λi − σj)\n\n.\n\n216 CHAPTER 7. DETERMINANTS\n\n(2) Prove that if H is an n x n Hadamard matrix (n > 2), then | det(H)| =n\".\n(3) Prove that if H is ann x n Hadamard matrix (n > 2), then\n\ndet (ji \") = (2n)\".\n\nProblem 7.10. Compute the product of the following determinants\n\na —b -—c —d\\|x -y -z -t\nb a -d clly a -t 2z\nc dad a —bl|z t aw -y\nd -c b allt -z y «\n\nto prove the following identity (due to Euler):\n\n(P4+RP4C4P)(a? +y? +240) = (ax + by + cz +dt) + (ay — bx + ct — dz)?\n+ (az — bt — cx + dy)? + (at + bz — cy + dx)’.\n\nProblem 7.11. Let A be an n x n matrix with integer entries. Prove that A! exists and\nhas integer entries if and only if det(A) = +1.\n\nProblem 7.12. Let A be an n x n real or complex matrix.\n(1) Prove that if A' = —A (A is skew-symmetric) and if n is odd, then det(A) = 0.\n(2) Prove that\n\n0 a ob e\n—a 0 de} _ 9\npb -d 0 f = (af — be + dc)’.\n—c -e -f 0\nProblem 7.13. A Cauchy matrix is a matrix of the form\n1 1 1\nAy —- 0, ATO At — On\n1 1 ALT 972 Fy\nAy — O71 Ay — J2 A2 — On\n1 1 | 1\nMn — 01 An — 02 An — On\n\nwhere \\; 4 o;, for all 7,7, with 1 < 7,7 <n. Prove that the determinant C,, of a Cauchy\nmatrix as above is given by\n\nC= [Tj-2 Wai — r;)(a; — 9)\n\nTie Tj — 9j)\n\n\n\n\n7.11. PROBLEMS 217\n\nProblem 7.14. Let (α1, . . . , αm+1) be a sequence of pairwise distinct scalars in R and let\n(β1, . . . , βm+1) be any sequence of scalars in R, not necessarily distinct.\n\n(1) Prove that there is a unique polynomial P of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\nHint . Remember Vandermonde!\n\n(2) Let Li(X) be the polynomial of degree m given by\n\nLi(X) =\n(X − α1) · · · (X − αi−1)(X − αi+1) · · · (X − αm+1)\n\n(αi − α1) · · · (αi − αi−1)(αi − αi+1) · · · (αi − αm+1)\n, 1 ≤ i ≤ m+ 1.\n\nThe polynomials Li(X) are known as Lagrange polynomial interpolants . Prove that\n\nLi(αj) = δi j 1 ≤ i, j ≤ m+ 1.\n\nProve that\nP (X) = β1L1(X) + · · ·+ βm+1Lm+1(X)\n\nis the unique polynomial of degree at most m such that\n\nP (αi) = βi, 1 ≤ i ≤ m+ 1.\n\n(3) Prove that L1(X), . . . , Lm+1(X) are linearly independent, and that they form a basis\nof all polynomials of degree at most m.\n\nHow is 1 (the constant polynomial 1) expressed over the basis (L1(X), . . . , Lm+1(X))?\n\nGive the expression of every polynomial P (X) of degree at most m over the basis\n(L1(X), . . . , Lm+1(X)).\n\n(4) Prove that the dual basis (L∗1, . . . , L\n∗\nm+1) of the basis (L1(X), . . . , Lm+1(X)) consists\n\nof the linear forms L∗i given by\nL∗i (P ) = P (αi),\n\nfor every polynomial P of degree at most m; this is simply evaluation at αi.\n\n\n\n218 CHAPTER 7. DETERMINANTS\n\n\n\nChapter 8\n\nGaussian Elimination,\nLU-Factorization, Cholesky\nFactorization, Reduced Row Echelon\nForm\n\nIn this chapter we assume that all vector spaces are over the field R. All results that do not\nrely on the ordering on R or on taking square roots hold for arbitrary fields.\n\n8.1 Motivating Example: Curve Interpolation\n\nCurve interpolation is a problem that arises frequently in computer graphics and in robotics\n(path planning). There are many ways of tackling this problem and in this section we will\ndescribe a solution using cubic splines . Such splines consist of cubic Bézier curves. They\nare often used because they are cheap to implement and give more flexibility than quadratic\nBézier curves.\n\nA cubic Bézier curve C(t) (in R2 or R3) is specified by a list of four control points\n(b0, b2, b2, b3) and is given parametrically by the equation\n\nC(t) = (1− t)3 b0 + 3(1− t)2t b1 + 3(1− t)t2 b2 + t3 b3.\n\nClearly, C(0) = b0, C(1) = b3, and for t ∈ [0, 1], the point C(t) belongs to the convex hull of\nthe control points b0, b1, b2, b3. The polynomials\n\n(1− t)3, 3(1− t)2t, 3(1− t)t2, t3\n\nare the Bernstein polynomials of degree 3.\n\nTypically, we are only interested in the curve segment corresponding to the values of t in\nthe interval [0, 1]. Still, the placement of the control points drastically affects the shape of the\ncurve segment, which can even have a self-intersection; See Figures 8.1, 8.2, 8.3 illustrating\nvarious configurations.\n\n219\n\n\n\n220 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.1: A “standard” Bézier curve.\n\nb0\n\nb1\n\nb2\n\nb3\n\nFigure 8.2: A Bézier curve with an inflection point.\n\n\n\n8.1. MOTIVATING EXAMPLE: CURVE INTERPOLATION 221\n\nb0\n\nb1b2\n\nb3\n\nFigure 8.3: A self-intersecting Bézier curve.\n\nInterpolation problems require finding curves passing through some given data points and\npossibly satisfying some extra constraints.\n\nA Bézier spline curve F is a curve which is made up of curve segments which are Bézier\ncurves, say C1, . . . , Cm (m ≥ 2). We will assume that F defined on [0,m], so that for\ni = 1, . . . ,m,\n\nF (t) = Ci(t− i+ 1), i− 1 ≤ t ≤ i.\n\nTypically, some smoothness is required between any two junction points, that is, between\nany two points Ci(1) and Ci+1(0), for i = 1, . . . ,m − 1. We require that Ci(1) = Ci+1(0)\n(C0-continuity), and typically that the derivatives of Ci at 1 and of Ci+1 at 0 agree up to\nsecond order derivatives. This is called C2-continuity , and it ensures that the tangents agree\nas well as the curvatures.\n\nThere are a number of interpolation problems, and we consider one of the most common\nproblems which can be stated as follows:\n\nProblem: Given N + 1 data points x0, . . . , xN , find a C2 cubic spline curve F such that\nF (i) = xi for all i, 0 ≤ i ≤ N (N ≥ 2).\n\nA way to solve this problem is to find N + 3 auxiliary points d−1, . . . , dN+1, called de\nBoor control points , from which N Bézier curves can be found. Actually,\n\nd−1 = x0 and dN+1 = xN\n\n\n\n222 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nso we only need to find N + 1 points d0, . . . , dN .\n\nIt turns out that the C2-continuity constraints on the N Bézier curves yield only N − 1\nequations, so d0 and dN can be chosen arbitrarily. In practice, d0 and dN are chosen according\nto various end conditions, such as prescribed velocities at x0 and xN . For the time being, we\nwill assume that d0 and dN are given.\n\nFigure 8.4 illustrates an interpolation problem involving N + 1 = 7 + 1 = 8 data points.\nThe control points d0 and d7 were chosen arbitrarily.\n\nx0 = d−1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\nFigure 8.4: A C2 cubic interpolation spline curve passing through the points x0, x1, x2, x3,\nx4, x5, x6, x7.\n\nIt can be shown that d1, . . . , dN−1 are given by the linear system\n7\n2\n\n1\n1 4 1 0\n\n. . . . . . . . .\n\n0 1 4 1\n1 7\n\n2\n\n\n\n\nd1\n\nd2\n...\n\ndN−2\n\ndN−1\n\n =\n\n\n6x1 − 3\n\n2\nd0\n\n6x2\n...\n\n6xN−2\n\n6xN−1 − 3\n2\ndN\n\n .\n\nWe will show later that the above matrix is invertible because it is strictly diagonally\ndominant.\n\n\n\n8.2. GAUSSIAN ELIMINATION 223\n\nOnce the above system is solved, the Bézier cubics C1, . . ., CN are determined as follows\n(we assume N ≥ 2): For 2 ≤ i ≤ N − 1, the control points (bi0, b\n\ni\n1, b\n\ni\n2, b\n\ni\n3) of Ci are given by\n\nbi0 = xi−1\n\nbi1 =\n2\n\n3\ndi−1 +\n\n1\n\n3\ndi\n\nbi2 =\n1\n\n3\ndi−1 +\n\n2\n\n3\ndi\n\nbi3 = xi.\n\nThe control points (b1\n0, b\n\n1\n1, b\n\n1\n2, b\n\n1\n3) of C1 are given by\n\nb1\n0 = x0\n\nb1\n1 = d0\n\nb1\n2 =\n\n1\n\n2\nd0 +\n\n1\n\n2\nd1\n\nb1\n3 = x1,\n\nand the control points (bN0 , b\nN\n1 , b\n\nN\n2 , b\n\nN\n3 ) of CN are given by\n\nbN0 = xN−1\n\nbN1 =\n1\n\n2\ndN−1 +\n\n1\n\n2\ndN\n\nbN2 = dN\n\nbN3 = xN .\n\nFigure 8.5 illustrates this process spline interpolation for N = 7.\n\nWe will now describe various methods for solving linear systems. Since the matrix of the\nabove system is tridiagonal, there are specialized methods which are more efficient than the\ngeneral methods. We will discuss a few of these methods.\n\n8.2 Gaussian Elimination\n\nLet A be an n × n matrix, let b ∈ Rn be an n-dimensional vector and assume that A is\ninvertible. Our goal is to solve the system Ax = b. Since A is assumed to be invertible,\nwe know that this system has a unique solution x = A−1b. Experience shows that two\ncounter-intuitive facts are revealed:\n\n(1) One should avoid computing the inverse A−1 of A explicitly. This is inefficient since\nit would amount to solving the n linear systems Au(j) = ej for j = 1, . . . , n, where\nej = (0, . . . , 1, . . . , 0) is the jth canonical basis vector of Rn (with a 1 is the jth slot).\nBy doing so, we would replace the resolution of a single system by the resolution of n\nsystems, and we would still have to multiply A−1 by b.\n\n\n\n224 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nx0 = d1\n\nx1\n\nx2\n\nx3\n\nx4\n\nx5\n\nx6\n\nx7 = d8\n\nd0\n\nd1\n\nd2\n\nd3\n\nd4\n\nd5\n\nd6\n\nd7\n\n1\n1b =\n\n1\n2b\n\nb\n2\n1\n\nb\n2\n2\n\nb\n\nb1\n3\n\nb2\n3\n\nb1\n4\n\nb2\n4\n\nb1\n5\n\nb2\n5\n\nb1\n6\n\nb2\n6\n\n1\n7\n\nb\n7\n2=\n\nFigure 8.5: A C2 cubic interpolation of x0, x1, x2, x3, x4, x5, x6, x7 with associated color\ncoded Bézier cubics.\n\n(2) One does not solve (large) linear systems by computing determinants (using Cramer’s\nformulae) since this method requires a number of additions (resp. multiplications)\nproportional to (n+ 1)! (resp. (n+ 2)!).\n\nThe key idea on which most direct methods (as opposed to iterative methods, that look\nfor an approximation of the solution) are based is that if A is an upper-triangular matrix,\nwhich means that aij = 0 for 1 ≤ j < i ≤ n (resp. lower-triangular, which means that\naij = 0 for 1 ≤ i < j ≤ n), then computing the solution x is trivial. Indeed, say A is an\nupper-triangular matrix\n\nA =\n\n\n\na1 1 a1 2 · · · a1n−2 a1n−1 a1n\n\n0 a2 2 · · · a2n−2 a2n−1 a2n\n\n0 0\n. . .\n\n...\n...\n\n...\n. . .\n\n...\n...\n\n0 0 · · · 0 an−1n−1 an−1n\n\n0 0 · · · 0 0 ann\n\n\n.\n\nThen det(A) = a1 1a2 2 · · · ann 6= 0, which implies that ai i 6= 0 for i = 1, . . . , n, and we can\nsolve the system Ax = b from bottom-up by back-substitution. That is, first we compute\n\n\n\n8.2. GAUSSIAN ELIMINATION 225\n\nxn from the last equation, next plug this value of xn into the next to the last equation and\ncompute xn−1 from it, etc. This yields\n\nxn = a−1\nnnbn\n\nxn−1 = a−1\nn−1n−1(bn−1 − an−1nxn)\n\n...\n\nx1 = a−1\n1 1 (b1 − a1 2x2 − · · · − a1nxn).\n\nNote that the use of determinants can be avoided to prove that if A is invertible then\nai i 6= 0 for i = 1, . . . , n. Indeed, it can be shown directly (by induction) that an upper (or\nlower) triangular matrix is invertible iff all its diagonal entries are nonzero.\n\nIf A is lower-triangular, we solve the system from top-down by forward-substitution.\n\nThus, what we need is a method for transforming a matrix to an equivalent one in upper-\ntriangular form. This can be done by elimination. Let us illustrate this method on the\nfollowing example:\n\n2x + y + z = 5\n4x − 6y = −2\n−2x + 7y + 2z = 9.\n\nWe can eliminate the variable x from the second and the third equation as follows: Subtract\ntwice the first equation from the second and add the first equation to the third. We get the\nnew system\n\n2x + y + z = 5\n− 8y − 2z = −12\n\n8y + 3z = 14.\n\nThis time we can eliminate the variable y from the third equation by adding the second\nequation to the third:\n\n2x + y + z = 5\n− 8y − 2z = −12\n\nz = 2.\n\nThis last system is upper-triangular. Using back-substitution, we find the solution: z = 2,\ny = 1, x = 1.\n\nObserve that we have performed only row operations. The general method is to iteratively\neliminate variables using simple row operations (namely, adding or subtracting a multiple of\na row to another row of the matrix) while simultaneously applying these operations to the\nvector b, to obtain a system, MAx = Mb, where MA is upper-triangular. Such a method is\ncalled Gaussian elimination. However, one extra twist is needed for the method to work in\nall cases: It may be necessary to permute rows, as illustrated by the following example:\n\nx + y + z = 1\nx + y + 3z = 1\n2x + 5y + 8z = 1.\n\n\n\n226 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nIn order to eliminate x from the second and third row, we subtract the first row from the\nsecond and we subtract twice the first row from the third:\n\nx + y + z = 1\n2z = 0\n\n3y + 6z = −1.\n\nNow the trouble is that y does not occur in the second row; so, we can’t eliminate y from\nthe third row by adding or subtracting a multiple of the second row to it. The remedy is\nsimple: Permute the second and the third row! We get the system:\n\nx + y + z = 1\n3y + 6z = −1\n\n2z = 0,\n\nwhich is already in triangular form. Another example where some permutations are needed\nis:\n\nz = 1\n−2x + 7y + 2z = 1\n4x − 6y = −1.\n\nFirst we permute the first and the second row, obtaining\n\n−2x + 7y + 2z = 1\nz = 1\n\n4x − 6y = −1,\n\nand then we add twice the first row to the third, obtaining:\n\n−2x + 7y + 2z = 1\nz = 1\n\n8y + 4z = 1.\n\nAgain we permute the second and the third row, getting\n\n−2x + 7y + 2z = 1\n8y + 4z = 1\n\nz = 1,\n\nan upper-triangular system. Of course, in this example, z is already solved and we could\nhave eliminated it first, but for the general method, we need to proceed in a systematic\nfashion.\n\nWe now describe the method of Gaussian elimination applied to a linear system Ax = b,\nwhere A is assumed to be invertible. We use the variable k to keep track of the stages of\nelimination. Initially, k = 1.\n\n\n\n8.2. GAUSSIAN ELIMINATION 227\n\n(1) The first step is to pick some nonzero entry ai 1 in the first column of A. Such an\nentry must exist, since A is invertible (otherwise, the first column of A would be the\nzero vector, and the columns of A would not be linearly independent. Equivalently, we\nwould have det(A) = 0). The actual choice of such an element has some impact on the\nnumerical stability of the method, but this will be examined later. For the time being,\nwe assume that some arbitrary choice is made. This chosen element is called the pivot\nof the elimination step and is denoted π1 (so, in this first step, π1 = ai 1).\n\n(2) Next we permute the row (i) corresponding to the pivot with the first row. Such a\nstep is called pivoting . So after this permutation, the first element of the first row is\nnonzero.\n\n(3) We now eliminate the variable x1 from all rows except the first by adding suitable\nmultiples of the first row to these rows. More precisely we add −ai 1/π1 times the first\nrow to the ith row for i = 2, . . . , n. At the end of this step, all entries in the first\ncolumn are zero except the first.\n\n(4) Increment k by 1. If k = n, stop. Otherwise, k < n, and then iteratively repeat Steps\n(1), (2), (3) on the (n− k + 1)× (n− k + 1) subsystem obtained by deleting the first\nk − 1 rows and k − 1 columns from the current system.\n\nIf we let A1 = A and Ak = (a\n(k)\ni j ) be the matrix obtained after k − 1 elimination steps\n\n(2 ≤ k ≤ n), then the kth elimination step is applied to the matrix Ak of the form\n\nAk =\n\n\n\na\n(k)\n1 1 a\n\n(k)\n1 2 · · · · · · · · · a\n\n(k)\n1n\n\n0 a\n(k)\n2 2 · · · · · · · · · a\n\n(k)\n2n\n\n...\n. . . . . .\n\n...\n...\n\n0 0 0 a\n(k)\nk k · · · a\n\n(k)\nk n\n\n...\n...\n\n...\n...\n\n...\n\n0 0 0 a\n(k)\nnk · · · a\n\n(k)\nnn\n\n\n.\n\nActually, note that\na\n\n(k)\ni j = a\n\n(i)\ni j\n\nfor all i, j with 1 ≤ i ≤ k − 2 and i ≤ j ≤ n, since the first k − 1 rows remain unchanged\nafter the (k − 1)th step.\n\nWe will prove later that det(Ak) = ± det(A). Consequently, Ak is invertible. The fact\nthat Ak is invertible iff A is invertible can also be shown without determinants from the fact\nthat there is some invertible matrix Mk such that Ak = MkA, as we will see shortly.\n\nSince Ak is invertible, some entry a\n(k)\ni k with k ≤ i ≤ n is nonzero. Otherwise, the last\n\nn − k + 1 entries in the first k columns of Ak would be zero, and the first k columns of\nAk would yield k vectors in Rk−1. But then the first k columns of Ak would be linearly\n\n\n\n228 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\ndependent and Ak would not be invertible, a contradiction. This situation is illustrated by\nthe following matrix for n = 5 and k = 3:\n\na\n(3)\n1 1 a\n\n(3)\n1 2 a\n\n(3)\n1 3 a\n\n(3)\n1 3 a\n\n(3)\n1 5\n\n0 a\n(3)\n2 2 a\n\n(3)\n2 3 a\n\n(3)\n2 4 a\n\n(3)\n2 5\n\n0 0 0 a\n(3)\n3 4 a\n\n(3)\n3 5\n\n0 0 0 a\n(3)\n4 4 a\n\n(3)\n4n\n\n0 0 0 a\n(3)\n5 4 a\n\n(3)\n5 5\n\n .\n\nThe first three columns of the above matrix are linearly dependent.\n\nSo one of the entries a\n(k)\ni k with k ≤ i ≤ n can be chosen as pivot, and we permute the kth\n\nrow with the ith row, obtaining the matrix α(k) = (α\n(k)\nj l ). The new pivot is πk = α\n\n(k)\nk k , and\n\nwe zero the entries i = k + 1, . . . , n in column k by adding −α(k)\ni k /πk times row k to row i.\n\nAt the end of this step, we have Ak+1. Observe that the first k − 1 rows of Ak are identical\nto the first k − 1 rows of Ak+1.\n\nThe process of Gaussian elimination is illustrated in schematic form below:\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 × × ×\n0 × × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 × ×\n\n =⇒\n\n\n× × × ×\n0 × × ×\n0 0 × ×\n0 0 0 ×\n\n .\n\n8.3 Elementary Matrices and Row Operations\n\nIt is easy to figure out what kind of matrices perform the elementary row operations used\nduring Gaussian elimination. The key point is that if A = PB, where A,B are m × n\nmatrices and P is a square matrix of dimension m, if (as usual) we denote the rows of A and\nB by A1, . . . , Am and B1, . . . , Bm, then the formula\n\naij =\nm∑\nk=1\n\npikbkj\n\ngiving the (i, j)th entry in A shows that the ith row of A is a linear combination of the rows\nof B:\n\nAi = pi1B1 + · · ·+ pimBm.\n\nTherefore, multiplication of a matrix on the left by a square matrix performs row opera-\ntions . Similarly, multiplication of a matrix on the right by a square matrix performs column\noperations\n\nThe permutation of the kth row with the ith row is achieved by multiplying A on the left\nby the transposition matrix P (i, k), which is the matrix obtained from the identity matrix\n\n\n\n8.3. ELEMENTARY MATRICES AND ROW OPERATIONS 229\n\nby permuting rows i and k, i.e.,\n\nP (i, k) =\n\n\n\n1\n1\n\n0 1\n1\n\n. . .\n\n1\n1 0\n\n1\n1\n\n\n.\n\nFor example, if m = 3,\n\nP (1, 3) =\n\n0 0 1\n0 1 0\n1 0 0\n\n ,\n\nthen\n\nP (1, 3)B =\n\n0 0 1\n0 1 0\n1 0 0\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n =\n\nb31 b32 · · · · · · · · · b3n\n\nb21 b22 · · · · · · · · · b2n\n\nb11 b12 · · · · · · · · · b1n\n\n .\n\nObserve that det(P (i, k)) = −1. Furthermore, P (i, k) is symmetric (P (i, k)> = P (i, k)), and\n\nP (i, k)−1 = P (i, k).\n\nDuring the permutation Step (2), if row k and row i need to be permuted, the matrix A\nis multiplied on the left by the matrix Pk such that Pk = P (i, k), else we set Pk = I.\n\nAdding β times row j to row i (with i 6= j) is achieved by multiplying A on the left by\nthe elementary matrix ,\n\nEi,j;β = I + βei j,\n\nwhere\n\n(ei j)k l =\n\n{\n1 if k = i and l = j\n0 if k 6= i or l 6= j,\n\ni.e.,\n\nEi,j;β =\n\n\n\n1\n1\n\n1\n1\n\n. . .\n\n1\nβ 1\n\n1\n1\n\n\nor Ei,j;β =\n\n\n\n1\n1\n\n1 β\n1\n\n. . .\n\n1\n1\n\n1\n1\n\n\n,\n\n\n\n230 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\non the left, i > j, and on the right, i < j. The index i is the index of the row that is changed\nby the multiplication. For example, if m = 3 and we want to add twice row 1 to row 3, since\nβ = 2, j = 1 and i = 3, we form\n\nE3,1;2 = I + 2e31 =\n\n1 0 0\n0 1 0\n0 0 1\n\n+\n\n0 0 0\n0 0 0\n2 0 0\n\n =\n\n1 0 0\n0 1 0\n2 0 1\n\n ,\n\nand calculate\n\nE3,1;2B =\n\n1 0 0\n0 1 0\n2 0 1\n\nb11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\nb31 b32 · · · · · · · · · b3n\n\n\n=\n\n b11 b12 · · · · · · · · · b1n\n\nb21 b22 · · · · · · · · · b2n\n\n2b11 + b31 2b12 + b32 · · · · · · · · · 2b1n + b3n\n\n .\n\nObserve that the inverse of Ei,j;β = I + βei j is Ei,j;−β = I − βei j and that det(Ei,j;β) = 1.\nTherefore, during Step 3 (the elimination step), the matrix A is multiplied on the left by a\nproduct Ek of matrices of the form Ei,k;βi,k , with i > k.\n\nConsequently, we see that\nAk+1 = EkPkAk,\n\nand then\nAk = Ek−1Pk−1 · · ·E1P1A.\n\nThis justifies the claim made earlier that Ak = MkA for some invertible matrix Mk; we can\npick\n\nMk = Ek−1Pk−1 · · ·E1P1,\n\na product of invertible matrices.\n\nThe fact that det(P (i, k)) = −1 and that det(Ei,j;β) = 1 implies immediately the fact\nclaimed above: We always have\n\ndet(Ak) = ± det(A).\n\nFurthermore, since\nAk = Ek−1Pk−1 · · ·E1P1A\n\nand since Gaussian elimination stops for k = n, the matrix\n\nAn = En−1Pn−1 · · ·E2P2E1P1A\n\nis upper-triangular. Also note that if we letM = En−1Pn−1 · · ·E2P2E1P1, then det(M) = ±1,\nand\n\ndet(A) = ± det(An).\n\nThe matrices P (i, k) and Ei,j;β are called elementary matrices . We can summarize the\nabove discussion in the following theorem:\n\n\n\n8.4. LU -FACTORIZATION 231\n\nTheorem 8.1. (Gaussian elimination) Let A be an n× n matrix (invertible or not). Then\nthere is some invertible matrix M so that U = MA is upper-triangular. The pivots are all\nnonzero iff A is invertible.\n\nProof. We already proved the theorem when A is invertible, as well as the last assertion.\nNow A is singular iff some pivot is zero, say at Stage k of the elimination. If so, we must\nhave a\n\n(k)\ni k = 0 for i = k, . . . , n; but in this case, Ak+1 = Ak and we may pick Pk = Ek = I.\n\nRemark: Obviously, the matrix M can be computed as\n\nM = En−1Pn−1 · · ·E2P2E1P1,\n\nbut this expression is of no use. Indeed, what we need is M−1; when no permutations are\nneeded, it turns out that M−1 can be obtained immediately from the matrices Ek’s, in fact,\nfrom their inverses, and no multiplications are necessary.\n\nRemark: Instead of looking for an invertible matrix M so that MA is upper-triangular, we\ncan look for an invertible matrix M so that MA is a diagonal matrix. Only a simple change\nto Gaussian elimination is needed. At every Stage k, after the pivot has been found and\npivoting been performed, if necessary, in addition to adding suitable multiples of the kth\nrow to the rows below row k in order to zero the entries in column k for i = k + 1, . . . , n,\nalso add suitable multiples of the kth row to the rows above row k in order to zero the\nentries in column k for i = 1, . . . , k − 1. Such steps are also achieved by multiplying on\nthe left by elementary matrices Ei,k;βi,k , except that i < k, so that these matrices are not\nlower-triangular matrices. Nevertheless, at the end of the process, we find that An = MA,\nis a diagonal matrix.\n\nThis method is called the Gauss-Jordan factorization. Because it is more expensive than\nGaussian elimination, this method is not used much in practice. However, Gauss-Jordan\nfactorization can be used to compute the inverse of a matrix A. Indeed, we find the jth\ncolumn of A−1 by solving the system Ax(j) = ej (where ej is the jth canonical basis vector\nof Rn). By applying Gauss-Jordan, we are led to a system of the form Djx\n\n(j) = Mjej, where\nDj is a diagonal matrix, and we can immediately compute x(j).\n\nIt remains to discuss the choice of the pivot, and also conditions that guarantee that no\npermutations are needed during the Gaussian elimination process. We begin by stating a\nnecessary and sufficient condition for an invertible matrix to have an LU -factorization (i.e.,\nGaussian elimination does not require pivoting).\n\n8.4 LU-Factorization\n\nDefinition 8.1. We say that an invertible matrix A has an LU-factorization if it can be\nwritten as A = LU , where U is upper-triangular invertible and L is lower-triangular, with\nLi i = 1 for i = 1, . . . , n.\n\n\n\n232 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nA lower-triangular matrix with diagonal entries equal to 1 is called a unit lower-triangular\nmatrix. Given an n×n matrix A = (ai j), for any k with 1 ≤ k ≤ n, let A(1 : k, 1 : k) denote\nthe submatrix of A whose entries are ai j, where 1 ≤ i, j ≤ k.1 For example, if A is the 5× 5\nmatrix\n\nA =\n\n\na11 a12 a13 a14 a15\n\na21 a22 a23 a24 a25\n\na31 a32 a33 a34 a35\n\na41 a42 a43 a44 a45\n\na51 a52 a53 a54 a55\n\n ,\n\nthen\n\nA(1 : 3, 1 : 3) =\n\na11 a12 a13\n\na21 a22 a23\n\na31 a32 a33\n\n .\n\nProposition 8.2. Let A be an invertible n × n-matrix. Then A has an LU-factorization\nA = LU iff every matrix A(1 : k, 1 : k) is invertible for k = 1, . . . , n. Furthermore, when A\nhas an LU-factorization, we have\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nwhere πk is the pivot obtained after k− 1 elimination steps. Therefore, the kth pivot is given\nby\n\nπk =\n\na11 = det(A(1 : 1, 1 : 1)) if k = 1\ndet(A(1 : k, 1 : k))\n\ndet(A(1 : k − 1, 1 : k − 1))\nif k = 2, . . . , n.\n\nProof. First assume that A = LU is an LU -factorization of A. We can write\n\nA =\n\n(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nL1 0\nL3 L4\n\n)(\nU1 U2\n\n0 U4\n\n)\n=\n\n(\nL1U1 L1U2\n\nL3U1 L3U2 + L4U4\n\n)\n,\n\nwhere L1, L4 are unit lower-triangular and U1, U4 are upper-triangular. (Note, A(1 : k, 1 : k),\nL1, and U1 are k×k matrices; A2 and U2 are k× (n−k) matrices; A3 and L3 are (n−k)×k\nmatrices; A4, L4, and U4 are (n− k)× (n− k) matrices.) Thus,\n\nA(1 : k, 1 : k) = L1U1,\n\nand since U is invertible, U1 is also invertible (the determinant of U is the product of the\ndiagonal entries in U , which is the product of the diagonal entries in U1 and U4). As L1 is\ninvertible (since its diagonal entries are equal to 1), we see that A(1 : k, 1 : k) is invertible\nfor k = 1, . . . , n.\n\nConversely, assume that A(1 : k, 1 : k) is invertible for k = 1, . . . , n. We just need to\nshow that Gaussian elimination does not need pivoting. We prove by induction on k that\nthe kth step does not need pivoting.\n\n1We are using Matlab’s notation.\n\n\n\n8.4. LU -FACTORIZATION 233\n\nThis holds for k = 1, since A(1 : 1, 1 : 1) = (a1 1), so a1 1 6= 0. Assume that no pivoting\nwas necessary for the first k − 1 steps (2 ≤ k ≤ n− 1). In this case, we have\n\nEk−1 · · ·E2E1A = Ak,\n\nwhere L = Ek−1 · · ·E2E1 is a unit lower-triangular matrix and Ak(1 : k, 1 : k) is upper-\ntriangular, so that LA = Ak can be written as(\n\nL1 0\nL3 L4\n\n)(\nA(1 : k, 1 : k) A2\n\nA3 A4\n\n)\n=\n\n(\nU1 B2\n\n0 B4\n\n)\n,\n\nwhere L1 is unit lower-triangular and U1 is upper-triangular. (Once again A(1 : k, 1 : k), L1,\nand U1 are k × k matrices; A2 and B2 are k × (n− k) matrices; A3 and L3 are (n− k)× k\nmatrices; A4, L4, and B4 are (n− k)× (n− k) matrices.) But then,\n\nL1A(1 : k, 1 : k)) = U1,\n\nwhere L1 is invertible (in fact, det(L1) = 1), and since by hypothesis A(1 : k, 1 : k) is\ninvertible, U1 is also invertible, which implies that (U1)kk 6= 0, since U1 is upper-triangular.\nTherefore, no pivoting is needed in Step k, establishing the induction step. Since det(L1) = 1,\nwe also have\n\ndet(U1) = det(L1A(1 : k, 1 : k)) = det(L1) det(A(1 : k, 1 : k)) = det(A(1 : k, 1 : k)),\n\nand since U1 is upper-triangular and has the pivots π1, . . . , πk on its diagonal, we get\n\ndet(A(1 : k, 1 : k)) = π1π2 · · · πk, k = 1, . . . , n,\n\nas claimed.\n\nRemark: The use of determinants in the first part of the proof of Proposition 8.2 can be\navoided if we use the fact that a triangular matrix is invertible iff all its diagonal entries are\nnonzero.\n\nCorollary 8.3. (LU-Factorization) Let A be an invertible n × n-matrix. If every matrix\nA(1 : k, 1 : k) is invertible for k = 1, . . . , n, then Gaussian elimination requires no pivoting\nand yields an LU-factorization A = LU .\n\nProof. We proved in Proposition 8.2 that in this case Gaussian elimination requires no\npivoting. Then since every elementary matrix Ei,k;β is lower-triangular (since we always\narrange that the pivot πk occurs above the rows that it operates on), since E−1\n\ni,k;β = Ei,k;−β\nand the Eks are products of Ei,k;βi,ks, from\n\nEn−1 · · ·E2E1A = U,\n\nwhere U is an upper-triangular matrix, we get\n\nA = LU,\n\nwhere L = E−1\n1 E−1\n\n2 · · ·E−1\nn−1 is a lower-triangular matrix. Furthermore, as the diagonal\n\nentries of each Ei,k;β are 1, the diagonal entries of each Ek are also 1.\n\n\n\n234 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nExample 8.1. The reader should verify that\n2 1 1 0\n4 3 3 1\n8 7 9 5\n6 7 9 8\n\n =\n\n\n1 0 0 0\n2 1 0 0\n4 3 1 0\n3 4 1 1\n\n\n\n\n2 1 1 0\n0 1 1 1\n0 0 2 2\n0 0 0 2\n\n\nis an LU -factorization.\n\nOne of the main reasons why the existence of an LU -factorization for a matrix A is\ninteresting is that if we need to solve several linear systems Ax = b corresponding to the\nsame matrix A, we can do this cheaply by solving the two triangular systems\n\nLw = b, and Ux = w.\n\nThere is a certain asymmetry in the LU -decomposition A = LU of an invertible matrix A.\nIndeed, the diagonal entries of L are all 1, but this is generally false for U . This asymmetry\ncan be eliminated as follows: if\n\nD = diag(u11, u22, . . . , unn)\n\nis the diagonal matrix consisting of the diagonal entries in U (the pivots), then we if let\nU ′ = D−1U , we can write\n\nA = LDU ′,\n\nwhere L is lower- triangular, U ′ is upper-triangular, all diagonal entries of both L and U ′\n\nare 1, and D is a diagonal matrix of pivots. Such a decomposition leads to the following\ndefinition.\n\nDefinition 8.2. We say that an invertible n×n matrix A has an LDU -factorization if it can\nbe written as A = LDU ′, where L is lower- triangular, U ′ is upper-triangular, all diagonal\nentries of both L and U ′ are 1, and D is a diagonal matrix.\n\nWe will see shortly than if A is real symmetric, then U ′ = L>.\n\nAs we will see a bit later, real symmetric positive definite matrices satisfy the condition of\nProposition 8.2. Therefore, linear systems involving real symmetric positive definite matrices\ncan be solved by Gaussian elimination without pivoting. Actually, it is possible to do better:\nthis is the Cholesky factorization.\n\nIf a square invertible matrix A has an LU -factorization, then it is possible to find L and U\nwhile performing Gaussian elimination. Recall that at Step k, we pick a pivot πk = a\n\n(k)\nik 6= 0\n\nin the portion consisting of the entries of index j ≥ k of the k-th column of the matrix Ak\nobtained so far, we swap rows i and k if necessary (the pivoting step), and then we zero the\nentries of index j = k + 1, . . . , n in column k. Schematically, we have the following steps:\n\n× × × × ×\n0 × × × ×\n0 × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n\n pivot\n=⇒\n\n\n× × × × ×\n0 a\n\n(k)\nik × × ×\n\n0 × × × ×\n0 × × × ×\n0 × × × ×\n\n elim\n=⇒\n\n\n× × × × ×\n0 × × × ×\n0 0 × × ×\n0 0 × × ×\n0 0 × × ×\n\n .\n\n\n\n8.4. LU -FACTORIZATION 235\n\nMore precisely, after permuting row k and row i (the pivoting step), if the entries in column\nk below row k are αk+1k, . . . , αnk, then we add −αjk/πk times row k to row j; this process\nis illustrated below: \n\na\n(k)\nkk\n\na\n(k)\nk+1k\n...\n\na\n(k)\nik\n...\n\na\n(k)\nnk\n\n\npivot\n=⇒\n\n\n\na\n(k)\nik\n\na\n(k)\nk+1k\n...\n\na\n(k)\nkk\n...\n\na\n(k)\nnk\n\n\n=\n\n\n\nπk\nαk+1k\n\n...\nαik\n...\nαnk\n\n\nelim\n=⇒\n\n\n\nπk\n0\n...\n0\n...\n0\n\n\n.\n\nThen if we write `jk = αjk/πk for j = k + 1, . . . , n, the kth column of L is\n\n0\n...\n0\n1\n\n`k+1k\n...\n`nk\n\n\n.\n\nObserve that the signs of the multipliers −αjk/πk have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n .\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the `ij:\n\nL−1 =\n\n\n1 0 0 · · · 0\n−`21 1 0 · · · 0\n−`31 −`32 1 · · · 0\n\n...\n...\n\n...\n. . . 0\n\n−`n1 −`n2 −`n3 · · · 1\n\n .\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = En−1 · · ·E1A,\n\n8.4. LU-FACTORIZATION 235\n\nMore precisely, after permuting row & and row 7 (the pivoting step), if the entries in column\nk below row k are Qp+iz,---,Qnk, then we add —a,;,/m, times row k to row j; this process\nis illustrated below:\n\n‘s i) (me) fe\nk+1k Qesik Ok+1k 0\npivot _ elim\na () al) Qik 0\n(k) (k) ray 0)\nOnk Onk mk\n\nThen if we write Cj, = ajx/m, for j =k+1,...,n, the kth column of L is\n\nObserve that the signs of the multipliers —a,,/a, have been flipped. Thus, we obtain the\nunit lower triangular matrix\n\n1 0 0\nly, 1 0\nL=|n 2 1\n\nFo OOO\n\nCnt ln2 lng\n\nIt is easy to see (and this is proven in Theorem 8.5) that the inverse of L is obtained from\nL by flipping the signs of the ¢,;:\n\n1 0 0 0\n\nbo} 1 0 0\n\nLot =| —f31 —l32 1 0\n: : 0\n\n—lny —ln2 ng 1\n\nFurthermore, if the result of Gaussian elimination (without pivoting) is U = E,_,--- FA,\n\n\n\n\n236 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nthen\n\nEk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · −`k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · −`nk 0 · · · 1\n\n\nand E−1\n\nk =\n\n\n\n1 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 1 0 · · · 0\n0 · · · `k+1k 1 · · · 0\n...\n\n...\n...\n\n...\n. . .\n\n...\n0 · · · `nk 0 · · · 1\n\n\n,\n\nso the kth column of Ek is the kth column of L−1.\n\nHere is an example illustrating the method.\n\nExample 8.2. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: The first pivot is π1 = 1 in row 1, and we substract\nrow 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n L1 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 0 0 1\n\n .\n\nThe next pivot is π2 = −2 in row 2, and we subtract row 2 from row 4 (and add 0 times row\n2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L2 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = L3 =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n .\n\n\n\n8.5. PA = LU FACTORIZATION 237\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n = A.\n\nWe now show how to extend the above method to deal with pivoting efficiently. This is\nthe PA = LU factorization.\n\n8.5 PA = LU Factorization\n\nThe following easy proposition shows that, in principle, A can be premultiplied by some\npermutation matrix P , so that PA can be converted to upper-triangular form without using\nany pivoting. Permutations are discussed in some detail in Section 30.3, but for now we\njust need this definition. For the precise connection between the notion of permutation (as\ndiscussed in Section 30.3) and permutation matrices, see Problem 8.16.\n\nDefinition 8.3. A permutation matrix is a square matrix that has a single 1 in every row\nand every column and zeros everywhere else.\n\nIt is shown in Section 30.3 that every permutation matrix is a product of transposition\nmatrices (the P (i, k)s), and that P is invertible with inverse P>.\n\nProposition 8.4. Let A be an invertible n × n-matrix. There is some permutation matrix\nP so that (PA)(1 : k, 1 : k) is invertible for k = 1, . . . , n.\n\nProof. The case n = 1 is trivial, and so is the case n = 2 (we swap the rows if necessary). If\nn ≥ 3, we proceed by induction. Since A is invertible, its columns are linearly independent;\nin particular, its first n− 1 columns are also linearly independent. Delete the last column of\nA. Since the remaining n− 1 columns are linearly independent, there are also n− 1 linearly\nindependent rows in the corresponding n × (n − 1) matrix. Thus, there is a permutation\nof these n rows so that the (n − 1) × (n − 1) matrix consisting of the first n − 1 rows is\ninvertible. But then there is a corresponding permutation matrix P1, so that the first n− 1\nrows and columns of P1A form an invertible matrix A′. Applying the induction hypothesis\nto the (n− 1)× (n− 1) matrix A′, we see that there some permutation matrix P2 (leaving\nthe nth row fixed), so that (P2P1A)(1 : k, 1 : k) is invertible, for k = 1, . . . , n − 1. Since A\nis invertible in the first place and P1 and P2 are invertible, P1P2A is also invertible, and we\nare done.\n\nRemark: One can also prove Proposition 8.4 using a clever reordering of the Gaussian\nelimination steps suggested by Trefethen and Bau [174] (Lecture 21). Indeed, we know that\n\n\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices Pi and products of elementary matrices\nEi, so that\n\nAn = En−1Pn−1 · · ·E2P2E1P1A,\n\nwhere U = An is upper-triangular. For example, when n = 4, we have E3P3E2P2E1P1A = U .\nWe can define new matrices E ′1, E\n\n′\n2, E\n\n′\n3 which are still products of elementary matrices so\n\nthat we have\nE ′3E\n\n′\n2E\n′\n1P3P2P1A = U.\n\nIndeed, if we let E ′3 = E3, E ′2 = P3E2P\n−1\n3 , and E ′1 = P3P2E1P\n\n−1\n2 P−1\n\n3 , we easily verify that\neach E ′k is a product of elementary matrices and that\n\nE ′3E\n′\n2E\n′\n1P3P2P1 = E3(P3E2P\n\n−1\n3 )(P3P2E1P\n\n−1\n2 P−1\n\n3 )P3P2P1 = E3P3E2P2E1P1.\n\nIt can also be proven that E ′1, E\n′\n2, E\n\n′\n3 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nE ′k = Pn−1 · · ·Pk+1EkP\n\n−1\nk+1 · · ·P−1\n\nn−1,\n\nand we have\nE ′n−1 · · ·E ′1Pn−1 · · ·P1A = U,\n\nwhere each E ′j is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU -factorization yields the matrices L,U ,\nand P , such that PA = LU . To describe this new method, since the diagonal entries of L\nare 1s, it is convenient to write\n\nL = I + Λ.\n\nThen in assembling the matrix Λ while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of Λ (really Λk−1) that we make on the rows of A\n(really Ak) during a pivoting step involving row k and row i. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand Λ. Here is an example illustrating this method.\n\nExample 8.3. Given\n\nA = A1 =\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n ,\n\nwe have the following sequence of steps: We initialize Λ0 = 0 and P0 = I4. The first pivot is\nπ1 = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\nA2 =\n\n\n1 1 1 0\n0 0 −2 0\n0 −2 −1 1\n0 −2 −1 −1\n\n Λ1 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P1 =\n\n\n1 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 1\n\n .\n\n238 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nif A is invertible, then there are permutation matrices P; and products of elementary matrices\nE;, so that\nAn = n—1P n—1 ute Fo P, EK, P,A,\n\nwhere U = A, is upper-triangular. For example, when n = 4, we have £3 P3E)P,E,P,A = U.\nWe can define new matrices E}, £5, £3 which are still products of elementary matrices so\nthat we have\n\nEEE P3P,P,A = U.\nIndeed, if we let E, = E3, BE, = P3E,P;', and E} = P3P,E,P;'P;*, we easily verify that\neach Ej, is a product of elementary matrices and that\n\nEEE) P3P2P, = E3(P3E2P;')(P3P)E, Py! Ps')P3P)P, = F3P3E2P)E,P,.\n\nIt can also be proven that FE}, £5, E4 are lower triangular (see Theorem 8.5).\n\nIn general, we let\nEy = Pyrite ++ Pei EePay Poh,\n\nand we have\n\nEly EL Py: RA=U,\nwhere each E* is a lower triangular matrix (see Theorem 8.5).\n\nIt is remarkable that if pivoting steps are necessary during Gaussian elimination, a very\nsimple modification of the algorithm for finding an LU-factorization yields the matrices L, U,\nand P, such that PA = LU. To describe this new method, since the diagonal entries of L\nare ls, it is convenient to write\n\nL=I+A.\n\nThen in assembling the matrix A while performing Gaussian elimination with pivoting, we\nmake the same transposition on the rows of A (really A;,_1) that we make on the rows of A\n(really A;,) during a pivoting step involving row k and row 7. We also assemble P by starting\nwith the identity matrix and applying to P the same row transpositions that we apply to A\nand A. Here is an example illustrating this method.\n\nExample 8.3. Given\n\n11 1 £0\n\n1 1 —-1 0\nA= A, = 1-1 0 14?\n\n1-1 0 —-1\n\nwe have the following sequence of steps: We initialize Ag = 0 and Po = Jy. The first pivot is\nm7, = 1 in row 1, and we subtract row 1 from rows 2, 3, and 4. We get\n\n11 1 +0 0000 100 0\n0 0 -2 0 100 0 0100\nAg 0 -2 -1 1 Ay 1000 B= 159 1 0\n0 —2 -1 -l 100 0 0001\n\n\n\n\n8.5. PA = LU FACTORIZATION 239\n\nThe next pivot is π2 = −2 in row 3, so we permute row 2 and 3; we also apply this permutation\nto Λ and P :\n\nA′3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 −2 −1 −1\n\n Λ′2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 0 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nNext we subtract row 2 from row 4 (and add 0 times row 2 to row 3). We get\n\nA3 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ2 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P2 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe next pivot is π3 = −2 in row 3, and since the fourth entry in column 3 is already a zero,\nwe add 0 times row 3 to row 4. We get\n\nA4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n Λ3 =\n\n\n0 0 0 0\n1 0 0 0\n1 0 0 0\n1 1 0 0\n\n P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nThe procedure is finished, and we have\n\nL = Λ3 + I =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n U = A4 =\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n P = P3 =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n .\n\nIt is easy to check that indeed\n\nLU =\n\n\n1 0 0 0\n1 1 0 0\n1 0 1 0\n1 1 0 1\n\n\n\n\n1 1 1 0\n0 −2 −1 1\n0 0 −2 0\n0 0 0 −2\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n\nand\n\nPA =\n\n\n1 0 0 0\n0 0 1 0\n0 1 0 0\n0 0 0 1\n\n\n\n\n1 1 1 0\n1 1 −1 0\n1 −1 0 1\n1 −1 0 −1\n\n =\n\n\n1 1 1 0\n1 −1 0 1\n1 1 −1 0\n1 −1 0 −1\n\n .\n\nUsing the idea in the remark before the above example, we can prove the theorem below\nwhich shows the correctness of the algorithm for computing P,L and U using a simple\nadaptation of Gaussian elimination.\n\n\n\n240 CHAPTER 8. GAUSSIAN ELIMINATION, LU, CHOLESKY, ECHELON FORM\n\nWe are not aware of a detailed proof of Theorem 8.5 in the standard texts. Although\nGolub and Van Loan [80] state a version of this theorem as their Theorem 3.1.4, they say\nthat “The proof is a messy subscripting argument.” Meyer [124] also provides a sketch of\nproof (see the end of Section 3.10). In view of this situation, we offer a complete proof.\nIt does involve a lot of subscripts and superscripts, but in our opinion, it contains some\ntechniques that go far beyond symbol manipulation.\n\nTheorem 8.5. For every invertible n× n-matrix A, the following hold:\n\n(1) There is some permutation matrix P , some upper-triangular matrix U , and some unit\nlower-triangular matrix L, so that PA = LU (recall, Li i = 1 for i = 1, . . . , n). Fur-\nthermore, if P = I, then L and U are unique and they are produced as a result of\nGaussian elimination without pivoting.\n\n(2) If En−1 . . . E1A = U is the result of Gaussian elimination without pivoting, write as\n\nusual Ak = Ek−1 . . . E1A (with Ak = (a\n(k)\nij )), and let `ik = a\n\n(k)\nik /a\n\n(k)\nkk , with 1 ≤ k ≤ n− 1\n\nand k + 1 ≤ i ≤ n. Then\n\nL =\n\n\n1 0 0 · · · 0\n`21 1 0 · · · 0\n`31 `32 1 · · · 0\n...\n\n...\n...\n\n. . . 0\n`n1 `n2 `n3 · · · 1\n\n ,\n\nwhere the kth column of L is the kth column of E−1\nk , for k = 1, . . . , n− 1.\n\n(3) If En−1Pn−1 · · ·E1P1A = U is the result of Gaussian elimination with some pivoting,\nwrite Ak = Ek−1Pk−1 · · ·E1P1A, and define Ek\n\nj , with 1 ≤ j ≤ n− 1 and j ≤ k ≤ n− 1,\nsuch that, for j = 1, . . . , n− 2,\n\nEj\nj = Ej\n\nEk\nj = PkE\n\nk−1\nj Pk, for k = j + 1, . . . , n− 1,\n\nand\nEn−1\nn−1 = En−1.\n\nThen,\n\nEk\nj = PkPk−1 · · ·Pj+1EjPj+1 · · ·Pk−1Pk\n\nU = En−1\nn−1 · · ·En−1\n\n1 Pn−1 · · ·P1A,\n\nand if we set\n\nP = Pn−1 · · ·P1\n\nL = (En−1\n1 )−1 · · · (En−1\n\nn−1)−1,\n\n\n\n8.5. PA = LU FACTORIZATION 241\n\nthen\n\nPA = LU. (†1)\n\nFurthermore,\n\n(Ek\nj )−1 = I + Ekj , 1 ≤ j ≤ n− 1, j ≤ k ≤ n− 1,\n\nwhere Ekj is a lower triangular matrix of the form\n\nEkj =\n\n\n\n0 · · · 0 0 · · · 0\n...\n\n. . .\n...\n\n...\n...\n\n...\n0 · · · 0 0 · · · 0\n\n0 · · · `\n(k)\nj+1j 0 · · · 0\n\n...\n...\n\n...\n...\n\n. . .\n...\n\n0 · · · `\n(k)\nnj 0 · · · 0\n\n\n,\n\nwe have\n\nEk\nj = I − Ekj ,\n\nand\n\nEkj = PkEk−1\nj , 1 ≤ j ≤ n− 2, j + 1 ≤ k ≤ n− 1,\n\nwhere Pk = I or else Pk = P (k, i) for some i such that k + 1 ≤ i ≤ n; if Pk 6= I, this\nmeans that (Ek\n\nj )−1 is obtained from (Ek−1\nj )−1 by permuting the entries on rows i and\n\nk in column j. Because the matrices (Ek\nj )−1 are all","extracted_metadata":{"X-TIKA:Parsed-By-Full-Set":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser","org.apache.tika.parser.ocr.TesseractOCRParser"],"pdf:docinfo:trapped":["False"],"X-TIKA:Parsed-By":["org.apache.tika.parser.DefaultParser","org.apache.tika.parser.pdf.PDFParser"],"xmpMM:DocumentID":["uuid:ce41e11f-d564-4756-b319-b610f878631b"],"pdf:PDFVersion":["1.5"],"pdf:docinfo:creator_tool":["TeX"],"access_permission:can_print_degraded":["true"],"pdf:docinfo:modified":["2020-03-10T20:05:09Z"],"xmp:CreateDate":["2019-08-02T23:26:13Z"],"pdf:hasXMP":["true"],"access_permission:extract_content":["true"],"dcterms:modified":["2020-03-10T20:05:09Z"],"dc:format":["application/pdf; version=1.5"],"pdf:docinfo:created":["2019-08-02T23:26:13Z"],"access_permission:extract_for_accessibility":["true"],"pdf:docinfo:custom:PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"],"access_permission:fill_in_form":["true"],"xmp:CreatorTool":["TeX"],"pdf:producer":["pdfTeX-1.40.17"],"Content-Type":["application/pdf"],"pdf:charsPerPage":["264","1","1263","2199","2131","2172","2059","1963","1915","2250","2056","1853","2008","2098","1054","10","22","23","1330","1775","1213","1058","1293","984","1576","1795","1042","1384","833","728","1496","1390","971","1656","1435","1217","1623","1155","1581","1434","1514","1191","20","0","659","1398","944","800","1316","1428","1296","2192","843","572","533","1323","1637","1545","1495","1940","2321","1417","1100","1421","1230","2050","1727","1956","618","952","1103","1573","1942","2105","2520","1646","1622","2210","1774","1636","1365","1283","1378","1691","1142","875","1677","1253","1163","1970","966","2065","1371","1692","1147","1112","1283","1196","929","1279","1080","1344","936","876","1158","41","1256","1172","775","1475","1080","1589","614","1132","1576","1159","2095","1710","1580","1247","1323","872","1335","1106","751","1060","1004","1307","1442","500","956","1539","547","1234","1239","779","1264","951","766","1556","1174","845","1477","983","1187","1773","537","119","1228","2084","1054","912","1109","1049","1101","1267","1445","1091","1337","1143","1455","642","1517","1909","1086","1505","1157","1061","1268","1366","1685","893","946","1377","1260","1457","1634","1491","437","22","1792","1695","1854","1639","1384","1458","1277","1167","1375","1241","1167","1699","1412","923","608","920","886","861","1146","1309","1511","1833","1614","1061","622","1298","1124","1198","1984","1373","1313","1243","850","1038","1276","954","1011","24","1187","149","1040","838","1218","970","1531","1036","1866","1517","771","1239","2200","1542","1736","1877","891","950","1922","1576","1015","1358"],"pdf:docinfo:producer":["pdfTeX-1.40.17"],"access_permission:can_print":["true"],"access_permission:modify_annotations":["true"],"pdf:unmappedUnicodeCharsPerPage":["1","0","1","1","1","1","0","0","0","0","3","0","0","2","2","0","0","0","0","3","31","30","0","9","0","2","20","15","23","25","25","10","9","1","0","1","2","4","6","4","3","1","0","0","0","1","14","0","3","26","0","37","0","0","0","3","8","2","0","1","3","22","83","47","51","0","6","11","0","1","11","13","7","2","10","7","7","7","11","11","0","7","16","3","20","16","0","15","15","6","3","0","0","0","6","6","0","0","0","0","0","2","2","1","1","0","1","3","9","1","1","3","10","3","0","1","2","7","25","32","22","6","9","0","0","11","14","7","2","1","0","6","0","5","8","0","6","12","11","7","0","20","28","2","2","0","10","0","7","9","5","3","33","16","40","88","55","0","7","0","9","10","14","17","8","3","0","1","6","15","3","39","10","15","38","3","15","24","5","0","1","4","3","8","13","3","2","11","6","0","21","27","9","12","23","23","0","80","48","7","8","7","2","25","1","2","4","9","5","0","3","29","19","1","46","56","0","0","0","0","0","0","0","2","1","0","0","1","5","0","0","8","8","10","16","4","3","20","2","7"],"pdf:encrypted":["false"],"PTEX.Fullbanner":["This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2"],"access_permission:assemble_document":["true"],"resourceName":["Algebra, Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning - 2019 (math-deep).pdf"],"pdf:hasXFA":["false"],"access_permission:can_modify":["true"],"xmp:MetadataDate":["2020-03-10T20:05:09Z"],"X-TIKA:EXCEPTION:write_limit_reached":["true"],"pdf:hasCollection":["false"],"xmpTPg:NPages":["1962"],"dcterms:created":["2019-08-02T23:26:13Z"],"xmp:ModifyDate":["2020-03-10T20:05:09Z"],"Content-Length":["20812100"],"pdf:hasMarkedContent":["false"]},"metadata_field_count":39,"attempts":1,"timestamp":1754065282.790042,"platform":"Linux","python_version":"3.13.5"}]