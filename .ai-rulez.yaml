metadata:
  name: "Python Text Extraction Libraries Benchmarks 2025"
  version: "1.0.0"
  description: "Comprehensive benchmarking suite for Python text extraction libraries"

outputs:
  - file: "CLAUDE.md"
    template: |
      # CLAUDE.md - Repository Knowledge Base

      ## Project Overview

      {{ .Description }}

      {{ range .Rules }}
      ## {{ .Name }}
      {{ .Content }}
      {{ end }}

      {{ range .Sections }}
      ## {{ .Title }}
      {{ .Content }}
      {{ end }}
  - file: "GEMINI.md"
    template: |
      # GEMINI.md - Repository Knowledge Base

      This file provides guidance to Gemini when working with code in this repository.

      ## Project Overview

      {{ .Description }}

      {{ range .Rules }}
      ## {{ .Name }}
      {{ .Content }}
      {{ end }}

      {{ range .Sections }}
      ## {{ .Title }}
      {{ .Content }}
      {{ end }}
  - file: ".cursorrules"
    template: |
      # Python Text Extraction Libraries Benchmarks 2025
      Comprehensive benchmarking suite for Python text extraction libraries

      {{ range .Rules }}
      ## {{ .Name }}
      {{ .Content }}
      {{ end }}

      {{ range .Sections }}
      ## {{ .Title }}
      {{ .Content }}
      {{ end }}
  - file: ".windsurfrules"
    template: |
      # Python Text Extraction Libraries Benchmarks 2025
      Comprehensive benchmarking suite for Python text extraction libraries

      {{ range .Rules }}
      ## {{ .Name }}
      {{ .Content }}
      {{ end }}

      {{ range .Sections }}
      ## {{ .Title }}
      {{ .Content }}
      {{ end }}

rules:
  - name: "Quick Reference"
    priority: 10
    content: |
      - **Main CLI**: `uv run python -m src.cli <command>`
      - **Test Documents**: `test_documents/` (94 files, ~210MB)
      - **CI Workflows**: `.github/workflows/benchmark-by-framework.yml` (recommended)
      - **Key Timeouts**: 300s per extraction, 150min per CI job
      - **Cache Clearing**: Kreuzberg cache cleared before each benchmark

  - name: "Development Commands"
    priority: 10
    content: |
      ### Dependencies
      - Install base dependencies: `uv sync`
      - Install specific framework: `uv sync --extra kreuzberg`
      - Install all compatible: `uv sync --extra all`
      - Install with OCR: `uv sync --extra kreuzberg-ocr`

      ### Running Benchmarks
      - Run all frameworks: `uv run python -m src.cli benchmark`
      - Test specific frameworks: `uv run python -m src.cli benchmark --framework kreuzberg_sync,extractous`
      - Test by category: `uv run python -m src.cli benchmark --category small,medium`
      - List available options: `uv run python -m src.cli list-frameworks`
      - Note: CLI includes ALL frameworks (including docling) when using --framework all

      ### Analysis and Reporting
      - Generate reports: `uv run python -m src.cli report --output-format html`
      - Create visualizations: `uv run python -m src.cli visualize`
      - Aggregate results: `uv run python -m src.cli aggregate`
      - Quality assessment: `uv run python -m src.cli quality-assess`

      ### Code Quality
      - Format code: `ruff format`
      - Lint code: `ruff check`
      - Fix linting issues: `ruff check --fix`
      - Type check: `mypy src/`

      ### Pre-commit
      - Install hooks: `uv run pre-commit install --hook-type commit-msg && uv run pre-commit install`
      - Run manually: `uv run pre-commit run --all-files`

  - name: "Architecture"
    priority: 9
    content: |
      ### Core Components
      1. **Benchmark Engine** (`benchmark.py`): Orchestrates benchmark execution
         - Supports both sync and async extractors
         - Thread pool execution for sync extractors
         - Comprehensive timeout and error handling

      2. **Extractors** (`extractors.py`): Framework wrappers
         - Protocol-based interface for consistency
         - Factory pattern for instantiation
         - Language configuration support

      3. **Profiler** (`profiler.py`): Performance monitoring
         - CPU usage sampling at 100ms intervals
         - Memory (RSS) tracking with peak detection
         - Context managers for clean resource management

      4. **Reporting** (`reporting.py`): Results analysis
         - Console tables with Rich
         - CSV exports for external analysis
         - Visualization charts (time, memory, success rate)
         - Colorblind-accessible visualizations

      5. **CLI** (`cli.py`): Command interface
         - Commands: benchmark, report, visualize, aggregate, quality-assess
         - Framework/category/file-type filtering
         - Configurable timeouts and iterations

      ### Adding New Frameworks
      1. Create extractor class in `extractors.py`
      2. Add to Framework enum in `types.py`
      3. Register in `get_extractor()` factory
      4. Add dependencies to `pyproject.toml`
      5. Configure language support in `get_language_config()`
      6. Add to FRAMEWORK_EXCLUSIONS in `config.py` if needed

  - name: "Testing Guidelines"
    priority: 8
    content: |
      ### Test Document Collection
      - **Total**: 94 documents (~210MB)
      - **Categories**: tiny (<100KB), small (100KB-1MB), medium (1-10MB), large (10-50MB), huge (>50MB)
      - **Formats**: PDF, DOCX, PPTX, XLSX, HTML, images, emails, text, data formats
      - **Languages**: English, Hebrew, German, Chinese, Japanese, Korean

      ### Performance Metrics
      - **Extraction Time**: Wall-clock time from start to completion
      - **Memory Usage**: Peak RSS (Resident Set Size)
      - **CPU Utilization**: Average percentage during processing
      - **Success Rate**: Percentage of successful extractions
      - **Throughput**: Files/second and MB/second

      ### Benchmark Methodology
      - Multiple iterations (default 3) per document
      - Cold-start performance (no warmup)
      - Resource monitoring at 50ms intervals
      - Timeout protection (300s per file)
      - Framework isolation in CI jobs

  - name: "Important Instructions"
    priority: 10
    content: |
      - Do what has been asked; nothing more, nothing less
      - NEVER create files unless they're absolutely necessary for achieving your goal
      - ALWAYS prefer editing an existing file to creating a new one
      - NEVER proactively create documentation files (*.md) or README files
      - Only create documentation files if explicitly requested by the User
      - NO framework recommendations - present data objectively
      - Focus on benchmarking accuracy and fairness
      - Focus on multi-format text extraction frameworks only
      - CLI benchmark command includes ALL frameworks by default (including docling)
      - CI excludes docling from default runs due to conflicts

  - name: "CI/CD and Deployment"
    priority: 7
    content: |
      ### GitHub Actions Workflows
      #### CI (ci.yml)
      - **Triggers**: Push to main, PRs
      - **Purpose**: Code validation, linting, type checking
      - **No benchmarks**: Only validates code quality

      #### Benchmarks (benchmark-by-framework.yml)
      - **Triggers**: Manual dispatch, releases, weekly schedule (Monday 6 AM UTC)
      - **Framework Isolation**: Each framework runs in separate job
      - **Timeouts**: 2 hours per framework, 300s per file
      - **Default Frameworks**: ALL (kreuzberg_sync, kreuzberg_async, markitdown, unstructured, extractous, docling)
      - **Artifacts**: Results stored for 90 days

      ### Deployment Pipeline
      1. Run benchmarks for each framework
      2. Aggregate results from successful jobs
      3. Generate charts and reports
      4. Deploy to GitHub Pages
      5. Store raw data in repository

      ### Key Optimizations
      - Clear Kreuzberg cache before each run
      - Continue on error for robustness
      - Parallel framework execution
      - Automatic result aggregation

  - name: "Data Management"
    priority: 6
    content: |
      ### Output Files
      - **Raw Results**: `results/results.json` (msgspec format)
      - **Summaries**: `results/summaries.json`
      - **CSV Exports**: `results/detailed_results.csv`, `results/summary_results.csv`
      - **Visualizations**: `results/charts/*.png`
      - **Interactive**: `results/charts/interactive_dashboard.html`

      ### Data Structure
      - BenchmarkResult: Per-file extraction details
      - BenchmarkSummary: Aggregated statistics by framework/category
      - AggregatedResults: Combined results from all frameworks
      - Quality metrics when enabled with --enable-quality-assessment

sections:
  - title: "Tested Frameworks"
    content: |
      ### Multi-Format Frameworks (6 total)
      1. **Kreuzberg** (v3.8.0+)
         - Sync/async APIs, multiple OCR backends
         - 71MB installation

      2. **Extractous** (v0.1.0+)
         - Rust-based with Python bindings
         - Native performance characteristics

      3. **Unstructured** (v0.18.5+)
         - 64+ file types support
         - 146MB installation

      4. **MarkItDown** (v0.0.1a2+)
         - Microsoft's Markdown converter
         - 251MB with ONNX Runtime

      5. **Docling** (v2.41.0+)
         - IBM Research's ML-based extraction
         - 1GB+ installation with PyTorch

      6. **Various Kreuzberg OCR backends**
         - kreuzberg_tesseract, kreuzberg_easyocr, kreuzberg_paddleocr
         - Both sync and async variants

  - title: "Performance Observations"
    content: |
      ### Framework Speed Rankings (medium PDFs)
      - Kreuzberg: ~2 minutes for 24 extractions
      - Markitdown: Moderate speed
      - Unstructured: Slower but reliable
      - Docling: Very slow on complex PDFs (>60 min timeout)

      ### Common Issues
      - **Docling Timeouts**: Switch to text export for better performance
      - **Memory Spikes**: Some frameworks use 1GB+ on large documents
      - **Cache Impact**: Kreuzberg cache must be cleared for fair comparison
      - **Language Config**: Affects OCR accuracy significantly

  - title: "Language Configuration"
    content: |
      ### Auto-Detection Based on Filenames
      - **Hebrew**: Files containing "hebrew", "israel", "tel_aviv"
      - **German**: Files containing "german", "germany", "berlin"
      - **Chinese**: Files containing "chinese", "china", "beijing", "chi_sim"
      - **Japanese**: Files containing "japanese", "japan", "jpn"
      - **Korean**: Files containing "korean", "korea", "kor"
      - **Default**: English with German and French as fallbacks

      ### Framework-Specific Handling
      - **Kreuzberg**: Configures Tesseract/EasyOCR/PaddleOCR with appropriate models
      - **Unstructured**: Passes language hints for OCR
      - **Docling**: Auto-detects using ML models
      - **MarkItDown**: Uses ONNX models with multilingual support

  - title: "Future Extensibility"
    content: |
      ### Adding New Frameworks
      1. Create extractor class inheriting from Protocol
      2. Add to Framework enum
      3. Register in factory function
      4. Add dependencies
      5. Configure language support
      6. Add format exclusions if needed

      ### Potential Enhancements
      - GPU acceleration support
      - Multi-threaded extraction
      - Streaming extraction for large files
      - Real-time monitoring dashboard
      - Cloud deployment options
      - API endpoint benchmarking
