name: Table Extraction Benchmarks

on:
  workflow_dispatch:
    inputs:
      frameworks:
        description: 'Frameworks to test (comma-separated)'
        required: false
        default: 'all'
        type: string
      categories:
        description: 'Categories to test (comma-separated)'
        required: false
        default: 'tiny,small,medium'
        type: string
      iterations:
        description: 'Number of iterations'
        required: false
        default: '3'
        type: string

  release:
    types: [published]

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "table-benchmark"
  cancel-in-progress: false

jobs:
  # Test Kreuzberg with GMFT
  kreuzberg-table-sync:
    runs-on: macos-14
    timeout-minutes: 480  # 8 hours - table extraction with GMFT is much slower
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies with GMFT
        run: |
          uv sync --extra ocr
          # Install Kreuzberg with GMFT support
          uv add "kreuzberg[gmft]>=4.0.0rc1"

      - name: Clear caches
        run: |
          rm -rf .kreuzberg || true

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_sync \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-kreuzberg-table-sync \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-kreuzberg-table-sync
          path: results-kreuzberg-table-sync/
          retention-days: 90

  # Test Kreuzberg Async with GMFT
  kreuzberg-table-async:
    runs-on: macos-14
    timeout-minutes: 480  # 8 hours - table extraction with GMFT is much slower
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies with GMFT
        run: |
          uv sync --extra ocr
          # Install Kreuzberg with GMFT support
          uv add "kreuzberg[gmft]>=4.0.0rc1"

      - name: Clear caches
        run: |
          rm -rf .kreuzberg || true

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_async \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-kreuzberg-table-async \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-kreuzberg-table-async
          path: results-kreuzberg-table-async/
          retention-days: 90

  # Test Docling (has built-in table extraction)
  docling-table:
    runs-on: macos-14
    timeout-minutes: 360  # 6 hours - Docling table extraction is slow but faster than GMFT
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --extra ocr

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework docling \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-docling-table \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-docling-table
          path: results-docling-table/
          retention-days: 90

  # Test MarkItDown
  markitdown-table:
    runs-on: macos-14
    timeout-minutes: 180  # 3 hours - MarkItDown is faster but may struggle with table complexity
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --extra ocr

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework markitdown \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-markitdown-table \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-markitdown-table
          path: results-markitdown-table/
          retention-days: 90

  # Test Unstructured
  unstructured-table:
    runs-on: macos-14
    timeout-minutes: 300  # 5 hours - Unstructured table extraction can be slow with hi-res strategy
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --extra ocr

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework unstructured \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-unstructured-table \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-unstructured-table
          path: results-unstructured-table/
          retention-days: 90

  # Test Extractous (limited table support)
  extractous-table:
    runs-on: macos-14
    timeout-minutes: 120  # 2 hours - Extractous is fast but has limited table support
    continue-on-error: true
    steps:
      - uses: actions/checkout@v4

      - name: Install system dependencies
        run: |
          brew install tesseract poppler pandoc

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: |
          uv sync --extra ocr

      - name: Run table extraction benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework extractous \
            --category ${{ github.event.inputs.categories || 'tiny,small,medium' }} \
            --iterations ${{ github.event.inputs.iterations || '3' }} \
            --timeout 900 \
            --output-dir results-extractous-table \
            --enable-quality-assessment \
            --table-extraction-only

      - name: Upload results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: results-extractous-table
          path: results-extractous-table/
          retention-days: 90

  # Aggregate results and generate reports
  aggregate-table-results:
    needs: [kreuzberg-table-sync, kreuzberg-table-async, docling-table, markitdown-table, unstructured-table, extractous-table]
    runs-on: macos-14
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "0.5.11"

      - name: Set up Python
        run: uv python install 3.13

      - name: Install dependencies
        run: uv sync --extra ocr

      - name: Download all results
        uses: actions/download-artifact@v4
        with:
          path: all-table-results

      - name: List downloaded artifacts
        run: |
          echo "Downloaded artifacts:"
          find all-table-results -name "*.json" | head -20

      - name: Aggregate results
        run: |
          uv run python -m src.cli aggregate \
            all-table-results/results-*/ \
            --output-dir aggregated-table-results

      - name: Generate reports
        run: |
          # Generate comprehensive reports
          uv run python -m src.cli report \
            --aggregated-file aggregated-table-results/aggregated_results.json \
            --output-dir table-reports \
            --format markdown --format json --format html

          # Generate visualizations
          uv run python -m src.cli visualize \
            --aggregated-file aggregated-table-results/aggregated_results.json \
            --output-dir table-visualizations

          # Generate table-specific analysis
          uv run python -m src.cli table-analysis \
            --results-dir aggregated-table-results \
            --output-dir table-analysis

      - name: Generate GitHub Pages index
        run: |
          python src/generate_index.py \
            aggregated-table-results/aggregated_results.json \
            table-index.html

      - name: Organize final output
        run: |
          mkdir -p table-benchmark-output

          # Copy main results
          cp -r aggregated-table-results/* table-benchmark-output/
          cp -r table-reports table-benchmark-output/
          cp -r table-visualizations table-benchmark-output/
          cp -r table-analysis table-benchmark-output/
          cp table-index.html table-benchmark-output/

          # Create summary
          echo "# Table Extraction Benchmark Results" > table-benchmark-output/README.md
          echo "" >> table-benchmark-output/README.md
          echo "This directory contains comprehensive table extraction benchmark results." >> table-benchmark-output/README.md
          echo "" >> table-benchmark-output/README.md
          echo "## Key Files:" >> table-benchmark-output/README.md
          echo "- \`table-index.html\`: Main results dashboard" >> table-benchmark-output/README.md
          echo "- \`table-analysis/\`: Table extraction analysis" >> table-benchmark-output/README.md
          echo "- \`table-reports/\`: Comprehensive reports" >> table-benchmark-output/README.md
          echo "- \`table-visualizations/\`: Performance charts" >> table-benchmark-output/README.md
          echo "" >> table-benchmark-output/README.md
          echo "Generated: $(date)" >> table-benchmark-output/README.md

      - name: Upload final results
        uses: actions/upload-artifact@v4
        with:
          name: table-benchmark-results
          path: table-benchmark-output/
          retention-days: 90

      - name: Upload to GitHub Pages (if main branch)
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: table-benchmark-output/

  # Deploy to GitHub Pages
  deploy-table-pages:
    needs: aggregate-table-results
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
