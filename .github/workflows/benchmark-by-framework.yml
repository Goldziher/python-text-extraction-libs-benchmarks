# ~keep Distributed benchmark pipeline for fair framework comparison
#
# Pipeline strategy:
# - Each framework runs in isolated jobs to prevent interference
# - Matrix strategy tests each framework across all document categories
# - Results are aggregated after all frameworks complete
# - Artifacts stored for 90 days with GitHub Pages deployment
name: Benchmark by Framework (General Text Extraction)

on:
  release:
    types: [published]  # Comprehensive benchmarks on releases
  schedule:
    - cron: '0 6 * * 1'  # Every Monday at 6 AM UTC
  workflow_dispatch:  # Manual triggering with parameters
    inputs:
      frameworks:
        description: 'Frameworks to test (comma-separated or "all")'
        required: false
        default: 'all'
        type: string
      categories:
        description: 'Categories to test (comma-separated or "all")'
        required: false
        default: 'tiny,small,medium,large,huge'  # All size categories for complete testing
        type: string
      iterations:
        description: 'Number of iterations per test'
        required: false
        default: '3'  # Multiple iterations for statistical significance
        type: string
      format_tier:
        description: 'Format tier to test (universal, common, or all)'
        required: false
        default: 'common'  # Balance between coverage and CI time
        type: choice
        options:
          - all
          - universal
          - common

jobs:
  # ~keep Parameter setup job: parse inputs into matrix-compatible JSON arrays
  prepare:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      frameworks: ${{ steps.setup.outputs.frameworks }}
      categories: ${{ steps.setup.outputs.categories }}
      iterations: ${{ steps.setup.outputs.iterations }}
      format_tier: ${{ steps.setup.outputs.format_tier }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Parameters
        id: setup
        run: |
          # Parse frameworks
          INPUT_FRAMEWORKS="${{ github.event.inputs.frameworks || 'all' }}"
          if [ "$INPUT_FRAMEWORKS" = "all" ]; then
            # All frameworks including docling (runs in isolation)
            FRAMEWORKS='["kreuzberg_sync","kreuzberg_async","markitdown","unstructured","extractous","docling"]'
          else
            # Convert comma-separated to JSON array
            FRAMEWORKS=$(echo "$INPUT_FRAMEWORKS" | sed 's/,/","/g' | sed 's/^/["/' | sed 's/$/"]/')
          fi

          # Parse categories
          INPUT_CATEGORIES="${{ github.event.inputs.categories || 'tiny,small,medium,large,huge' }}"
          if [ "$INPUT_CATEGORIES" = "all" ]; then
            CATEGORIES='["tiny","small","medium","large","huge"]'
          else
            # Convert comma-separated to JSON array
            CATEGORIES=$(echo "$INPUT_CATEGORIES" | sed 's/,/","/g' | sed 's/^/["/' | sed 's/$/"]/')
          fi

          ITERATIONS="${{ github.event.inputs.iterations || '3' }}"

          # Format tier - default to 'all' for releases to test complete capabilities
          FORMAT_TIER="${{ github.event.inputs.format_tier || 'all' }}"

          echo "frameworks=$FRAMEWORKS" >> $GITHUB_OUTPUT
          echo "categories=$CATEGORIES" >> $GITHUB_OUTPUT
          echo "iterations=$ITERATIONS" >> $GITHUB_OUTPUT
          echo "format_tier=$FORMAT_TIER" >> $GITHUB_OUTPUT

          echo "Frameworks: $FRAMEWORKS"
          echo "Categories: $CATEGORIES"
          echo "Iterations: $ITERATIONS"
          echo "Format Tier: $FORMAT_TIER"

  # ~keep Framework isolation: each framework runs in separate job to prevent interference
  # This ensures fair comparison by eliminating shared memory, dependencies, or caching effects
  benchmark-kreuzberg-sync:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_sync')
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 hours max per framework/category combination
    strategy:
      fail-fast: false  # Continue testing other categories if one fails
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}  # Test each size category
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[kreuzberg]"
          uv pip install -e ".[kreuzberg-ocr]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          source .venv/bin/activate
          python -c "import kreuzberg; print(kreuzberg.__version__)"

      - name: Clear Kreuzberg Cache
        run: |
          echo "Clearing Kreuzberg cache directories..."
          rm -rf ~/.kreuzberg .kreuzberg
          echo "Cache cleared"

      - name: Run Benchmark
        run: |
          source .venv/bin/activate
          python -m src.cli benchmark \
            --framework kreuzberg_sync \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_sync-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_sync-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_sync-${{ matrix.category }}/
          retention-days: 30

  benchmark-kreuzberg-async:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_async')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[kreuzberg]"
          uv pip install -e ".[kreuzberg-ocr]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          source .venv/bin/activate
          python -c "import kreuzberg; print(kreuzberg.__version__)"

      - name: Clear Kreuzberg Cache
        run: |
          echo "Clearing Kreuzberg cache directories..."
          rm -rf ~/.kreuzberg .kreuzberg
          echo "Cache cleared"

      - name: Run Benchmark
        run: |
          source .venv/bin/activate
          python -m src.cli benchmark \
            --framework kreuzberg_async \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_async-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_async-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_async-${{ matrix.category }}/
          retention-days: 30


  # NOTE: Docling has conflicting dependencies with kreuzberg (lxml version)
  # Run this job separately by specifying: --frameworks docling
  benchmark-docling:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'docling')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[docling]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          source .venv/bin/activate
          timeout 7200 python -m src.cli benchmark \
            --framework docling \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-docling-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-docling-${{ matrix.category }}"
            echo '{"framework": "docling", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-docling-${{ matrix.category }}/benchmark_failure.json"
            echo "Docling benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-docling-${{ matrix.category }}-${{ github.run_id }}
          path: results-docling-${{ matrix.category }}/
          retention-days: 30

  benchmark-markitdown:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'markitdown')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[markitdown]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          source .venv/bin/activate
          timeout 7200 python -m src.cli benchmark \
            --framework markitdown \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-markitdown-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-markitdown-${{ matrix.category }}"
            echo '{"framework": "markitdown", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-markitdown-${{ matrix.category }}/benchmark_failure.json"
            echo "MarkItDown benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-markitdown-${{ matrix.category }}-${{ github.run_id }}
          path: results-markitdown-${{ matrix.category }}/
          retention-days: 30

  benchmark-unstructured:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'unstructured')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[unstructured]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark
        run: |
          source .venv/bin/activate
          python -m src.cli benchmark \
            --framework unstructured \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-unstructured-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-unstructured-${{ matrix.category }}-${{ github.run_id }}
          path: results-unstructured-${{ matrix.category }}/
          retention-days: 30


  benchmark-extractous:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'extractous')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          uv pip install -e ".[extractous]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          source .venv/bin/activate
          timeout 7200 python -m src.cli benchmark \
            --framework extractous \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 1200 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-extractous-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-extractous-${{ matrix.category }}"
            echo '{"framework": "extractous", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-extractous-${{ matrix.category }}/benchmark_failure.json"
            echo "Extractous benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-extractous-${{ matrix.category }}-${{ github.run_id }}
          path: results-extractous-${{ matrix.category }}/
          retention-days: 30




  # ~keep Aggregation job: combine results from distributed framework jobs
  # Runs even if some frameworks fail to provide partial results
  aggregate:
    needs:
      - prepare
      - benchmark-kreuzberg-sync
      - benchmark-kreuzberg-async
      - benchmark-docling
      - benchmark-markitdown
      - benchmark-unstructured
      - benchmark-extractous
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: always() && needs.prepare.result == 'success'  # Continue even if some frameworks failed
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv venv
          source .venv/bin/activate
          uv pip install -e .
          # Install all extras for aggregation/reporting
          uv pip install -e ".[all]"
          uv pip install pytest pytest-asyncio pytest-timeout

      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-*-${{ github.run_id }}  # Download from all framework jobs
          path: all-results/

      - name: List Downloaded Results
        run: |
          echo "Downloaded benchmark results:"
          find all-results/ -name "*.json" -type f | sort
          echo "Total result files: $(find all-results/ -name "*.json" -type f | wc -l)"

      - name: Aggregate Results
        run: |
          # ~keep Combine distributed results with graceful failure handling
          # Separate successful results from failure markers for proper aggregation
          RESULT_DIRS=""
          FAILURE_DIRS=""
          for dir in all-results/*/; do
            if [ -f "${dir}benchmark_results.json" ]; then
              RESULT_DIRS="$RESULT_DIRS $dir"  # Valid benchmark data
            elif [ -f "${dir}benchmark_failure.json" ]; then
              FAILURE_DIRS="$FAILURE_DIRS $dir"  # Framework/category failed
              echo "Found failure marker in: $dir"
            fi
          done

          # Report framework failures for transparency
          if [ -n "$FAILURE_DIRS" ]; then
            echo "⚠️ Found $(echo $FAILURE_DIRS | wc -w) failed benchmark runs:"
            for dir in $FAILURE_DIRS; do
              echo "  - $(basename $dir): $(cat ${dir}benchmark_failure.json)"
            done
          fi

          # Aggregate available results or create empty structure
          if [ -n "$RESULT_DIRS" ]; then
            echo "Aggregating results from: $RESULT_DIRS"
            source .venv/bin/activate
            python -m src.cli aggregate $RESULT_DIRS --output-dir aggregated-results
            echo "✅ Successfully aggregated $(echo $RESULT_DIRS | wc -w) result directories"
          else
            echo "⚠️ No valid result directories found - creating empty aggregated results"
            mkdir -p aggregated-results
            echo '{"frameworks": [], "categories": [], "results": [], "metadata": {"timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'", "note": "No benchmark results available"}}' > aggregated-results/aggregated_results.json
            echo '[]' > aggregated-results/all_summaries.json
          fi

      - name: Generate Reports
        run: |
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            echo "Generating comprehensive reports..."
            mkdir -p final-reports

            # Try to generate reports, but continue even if it fails
            if python -m src.cli report \
              --aggregated-file aggregated-results/aggregated_results.json \
              --output-dir final-reports \
              --format markdown \
              --format json \
              --format html; then
              echo "✅ Reports generated successfully"
            else
              echo "⚠️ Report generation failed, creating basic report"
              echo "# Benchmark Results\n\nNo valid results available for reporting." > final-reports/benchmark_report.md
              echo '{"error": "No results available"}' > final-reports/benchmark_metrics.json
            fi

            ls -la final-reports/
          else
            echo "⚠️ No aggregated results found, creating empty reports"
            mkdir -p final-reports
            echo "# Benchmark Results\n\nNo benchmark results available." > final-reports/benchmark_report.md
            echo '{"error": "No aggregated results file"}' > final-reports/benchmark_metrics.json
          fi

      - name: Generate Visualizations
        run: |
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            echo "Generating comprehensive visualizations..."
            mkdir -p final-visualizations

            # Try to generate visualizations, but continue even if it fails
            if python -m src.cli visualize \
              --aggregated-file aggregated-results/aggregated_results.json \
              --output-dir final-visualizations; then
              echo "✅ Visualizations generated successfully"
            else
              echo "⚠️ Visualization generation failed, creating placeholder"
              echo '{"error": "No visualizations available"}' > final-visualizations/placeholder.json
            fi

            ls -la final-visualizations/
          else
            echo "⚠️ No aggregated results found, creating empty visualizations"
            mkdir -p final-visualizations
            echo '{"error": "No aggregated results file"}' > final-visualizations/placeholder.json
          fi

      - name: Run Metadata Analysis
        run: |
          echo "📊 Running metadata analysis..."
          mkdir -p metadata-analysis

          # Find the main results file for metadata analysis
          RESULTS_FILE=""
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            # Use aggregated results if available
            RESULTS_FILE="aggregated-results/aggregated_results.json"
          else
            # Look for individual results files
            for file in all-results/*/benchmark_results.json; do
              if [ -f "$file" ]; then
                RESULTS_FILE="$file"
                break
              fi
            done
          fi

          if [ -n "$RESULTS_FILE" ]; then
            echo "Using results file: $RESULTS_FILE"
            if python -m src.cli metadata-analysis \
              --results-dir "$(dirname "$RESULTS_FILE")" \
              --output-dir metadata-analysis; then
              echo "✅ Metadata analysis completed successfully"
            else
              echo "⚠️ Metadata analysis failed, continuing with other analyses"
            fi
          else
            echo "⚠️ No results file found for metadata analysis"
            echo '{"error": "No results file available for metadata analysis"}' > metadata-analysis/placeholder.json
          fi

      - name: Run Table Analysis
        run: |
          echo "📊 Running table analysis..."
          mkdir -p table-analysis

          # Find the main results file for table analysis
          RESULTS_FILE=""
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            # Use aggregated results if available
            RESULTS_FILE="aggregated-results/aggregated_results.json"
          else
            # Look for individual results files
            for file in all-results/*/benchmark_results.json; do
              if [ -f "$file" ]; then
                RESULTS_FILE="$file"
                break
              fi
            done
          fi

          if [ -n "$RESULTS_FILE" ]; then
            echo "Using results file: $RESULTS_FILE"
            if python -m src.cli table-analysis \
              --results-dir "$(dirname "$RESULTS_FILE")" \
              --output-dir table-analysis; then
              echo "✅ Table analysis completed successfully"
            else
              echo "⚠️ Table analysis failed, continuing with other analyses"
            fi
          else
            echo "⚠️ No results file found for table analysis"
            echo '{"error": "No results file available for table analysis"}' > table-analysis/placeholder.json
          fi

      - name: Create Summary
        run: |
          echo "## 📊 Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Framework-Specific Jobs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count successful and failed results
          COMPLETED_COUNT=0
          FAILED_COUNT=0

          # List which framework jobs completed
          for framework in kreuzberg_sync kreuzberg_async docling markitdown unstructured extractous; do
            if ls all-results/benchmark-${framework}-*/ >/dev/null 2>&1; then
              # Count categories that completed for this framework
              CATEGORIES=$(ls all-results/benchmark-${framework}-*/ | wc -l)
              echo "✅ **${framework}**: Completed ($CATEGORIES categories)" >> $GITHUB_STEP_SUMMARY
              COMPLETED_COUNT=$((COMPLETED_COUNT + CATEGORIES))
            else
              echo "❌ **${framework}**: Failed or skipped" >> $GITHUB_STEP_SUMMARY
              FAILED_COUNT=$((FAILED_COUNT + 1))
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Overall Status" >> $GITHUB_STEP_SUMMARY
          echo "- **✅ Successful runs**: $COMPLETED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **❌ Failed/skipped**: $FAILED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📋 **Reports and artifacts generated from available results**" >> $GITHUB_STEP_SUMMARY

      - name: Upload Aggregated Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-benchmark-results-${{ github.run_id }}
          path: aggregated-results/
          retention-days: 90

      - name: Upload Final Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-reports-${{ github.run_id }}
          path: final-reports/
          retention-days: 90

      - name: Upload Visualizations
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-visualizations-${{ github.run_id }}
          path: final-visualizations/
          retention-days: 90

      - name: Upload Metadata Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metadata-analysis-${{ github.run_id }}
          path: metadata-analysis/
          retention-days: 90

      - name: Upload Table Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: table-analysis-${{ github.run_id }}
          path: table-analysis/
          retention-days: 90
