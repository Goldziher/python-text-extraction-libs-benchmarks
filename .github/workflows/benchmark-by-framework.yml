name: Benchmark by Framework (General Text Extraction)

on:
  release:
    types: [published]
  pull_request:
    branches: [main]
  workflow_dispatch:
    inputs:
      frameworks:
        description: 'Frameworks to test (comma-separated or "all")'
        required: false
        default: 'all'
        type: string
      categories:
        description: 'Categories to test (comma-separated or "all")'
        required: false
        default: 'tiny,small,medium'
        type: string
      iterations:
        description: 'Number of iterations per test'
        required: false
        default: '3'
        type: string
      format_tier:
        description: 'Format tier to test (universal, common, or all)'
        required: false
        default: 'common'
        type: choice
        options:
          - all
          - universal
          - common

jobs:
  prepare:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      frameworks: ${{ steps.setup.outputs.frameworks }}
      categories: ${{ steps.setup.outputs.categories }}
      iterations: ${{ steps.setup.outputs.iterations }}
      format_tier: ${{ steps.setup.outputs.format_tier }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Parameters
        id: setup
        run: |
          # Parse frameworks
          INPUT_FRAMEWORKS="${{ github.event.inputs.frameworks || 'all' }}"
          if [ "$INPUT_FRAMEWORKS" = "all" ]; then
            FRAMEWORKS='["kreuzberg_sync","kreuzberg_async","kreuzberg_extractous_sync","kreuzberg_extractous_async","docling","markitdown","unstructured","extractous"]'
          else
            # Convert comma-separated to JSON array
            FRAMEWORKS=$(echo "$INPUT_FRAMEWORKS" | sed 's/,/","/g' | sed 's/^/["/' | sed 's/$/"]/')
          fi

          # Parse categories
          INPUT_CATEGORIES="${{ github.event.inputs.categories || 'tiny,small,medium' }}"
          if [ "$INPUT_CATEGORIES" = "all" ]; then
            CATEGORIES='["tiny","small","medium","large","huge"]'
          else
            # Convert comma-separated to JSON array
            CATEGORIES=$(echo "$INPUT_CATEGORIES" | sed 's/,/","/g' | sed 's/^/["/' | sed 's/$/"]/')
          fi

          ITERATIONS="${{ github.event.inputs.iterations || '3' }}"

          # Format tier - default to 'all' for releases to test complete capabilities
          FORMAT_TIER="${{ github.event.inputs.format_tier || 'all' }}"

          echo "frameworks=$FRAMEWORKS" >> $GITHUB_OUTPUT
          echo "categories=$CATEGORIES" >> $GITHUB_OUTPUT
          echo "iterations=$ITERATIONS" >> $GITHUB_OUTPUT
          echo "format_tier=$FORMAT_TIER" >> $GITHUB_OUTPUT

          echo "Frameworks: $FRAMEWORKS"
          echo "Categories: $CATEGORIES"
          echo "Iterations: $ITERATIONS"
          echo "Format Tier: $FORMAT_TIER"

  # Separate job for each framework
  benchmark-kreuzberg-sync:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_sync')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Clone and install Kreuzberg v4 RC1 Enhanced
        run: |
          # Clone Kreuzberg repository
          git clone https://github.com/Goldziher/kreuzberg.git kreuzberg-source
          cd kreuzberg-source
          git checkout feature/enhanced-benchmarking

          # Install our benchmark dependencies first
          cd ..
          uv sync --all-packages --all-extras --dev

          # Install Kreuzberg from source WITHOUT extractous support
          uv add ./kreuzberg-source --extra gmft --extra cli

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          uv run python -c "import kreuzberg; print(kreuzberg.__version__)"

      - name: Run Benchmark (Kreuzberg v4 RC1 Enhanced without Extractous)
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_sync \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_sync-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_sync-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_sync-${{ matrix.category }}/
          retention-days: 30

  benchmark-kreuzberg-async:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_async')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Clone and install Kreuzberg v4 RC1 Enhanced
        run: |
          # Clone Kreuzberg repository
          git clone https://github.com/Goldziher/kreuzberg.git kreuzberg-source
          cd kreuzberg-source
          git checkout feature/enhanced-benchmarking

          # Install our benchmark dependencies first
          cd ..
          uv sync --all-packages --all-extras --dev

          # Install Kreuzberg from source WITHOUT extractous support
          uv add ./kreuzberg-source --extra gmft --extra cli

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          uv run python -c "import kreuzberg; print(kreuzberg.__version__)"

      - name: Run Benchmark (Kreuzberg v4 RC1 Enhanced without Extractous)
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_async \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_async-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_async-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_async-${{ matrix.category }}/
          retention-days: 30

  benchmark-kreuzberg-extractous-sync:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_extractous_sync')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Clone and install Kreuzberg v4 RC1 Enhanced with Extractous
        run: |
          # Clone Kreuzberg repository
          git clone https://github.com/Goldziher/kreuzberg.git kreuzberg-source
          cd kreuzberg-source
          git checkout feature/enhanced-benchmarking

          # Install our benchmark dependencies first
          cd ..
          uv sync --all-packages --all-extras --dev

          # Install Kreuzberg from source WITH extractous support
          uv add ./kreuzberg-source --extra extractous --extra gmft --extra cli

          # Ensure extractous is installed (needed for Kreuzberg v4 extractous integration)
          uv add extractous

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          uv run python -c "import kreuzberg; print(kreuzberg.__version__)"
          echo "Extractous support:"
          uv run python -c "
          try:
              import extractous
              print('✅ Extractous directly importable')
          except ImportError:
              print('⚠️  Extractous not directly importable')

          # Test if Kreuzberg can use extractous
          try:
              from src.extractors import KreuzbergExtractousSyncExtractor
              extractor = KreuzbergExtractousSyncExtractor()
              print('✅ Kreuzberg extractous extractor works')
          except Exception as e:
              print(f'❌ Kreuzberg extractous extractor failed: {e}')
              exit(1)
          " || exit 1

      - name: Run Benchmark (Kreuzberg v4 RC1 Enhanced with Extractous)
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_extractous_sync \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_extractous_sync-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_extractous_sync-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_extractous_sync-${{ matrix.category }}/
          retention-days: 30

  benchmark-kreuzberg-extractous-async:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'kreuzberg_extractous_async')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Clone and install Kreuzberg v4 RC1 Enhanced with Extractous
        run: |
          # Clone Kreuzberg repository
          git clone https://github.com/Goldziher/kreuzberg.git kreuzberg-source
          cd kreuzberg-source
          git checkout feature/enhanced-benchmarking

          # Install our benchmark dependencies first
          cd ..
          uv sync --all-packages --all-extras --dev

          # Install Kreuzberg from source WITH extractous support
          uv add ./kreuzberg-source --extra extractous --extra gmft --extra cli

          # Ensure extractous is installed (needed for Kreuzberg v4 extractous integration)
          uv add extractous

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10
          echo "Kreuzberg version:"
          uv run python -c "import kreuzberg; print(kreuzberg.__version__)"
          echo "Extractous support:"
          uv run python -c "
          try:
              import extractous
              print('✅ Extractous directly importable')
          except ImportError:
              print('⚠️  Extractous not directly importable')

          # Test if Kreuzberg can use extractous
          try:
              from src.extractors import KreuzbergExtractousSyncExtractor
              extractor = KreuzbergExtractousSyncExtractor()
              print('✅ Kreuzberg extractous extractor works')
          except Exception as e:
              print(f'❌ Kreuzberg extractous extractor failed: {e}')
              exit(1)
          " || exit 1

      - name: Run Benchmark (Kreuzberg v4 RC1 Enhanced with Extractous)
        run: |
          uv run python -m src.cli benchmark \
            --framework kreuzberg_extractous_async \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-kreuzberg_extractous_async-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-kreuzberg_extractous_async-${{ matrix.category }}-${{ github.run_id }}
          path: results-kreuzberg_extractous_async-${{ matrix.category }}/
          retention-days: 30

  benchmark-docling:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'docling')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-packages --all-extras --dev

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          timeout 7200 uv run python -m src.cli benchmark \
            --framework docling \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-docling-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-docling-${{ matrix.category }}"
            echo '{"framework": "docling", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-docling-${{ matrix.category }}/benchmark_failure.json"
            echo "Docling benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-docling-${{ matrix.category }}-${{ github.run_id }}
          path: results-docling-${{ matrix.category }}/
          retention-days: 30

  benchmark-markitdown:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'markitdown')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-packages --all-extras --dev

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          timeout 7200 uv run python -m src.cli benchmark \
            --framework markitdown \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-markitdown-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-markitdown-${{ matrix.category }}"
            echo '{"framework": "markitdown", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-markitdown-${{ matrix.category }}/benchmark_failure.json"
            echo "MarkItDown benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-markitdown-${{ matrix.category }}-${{ github.run_id }}
          path: results-markitdown-${{ matrix.category }}/
          retention-days: 30

  benchmark-unstructured:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'unstructured')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-packages --all-extras --dev

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark
        run: |
          uv run python -m src.cli benchmark \
            --framework unstructured \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-unstructured-${{ matrix.category }}"
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-unstructured-${{ matrix.category }}-${{ github.run_id }}
          path: results-unstructured-${{ matrix.category }}/
          retention-days: 30


  benchmark-extractous:
    needs: prepare
    if: contains(needs.prepare.outputs.frameworks, 'extractous')
    runs-on: ubuntu-latest
    timeout-minutes: 360
    strategy:
      fail-fast: false
      matrix:
        category: ${{ fromJson(needs.prepare.outputs.categories) }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-packages --all-extras --dev

      - name: Install APT packages
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            pandoc \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-fra \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            tesseract-ocr-jpn \
            tesseract-ocr-kor \
            poppler-utils \
            libmagic1

      - name: Verify installations
        run: |
          echo "Pandoc version:"
          pandoc --version | head -1
          echo "Tesseract version:"
          tesseract --version | head -1
          echo "Available Tesseract languages:"
          tesseract --list-langs | head -10

      - name: Run Benchmark with timeout handling
        run: |
          # Use timeout command to enforce 2-hour limit and create failure results
          timeout 7200 uv run python -m src.cli benchmark \
            --framework extractous \
            --category ${{ matrix.category }} \
            --iterations ${{ needs.prepare.outputs.iterations }} \
            --warmup-runs 1 \
            --timeout 300 \
            --continue-on-error \
            --format-tier ${{ needs.prepare.outputs.format_tier }} \
            --enable-quality-assessment \
            --output-dir "results-extractous-${{ matrix.category }}" || {
            # If timeout or failure, create empty results directory with failure marker
            mkdir -p "results-extractous-${{ matrix.category }}"
            echo '{"framework": "extractous", "category": "${{ matrix.category }}", "status": "timeout_or_failure", "timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'"}' > "results-extractous-${{ matrix.category }}/benchmark_failure.json"
            echo "Extractous benchmark failed or timed out after 2 hours - recorded as failure"
          }
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-extractous-${{ matrix.category }}-${{ github.run_id }}
          path: results-extractous-${{ matrix.category }}/
          retention-days: 30

  # Aggregate all results after benchmarks complete
  aggregate:
    needs:
      - prepare
      - benchmark-kreuzberg-sync
      - benchmark-kreuzberg-async
      - benchmark-kreuzberg-extractous-sync
      - benchmark-kreuzberg-extractous-async
      - benchmark-docling
      - benchmark-markitdown
      - benchmark-unstructured
      - benchmark-extractous
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: always() && needs.prepare.result == 'success'
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install uv
        uses: astral-sh/setup-uv@v6
        with:
          enable-cache: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.13"

      - name: Install Dependencies
        run: |
          uv sync --all-packages --all-extras --dev

      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-*-${{ github.run_id }}
          path: all-results/

      - name: List Downloaded Results
        run: |
          echo "Downloaded benchmark results:"
          find all-results/ -name "*.json" -type f | sort
          echo "Total result files: $(find all-results/ -name "*.json" -type f | wc -l)"

      - name: Aggregate Results
        run: |
          # Find all result directories (including those with failure markers)
          RESULT_DIRS=""
          FAILURE_DIRS=""
          for dir in all-results/*/; do
            if [ -f "${dir}benchmark_results.json" ]; then
              RESULT_DIRS="$RESULT_DIRS $dir"
            elif [ -f "${dir}benchmark_failure.json" ]; then
              FAILURE_DIRS="$FAILURE_DIRS $dir"
              echo "Found failure marker in: $dir"
            fi
          done

          # Report failures
          if [ -n "$FAILURE_DIRS" ]; then
            echo "⚠️ Found $(echo $FAILURE_DIRS | wc -w) failed benchmark runs:"
            for dir in $FAILURE_DIRS; do
              echo "  - $(basename $dir): $(cat ${dir}benchmark_failure.json)"
            done
          fi

          if [ -n "$RESULT_DIRS" ]; then
            echo "Aggregating results from: $RESULT_DIRS"
            uv run python -m src.cli aggregate $RESULT_DIRS --output-dir aggregated-results
            echo "✅ Successfully aggregated $(echo $RESULT_DIRS | wc -w) result directories"
          else
            echo "⚠️ No valid result directories found - creating empty aggregated results"
            mkdir -p aggregated-results
            echo '{"frameworks": [], "categories": [], "results": [], "metadata": {"timestamp": "'$(date -u +%Y-%m-%dT%H:%M:%SZ)'", "note": "No benchmark results available"}}' > aggregated-results/aggregated_results.json
            echo '[]' > aggregated-results/all_summaries.json
          fi

      - name: Generate Reports
        run: |
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            echo "Generating comprehensive reports..."
            mkdir -p final-reports

            # Try to generate reports, but continue even if it fails
            if uv run python -m src.cli report \
              --aggregated-file aggregated-results/aggregated_results.json \
              --output-dir final-reports \
              --format markdown \
              --format json \
              --format html; then
              echo "✅ Reports generated successfully"
            else
              echo "⚠️ Report generation failed, creating basic report"
              echo "# Benchmark Results\n\nNo valid results available for reporting." > final-reports/benchmark_report.md
              echo '{"error": "No results available"}' > final-reports/benchmark_metrics.json
            fi

            ls -la final-reports/
          else
            echo "⚠️ No aggregated results found, creating empty reports"
            mkdir -p final-reports
            echo "# Benchmark Results\n\nNo benchmark results available." > final-reports/benchmark_report.md
            echo '{"error": "No aggregated results file"}' > final-reports/benchmark_metrics.json
          fi

      - name: Generate Visualizations
        run: |
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            echo "Generating comprehensive visualizations..."
            mkdir -p final-visualizations

            # Try to generate visualizations, but continue even if it fails
            if uv run python -m src.cli visualize \
              --aggregated-file aggregated-results/aggregated_results.json \
              --output-dir final-visualizations; then
              echo "✅ Visualizations generated successfully"
            else
              echo "⚠️ Visualization generation failed, creating placeholder"
              echo '{"error": "No visualizations available"}' > final-visualizations/placeholder.json
            fi

            ls -la final-visualizations/
          else
            echo "⚠️ No aggregated results found, creating empty visualizations"
            mkdir -p final-visualizations
            echo '{"error": "No aggregated results file"}' > final-visualizations/placeholder.json
          fi

      - name: Run Metadata Analysis
        run: |
          echo "📊 Running metadata analysis..."
          mkdir -p metadata-analysis

          # Find the main results file for metadata analysis
          RESULTS_FILE=""
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            # Use aggregated results if available
            RESULTS_FILE="aggregated-results/aggregated_results.json"
          else
            # Look for individual results files
            for file in all-results/*/benchmark_results.json; do
              if [ -f "$file" ]; then
                RESULTS_FILE="$file"
                break
              fi
            done
          fi

          if [ -n "$RESULTS_FILE" ]; then
            echo "Using results file: $RESULTS_FILE"
            if uv run python -m src.cli metadata-analysis \
              --results-dir "$(dirname "$RESULTS_FILE")" \
              --output-dir metadata-analysis; then
              echo "✅ Metadata analysis completed successfully"
            else
              echo "⚠️ Metadata analysis failed, continuing with other analyses"
            fi
          else
            echo "⚠️ No results file found for metadata analysis"
            echo '{"error": "No results file available for metadata analysis"}' > metadata-analysis/placeholder.json
          fi

      - name: Run Table Analysis
        run: |
          echo "📊 Running table analysis..."
          mkdir -p table-analysis

          # Find the main results file for table analysis
          RESULTS_FILE=""
          if [ -f "aggregated-results/aggregated_results.json" ]; then
            # Use aggregated results if available
            RESULTS_FILE="aggregated-results/aggregated_results.json"
          else
            # Look for individual results files
            for file in all-results/*/benchmark_results.json; do
              if [ -f "$file" ]; then
                RESULTS_FILE="$file"
                break
              fi
            done
          fi

          if [ -n "$RESULTS_FILE" ]; then
            echo "Using results file: $RESULTS_FILE"
            if uv run python -m src.cli table-analysis \
              --results-dir "$(dirname "$RESULTS_FILE")" \
              --output-dir table-analysis; then
              echo "✅ Table analysis completed successfully"
            else
              echo "⚠️ Table analysis failed, continuing with other analyses"
            fi
          else
            echo "⚠️ No results file found for table analysis"
            echo '{"error": "No results file available for table analysis"}' > table-analysis/placeholder.json
          fi

      - name: Create Summary
        run: |
          echo "## 📊 Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Framework-Specific Jobs" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Count successful and failed results
          COMPLETED_COUNT=0
          FAILED_COUNT=0

          # List which framework jobs completed
          for framework in kreuzberg_sync kreuzberg_async kreuzberg_extractous_sync kreuzberg_extractous_async docling markitdown unstructured extractous; do
            if ls all-results/benchmark-${framework}-*/ >/dev/null 2>&1; then
              # Count categories that completed for this framework
              CATEGORIES=$(ls all-results/benchmark-${framework}-*/ | wc -l)
              echo "✅ **${framework}**: Completed ($CATEGORIES categories)" >> $GITHUB_STEP_SUMMARY
              COMPLETED_COUNT=$((COMPLETED_COUNT + CATEGORIES))
            else
              echo "❌ **${framework}**: Failed or skipped" >> $GITHUB_STEP_SUMMARY
              FAILED_COUNT=$((FAILED_COUNT + 1))
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Overall Status" >> $GITHUB_STEP_SUMMARY
          echo "- **✅ Successful runs**: $COMPLETED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "- **❌ Failed/skipped**: $FAILED_COUNT" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "📋 **Reports and artifacts generated from available results**" >> $GITHUB_STEP_SUMMARY

      - name: Upload Aggregated Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-benchmark-results-${{ github.run_id }}
          path: aggregated-results/
          retention-days: 90

      - name: Upload Final Reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-reports-${{ github.run_id }}
          path: final-reports/
          retention-days: 90

      - name: Upload Visualizations
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-visualizations-${{ github.run_id }}
          path: final-visualizations/
          retention-days: 90

      - name: Upload Metadata Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metadata-analysis-${{ github.run_id }}
          path: metadata-analysis/
          retention-days: 90

      - name: Upload Table Analysis
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: table-analysis-${{ github.run_id }}
          path: table-analysis/
          retention-days: 90
