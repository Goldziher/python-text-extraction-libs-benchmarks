name: Comprehensive Benchmarks

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  workflow_dispatch:
    inputs:
      iterations:
        description: 'Number of benchmark iterations'
        required: false
        default: '3'
      frameworks:
        description: 'Comma-separated list of frameworks to test'
        required: false
        default: 'all'
      categories:
        description: 'Comma-separated list of categories to test'
        required: false
        default: 'all'

jobs:
  prepare:
    runs-on: ubuntu-latest
    outputs:
      framework-matrix: ${{ steps.set-matrix.outputs.frameworks }}
      category-matrix: ${{ steps.set-matrix.outputs.categories }}
    steps:
      - uses: actions/checkout@v4

      - name: Set up job matrix
        id: set-matrix
        run: |
          # Default frameworks
          if [[ "${{ github.event.inputs.frameworks }}" == "all" || -z "${{ github.event.inputs.frameworks }}" ]]; then
            FRAMEWORKS='["kreuzberg_sync","kreuzberg_async","docling","markitdown","unstructured"]'
          else
            # Convert comma-separated to JSON array
            FRAMEWORKS=$(echo "${{ github.event.inputs.frameworks }}" | jq -R -s -c 'split(",") | map(gsub("^\\s+|\\s+$";""))')
          fi

          # Default categories - start with a subset for faster CI
          if [[ "${{ github.event.inputs.categories }}" == "all" || -z "${{ github.event.inputs.categories }}" ]]; then
            CATEGORIES='["tiny","small","pdf_standard","office","text"]'
          else
            # Convert comma-separated to JSON array
            CATEGORIES=$(echo "${{ github.event.inputs.categories }}" | jq -R -s -c 'split(",") | map(gsub("^\\s+|\\s+$";""))')
          fi

          echo "frameworks=$FRAMEWORKS" >> $GITHUB_OUTPUT
          echo "categories=$CATEGORIES" >> $GITHUB_OUTPUT

  benchmark:
    needs: prepare
    runs-on: ${{ matrix.os }}
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        framework: ${{ fromJson(needs.prepare.outputs.framework-matrix) }}
        category: ${{ fromJson(needs.prepare.outputs.category-matrix) }}
        exclude:
          # Skip some combinations for Windows to reduce CI time
          - os: windows-latest
            category: large
          - os: windows-latest
            category: huge

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install uv
        uses: astral-sh/setup-uv@v3
        with:
          enable-cache: true

      - name: Install system dependencies (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            tesseract-ocr \
            tesseract-ocr-eng \
            tesseract-ocr-deu \
            tesseract-ocr-heb \
            tesseract-ocr-chi-sim \
            poppler-utils \
            libmagic1

      - name: Install system dependencies (macOS)
        if: runner.os == 'macOS'
        run: |
          brew install tesseract poppler libmagic
          brew install tesseract-lang

      - name: Install system dependencies (Windows)
        if: runner.os == 'Windows'
        run: |
          choco install tesseract poppler

      - name: Cache test documents
        uses: actions/cache@v4
        with:
          path: test_documents
          key: test-docs-${{ hashFiles('test_documents/**') }}
          restore-keys: |
            test-docs-

      - name: Install dependencies
        run: |
          uv pip install -e .

      - name: Run benchmarks
        id: benchmark
        run: |
          python -m src.cli benchmark \
            --framework ${{ matrix.framework }} \
            --category ${{ matrix.category }} \
            --iterations ${{ github.event.inputs.iterations || 3 }} \
            --output-dir results/${{ matrix.os }}/${{ matrix.framework }}/${{ matrix.category }}
        env:
          PYTHONUNBUFFERED: "1"

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.os }}-${{ matrix.framework }}-${{ matrix.category }}
          path: results/
          retention-days: 30

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: logs-${{ matrix.os }}-${{ matrix.framework }}-${{ matrix.category }}
          path: |
            *.log
            results/**/*.log
          retention-days: 7

  aggregate:
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install uv
        uses: astral-sh/setup-uv@v3

      - name: Install dependencies
        run: |
          uv pip install -e .

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: results-*
          merge-multiple: true

      - name: Aggregate results
        run: |
          python -m src.cli aggregate \
            --input-dir . \
            --output-dir aggregated-results

      - name: Generate report
        run: |
          python -m src.cli report \
            --results-dir aggregated-results \
            --output-format json,markdown,html

      - name: Upload final report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ github.run_number }}
          path: aggregated-results/
          retention-days: 90

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Read the markdown report
            const reportPath = path.join('aggregated-results', 'benchmark_report.md');
            let report = '## Benchmark Results\n\n';

            if (fs.existsSync(reportPath)) {
              report += fs.readFileSync(reportPath, 'utf8');
            } else {
              report += 'No benchmark report was generated.';
            }

            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  performance-tracking:
    needs: aggregate
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: write
    steps:
      - uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download benchmark report
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-report-*

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Text Extraction Benchmarks
          tool: 'customBiggerIsBetter'
          output-file-path: benchmark-report-*/benchmark_metrics.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          benchmark-data-dir-path: 'benchmarks'

  cleanup:
    needs: [aggregate, performance-tracking]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Delete intermediate artifacts
        uses: geekyeggo/delete-artifact@v4
        with:
          name: results-*
          failOnError: false
