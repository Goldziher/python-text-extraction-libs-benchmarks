# Python Text Extraction Libraries Benchmarks 2025

> **ğŸ¯ [ğŸ“Š VIEW LIVE BENCHMARK RESULTS â†’](https://goldziher.github.io/python-text-extraction-libs-benchmarks/)**

Automated performance benchmarking of Python text extraction frameworks with real-time updates.

## ğŸ† What You'll Find in the Results

- **âš¡ Performance Comparison** - Speed, memory usage, and success rates across all frameworks
- **ğŸ“Š Interactive Charts** - Visual breakdowns by file type, size category, and framework
- **ğŸ” Detailed Metrics** - Per-file results, error analysis, and resource utilization
- **ğŸ“ˆ Trend Analysis** - Performance changes over iterations and time
- **ğŸ¯ Framework Recommendations** - Guidance for choosing the right tool

## ğŸ”¬ Framework Assessment

### âš¡ **Kreuzberg** (71MB, 20 deps)

**Best for: Production workloads, edge computing, cloud functions**

- Fastest extraction speeds (35+ files/second)
- Both sync and async APIs with OCR support
- **Lightweight**: Perfect for AWS Lambda, edge functions, serverless
- **Smallest footprint**: 71MB with only 20 dependencies
- Handles all document types reliably

### ğŸ¢ **Unstructured** (146MB, 54 deps)

**Best for: Enterprise applications, mixed document types**

- Most reliable overall (88%+ success rate)
- Handles complex layouts well
- Moderate speed, good for batch processing
- **Moderate footprint**: 146MB installation with 54 dependencies

### ğŸ“ **MarkItDown** (251MB, 25 deps)

**Best for: Simple documents, LLM preprocessing**

- Good for basic PDF and Office documents
- **Limitation**: Struggles with large/complex files (>10MB)
- **ONNX Runtime included**: 251MB (includes ML inference models)
- Optimized for Markdown output

### ğŸ”¬ **Docling** (1,032MB, 88 deps)

**Best for: Research environments, ML workflows**

- Advanced document understanding with ML models
- **Major limitation**: Extremely slow (often 60+ minutes per file)
- **Frequent failures** on medium-sized documents due to timeouts
- **Heaviest install**: 1GB+ with PyTorch, transformers, vision models

## ğŸ“Š Test Coverage

- **94 Documents** - PDFs, Word docs, HTML, images, and more (~210MB total)
- **Real-world Files** - From tiny text files to 59MB academic papers
- **5 Size Categories** - Tiny (\<100KB), Small (100KB-1MB), Medium (1-10MB), Large (10-50MB), Huge (>50MB)
- **Multi-language** - English, Hebrew, German, Chinese, Japanese, Korean
- **CPU-only Processing** - No GPU acceleration for fair comparison
- **Comprehensive Metrics** - Speed, memory usage, success rates, installation sizes

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/Goldziher/python-text-extraction-libs-benchmarks.git
cd python-text-extraction-libs-benchmarks

# Install dependencies
uv sync --all-extras

# Run benchmarks (specific framework and category)
uv run python -m src.cli benchmark --framework kreuzberg_sync --category small

# Generate reports
uv run python -m src.cli report --output-format html
```

## ğŸ“– Documentation

### Core Commands

```bash
# List available frameworks
uv run python -m src.cli list-frameworks

# Run comprehensive benchmarks
uv run python -m src.cli benchmark --framework all --category tiny,small

# Aggregate multiple benchmark runs
uv run python -m src.cli aggregate results/ --output-dir aggregated/

# Generate visualizations
uv run python -m src.cli visualize --aggregated-file results.json
```

### Configuration

The benchmark suite automatically detects document languages and configures frameworks accordingly. See `CLAUDE.md` for detailed configuration options and framework-specific settings.

## ğŸ¤ Contributing

1. **Add New Frameworks** - Implement the extractor interface in `src/extractors.py`
1. **Improve Tests** - Add test documents to `test_documents/`
1. **Enhance Visualizations** - Modify `src/visualize.py` for new chart types
1. **Report Issues** - Use GitHub Issues for bugs and feature requests

## ğŸ“‹ Project Structure

```
python-text-extraction-libs-benchmarks-2025/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ benchmark.py        # Core benchmarking engine
â”‚   â”œâ”€â”€ extractors.py       # Framework implementations
â”‚   â”œâ”€â”€ visualize.py        # Chart generation
â”‚   â””â”€â”€ cli.py             # Command-line interface
â”œâ”€â”€ test_documents/         # 94 test files (~210MB)
â”œâ”€â”€ .github/workflows/      # CI/CD automation
â””â”€â”€ CLAUDE.md              # Detailed technical documentation
```

## ğŸ”§ Technical Details

- **Python 3.13+** with modern async/await patterns
- **msgspec** for fast JSON serialization
- **plotly/matplotlib** for comprehensive visualizations
- **GitHub Actions** for automated benchmarking
- **uv** for fast dependency management

## ğŸ“Š Performance Highlights

Based on our latest benchmarks:

### ğŸ† **Winners by Category**

- **Speed**: Kreuzberg (35+ files/second)
- **Reliability**: Unstructured (88%+ success rate)
- **Installation Size**: Kreuzberg (71MB, 20 deps vs Docling's 1GB+, 88 deps)
- **Enterprise Features**: Unstructured

### âš ï¸ **Key Limitations**

- **Docling**: Often fails/times out on medium files (>1MB)
- **MarkItDown**: Struggles with large/complex documents (>10MB)
- **All frameworks**: Performance varies significantly by document type

### ğŸ¯ **Quick Recommendations**

- **High-volume production & edge computing**: Choose Kreuzberg
- **Enterprise/mixed docs**: Choose Unstructured
- **Simple docs for LLMs**: Choose Kreuzberg
- **Research/ML workflows**: Choose Kreuzberg with fallback to Unstructured

See the [live results](https://goldziher.github.io/python-text-extraction-libs-benchmarks/) for detailed comparisons and failure analysis.

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Framework maintainers for building excellent tools
- Contributors who added test documents and improvements
- The Python community for feedback and suggestions

______________________________________________________________________

**ğŸ”— Links:**

- [ğŸ“Š Live Results Dashboard](https://goldziher.github.io/python-text-extraction-libs-benchmarks/)
- [âš™ï¸ GitHub Actions](https://github.com/Goldziher/python-text-extraction-libs-benchmarks/actions)
- [ğŸ“– Technical Documentation](CLAUDE.md)
