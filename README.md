# Python Text Extraction Libraries Benchmarks 2025

> **ğŸ¯ [ğŸ“Š VIEW LIVE BENCHMARK RESULTS â†’](https://goldziher.github.io/python-text-extraction-libs-benchmarks/)**

Automated performance benchmarking of Python text extraction frameworks with real-time updates.

## ğŸ† What You'll Find in the Results

- **âš¡ Performance Comparison** - Speed, memory usage, and success rates across all frameworks
- **ğŸ“Š Interactive Charts** - Visual breakdowns by file type, size category, and framework
- **ğŸ” Detailed Metrics** - Per-file results, error analysis, and resource utilization
- **ğŸ“ˆ Trend Analysis** - Performance changes over iterations and time
- **ğŸ¯ Framework Recommendations** - Guidance for choosing the right tool

## ğŸ”¬ Tested Frameworks

### Speed Champions
- **Kreuzberg** (sync/async) - Lightning fast with OCR capabilities
- **MarkItDown** - Microsoft's lightweight converter

### Quality Leaders  
- **Unstructured** - Enterprise-grade reliability
- **Docling** - IBM's research-backed solution

## ğŸ“Š Test Coverage

- **1,000+ Documents** - PDFs, Word docs, HTML, images, and more
- **Real-world Files** - From tiny text files to 59MB academic papers
- **Multi-language** - English, Hebrew, German, Chinese, Japanese, Korean
- **CPU-only Processing** - No GPU acceleration for fair comparison

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/Goldziher/python-text-extraction-libs-benchmarks.git
cd python-text-extraction-libs-benchmarks

# Install dependencies
uv sync --all-extras

# Run benchmarks (specific framework and category)
uv run python -m src.cli benchmark --framework kreuzberg_sync --category small

# Generate reports
uv run python -m src.cli report --output-format html
```

## ğŸ“– Documentation

### Core Commands

```bash
# List available frameworks
uv run python -m src.cli list-frameworks

# Run comprehensive benchmarks
uv run python -m src.cli benchmark --framework all --category tiny,small

# Aggregate multiple benchmark runs
uv run python -m src.cli aggregate results/ --output-dir aggregated/

# Generate visualizations
uv run python -m src.cli visualize --aggregated-file results.json
```

### Configuration

The benchmark suite automatically detects document languages and configures frameworks accordingly. See `CLAUDE.md` for detailed configuration options and framework-specific settings.

## ğŸ¤ Contributing

1. **Add New Frameworks** - Implement the extractor interface in `src/extractors.py`
2. **Improve Tests** - Add test documents to `test_documents/`
3. **Enhance Visualizations** - Modify `src/visualize.py` for new chart types
4. **Report Issues** - Use GitHub Issues for bugs and feature requests

## ğŸ“‹ Project Structure

```
python-text-extraction-libs-benchmarks-2025/
â”œâ”€â”€ src/                    # Main source code
â”‚   â”œâ”€â”€ benchmark.py        # Core benchmarking engine
â”‚   â”œâ”€â”€ extractors.py       # Framework implementations
â”‚   â”œâ”€â”€ visualize.py        # Chart generation
â”‚   â””â”€â”€ cli.py             # Command-line interface
â”œâ”€â”€ test_documents/         # 1000+ test files
â”œâ”€â”€ .github/workflows/      # CI/CD automation
â””â”€â”€ CLAUDE.md              # Detailed technical documentation
```

## ğŸ”§ Technical Details

- **Python 3.13+** with modern async/await patterns
- **msgspec** for fast JSON serialization
- **plotly/matplotlib** for comprehensive visualizations
- **GitHub Actions** for automated benchmarking
- **uv** for fast dependency management

## ğŸ“Š Performance Highlights

Based on our latest benchmarks:

- **Fastest**: Kreuzberg (35+ files/second)
- **Most Reliable**: Unstructured (88%+ success rate)
- **Best Balance**: Framework choice depends on your specific use case

See the [live results](https://goldziher.github.io/python-text-extraction-libs-benchmarks/) for detailed comparisons.

## ğŸ“œ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- Framework maintainers for building excellent tools
- Contributors who added test documents and improvements
- The Python community for feedback and suggestions

---

**ğŸ”— Links:**
- [ğŸ“Š Live Results Dashboard](https://goldziher.github.io/python-text-extraction-libs-benchmarks/)
- [âš™ï¸ GitHub Actions](https://github.com/Goldziher/python-text-extraction-libs-benchmarks/actions)
- [ğŸ“– Technical Documentation](CLAUDE.md)